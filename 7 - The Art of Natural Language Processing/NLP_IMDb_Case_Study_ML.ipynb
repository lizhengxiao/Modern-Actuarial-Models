{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *The Art of Natural Language Processing: Machine Learning for the Case Study*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authors: Andrea Ferrario, Mara NÃ¤gelin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date: February 2020** (updated September 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to run the machine learning modeling in the Classical and Modern Approaches, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Getting started with Python and Jupyter Notebook](#started)\n",
    "2. [Import data](#import)\n",
    "3. [Duplicated reviews](#duplicated)\n",
    "4. [Data preprocessing](#preprocessing)\n",
    "5. [POS-tagging](#POS)\n",
    "6. [Pre-trained word embeddings](#emb)\n",
    "7. [Data analytics](#analytics)  \n",
    "    7.1. [A quick check of data structure](#check)  \n",
    "    7.2. [Basic linguistic analysis of movie reviews](#basic)\n",
    "8. [Machine learning](#ML)  \n",
    "    8.1. [Adaptive boosting (ADA)](#ADA)  \n",
    "    .......8.1.1. [Bag-of-words](#ADA_BOW)  \n",
    "    .......8.1.2. [Bag-of-POS](#ADA_BOP)  \n",
    "    .......8.1.3. [Embeddings](#ADA_E)  \n",
    "    8.2. [Random forests (RF)](#RF)  \n",
    "    .......8.2.1. [Bag-of-words](#RF_BOW)  \n",
    "    .......8.2.2. [Bag-of-POS](#RF_BOP)  \n",
    "    .......8.2.3. [Embeddings](#RF_E)  \n",
    "    8.3. [Extreme gradient boosting (XGB)](#XGB)  \n",
    "    .......8.3.1. [Bag-of-words](#XGB_BOW)  \n",
    "    .......8.3.2. [Bag-of-POS](#XGB_BOP)  \n",
    "    .......8.3.3. [Embeddings](#XGB_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting started with Python and Jupyter Notebook<a name=\"started\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data<a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:54\n"
     ]
    }
   ],
   "source": [
    "# we use the import function, as in Chapter 8 of Raschka's book (see the tutorial)\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "basepath = '...' # insert basepath, where original data are stored\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Duplicated reviews<a name=\"duplicated\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "duplicates = df[df.duplicated()]  #equivalent to keep = first. Duplicated rows, except the first entry, are marked as 'True'\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33       I was fortunate to attend the London premier o...\n",
       "177      I've been strangely attracted to this film sin...\n",
       "939      The Andrew Davies adaptation of the Sarah Wate...\n",
       "1861     <br /><br />First of all, I reviewed this docu...\n",
       "1870     Spheeris debut must be one of the best music d...\n",
       "                               ...                        \n",
       "49412    There is no way to avoid a comparison between ...\n",
       "49484    **SPOILERS** I rented \"Tesis\" (or \"Thesis\" in ...\n",
       "49842    'Dead Letter Office' is a low-budget film abou...\n",
       "49853    This movie had a IMDB rating of 8.1 so I expec...\n",
       "49864    You know all those letters to \"Father Christma...\n",
       "Name: review, Length: 418, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a check on the duplicated review\n",
    "duplicates.review   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates: \n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check\n",
    "df[df.duplicated(subset='review')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data preprocessing<a name=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have always liked this film and I\\'m glad it\\'s available finally on DVD so more viewers can see what I have been telling them all these years. Story is about a high school virgin named Gary (Lawrence Monoson) who works at a pizza place as a delivery boy and he hangs out with his friends David (Joe Rubbo) and Rick (Steve Antin). Gary notices Karen (Diane Franklin) who is the new girl in school and one morning he gives her a ride and by this time he is totally in love. That night at a party he see\\'s Rick with Karen and now he is jealous of his best friend but doesn\\'t tell anyone of his true feelings.<br /><br />*****SPOILER ALERT*****<br /><br />Rick asks Gary if he can borrow his Grandmothers vacant home but Gary makes up an excuse so that Rick can\\'t get Karen alone. But one night Rick brags to Gary that he nailed her at the football field and Gary becomes enraged. A few days later in the school library Gary see\\'s Rick and Karen arguing and he asks Karen what is wrong. She tells him that she\\'s pregnant and that Rick has dumped her. Gary helps her by taking her to his Grandmothers home and paying for her abortion. Finally, Gary tells Karen how he really feels about her and she seems receptive to his feelings but later at her birthday party he walks in on Karen and Rick together again. Gary drives off without the girl! This film ends with a much more realistic version of how life really is. No matter how nice you are you don\\'t necessarily get the girl.<br /><br />This film was directed by Boaz Davidson who would go on to be a pretty competent action film director and he did two things right with this movie. First, he made sure that there was plenty of gratuitous nudity so that this was marketable to the young males that usually go to these films. Secondly, he had the film end with young Gary without Karen and I think the males in the audience can relate to being screwed over no matter how hard you try and win a girls heart. Yes, this film is silly and exploitive but it is funny and sexy. Actress Louisa Moritz almost steals the film as the sexy Carmela. Moritz was always a popular \"B\" level actress and you might remember her in \"One Flew Over The Cuckoo\\'s Nest\". Like \"Fast Times at Ridgemont High\" this has a very good soundtrack and the songs being played reflect what is going on in the story. But at the heart of this film is two very good performances by Monoson and Franklin. There is nudity required by Franklin but she still conveys the sorrow of a young girl who gets dumped at a crucial time. She\\'s always been a good actress and her natural charm is very evident in this film. But this is still Monoson\\'s story and you can\\'t help but feel for this guy. When the film ends it\\'s his performance that stays with you. It\\'s a solid job of acting that makes this more than just a teen sex comedy. Even with the silly scenarios of teens trying to have sex this film still manages to achieve what it wants. Underrated comedy hits the bullseye.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of 'raw' review: we have all sort of HTML markup\n",
    "df.loc[500, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing by Raschka, Chpater 8 (see tutorial)\n",
    "# we remove all markups, substitute non-alphanumeric characters (including \n",
    "# underscore) with whitespaces, and remove the nose from emoticons\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have always liked this film and i m glad it s available finally on dvd so more viewers can see what i have been telling them all these years story is about a high school virgin named gary lawrence monoson who works at a pizza place as a delivery boy and he hangs out with his friends david joe rubbo and rick steve antin gary notices karen diane franklin who is the new girl in school and one morning he gives her a ride and by this time he is totally in love that night at a party he see s rick with karen and now he is jealous of his best friend but doesn t tell anyone of his true feelings spoiler alert rick asks gary if he can borrow his grandmothers vacant home but gary makes up an excuse so that rick can t get karen alone but one night rick brags to gary that he nailed her at the football field and gary becomes enraged a few days later in the school library gary see s rick and karen arguing and he asks karen what is wrong she tells him that she s pregnant and that rick has dumped her gary helps her by taking her to his grandmothers home and paying for her abortion finally gary tells karen how he really feels about her and she seems receptive to his feelings but later at her birthday party he walks in on karen and rick together again gary drives off without the girl this film ends with a much more realistic version of how life really is no matter how nice you are you don t necessarily get the girl this film was directed by boaz davidson who would go on to be a pretty competent action film director and he did two things right with this movie first he made sure that there was plenty of gratuitous nudity so that this was marketable to the young males that usually go to these films secondly he had the film end with young gary without karen and i think the males in the audience can relate to being screwed over no matter how hard you try and win a girls heart yes this film is silly and exploitive but it is funny and sexy actress louisa moritz almost steals the film as the sexy carmela moritz was always a popular b level actress and you might remember her in one flew over the cuckoo s nest like fast times at ridgemont high this has a very good soundtrack and the songs being played reflect what is going on in the story but at the heart of this film is two very good performances by monoson and franklin there is nudity required by franklin but she still conveys the sorrow of a young girl who gets dumped at a crucial time she s always been a good actress and her natural charm is very evident in this film but this is still monoson s story and you can t help but feel for this guy when the film ends it s his performance that stays with you it s a solid job of acting that makes this more than just a teen sex comedy even with the silly scenarios of teens trying to have sex this film still manages to achieve what it wants underrated comedy hits the bullseye '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking again the same review\n",
    "df.loc[500, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data as csv \n",
    "path = '...'  # insert path\n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. POS - tagging<a name=\"POS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we apply POS-tagging on (deduplicated and) pre-processed data - let us import them\n",
    "path = '...' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the NLTK resources\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# introduction of POS tagger per NLTK token\n",
    "def pos_tags(text):\n",
    "    text_processed = word_tokenize(text)\n",
    "    return \"-\".join( tag for (word, tag) in nltk.pos_tag(text_processed))\n",
    "\n",
    "# applying POS tagger to data \n",
    "############################################\n",
    "df['text_pos']=df.apply(lambda x: pos_tags(x['review']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save POS-tagged data as csv \n",
    "path = '...' # insert path \n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pre-trained word embeddings<a name=\"emb\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we apply embeddings on de-duplicated and pre-processed data - let us import them\n",
    "path = '...' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-trained word embedding model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md') # load the model first if necessary: python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we stack (like a numpy vertical stack) the 300 variables obtained from averaging the embedding of each df.review entry\n",
    "# WARNING: this is computationally expensive. Alternatively try with the smaller model en_core_web_sm\n",
    "import numpy as np\n",
    "emb = np.vstack(df.review.apply(lambda x: nlp(x).vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49582, 300)\n",
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15'\n",
      " '16' '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29'\n",
      " '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43'\n",
      " '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57'\n",
      " '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
      " '72' '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85'\n",
      " '86' '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99'\n",
      " '100' '101' '102' '103' '104' '105' '106' '107' '108' '109' '110' '111'\n",
      " '112' '113' '114' '115' '116' '117' '118' '119' '120' '121' '122' '123'\n",
      " '124' '125' '126' '127' '128' '129' '130' '131' '132' '133' '134' '135'\n",
      " '136' '137' '138' '139' '140' '141' '142' '143' '144' '145' '146' '147'\n",
      " '148' '149' '150' '151' '152' '153' '154' '155' '156' '157' '158' '159'\n",
      " '160' '161' '162' '163' '164' '165' '166' '167' '168' '169' '170' '171'\n",
      " '172' '173' '174' '175' '176' '177' '178' '179' '180' '181' '182' '183'\n",
      " '184' '185' '186' '187' '188' '189' '190' '191' '192' '193' '194' '195'\n",
      " '196' '197' '198' '199' '200' '201' '202' '203' '204' '205' '206' '207'\n",
      " '208' '209' '210' '211' '212' '213' '214' '215' '216' '217' '218' '219'\n",
      " '220' '221' '222' '223' '224' '225' '226' '227' '228' '229' '230' '231'\n",
      " '232' '233' '234' '235' '236' '237' '238' '239' '240' '241' '242' '243'\n",
      " '244' '245' '246' '247' '248' '249' '250' '251' '252' '253' '254' '255'\n",
      " '256' '257' '258' '259' '260' '261' '262' '263' '264' '265' '266' '267'\n",
      " '268' '269' '270' '271' '272' '273' '274' '275' '276' '277' '278' '279'\n",
      " '280' '281' '282' '283' '284' '285' '286' '287' '288' '289' '290' '291'\n",
      " '292' '293' '294' '295' '296' '297' '298' '299']\n"
     ]
    }
   ],
   "source": [
    "# embeddings into a dataframe\n",
    "emb = pd.DataFrame(emb, columns = np.array([str(x) for x in range(0, 299 + 1)]) )\n",
    "print(emb.shape)\n",
    "print(emb.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join embeddings with dataframe\n",
    "df_embed = pd.concat([df, emb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 302)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the resulting dataframe\n",
    "df_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word embedding data as csv \n",
    "path = '...' # insert path\n",
    "df_embed.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data analytics<a name=\"analytics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reproduce main data analytics results in Section 6.3 of the tutorial. We use the preprocessed and deduplicated data, for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. A quick check of data structure<a name=\"check\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "import pandas as pd\n",
    "\n",
    "path = '...' # insert path for deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported data structure\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns in data\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>i went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>actor turned director bill paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>as a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>i saw this film in a sneak preview and it is d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bill paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>i saw this film on september 1st 2005 in india...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>maybe i m reading into this too much but i won...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>i felt this film did have many good qualities ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>this movie is amazing because the fact that th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>quitting may be as much about exiting a pre o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  i went and saw this movie last night after bei...          1\n",
       "1  actor turned director bill paxton follows up h...          1\n",
       "2  as a recreational golfer with some knowledge o...          1\n",
       "3  i saw this film in a sneak preview and it is d...          1\n",
       "4  bill paxton has taken the true story of the 19...          1\n",
       "5  i saw this film on september 1st 2005 in india...          1\n",
       "6  maybe i m reading into this too much but i won...          1\n",
       "7  i felt this film did have many good qualities ...          1\n",
       "8  this movie is amazing because the fact that th...          1\n",
       "9   quitting may be as much about exiting a pre o...          1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported data: first 10 entries\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24884\n",
       "0    24698\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts of rviews per sentiment value\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Basic linguistic analysis of movie reviews<a name=\"basic\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>i went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>actor turned director bill paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>as a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>i saw this film in a sneak preview and it is d...</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bill paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>i saw this film on september 1st 2005 in india...</td>\n",
       "      <td>1</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>maybe i m reading into this too much but i won...</td>\n",
       "      <td>1</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>i felt this film did have many good qualities ...</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>this movie is amazing because the fact that th...</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>quitting may be as much about exiting a pre o...</td>\n",
       "      <td>1</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  word_count\n",
       "0  i went and saw this movie last night after bei...          1         153\n",
       "1  actor turned director bill paxton follows up h...          1         353\n",
       "2  as a recreational golfer with some knowledge o...          1         247\n",
       "3  i saw this film in a sneak preview and it is d...          1         128\n",
       "4  bill paxton has taken the true story of the 19...          1         206\n",
       "5  i saw this film on september 1st 2005 in india...          1         318\n",
       "6  maybe i m reading into this too much but i won...          1         344\n",
       "7  i felt this film did have many good qualities ...          1         144\n",
       "8  this movie is amazing because the fact that th...          1         174\n",
       "9   quitting may be as much about exiting a pre o...          1         959"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show distribution of review lenghts \n",
    "# we strip leading and trailing whitespaces and tokenize by whitespace\n",
    "df['word_count'] = df['review'].apply(lambda x: len(x.strip().split(\" \")))\n",
    "df[['review','sentiment', 'word_count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    49582.000000\n",
      "mean       235.660340\n",
      "std        174.444773\n",
      "min          6.000000\n",
      "25%        129.000000\n",
      "50%        177.000000\n",
      "75%        286.000000\n",
      "max       2498.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summary statistics of word counts\n",
    "print(df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show histograms of word counts divided per sentiment\n",
    "from matplotlib import pyplot\n",
    "\n",
    "x = df[df['sentiment']==0].word_count\n",
    "y = df[df['sentiment']==1].word_count\n",
    "\n",
    "pyplot.hist(x, bins=50, alpha=0.5, label='negative sentiment reviews')\n",
    "pyplot.hist(y, bins=50, alpha=0.5, label='positive sentiment reviews')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    24698.000000\n",
      "mean       234.109847\n",
      "std        168.079121\n",
      "min          6.000000\n",
      "25%        131.000000\n",
      "50%        178.000000\n",
      "75%        283.000000\n",
      "max       1550.000000\n",
      "Name: word_count, dtype: float64\n",
      "count    24884.000000\n",
      "mean       237.199244\n",
      "std        180.531262\n",
      "min         10.000000\n",
      "25%        127.000000\n",
      "50%        176.000000\n",
      "75%        288.000000\n",
      "max       2498.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summary of distributions of word counts\n",
    "print(x.describe())\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19476</td>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21333</td>\n",
       "      <td>read the book forget the movie</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "19476  primary plot primary direction poor interpreta...          0   \n",
       "21333                    read the book forget the movie           0   \n",
       "\n",
       "       word_count  \n",
       "19476           6  \n",
       "21333           6  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. word_counts=6 or 1550 or 2498 )\n",
    "df[df['word_count']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>i went and saw this movie last night after bei...</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>4.091503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>actor turned director bill paxton follows up h...</td>\n",
       "      <td>353</td>\n",
       "      <td>1</td>\n",
       "      <td>4.501416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>as a recreational golfer with some knowledge o...</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>4.607287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>i saw this film in a sneak preview and it is d...</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>4.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bill paxton has taken the true story of the 19...</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>4.723301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>i saw this film on september 1st 2005 in india...</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "      <td>4.544025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>maybe i m reading into this too much but i won...</td>\n",
       "      <td>344</td>\n",
       "      <td>1</td>\n",
       "      <td>4.270349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>i felt this film did have many good qualities ...</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>4.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>this movie is amazing because the fact that th...</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>4.436782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>quitting may be as much about exiting a pre o...</td>\n",
       "      <td>959</td>\n",
       "      <td>1</td>\n",
       "      <td>4.503650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment  \\\n",
       "0  i went and saw this movie last night after bei...         153          1   \n",
       "1  actor turned director bill paxton follows up h...         353          1   \n",
       "2  as a recreational golfer with some knowledge o...         247          1   \n",
       "3  i saw this film in a sneak preview and it is d...         128          1   \n",
       "4  bill paxton has taken the true story of the 19...         206          1   \n",
       "5  i saw this film on september 1st 2005 in india...         318          1   \n",
       "6  maybe i m reading into this too much but i won...         344          1   \n",
       "7  i felt this film did have many good qualities ...         144          1   \n",
       "8  this movie is amazing because the fact that th...         174          1   \n",
       "9   quitting may be as much about exiting a pre o...         959          1   \n",
       "\n",
       "   avg_word  \n",
       "0  4.091503  \n",
       "1  4.501416  \n",
       "2  4.607287  \n",
       "3  4.085938  \n",
       "4  4.723301  \n",
       "5  4.544025  \n",
       "6  4.270349  \n",
       "7  4.652778  \n",
       "8  4.436782  \n",
       "9  4.503650  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average word length (again, we tokenize by whitespaces)\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df['avg_word'] = df['review'].apply(lambda x: avg_word(x.strip()))\n",
    "df[['review','word_count', 'sentiment', 'avg_word']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    24698.000000\n",
      "mean         4.266094\n",
      "std          0.287540\n",
      "min          2.917808\n",
      "25%          4.073807\n",
      "50%          4.253968\n",
      "75%          4.447635\n",
      "max          7.500000\n",
      "Name: avg_word, dtype: float64\n",
      "\n",
      "count    24884.000000\n",
      "mean         4.325630\n",
      "std          0.318722\n",
      "min          3.137931\n",
      "25%          4.114986\n",
      "50%          4.316667\n",
      "75%          4.526851\n",
      "max         11.673077\n",
      "Name: avg_word, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distributions of word lengths conditional per sentiment\n",
    "x = df[df['sentiment']==0].avg_word\n",
    "y = df[df['sentiment']==1].avg_word\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11985</td>\n",
       "      <td>whoops looks like it s gonna cost you a whopp...</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>11.673077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "11985   whoops looks like it s gonna cost you a whopp...          1   \n",
       "\n",
       "       word_count   avg_word  \n",
       "11985          52  11.673077  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. avg_word>=11)\n",
    "df[df['avg_word']>=11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>i went and saw this movie last night after bei...</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>4.091503</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>actor turned director bill paxton follows up h...</td>\n",
       "      <td>353</td>\n",
       "      <td>1</td>\n",
       "      <td>4.501416</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>as a recreational golfer with some knowledge o...</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>4.607287</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>i saw this film in a sneak preview and it is d...</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>4.085938</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bill paxton has taken the true story of the 19...</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>4.723301</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>i saw this film on september 1st 2005 in india...</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "      <td>4.544025</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>maybe i m reading into this too much but i won...</td>\n",
       "      <td>344</td>\n",
       "      <td>1</td>\n",
       "      <td>4.270349</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>i felt this film did have many good qualities ...</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>4.652778</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>this movie is amazing because the fact that th...</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>4.436782</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>quitting may be as much about exiting a pre o...</td>\n",
       "      <td>959</td>\n",
       "      <td>1</td>\n",
       "      <td>4.503650</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment  \\\n",
       "0  i went and saw this movie last night after bei...         153          1   \n",
       "1  actor turned director bill paxton follows up h...         353          1   \n",
       "2  as a recreational golfer with some knowledge o...         247          1   \n",
       "3  i saw this film in a sneak preview and it is d...         128          1   \n",
       "4  bill paxton has taken the true story of the 19...         206          1   \n",
       "5  i saw this film on september 1st 2005 in india...         318          1   \n",
       "6  maybe i m reading into this too much but i won...         344          1   \n",
       "7  i felt this film did have many good qualities ...         144          1   \n",
       "8  this movie is amazing because the fact that th...         174          1   \n",
       "9   quitting may be as much about exiting a pre o...         959          1   \n",
       "\n",
       "   avg_word  stopwords  \n",
       "0  4.091503         82  \n",
       "1  4.501416        167  \n",
       "2  4.607287        123  \n",
       "3  4.085938         71  \n",
       "4  4.723301         91  \n",
       "5  4.544025        144  \n",
       "6  4.270349        184  \n",
       "7  4.652778         68  \n",
       "8  4.436782         93  \n",
       "9  4.503650        460  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words statistics - stopword from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['stopwords'] = df['review'].apply(lambda x: len([x for x in x.strip().split() if x in stop]))\n",
    "df[['review','word_count', 'sentiment', 'avg_word', 'stopwords']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    24698.000000\n",
      "mean       116.211556\n",
      "std         83.499135\n",
      "min          0.000000\n",
      "25%         65.000000\n",
      "50%         89.000000\n",
      "75%        140.000000\n",
      "max        726.000000\n",
      "Name: stopwords, dtype: float64\n",
      "\n",
      "count    24884.000000\n",
      "mean       115.732238\n",
      "std         87.736974\n",
      "min          3.000000\n",
      "25%         62.000000\n",
      "50%         87.000000\n",
      "75%        140.000000\n",
      "max       1208.000000\n",
      "Name: stopwords, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distributions of stop words conditional per sentiment\n",
    "x = df[df['sentiment']==0].stopwords\n",
    "y = df[df['sentiment']==1].stopwords\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19476</td>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "19476  primary plot primary direction poor interpreta...          0   \n",
       "\n",
       "       word_count  avg_word  stopwords  \n",
       "19476           6       7.5          0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. stopwords==0)\n",
    "df[df['stopwords']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Machine Learning<a name=\"ML\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replicate the machine learning pipelines from the tutorial, Section 6.4 (Classical and Modern Approaches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: as mentioned in the tutorial, the following cross-validation routines are computationally intensive. We recommend to sub-sample data and/or use HPC infrastructure (specifying the parameter njobs in GridSearch() accordingly). Test runs can be launched on reduced hyperparameter grids, as well. \n",
    "Note that we ran all the machine learning routines presented in this section on the ETH High Performance Computing (HPC) infrastructure [Euler](https://scicomp.ethz.ch/wiki/Euler), by submitting all jobs to a virtual machine consisting of 32 cores with 3072 MB RAM per core (total RAM: 98.304 GB). Therefore, notebook cell outputs are not available for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Adaptive boosting (ADA)<a name=\"ADA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the adaptive boosting (ADA) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. Bag-of-words<a name=\"ADA_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)     \n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],           # choose (1, 2) to compute 2-grams\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                          \n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]\n",
    "              }\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)               # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. Bag-of-POS<a name=\"ADA_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).text_pos\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).text_pos\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. Embeddings<a name=\"ADA_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)      # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 700, 900, 1000],\n",
    "#              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "pipe = Pipeline([('clf', AdaBoostClassifier(base_estimator=tree))])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Random Forests (RF)<a name=\"RF\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the random forests (RF) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Bag-of-words<a name=\"RF_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],    # for 2-grams:(1, 2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                           \n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)       # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Bag-of-POS<a name=\"RF_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).text_pos\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).text_pos\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3. Embeddings<a name=\"RF_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)     # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "             }\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 600, 800, 1000],\n",
    "#              'clf__max_depth': [1, 5, 10, 20]}\n",
    "\n",
    "pipe = Pipeline([('clf', RandomForestClassifier())\n",
    "               ])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)               # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Extreme gradient boosting (XGB)<a name=\"XGB\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the extreme gradient boosting (XGB) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings). We can use the cell below to install xgboost, if other imports failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing xgboost REMARK: run this cell only if other imports failed. Delete it in case xgboost has been already imported\n",
    "import pip\n",
    "pip.main(['install', 'xgboost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. Bag-of-words<a name=\"XGB_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to preprocessed and deduplicated data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)]    # for 2-grams: (1,2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                           \n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)                 # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. Bag-of-POS<a name=\"XGB_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3. Embeddings<a name=\"XGB_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)     # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "pipe = Pipeline([('clf', XGBClassifier())\n",
    "                ])\n",
    "                    \n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp_chapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
