{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *The Art of Natural Language Processing: Machine Learning for the Case Study*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authors: Andrea Ferrario, Mara NÃ¤gelin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date: February 2020** (updated September 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to run the machine learning modeling in the Classical and Modern Approaches, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Getting started with Python and Jupyter Notebook](#started)\n",
    "2. [Import data](#import)\n",
    "3. [Duplicated reviews](#duplicated)\n",
    "4. [Data preprocessing](#preprocessing)\n",
    "5. [POS-tagging](#POS)\n",
    "6. [Pre-trained word embeddings](#emb)\n",
    "7. [Data analytics](#analytics)  \n",
    "    7.1. [A quick check of data structure](#check)  \n",
    "    7.2. [Basic linguistic analysis of movie reviews](#basic)\n",
    "8. [Machine learning](#ML)  \n",
    "    8.1. [Adaptive boosting (ADA)](#ADA)  \n",
    "    .......8.1.1. [Bag-of-words](#ADA_BOW)  \n",
    "    .......8.1.2. [Bag-of-POS](#ADA_BOP)  \n",
    "    .......8.1.3. [Embeddings](#ADA_E)  \n",
    "    8.2. [Random forests (RF)](#RF)  \n",
    "    .......8.2.1. [Bag-of-words](#RF_BOW)  \n",
    "    .......8.2.2. [Bag-of-POS](#RF_BOP)  \n",
    "    .......8.2.3. [Embeddings](#RF_E)  \n",
    "    8.3. [Extreme gradient boosting (XGB)](#XGB)  \n",
    "    .......8.3.1. [Bag-of-words](#XGB_BOW)  \n",
    "    .......8.3.2. [Bag-of-POS](#XGB_BOP)  \n",
    "    .......8.3.3. [Embeddings](#XGB_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting started with Python and Jupyter Notebook<a name=\"started\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data<a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the import function, as in Chapter 8 of Raschka's book (see the tutorial)\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "basepath = './toymdb/' # insert basepath, where original data are stored\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Duplicated reviews<a name=\"duplicated\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "duplicates = df[df.duplicated()]  #equivalent to keep = first. Duplicated rows, except the first entry, are marked as 'True'\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: review, dtype: object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a check on the duplicated review\n",
    "duplicates.review   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates: \n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check\n",
    "df[df.duplicated(subset='review')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data preprocessing<a name=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great movie, great actors, great soundtrack! I loved it! Settings are perfect, dialogues, situations, storyline... all together mixed to give this masterpiece! Clooney and Turturro are magnificent and the Soggy Bottom Boys are simply charming and contagious with their music! :)'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of 'raw' review: we have all sort of HTML markup\n",
    "df.loc[0, 'review'] # 0 to 499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing by Raschka, Chpater 8 (see tutorial)\n",
    "# we remove all markups, substitute non-alphanumeric characters (including \n",
    "# underscore) with whitespaces, and remove the nose from emoticons\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great movie great actors great soundtrack i loved it settings are perfect dialogues situations storyline all together mixed to give this masterpiece clooney and turturro are magnificent and the soggy bottom boys are simply charming and contagious with their music :)'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking again the same review\n",
    "df.loc[0, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data as csv \n",
    "# path = './toymdb/0_data_pre.csv'  # insert path\n",
    "# df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. POS - tagging<a name=\"POS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we apply POS-tagging on (deduplicated and) pre-processed data - let us import them\n",
    "path = './toymdb/0_data_pre.csv' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the NLTK resources\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# introduction of POS tagger per NLTK token\n",
    "def pos_tags(text):\n",
    "    text_processed = word_tokenize(text)\n",
    "    return \"-\".join( tag for (word, tag) in nltk.pos_tag(text_processed))\n",
    "\n",
    "# applying POS tagger to data \n",
    "############################################\n",
    "df['text_pos']=df.apply(lambda x: pos_tags(x['review']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save POS-tagged data as csv \n",
    "# path = './toymdb/0_data_pos.csv' # insert path \n",
    "# df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pre-trained word embeddings<a name=\"emb\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we apply embeddings on de-duplicated and pre-processed data - let us import them\n",
    "path = './toymdb/0_data_pre.csv' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-trained word embedding model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # load the model first if necessary: python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we stack (like a numpy vertical stack) the 300 variables obtained from averaging the embedding of each df.review entry\n",
    "# WARNING: this is computationally expensive. Alternatively try with the smaller model en_core_web_sm\n",
    "import numpy as np\n",
    "emb = np.vstack(df.review.apply(lambda x: nlp(x).vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 96)\n",
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15'\n",
      " '16' '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29'\n",
      " '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43'\n",
      " '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57'\n",
      " '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
      " '72' '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85'\n",
      " '86' '87' '88' '89' '90' '91' '92' '93' '94' '95']\n"
     ]
    }
   ],
   "source": [
    "# embeddings into a dataframe\n",
    "emb = pd.DataFrame(emb, columns = np.array([str(x) for x in range(0, 96)]) )\n",
    "print(emb.shape)\n",
    "print(emb.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join embeddings with dataframe\n",
    "df_embed = pd.concat([df, emb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 98)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the resulting dataframe\n",
    "df_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word embedding data as csv \n",
    "# path = './toymdb/0_data_embed.csv' # insert path\n",
    "# df_embed.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data analytics<a name=\"analytics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reproduce main data analytics results in Section 6.3 of the tutorial. We use the preprocessed and deduplicated data, for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. A quick check of data structure<a name=\"check\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_pre.csv' # insert path for deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported data structure\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns in data\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great movie great actors great soundtrack i lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the end of suburbia is an important documentar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the intricate plot great visuals the world s g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is one of the few movies of this type i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>luc besson s first work is also his first fora...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rarely does a film capture such intense drama ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i kid you not yes who s that girl has the dist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a boy who adores maurice richard of the montre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the reason i think this movie is fabulous is t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spoiler alert i worked as an extra on this lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  great movie great actors great soundtrack i lo...          1\n",
       "1  the end of suburbia is an important documentar...          1\n",
       "2  the intricate plot great visuals the world s g...          1\n",
       "3  this is one of the few movies of this type i h...          1\n",
       "4  luc besson s first work is also his first fora...          1\n",
       "5  rarely does a film capture such intense drama ...          1\n",
       "6  i kid you not yes who s that girl has the dist...          1\n",
       "7  a boy who adores maurice richard of the montre...          1\n",
       "8  the reason i think this movie is fabulous is t...          1\n",
       "9  spoiler alert i worked as an extra on this lif...          1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported data: first 10 entries\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    250\n",
       "1    250\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts of rviews per sentiment value\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Basic linguistic analysis of movie reviews<a name=\"basic\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great movie great actors great soundtrack i lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the end of suburbia is an important documentar...</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the intricate plot great visuals the world s g...</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is one of the few movies of this type i h...</td>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>luc besson s first work is also his first fora...</td>\n",
       "      <td>1</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rarely does a film capture such intense drama ...</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i kid you not yes who s that girl has the dist...</td>\n",
       "      <td>1</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a boy who adores maurice richard of the montre...</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the reason i think this movie is fabulous is t...</td>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spoiler alert i worked as an extra on this lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  word_count\n",
       "0  great movie great actors great soundtrack i lo...          1          41\n",
       "1  the end of suburbia is an important documentar...          1         161\n",
       "2  the intricate plot great visuals the world s g...          1          54\n",
       "3  this is one of the few movies of this type i h...          1         117\n",
       "4  luc besson s first work is also his first fora...          1         386\n",
       "5  rarely does a film capture such intense drama ...          1         113\n",
       "6  i kid you not yes who s that girl has the dist...          1         537\n",
       "7  a boy who adores maurice richard of the montre...          1          57\n",
       "8  the reason i think this movie is fabulous is t...          1         194\n",
       "9  spoiler alert i worked as an extra on this lif...          1         239"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show distribution of review lenghts \n",
    "# we strip leading and trailing whitespaces and tokenize by whitespace\n",
    "df['word_count'] = df['review'].apply(lambda x: len(x.strip().split(\" \")))\n",
    "df[['review','sentiment', 'word_count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     500.000000\n",
      "mean      228.182000\n",
      "std       172.093432\n",
      "min         6.000000\n",
      "25%       126.000000\n",
      "50%       170.500000\n",
      "75%       274.000000\n",
      "max      1032.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summary statistics of word counts\n",
    "print(df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show histograms of word counts divided per sentiment\n",
    "from matplotlib import pyplot\n",
    "\n",
    "x = df[df['sentiment']==0].word_count\n",
    "y = df[df['sentiment']==1].word_count\n",
    "\n",
    "pyplot.hist(x, bins=50, alpha=0.5, label='negative sentiment reviews')\n",
    "pyplot.hist(y, bins=50, alpha=0.5, label='positive sentiment reviews')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     250.000000\n",
      "mean      225.304000\n",
      "std       167.889365\n",
      "min         6.000000\n",
      "25%       128.000000\n",
      "50%       173.000000\n",
      "75%       273.250000\n",
      "max      1032.000000\n",
      "Name: word_count, dtype: float64\n",
      "count    250.000000\n",
      "mean     231.060000\n",
      "std      176.487301\n",
      "min       26.000000\n",
      "25%      125.000000\n",
      "50%      169.000000\n",
      "75%      274.000000\n",
      "max      995.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summary of distributions of word counts\n",
    "print(x.describe())\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment  word_count\n",
       "238  primary plot primary direction poor interpreta...          0           6"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. word_counts=6 or 1550 or 2498 )\n",
    "df[df['word_count']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great movie great actors great soundtrack i lo...</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>5.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the end of suburbia is an important documentar...</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>4.981366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the intricate plot great visuals the world s g...</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4.240741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is one of the few movies of this type i h...</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>luc besson s first work is also his first fora...</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "      <td>4.430052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rarely does a film capture such intense drama ...</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>4.221239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i kid you not yes who s that girl has the dist...</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "      <td>4.292365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a boy who adores maurice richard of the montre...</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the reason i think this movie is fabulous is t...</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>4.067010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spoiler alert i worked as an extra on this lif...</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>4.213389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment  \\\n",
       "0  great movie great actors great soundtrack i lo...          41          1   \n",
       "1  the end of suburbia is an important documentar...         161          1   \n",
       "2  the intricate plot great visuals the world s g...          54          1   \n",
       "3  this is one of the few movies of this type i h...         117          1   \n",
       "4  luc besson s first work is also his first fora...         386          1   \n",
       "5  rarely does a film capture such intense drama ...         113          1   \n",
       "6  i kid you not yes who s that girl has the dist...         537          1   \n",
       "7  a boy who adores maurice richard of the montre...          57          1   \n",
       "8  the reason i think this movie is fabulous is t...         194          1   \n",
       "9  spoiler alert i worked as an extra on this lif...         239          1   \n",
       "\n",
       "   avg_word  \n",
       "0  5.512195  \n",
       "1  4.981366  \n",
       "2  4.240741  \n",
       "3  4.333333  \n",
       "4  4.430052  \n",
       "5  4.221239  \n",
       "6  4.292365  \n",
       "7  4.964912  \n",
       "8  4.067010  \n",
       "9  4.213389  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average word length (again, we tokenize by whitespaces)\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df['avg_word'] = df['review'].apply(lambda x: avg_word(x.strip()))\n",
    "df[['review','word_count', 'sentiment', 'avg_word']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    250.000000\n",
      "mean       4.292849\n",
      "std        0.365534\n",
      "min        3.500000\n",
      "25%        4.079142\n",
      "50%        4.295587\n",
      "75%        4.509106\n",
      "max        7.500000\n",
      "Name: avg_word, dtype: float64\n",
      "\n",
      "count    250.000000\n",
      "mean       4.316774\n",
      "std        0.340776\n",
      "min        3.385093\n",
      "25%        4.099108\n",
      "50%        4.295241\n",
      "75%        4.542389\n",
      "max        5.512195\n",
      "Name: avg_word, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distributions of word lengths conditional per sentiment\n",
    "x = df[df['sentiment']==0].avg_word\n",
    "y = df[df['sentiment']==1].avg_word\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment  word_count  \\\n",
       "238  primary plot primary direction poor interpreta...          0           6   \n",
       "\n",
       "     avg_word  stopwords  \n",
       "238       7.5          0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. avg_word>=11)\n",
    "df[df['avg_word']>=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great movie great actors great soundtrack i lo...</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>5.512195</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the end of suburbia is an important documentar...</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>4.981366</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the intricate plot great visuals the world s g...</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4.240741</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is one of the few movies of this type i h...</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>luc besson s first work is also his first fora...</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "      <td>4.430052</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rarely does a film capture such intense drama ...</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>4.221239</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i kid you not yes who s that girl has the dist...</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "      <td>4.292365</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a boy who adores maurice richard of the montre...</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4.964912</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the reason i think this movie is fabulous is t...</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>4.067010</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spoiler alert i worked as an extra on this lif...</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>4.213389</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment  \\\n",
       "0  great movie great actors great soundtrack i lo...          41          1   \n",
       "1  the end of suburbia is an important documentar...         161          1   \n",
       "2  the intricate plot great visuals the world s g...          54          1   \n",
       "3  this is one of the few movies of this type i h...         117          1   \n",
       "4  luc besson s first work is also his first fora...         386          1   \n",
       "5  rarely does a film capture such intense drama ...         113          1   \n",
       "6  i kid you not yes who s that girl has the dist...         537          1   \n",
       "7  a boy who adores maurice richard of the montre...          57          1   \n",
       "8  the reason i think this movie is fabulous is t...         194          1   \n",
       "9  spoiler alert i worked as an extra on this lif...         239          1   \n",
       "\n",
       "   avg_word  stopwords  \n",
       "0  5.512195         14  \n",
       "1  4.981366         65  \n",
       "2  4.240741         23  \n",
       "3  4.333333         53  \n",
       "4  4.430052        182  \n",
       "5  4.221239         59  \n",
       "6  4.292365        262  \n",
       "7  4.964912         25  \n",
       "8  4.067010        121  \n",
       "9  4.213389        114  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words statistics - stopword from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['stopwords'] = df['review'].apply(lambda x: len([x for x in x.strip().split() if x in stop]))\n",
    "df[['review','word_count', 'sentiment', 'avg_word', 'stopwords']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    250.00000\n",
      "mean     110.61600\n",
      "std       82.60588\n",
      "min        0.00000\n",
      "25%       62.00000\n",
      "50%       85.00000\n",
      "75%      136.00000\n",
      "max      523.00000\n",
      "Name: stopwords, dtype: float64\n",
      "\n",
      "count    250.000000\n",
      "mean     113.608000\n",
      "std       86.026978\n",
      "min        8.000000\n",
      "25%       62.000000\n",
      "50%       87.000000\n",
      "75%      130.750000\n",
      "max      491.000000\n",
      "Name: stopwords, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distributions of stop words conditional per sentiment\n",
    "x = df[df['sentiment']==0].stopwords\n",
    "y = df[df['sentiment']==1].stopwords\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment  word_count  \\\n",
       "238  primary plot primary direction poor interpreta...          0           6   \n",
       "\n",
       "     avg_word  stopwords  \n",
       "238       7.5          0  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. stopwords==0)\n",
    "df[df['stopwords']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Machine Learning<a name=\"ML\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replicate the machine learning pipelines from the tutorial, Section 6.4 (Classical and Modern Approaches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: as mentioned in the tutorial, the following cross-validation routines are computationally intensive. We recommend to sub-sample data and/or use HPC infrastructure (specifying the parameter njobs in GridSearch() accordingly). Test runs can be launched on reduced hyperparameter grids, as well. \n",
    "Note that we ran all the machine learning routines presented in this section on the ETH High Performance Computing (HPC) infrastructure [Euler](https://scicomp.ethz.ch/wiki/Euler), by submitting all jobs to a virtual machine consisting of 32 cores with 3072 MB RAM per core (total RAM: 98.304 GB). Therefore, notebook cell outputs are not available for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Adaptive boosting (ADA)<a name=\"ADA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the adaptive boosting (ADA) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. Bag-of-words<a name=\"ADA_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (500,)\n",
      "X_test shape check:  (500,)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-e97d1e5324a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m###########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mgs_lr_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# best estimator - test performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \"\"\"\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SAMME.R'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \"\"\"\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_pre.csv'  # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)     \n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],           # choose (1, 2) to compute 2-grams\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                          \n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]\n",
    "              }\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)               # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. Bag-of-POS<a name=\"ADA_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_pos.csv'  # insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).text_pos\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).text_pos\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. Embeddings<a name=\"ADA_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_embed.csv'  # insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)      # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 700, 900, 1000],\n",
    "#              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "pipe = Pipeline([('clf', AdaBoostClassifier(base_estimator=tree))])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Random Forests (RF)<a name=\"RF\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the random forests (RF) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Bag-of-words<a name=\"RF_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_pre.csv'  # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],    # for 2-grams:(1, 2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                           \n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)       # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Bag-of-POS<a name=\"RF_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_pos.csv'  # insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).text_pos\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).text_pos\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3. Embeddings<a name=\"RF_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_embed.csv'  # insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)     # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "             }\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 600, 800, 1000],\n",
    "#              'clf__max_depth': [1, 5, 10, 20]}\n",
    "\n",
    "pipe = Pipeline([('clf', RandomForestClassifier())\n",
    "               ])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)               # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Extreme gradient boosting (XGB)<a name=\"XGB\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the extreme gradient boosting (XGB) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings). We can use the cell below to install xgboost, if other imports failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing xgboost REMARK: run this cell only if other imports failed. Delete it in case xgboost has been already imported\n",
    "import pip\n",
    "pip.main(['install', 'xgboost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. Bag-of-words<a name=\"XGB_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_pre.csv'  # insert path to preprocessed and deduplicated data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)]    # for 2-grams: (1,2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                           \n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)                 # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. Bag-of-POS<a name=\"XGB_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_pos.csv'  # insert data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3. Embeddings<a name=\"XGB_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = './toymdb/0_data_embed.csv'  # insert data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)     # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "pipe = Pipeline([('clf', XGBClassifier())\n",
    "                ])\n",
    "                    \n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv)        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
