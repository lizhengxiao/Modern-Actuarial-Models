{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *The Art of Natural Language Processing: NLP Pipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authors: Andrea Ferrario, Mara Nägelin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date: February 2020** (updated September 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to test NLP preprocessing pipelines, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Getting started with Python and Jupyter Notebook](#started)\n",
    "2. [Test sentence](#test)\n",
    "3. [NLP preprocessing pipelines](#pipeline)  \n",
    "    3.1. [Conversion of text to lowercase](#lower)  \n",
    "    3.2. [Tokenizers](#tokenizers)  \n",
    "    3.3. [Stopwords removal](#stopwords)  \n",
    "    3.4. [Part-of-speech tagging](#POS)  \n",
    "    3.5. [Stemming and lemmatization](#stemming)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting started with Python and Jupyter Notebook<a name=\"started\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test sentence<a name=\"test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the test sentence to be preprocessed with NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In H.P. Lovecraft's short story 'The Call of Cthulhu', the author states that in S. Latitude 47° 9', W. Longitude 126° 43' the great Cthulhu dreams in the sea-bottom city of R'lyeh.\n"
     ]
    }
   ],
   "source": [
    "text = \"In H.P. Lovecraft's short story 'The Call of Cthulhu', the author states that in S. Latitude 47° 9', W. Longitude 126° 43' the great Cthulhu dreams in the sea-bottom city of R'lyeh.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the NLP pipeline:\n",
    "- conversion of text to lowercase;\n",
    "- tokenization, i.e. split of all strings of text into tokens;\n",
    "- part-of-speech (POS) tagging of tokenized text;\n",
    "- stopwords removal;\n",
    "- stemming or lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NLP Preprocessing Pipeline<a name=\"pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Conversion of text to lowercase<a name=\"lower\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply lowercase to the test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"in h.p. lovecraft's short story 'the call of cthulhu', the author states that in s. latitude 47° 9', w. longitude 126° 43' the great cthulhu dreams in the sea-bottom city of r'lyeh.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Tokenizers<a name=\"tokenizers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'H.P.', \"Lovecraft's\", 'short', 'story', \"'The\", 'Call', 'of', \"Cthulhu',\", 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47°', \"9',\", 'W.', 'Longitude', '126°', \"43'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea-bottom', 'city', 'of', \"R'lyeh.\"]\n"
     ]
    }
   ],
   "source": [
    "# 1. whitespace tokenizer\n",
    "#########################\n",
    "import re\n",
    "white_tok = text.split()\n",
    "print(white_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\namara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'H.P', '.', 'Lovecraft', \"'s\", 'short', 'story', \"'The\", 'Call', 'of', 'Cthulhu', \"'\", ',', 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47°', '9', \"'\", ',', 'W.', 'Longitude', '126°', '43', \"'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea-bottom', 'city', 'of', \"R'lyeh\", '.']\n"
     ]
    }
   ],
   "source": [
    "# 2. Natural Language Tool Kit tokenizer\n",
    "########################################\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokens_NLTK = word_tokenize(text)\n",
    "print(tokens_NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "H.P.\n",
      "Lovecraft\n",
      "'s\n",
      "short\n",
      "story\n",
      "'\n",
      "The\n",
      "Call\n",
      "of\n",
      "Cthulhu\n",
      "'\n",
      ",\n",
      "the\n",
      "author\n",
      "states\n",
      "that\n",
      "in\n",
      "S.\n",
      "Latitude\n",
      "47\n",
      "°\n",
      "9\n",
      "'\n",
      ",\n",
      "W.\n",
      "Longitude\n",
      "126\n",
      "°\n",
      "43\n",
      "'\n",
      "the\n",
      "great\n",
      "Cthulhu\n",
      "dreams\n",
      "in\n",
      "the\n",
      "sea\n",
      "-\n",
      "bottom\n",
      "city\n",
      "of\n",
      "R'lyeh\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# 3. SpaCy tokenizer\n",
    "####################\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['126', '43', '47', 'author', 'bottom', 'call', 'city', 'cthulhu', 'dreams', 'great', 'in', 'latitude', 'longitude', 'lovecraft', 'lyeh', 'of', 'sea', 'short', 'states', 'story', 'that', 'the']\n"
     ]
    }
   ],
   "source": [
    "# 4. sklearn vectorizer\n",
    "#######################\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "textt = [text]\n",
    "X = vectorizer.fit_transform(textt)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Stopwords removal<a name=\"stopwords\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove stopwords using NLTK, SpaCy and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "NLTK tokenized test sentence: ['In', 'H.P', '.', 'Lovecraft', \"'s\", 'short', 'story', \"'The\", 'Call', 'of', 'Cthulhu', \"'\", ',', 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47°', '9', \"'\", ',', 'W.', 'Longitude', '126°', '43', \"'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea-bottom', 'city', 'of', \"R'lyeh\", '.']\n",
      "-----------------------------\n",
      "NLTK tokenized test sentence after stowords removal: ['In', 'H.P', '.', 'Lovecraft', \"'s\", 'short', 'story', \"'The\", 'Call', 'Cthulhu', \"'\", ',', 'author', 'states', 'S.', 'Latitude', '47°', '9', \"'\", ',', 'W.', 'Longitude', '126°', '43', \"'\", 'great', 'Cthulhu', 'dreams', 'sea-bottom', 'city', \"R'lyeh\", '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\namara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# 1. stopwords removal with the Natural Language Tool Kit (NLTK)\n",
    "################################################################\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# we tokenize the test sentence\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word not in stop]\n",
    "\n",
    "print('-----------------------------')\n",
    "print('NLTK tokenized test sentence:', tokens)\n",
    "print('-----------------------------')\n",
    "print('NLTK tokenized test sentence after stowords removal:', filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that', 'in', 'the', 'of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed stopwords\n",
    "list(set(tokens) - set(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "SpaCy tokenized test sentence: ['In', 'H.P.', 'Lovecraft', \"'s\", 'short', 'story', \"'\", 'The', 'Call', 'of', 'Cthulhu', \"'\", ',', 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47', '°', '9', \"'\", ',', 'W.', 'Longitude', '126', '°', '43', \"'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea', '-', 'bottom', 'city', 'of', \"R'lyeh\", '.']\n",
      "-----------------------------\n",
      "SpaCy tokenized test sentence after stowords removal: ['In', 'H.P.', 'Lovecraft', \"'s\", 'short', 'story', \"'\", 'The', 'Call', 'Cthulhu', \"'\", ',', 'author', 'states', 'S.', 'Latitude', '47', '°', '9', \"'\", ',', 'W.', 'Longitude', '126', '°', '43', \"'\", 'great', 'Cthulhu', 'dreams', 'sea', '-', 'city', \"R'lyeh\", '.']\n"
     ]
    }
   ],
   "source": [
    "# 2. stopwords removal with SpaCy\n",
    "#################################\n",
    "import spacy\n",
    "\n",
    "# tokenization\n",
    "text_spacy = nlp(text)\n",
    "\n",
    "token_list = []\n",
    "for token in text_spacy:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "# stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# create list of word tokens after removing stopwords note that .vocab() looks at the lexeme of each token \n",
    "filtered_sentence =[] \n",
    "\n",
    "for word in token_list:\n",
    "    lexeme = nlp.vocab[word]   \n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word) \n",
    "        \n",
    "print('-----------------------------')\n",
    "print('SpaCy tokenized test sentence:', token_list)\n",
    "print('-----------------------------')\n",
    "print('SpaCy tokenized test sentence after stowords removal:', filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bottom', 'that', 'the', 'in', 'of']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed stopwords\n",
    "list(set(token_list) - set(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "CountVectorizer() tokenized test sentence: ['126', '43', '47', 'author', 'bottom', 'call', 'city', 'cthulhu', 'dreams', 'great', 'in', 'latitude', 'longitude', 'lovecraft', 'lyeh', 'of', 'sea', 'short', 'states', 'story', 'that', 'the']\n",
      "-----------------------------\n",
      "CountVectorizer() tokenized test sentence after stowords removal: ['126', '43', '47', 'author', 'city', 'cthulhu', 'dreams', 'great', 'latitude', 'longitude', 'lovecraft', 'lyeh', 'sea', 'short', 'states', 'story']\n"
     ]
    }
   ],
   "source": [
    "# 3. stopwords removal with sklearn and TfidfVectorizer()\n",
    "########################################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [text]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_stop = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_stop = vectorizer_stop.fit_transform(corpus)\n",
    "\n",
    "print('-----------------------------')\n",
    "print('CountVectorizer() tokenized test sentence:', vectorizer.get_feature_names())\n",
    "print('-----------------------------')\n",
    "print('CountVectorizer() tokenized test sentence after stowords removal:', vectorizer_stop.get_feature_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bottom', 'call', 'in', 'of', 'that', 'the'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed stopwords\n",
    "set(vectorizer.get_feature_names()) - set(vectorizer_stop.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Part-of-speech tagging<a name=\"POS\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform Part-Of-Speech (POS) tagging using the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\namara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('H.P', 'NNP'), ('.', '.'), ('Lovecraft', 'NNP'), (\"'s\", 'POS'), ('short', 'JJ'), ('story', 'NN'), (\"'The\", 'POS'), ('Call', 'NNP'), ('of', 'IN'), ('Cthulhu', 'NNP'), (\"'\", 'POS'), (',', ','), ('the', 'DT'), ('author', 'NN'), ('states', 'VBZ'), ('that', 'IN'), ('in', 'IN'), ('S.', 'NNP'), ('Latitude', 'NNP'), ('47°', 'CD'), ('9', 'CD'), (\"'\", \"''\"), (',', ','), ('W.', 'NNP'), ('Longitude', 'NNP'), ('126°', 'CD'), ('43', 'CD'), (\"'\", \"''\"), ('the', 'DT'), ('great', 'JJ'), ('Cthulhu', 'NNP'), ('dreams', 'NN'), ('in', 'IN'), ('the', 'DT'), ('sea-bottom', 'JJ'), ('city', 'NN'), ('of', 'IN'), (\"R'lyeh\", 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# introduction of POS tagger per NLTK token (word_tokenize is the tokenizer we choose)\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tokens_NLTK = word_tokenize(text)\n",
    "print(pos_tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Stemming and lemmatization<a name=\"stemming\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform (Porter) stemming and lemmatization on the test sentence, after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'h.p', '.', 'lovecraft', \"'s\", 'short', 'stori', \"'the\", 'call', 'of', 'cthulhu', \"'\", ',', 'the', 'author', 'state', 'that', 'in', 'S.', 'latitud', '47°', '9', \"'\", ',', 'W.', 'longitud', '126°', '43', \"'\", 'the', 'great', 'cthulhu', 'dream', 'in', 'the', 'sea-bottom', 'citi', 'of', \"r'lyeh\", '.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK Porter stemming on tokenized test sentence \n",
    "#################################################\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in word_tokenize(text)]\n",
    "\n",
    "# use stemming on NLTK tokenizer text\n",
    "stem_tokens = tokenizer_porter(text)\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\namara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In***H.P***.***Lovecraft***'s***short***story***'The***Call***of***Cthulhu***'***,***the***author***state***that***in***S.***Latitude***47°***9***'***,***W.***Longitude***126°***43***'***the***great***Cthulhu***dream***in***the***sea-bottom***city***of***R'lyeh***.\n"
     ]
    }
   ],
   "source": [
    "# NLTK lemmatization (WordNet database) on tokenized test sentence\n",
    "##################################################################\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Wordnet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK tokenizer\n",
    "word_list = nltk.word_tokenize(text)\n",
    "\n",
    "# lemmatization of the list of words and join - we lemmatize verbs (therefore 'v') and we use '***' as separator\n",
    "lemmatized_output = '***'.join([lemmatizer.lemmatize(w, 'v') for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
