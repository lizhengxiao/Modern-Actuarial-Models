{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *The Art of Natural Language Processing: RNNs for the Case Study*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authors: Andrea Ferrario, Mara Nägelin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date: February 2020** (updated September 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to run the RNNs in the Contemporary Approach, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Getting started with Python and Jupyter Notebook](#datagen)\n",
    "2. [Import data](#datagen)\n",
    "3. [Data preprocessing](#dataprep)  \n",
    "    3.1. [Remove duplicates](#remdup)  \n",
    "    3.2. [Shuffle the data](#shuffle)  \n",
    "    3.3. [Minimal preprocessing (detailed)](#prep_det)  \n",
    "    3.4. [Minimal preprocessing with Keras](#prep_keras)  \n",
    "4. [Deep learning](#DL)  \n",
    "    4.1. [Train test split](#trainsplit)  \n",
    "    4.2. [Define the model](#modeldef)  \n",
    "    .......4.2.1. [Shallow LSTM](#LSTM1)  \n",
    "    .......4.2.2. [Shallow GRU](#GRU1)  \n",
    "    .......4.2.3. [Deep LSTM](#LSTM2)    \n",
    "    4.3. [Train the model](#trainmodel)  \n",
    "    4.4. [Evaluate the model on test data](#evalmodel)  \n",
    "5. [Final remarks](#fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"started\"></a>\n",
    "# 1. Getting started with Python and Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='datagen'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the raw data from the original 50'000 text files and save to a dataframe. This only needs to be run once. After that, one can start directly with [Section 2](#dataprep). The following code snippet is based on the book `Python Machine Learning` by Raschka and Mirjalili, Chapter 8 (see tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:46\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "basepath = 'path_to_extracted_data/aclImdb/' # TODO: update to point to your data repository\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv \n",
    "path = \"path_to_save_data/movie_data.csv\" # TODO: update to your path\n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataprep'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we preprare the raw data such that in can be used as input for a neural network. Again, we follow the example of Raschka and Mirjalili (Chapter 16). We perform the following steps:\n",
    "\n",
    "* We remove all duplicates.\n",
    "* We shuffle the data in a random permutation.\n",
    "* We apply only minimal preprocessing (i.e. convert to lowercase and split on whitespaces and punctuation).\n",
    "* We map each word bijectively to an integer value.\n",
    "* We set each review to an equal length $T$ by padding with $0$ or slicing as required.\n",
    "\n",
    "The last three steps are written out in detail in [Section 2.3.](#prep_det) to give the reader an understanding of what exactly happens to the data. However, they can also be carried out — almost equivalently — using the high-level `text.preprocessing` functionalities of the `tensorflow.keras` module, see [Section 2.4.](#prep_keras) The user needs to run only one of these two subsections to preprocess the data.\n",
    "\n",
    "The transformed data is stored in a dataframe for convenience. Hence [Section 2](#dataprep) also needs to be run only once, and after one can start jump directly to [Section 3](#DL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following can be used to reimport the dataframe with the raw data generated in [Section 1](#datagen) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = 'path_to_save_data/movie_data.csv' # TODO: update to your path  \n",
    "df = pd.read_csv(path, encoding='utf-8') # read in the dataframe stored as csv\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='remdup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates - we found them, even with HTML markup...\n",
    "duplicates = df[df.duplicated()]  # Duplicated rows, except the first entry, are marked as 'True'\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33       I was fortunate to attend the London premier o...\n",
       "177      I've been strangely attracted to this film sin...\n",
       "939      The Andrew Davies adaptation of the Sarah Wate...\n",
       "1861     <br /><br />First of all, I reviewed this docu...\n",
       "1870     Spheeris debut must be one of the best music d...\n",
       "                               ...                        \n",
       "49412    There is no way to avoid a comparison between ...\n",
       "49484    **SPOILERS** I rented \"Tesis\" (or \"Thesis\" in ...\n",
       "49842    'Dead Letter Office' is a low-budget film abou...\n",
       "49853    This movie had a IMDB rating of 8.1 so I expec...\n",
       "49864    You know all those letters to \"Father Christma...\n",
       "Name: review, Length: 418, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a check on the duplicated review\n",
    "duplicates.review   # some appear more than once, as they originally appear 3 or more times in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates: 49582 + 418 = 50000\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check\n",
    "df[df.duplicated(subset='review')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shuffle'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shuffle the data to ensure randomness in the training input\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df = df.reset_index(drop=True) # reset the index after the permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prep_det'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Minimal preprocessing (detailed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippets are in part adapted from Raschka and Mirjalili (Chapter 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:07:16\n"
     ]
    }
   ],
   "source": [
    "# Minimal preprocessing and generating word counts:\n",
    "#  - we surround all punctuation by whitespaces\n",
    "#  - all text is converted to lowercase\n",
    "#  - word counts are generated by splitting the text on whitespaces\n",
    "import pyprind\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' '  \n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split()) # splitting on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 102966\n"
     ]
    }
   ],
   "source": [
    "# get the size of the vocabulary\n",
    "print('Number of unique words:', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appear more than once: 62923\n",
      "Number of words that appear more than 30 times: 15282\n"
     ]
    }
   ],
   "source": [
    "# investigate how many words appear only rarely in the reviews\n",
    "print('Number of words that appear more than once:', \n",
    "      len([k for k, v in counts.items() if v > 1]))\n",
    "print('Number of words that appear more than 30 times:', \n",
    "      len([k for k, v in counts.items() if v > 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hence we use only the 15'000 most common in our vocabulary \n",
    "# this will make training more efficient without loosing too much information\n",
    "vocab_size = 15000\n",
    "\n",
    "# create a dictionary with word:integer pairs for all unique words\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "word_counts = word_counts[0:vocab_size]\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    }
   ],
   "source": [
    "# Mapping words to integers\n",
    "# create a list with all reviews in integer coded form\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] \n",
    "                           for word in review.split() \n",
    "                           if word in word_to_int.keys()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length of mapped reviews: 213.0\n"
     ]
    }
   ],
   "source": [
    "# get the median length of the mapped review sequences to inform the choice of sequence_length\n",
    "print('Median length of mapped reviews:',\n",
    "      np.median([len(review) for review in mapped_reviews]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rev_lengths = np.array([len(review) for review in mapped_reviews])\n",
    "plt.hist(rev_lengths[rev_lengths < 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding: set sequence length and ensure all mapped reviews are coerced to required length\n",
    "# if sequence length < T: left-pad with zeros\n",
    "# if sequence length > T: use the last T elements\n",
    "sequence_length = 200  # (Known as T in our RNN formulae)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>961</td>\n",
       "      <td>306</td>\n",
       "      <td>581</td>\n",
       "      <td>82</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1945</td>\n",
       "      <td>1209</td>\n",
       "      <td>2</td>\n",
       "      <td>1881</td>\n",
       "      <td>3</td>\n",
       "      <td>2612</td>\n",
       "      <td>3</td>\n",
       "      <td>5244</td>\n",
       "      <td>2218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>147</td>\n",
       "      <td>1074</td>\n",
       "      <td>25</td>\n",
       "      <td>463</td>\n",
       "      <td>6240</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>2053</td>\n",
       "      <td>9122</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3996</td>\n",
       "      <td>678</td>\n",
       "      <td>105</td>\n",
       "      <td>1908</td>\n",
       "      <td>2667</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>360</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2728</td>\n",
       "      <td>16</td>\n",
       "      <td>4074</td>\n",
       "      <td>40</td>\n",
       "      <td>493</td>\n",
       "      <td>7</td>\n",
       "      <td>89</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1440</td>\n",
       "      <td>19</td>\n",
       "      <td>723</td>\n",
       "      <td>19</td>\n",
       "      <td>1855</td>\n",
       "      <td>19</td>\n",
       "      <td>1440</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>1201</td>\n",
       "      <td>19</td>\n",
       "      <td>2294</td>\n",
       "      <td>6281</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>6112</td>\n",
       "      <td>771</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment   0     1    2    3   4     5     6     7   8  ...   190   191  \\\n",
       "0          1   4   961  306  581  82    97     1    76   2  ...    11  1945   \n",
       "1          1   0     0    0    0   0     0     0     0   0  ...    14     8   \n",
       "2          1   3   359    1  109   3    44  2053  9122   6  ...  3996   678   \n",
       "3          1   0     0    0    0   0     0     0     0   0  ...     1  2728   \n",
       "4          0  19  1440   19  723  19  1855    19  1440  19  ...  1201    19   \n",
       "\n",
       "    192   193   194  195   196   197   198   199  \n",
       "0  1209     2  1881    3  2612     3  5244  2218  \n",
       "1    21   147  1074   25   463  6240    41    41  \n",
       "2   105  1908  2667    2     5   360    24     2  \n",
       "3    16  4074    40  493     7    89    14     2  \n",
       "4  2294  6281    10   64  6112   771   100     2  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with processed data\n",
    "df_processed = pd.concat([df['sentiment'],pd.DataFrame(sequences)], axis=1)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prep_keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Minimal preprocessing with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1786</td>\n",
       "      <td>1003</td>\n",
       "      <td>1427</td>\n",
       "      <td>3</td>\n",
       "      <td>832</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>1132</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1923</td>\n",
       "      <td>1192</td>\n",
       "      <td>1859</td>\n",
       "      <td>2590</td>\n",
       "      <td>5216</td>\n",
       "      <td>2194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>263</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>133</td>\n",
       "      <td>1057</td>\n",
       "      <td>18</td>\n",
       "      <td>448</td>\n",
       "      <td>6213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10587</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>1133</td>\n",
       "      <td>1637</td>\n",
       "      <td>374</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>997</td>\n",
       "      <td>3971</td>\n",
       "      <td>663</td>\n",
       "      <td>93</td>\n",
       "      <td>1886</td>\n",
       "      <td>2643</td>\n",
       "      <td>3</td>\n",
       "      <td>345</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2706</td>\n",
       "      <td>11</td>\n",
       "      <td>4049</td>\n",
       "      <td>31</td>\n",
       "      <td>478</td>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10588</td>\n",
       "      <td>3559</td>\n",
       "      <td>461</td>\n",
       "      <td>4106</td>\n",
       "      <td>14422</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>3836</td>\n",
       "      <td>560</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>13</td>\n",
       "      <td>1183</td>\n",
       "      <td>2271</td>\n",
       "      <td>6295</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>6085</td>\n",
       "      <td>755</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment      0     1      2     3      4     5     6     7    8  ...  \\\n",
       "0          1   1786  1003   1427     3    832     7    20    48    3  ...   \n",
       "1          1      0     0      0     0      0     0     0     0    0  ...   \n",
       "2          1      6     6  10587    27      3  1133  1637   374   26  ...   \n",
       "3          1      0     0      0     0      0     0     0     0    0  ...   \n",
       "4          0  10588  3559    461  4106  14422   116     2  3836  560  ...   \n",
       "\n",
       "   190   191   192   193   194   195   196   197   198   199  \n",
       "0   20  1132     8     8  1923  1192  1859  2590  5216  2194  \n",
       "1   23   263     9     6    14   133  1057    18   448  6213  \n",
       "2   12   997  3971   663    93  1886  2643     3   345    17  \n",
       "3   10     1  2706    11  4049    31   478     5    77     9  \n",
       "4  117    13  1183  2271  6295     7    53  6085   755    88  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 15000\n",
    "\n",
    "# map words to integers including minimal preprocessing\n",
    "tokenizer = Tokenizer(num_words=vocab_size, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # filters out all punctuation other than '\n",
    "                      lower=True, # convert to lowercase\n",
    "                      split=' ') # split on whitespaces\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "list_tokenized = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "# Padding to sequence_length\n",
    "sequence_length = 200\n",
    "sequences = pad_sequences(list_tokenized, maxlen=sequence_length)\n",
    "\n",
    "# create df with processed data\n",
    "df_processed = pd.concat([df['sentiment'], pd.DataFrame(sequences)], axis=1)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "<br>\n",
    "Lastly, we save the fully preprocesssed data to csv for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv \n",
    "path = \"path_to_save_data/movie_data_processed.csv\" # TODO: update to your path\n",
    "df_processed.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DL'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we reproduce the results from Section 6.4.6-6.4.7 of the tutorial. We split the preprocessed data into a training and a testing set and define our RNN model (three different possible architectures are given as examples, see [Sections 4.2.1.-4.2.3](#LSTM1). The model is compiled and trained using the high-level `tensorflow.keras` API. Finally, the development of loss and accuracy during the training is plotted and the fitted model is evaluated on the test data.  \n",
    "\n",
    "**WARNING:** Note that training with a large training dataset for a large number of epochs is computationally intensive and might easily take a couple of hours on a normal CPU machine. We recommend subsetting the training and testing datasets and/or using an HPC infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure that all keras functionalities work as intended\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 201)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = \"path_to_save_data/movie_data_processed.csv\" # TODO: update to your path\n",
    "df_processed = pd.read_csv(path, encoding='utf-8')\n",
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainsplit'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/20 train test split: 39666 / 9916\n"
     ]
    }
   ],
   "source": [
    "# get the number of samples for the training and test datasets\n",
    "perc_train = 0.8\n",
    "n_train = round(df_processed.shape[0]*perc_train)\n",
    "n_test = round(df_processed.shape[0]*(1-perc_train))\n",
    "\n",
    "print(str(int(perc_train*100))+'/'+str(int(100-perc_train*100))+' train test split:', \n",
    "      n_train, '/', n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape check X, y: (39666, 200) (39666,)\n",
      "Testing data shape check X, y: (9916, 200) (9916,)\n"
     ]
    }
   ],
   "source": [
    "# create the training and testing datasets\n",
    "X_train = np.array(df_processed.head(n_train).drop('sentiment', axis=1)) # replace with n_train\n",
    "y_train = df_processed.head(n_train).sentiment.values\n",
    "\n",
    "X_test = np.array(df_processed.tail(n_test).drop('sentiment', axis=1)) # replace with n_test\n",
    "y_test = df_processed.tail(n_test).sentiment.values\n",
    "\n",
    "print('Training data shape check X, y:', X_train.shape, y_train.shape)\n",
    "print('Testing data shape check X, y:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeldef'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14999\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# get the total number of words in vocabulary (+1 for the padding with 0)\n",
    "vocab_size = df_processed.drop('sentiment', axis=1).values.max() + 1\n",
    "print('Vocabulary size:', vocab_size-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the following subsections defines a distinct model architecture. The user can select and run one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LSTM1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Shallow LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a shallow RNN with just one LSTM layer. The same architecture was used for the example by Raschka and Marjili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a LSTM layer with 128 internal units\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GRU1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Shallow GRU architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the same shallow RNN as above with a GRU layer instead of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a GRU layer with 128 internal units\n",
    "model.add(layers.GRU(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LSTM3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Deep LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily deepen our network by stacking a second LSTM layer on top of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a LSTM layer with 128 internal units\n",
    "# Return sequences so we can stack the the next LSTM layer on top\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Add a second LSTM layer\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainmodel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 256)         3840000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         197120    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,168,833\n",
      "Trainable params: 4,168,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print the summary of the model we have defined\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# we select here the optimizer, loss and metric to be used in training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for early stopping during training\n",
    "# stop training when the validation loss `val_accuracy` is no longer improving\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=1e-3,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: as mentioned in the tutorial, the following training routine is computationally intensive. We recommend to sub-sample data in [Section 4.1.](#trainsplit) and/or use HPC infrastructure. \n",
    "Note that we ran all the machine learning routines presented in this section on the ETH High Performance Computing (HPC) infrastructure [Euler](https://scicomp.ethz.ch/wiki/Euler), by submitting all jobs to a virtual machine consisting of 32 cores with 3072 MB RAM per core (total RAM: 98.304 GB). Therefore, notebook outputs are not available for the subesquent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_split=0.2,\n",
    "                    epochs=30, \n",
    "                    batch_size=256,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the development of the accuracy over epochs to see the training progress\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the development of the loss over epochs to see the training progress\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evalmodel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# evalute model to get accuracy and loss on test data\n",
    "results = model.evaluate(X_test, y_test, batch_size=256, verbose=0)\n",
    "\n",
    "# calculate AUC on test data\n",
    "y_pred = model.predict_proba(X_test, batch_size=256)\n",
    "auc_res = roc_auc_score(y_test, y_pred[:, 0])\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])\n",
    "print('Test AUC:', auc_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example RNNs are simple architectures where none of the parameters were optimized for performance. In order to further improve the model accuracy, we could for example\n",
    "* play around with the network architecture    \n",
    "     (e.g. the depth of the network, the type of layers used, the number of hidden units within a layer, the activation functions used, ...)\n",
    "* fine-tune the training parameters   \n",
    "     (i.e. the number of epochs, batch size, ...)\n",
    "* perform more elaborate preprocessing on the data  \n",
    "    (e.g. excluding stopwords, see also the two other Notebooks and Section 1 of the tutorial)\n",
    "* use a the weights of an already trained embedding for our embedding layer  \n",
    "    (either as nontrainable fixed weights or with transfer learning, compare the Notebook ```NLP_IMDb_Case_Study_ML.ipynb``` and Section 3 of the tutorial)  \n",
    "\n",
    "Finally, note that the size of the dataset is arguably still too small to allow for much improvement over the presented architectures and results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp_chapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
