# å¾ªç¯ç¥ç»ç½‘ç»œä¸æ­»äº¡ç‡é¢„æµ‹ {#rnn}

*æ–¹ç‰æ˜•ã€é²ç‘¶ã€é«˜å…‰è¿œ*

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = F)
```

ğŸ˜· æ–°å† è‚ºç‚æ­»äº¡ç‡æ•°æ®ï¼š<https://mpidr.shinyapps.io/stmortality/>

## Lee-Carter Model

Lee Carteræ¨¡å‹ä¸­ï¼Œæ­»äº¡åŠ›ï¼ˆforce of mortalityï¼‰çš„å®šä¹‰å¦‚ä¸‹ï¼š  
$$\log \left(m_{t, x}\right)=a_{x}+b_{x} k_{t}$$
å…¶ä¸­ï¼Œ  

- $m_{t, x}>0$ æ˜¯ $x$ å²çš„äººåœ¨æ—¥å†å¹´ $t$ çš„æ­»äº¡ç‡ï¼ˆmortality rateï¼‰,   

- $a_{x}$ æ˜¯ $x$ å²çš„äººçš„å¹³å‡å¯¹æ•°æ­»äº¡ç‡,    

- $b_{x}$ æ˜¯æ­»äº¡ç‡å˜åŒ–çš„å¹´é¾„å› ç´ , 

- $\left(k_{t}\right)_{t}$ æ˜¯æ­»äº¡ç‡å˜åŒ–çš„æ—¥å†å¹´å› ç´ .  
  
ç”¨ $M_{t, x}$ è¡¨ç¤ºæŸä¸€æ€§åˆ«æ­»äº¡ç‡çš„è§‚å¯Ÿå€¼ï¼ˆraw mortality ratesï¼‰.  
æˆ‘ä»¬å¯¹å¯¹æ•°æ­»äº¡ç‡ $\log \left(M_{t, x}\right)$  ä¸­å¿ƒåŒ–å¤„ç†ï¼š  
$$\log \left(M_{t, x}^{\circ}\right)=\log \left(M_{t, x}\right)-\widehat{a}_{x}=\log \left(M_{t, x}\right)-\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)$$

å…¶ä¸­ï¼Œ  

- $\mathcal{T}$ ä¸ºè®­ç»ƒé›†ä¸­æ—¥å†å¹´çš„é›†åˆ,  

- $\widehat{a}_{x}=\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)$ æ˜¯å¹³å‡å¯¹æ•°æ­»äº¡ç‡ $a_{x}$ çš„ä¼°è®¡.  
  
å¯¹äº$b_x,k_t$ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ±‚å¦‚ä¸‹æœ€ä¼˜åŒ–é—®é¢˜ï¼š  
$$\underset{\left(b_{x}\right)_{x},\left(k_{t}\right)_{t}}{\arg \min } \sum_{t, x}\left(\log \left(M_{t, x}^{\circ}\right)-b_{x} k_{t}\right)^{2}ã€‚$$

å®šä¹‰çŸ©é˜µ $A=\left(\log \left(M_{t, x}^{\circ}\right)\right)_{x, t}$ã€‚ä¸Šè¿°æœ€ä¼˜åŒ–é—®é¢˜å¯ä»¥é€šè¿‡å¯¹$A$è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è§£å†³$$A=U\Lambda V^\intercal,$$
å…¶ä¸­$U$ç§°ä¸ºå·¦å¥‡å¼‚çŸ©é˜µï¼Œå¯¹è§’çŸ©é˜µ$\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_T)$ä¸­çš„å¯¹è§’å…ƒç´ $\lambda_1\geq\lambda_2\geq\ldots\geq\lambda_T\geq0$ç§°ä¸ºå¥‡å¼‚å€¼ï¼Œ$V$ç§°ä¸ºå³å¥‡å¼‚çŸ©é˜µã€‚

- $A$ çš„ç¬¬ä¸€ä¸ªå·¦å¥‡å¼‚å‘é‡$U_{\cdot,1}$ä¸ç¬¬ä¸€ä¸ªå¥‡å¼‚å€¼$\lambda_1$ç›¸ä¹˜ï¼Œå¯ä»¥å¾—åˆ° $\left(b_{x}\right)_{x}$ çš„ä¸€ä¸ªä¼°è®¡ $\left(\widehat{b}_{x}\right)_{x}$ã€‚

- $A$ çš„ç¬¬ä¸€ä¸ªå³å¥‡å¼‚å‘é‡$V_{\cdot,1}$ç»™å‡ºäº† $\left(k_{t}\right)_{t}$ çš„ä¸€ä¸ªä¼°è®¡ $\left(\widehat{k}_{t}\right)_{t}$ã€‚

ä¸ºäº†æ±‚è§£ç»“æœçš„å”¯ä¸€æ€§ï¼Œå¢åŠ çº¦æŸï¼š  
$$\sum_{x} \hat{b}_{x}=1 \quad \text { and } \quad \sum_{t \in \mathcal{T}} \hat{k}_{t}=0$$
è‡³æ­¤å³å¯è§£å‡ºå”¯ä¸€çš„ $\left(\hat{a}_{x}, \hat{b}_{x}\right)_{x}, \left(\hat{k}_{t}\right)_{t}$ . è¿™å°±æ˜¯ç»å…¸çš„LCæ¨¡å‹æ„å»ºæ–¹æ³•.

## æ™®é€šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆrecurrent neural networkï¼‰

**è¾“å…¥å˜é‡ï¼ˆInputï¼‰** :  $\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$ with components $\boldsymbol{x}_{t} \in \mathbb{R}^{\tau_{0}}$ at times $t=1, \ldots, T$ (in time series structure).

**è¾“å‡ºå˜é‡ï¼ˆOutputï¼‰**: $y \in \mathcal{Y} \subset \mathbb{R}$ .  
  
é¦–å…ˆçœ‹ä¸€ä¸ªå…·æœ‰ $\tau_{1} \in \mathbb{N}$ ä¸ªéšå±‚ç¥ç»å…ƒï¼ˆhidden neuronsï¼‰å’Œå•ä¸€éšå±‚ï¼ˆhidden layerï¼‰çš„RNN. éšå±‚ç”±å¦‚ä¸‹æ˜ å°„ï¼ˆmappingï¼‰å®šä¹‰ï¼š
$$\boldsymbol{z}^{(1)}: \mathbb{R}^{\tau_{0} \times \tau_{1}} \rightarrow \mathbb{R}^{\tau_{1}}, \quad\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) \mapsto \boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right)$$
å…¶ä¸­ä¸‹æ ‡ $t$ è¡¨ç¤ºæ—¶é—´,ä¸Šæ ‡ (1) è¡¨ç¤ºç¬¬ä¸€éšå±‚ï¼ˆæœ¬ä¾‹ä¸­ä¹Ÿæ˜¯å”¯ä¸€éšå±‚ï¼‰.

éšå±‚ç»“æ„æ„é€ å¦‚ä¸‹ï¼š  
$$
\begin{aligned}
\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) =&\left(\phi\left(\left\langle\boldsymbol{w}_{1}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{1}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right),  \ldots, \phi\left(\left\langle\boldsymbol{w}_{\tau_{1}}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{\tau_{1}}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)\right)^{\top} \\
\stackrel{\text { def. }}{=} &\phi\left(\left\langle W^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle U^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)
\end{aligned}
$$
å…¶ä¸­ç¬¬  $1 \leq j \leq \tau_{1}$ ä¸ªç¥ç»å…ƒçš„ç»“æ„ä¸ºï¼š    
$$\phi\left(\left\langle\boldsymbol{w}_{j}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{j}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)=\phi\left(w_{j, 0}^{(1)}+\sum_{l=1}^{\tau_{0}} w_{j, l}^{(1)} x_{t, l}+\sum_{l=1}^{\tau_{1}} u_{j, l}^{(1)} z_{t-1, l}\right)$$

- $\phi: \mathbb{R} \rightarrow \mathbb{R}$ æ˜¯éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰
- ç½‘ç»œå‚æ•°ï¼ˆnetwork parametersï¼‰ä¸º $$W^{(1)}=\left(\boldsymbol{w}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau \times\left(\tau_{0}+1\right)} \text{(including an intercept)}$$  $$U^{(1)}=\left(\boldsymbol{u}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} \text{(excluding an intercept)}$$



é™¤äº†ä¸Šè¿°å•éšå±‚çš„ç»“æ„ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è½»æ¾åœ°è®¾è®¡å¤šéšå±‚çš„RNN.  
  
ä¾‹å¦‚ï¼ŒåŒéšå±‚çš„RNNç»“æ„å¯ä»¥ä¸º:  

- **1st variant** : ä»…å…è®¸åŒçº§éšå±‚ä¹‹é—´çš„å¾ªç¯
$$
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right) \\
\boldsymbol{z}_{t}^{(2)} &=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
$$

- **2nd variant** : å…è®¸è·¨çº§éšå±‚å¾ªç¯
$$
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
$$

- **3rd variant** : å…è®¸äºŒçº§éšå±‚ä¸è¾“å…¥å±‚ $\boldsymbol{x}_{t}$ è¿›è¡Œå¾ªç¯
$$
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &=\boldsymbol{z}^{(2)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
$$


## é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œï¼ˆLong short-term memoryï¼‰

ä»¥ä¸Šplain vanilla RNN æ— æ³•å¤„ç†é•¿è·ç¦»ä¾èµ–å’Œä¸”æœ‰æ¢¯åº¦æ¶ˆæ•£çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼ŒHochreiter-Schmidhuber (1997)æå‡ºäº†é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œ(Long Short Term Memory Network, LSTM)ã€‚  

### æ¿€æ´»å‡½æ•°ï¼ˆActivation functionsï¼‰

LSTM ç”¨åˆ°3ç§ä¸åŒçš„ **æ¿€æ´»å‡½æ•°ï¼ˆactivation functionsï¼‰**:

1. Sigmoidå‡½æ•°ï¼ˆSigmoid functionï¼‰  
$$\phi_{\sigma}(x)=\frac{1}{1+e^{-x}} \in(0,1)$$

2. åŒæ›²æ­£åˆ‡å‡½æ•°ï¼ˆHyberbolic tangent functionï¼‰
$$\phi_{\tanh }(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=2 \phi_{\sigma}(2 x)-1 \in(-1,1)$$
3. ä¸€èˆ¬çš„æ¿€æ´»å‡½æ•°ï¼ˆGeneral activation functionï¼‰
$$\phi: \mathbb{R} \rightarrow \mathbb{R}$$ 


### Gates and cell state 

ä»¤ $\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}$ è¡¨ç¤ºæ—¶é—´ $(t-1)$ æ—¶çš„**æ´»åŒ–çŠ¶æ€ï¼ˆneuron activationsï¼‰**.  æˆ‘ä»¬å®šä¹‰3ä¸­ä¸åŒçš„ **é—¨ï¼ˆgatesï¼‰**, ç”¨æ¥å†³å®šä¼ æ’­åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çš„ä¿¡æ¯é‡ï¼š

- **é—å¿˜é—¨ï¼ˆForget gateï¼‰** (loss of memory rate):
$$\boldsymbol{f}_{t}^{(1)}=\boldsymbol{f}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{f}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{f}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept) $, U_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$ (excluding an intercept $),$ and where the activation function is evaluated element wise.

- **è¾“å…¥é—¨ï¼ˆInput gateï¼‰** (memory update rate):
$$\boldsymbol{i}_{t}^{(1)}=\boldsymbol{i}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{i}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{i}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$.

- **è¾“å‡ºé—¨ï¼ˆOutput gateï¼‰** (release of memory information rate):
$$\boldsymbol{o}_{t}^{(1)}=\boldsymbol{o}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{o}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{o}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept) $, U_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$.

æ³¨æ„ï¼šä»¥ä¸Šä¸‰ç§é—¨çš„åå­—å¹¶ä¸ä»£è¡¨ç€å®ƒä»¬åœ¨å®é™…ä¸­çš„ä½œç”¨ï¼Œå®ƒä»¬çš„ä½œç”¨ç”±ç½‘ç»œå‚æ•°å†³å®šï¼Œè€Œç½‘ç»œå‚æ•°æ˜¯ä»æ•°æ®ä¸­å­¦åˆ°çš„ã€‚

ä»¤ $\left(\boldsymbol{c}_{t}^{(1)}\right)_{t}$ è¡¨ç¤º **ç»†èƒçŠ¶æ€ï¼ˆcell stateï¼‰** , ç”¨ä»¥å‚¨å­˜å·²è·å¾—çš„ç›¸å…³ä¿¡æ¯.  

ç»†èƒçŠ¶æ€çš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š    
$$\begin{aligned}
\boldsymbol{c}_{t}^{(1)}&=\boldsymbol{c}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)\\&=\boldsymbol{f}_{t}^{(1)} \circ \boldsymbol{c}_{t-1}^{(1)}+\boldsymbol{i}_{t}^{(1)} \circ \phi_{\tanh }\left(\left\langle W_{c}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{c}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}
\end{aligned}$$
for network parameters $W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},$ and $\circ$
denotes the Hadamard product (element wise product). 
  
æœ€åï¼Œæˆ‘ä»¬æ›´æ–°æ—¶åˆ» $t$ æ—¶çš„æ´»åŒ–çŠ¶æ€ $\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}$.  
$$\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)=\boldsymbol{o}_{t}^{(1)} \circ \phi\left(\boldsymbol{c}_{t}^{(1)}\right) \in \mathbb{R}^{\tau_{1}}$$

è‡³æ­¤ï¼Œ

- æ¶‰åŠçš„å…¨éƒ¨ç½‘ç»œå‚æ•°æœ‰:  $$W_{f}^{\top}, W_{i}^{\top}, W_{o}^{\top}, W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}ï¼Œ~~ U_{f}^{\top}, U_{i}^{\top}, U_{o}^{\top}, U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} .$$   

- ä¸€ä¸ªLSTMå±‚éœ€è¦ $4\left(\left(\tau_{0}+1\right) \tau_{1}+\tau_{1}^{2}\right)$ ä¸ªç½‘ç»œå‚æ•°ã€‚

- ä»¥ä¸Šå®šä¹‰çš„å¤æ‚æ˜ å°„åœ¨kerasé€šè¿‡å‡½æ•°`layer_lstm()`å³å¯å®ç°ã€‚

- è¿™äº›å‚æ•°å‡ç”±æ¢¯åº¦ä¸‹é™çš„å˜å¼ç®—æ³•ï¼ˆa variant of the gradient descent algorithmï¼‰å­¦ä¹ å¾—.

### Output Function

åŸºäº $\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$ , æˆ‘ä»¬æ¥é¢„æµ‹å®šä¹‰åœ¨ $\mathcal{Y} \subset \mathbb{R}$ çš„éšæœºå˜é‡ $Y_{T}$ .

$$\widehat{Y}_{T}=\widehat{Y}_{T}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{T}^{(1)}\right\rangle \in \mathcal{Y}$$
å…¶ä¸­ï¼Œ   

- $z_{T}^{(1)}$ æ˜¯æœ€æ–°çš„éšå±‚ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ï¼ˆhidden neuron activationï¼‰

- $\boldsymbol{w} \in \mathbb{R}^{\tau_{1}+1}$ æ˜¯è¾“å‡ºæƒé‡(again including an intercept component)

- $\varphi: \mathbb{R} \rightarrow \mathcal{Y}$ æ˜¯ä¸€ä¸ªæ°å½“çš„è¾“å‡ºæ¿€æ´»å‡½æ•°ï¼Œé€‰æ‹©æ—¶éœ€è¦è€ƒè™‘$y$çš„å–å€¼èŒƒå›´ã€‚

### Time-distributed Layer

ä»¥ä¸Šåªè€ƒè™‘äº†æ ¹æ®æœ€æ–°çš„çŠ¶æ€ $\boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$ æ‰€ç¡®å®šçš„å•ä¸€çš„è¾“å‡º $Y_{T}$.    
  
ä½†æ˜¯æˆ‘ä»¬å¯ä»¥è€ƒè™‘ **æ‰€æœ‰** éšå±‚ç¥ç»å…ƒçŠ¶æ€:  
$$\boldsymbol{z}_{1}^{(1)}\left(\boldsymbol{x}_{1}\right), \boldsymbol{z}_{2}^{(1)}\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right), \boldsymbol{z}_{3}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{3}\right), \ldots, \boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$$
æ¯ä¸€ä¸ªçŠ¶æ€ $\boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)$ éƒ½å¯ä»¥ä½œä¸ºè§£é‡Šå˜é‡ï¼Œç”¨ä»¥ä¼°è®¡ $t$ æ—¶æ‰€å¯¹åº”çš„ $Y_{t}$ :

$$\widehat{Y}_{t}=\widehat{Y}_{t}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\right\rangle=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)\right\rangle$$
å…¶ä¸­è¿‡æ»¤å™¨ï¼ˆfilterï¼‰ $\varphi\langle\boldsymbol{w}, \cdot\rangle$ å¯¹æ‰€æœ‰æ—¶é—´ $t$ å–ç›¸åŒå‡½æ•°.
  
**å°ç»“ï¼šLSTMçš„ä¼˜åŠ¿**

- æ—¶é—´åºåˆ—ç»“æ„å’Œå› æœå…³ç³»éƒ½å¯ä»¥å¾—åˆ°æ­£ç¡®çš„ååº” 

- ç”±äºå‚æ•°ä¸ä¾èµ–æ—¶é—´ï¼ŒLSTMå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‹“å±•åˆ°æœªæ¥æ—¶é—´æ®µ

## é—¨æ§å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆGated Recurrent Unitï¼‰

å¦ä¸€ä¸ªæ¯”è¾ƒçƒ­é—¨çš„RNNç»“æ„æ˜¯ï¼šé—¨æ§å¾ªç¯å•å…ƒï¼ˆgated recurrent unit, GRU), ç”±Cho et al. (2014) æå‡ºï¼Œå®ƒæ¯”LSTMæ›´åŠ ç®€æ´ï¼Œä½†åŒæ ·å¯ä»¥ç¼“è§£plain vanilla RNNä¸­æ¢¯åº¦æ¶ˆæ•£çš„é—®é¢˜ã€‚

### Gates

GRUåªä½¿ç”¨2ä¸ªä¸åŒçš„**é—¨ï¼ˆgatesï¼‰**. ä»¤ $\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}$ è¡¨ç¤º $(t-1)$ æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€.

- **Reset gate**: ç±»ä¼¼äºLSTMä¸­çš„é—å¿˜é—¨
$$\boldsymbol{r}_{t}^{(1)}=\boldsymbol{r}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{r}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{r}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$.

- **Update gate**: ç±»ä¼¼äºLSTMä¸­çš„è¾“å…¥é—¨
$$\boldsymbol{u}_{t}^{(1)}=\boldsymbol{u}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{u}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{u}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$

### Neuron Activations

ä»¥ä¸Šé—¨å˜é‡çš„ä½œç”¨æ˜¯ï¼Œå·²çŸ¥ $t-1$ æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ $\boldsymbol{z}_{t-1}^{(1)}$, è®¡ç®— $t$ æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ $\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}$ . æˆ‘ä»¬é€‰ç”¨å¦‚ä¸‹ç»“æ„ï¼š 
$$\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\boldsymbol{r}_{t}^{(1)} \circ \boldsymbol{z}_{t-1}^{(1)}+\left(\mathbf{1}-\boldsymbol{r}_{t}^{(1)}\right) \circ \phi\left(\left\langle W, \boldsymbol{x}_{t}\right\rangle+\boldsymbol{u}_{t} \circ\left\langle U, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}$$
for network parameters $W^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept) $, U^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},$ and where $\circ$ denotes the Hadamard product.
  
GRUç½‘ç»œæ¯”LSTMç½‘ç»œçš„ç»“æ„æ›´ç®€æ´ï¼Œè€Œä¸”ä¼šäº§ç”Ÿç›¸è¿‘çš„ç»“æœã€‚
ä½†æ˜¯ï¼ŒGRUåœ¨ç¨³å¥æ€§ä¸Šæœ‰è¾ƒå¤§ç¼ºé™·ï¼Œå› æ­¤ç°é˜¶æ®µLSTMçš„ä½¿ç”¨æ›´ä¸ºå¹¿æ³›. 

## æ¡ˆä¾‹åˆ†æï¼ˆCase studyï¼‰

æœ¬æ¡ˆä¾‹çš„æ•°æ®æ¥æºäºHuman Mortality Database (HMD)ä¸­çš„æ•°æ®ï¼Œé€‰æ‹©ç‘å£«äººå£æ•°æ®(HMDä¸­æ ‡è®°ä¸ºâ€œCHEâ€)ä½œä¸ºç¤ºä¾‹ã€‚

### æ•°æ®æè¿°

æ•°æ®åŒ…å«7ä¸ªå˜é‡ï¼Œå„å˜é‡è¯´æ˜å¦‚ä¸‹ï¼š

|å˜é‡|ç±»å‹|è¯´æ˜|
|:---:|:---:|---|
|Gender|factor|ä¸¤ç§æ€§åˆ«â€”â€”ç”·æ€§å’Œå¥³æ€§|
|Year|int|æ—¥å†å¹´ï¼Œ1950å¹´åˆ°2016å¹´|
|Age|int|å¹´é¾„èŒƒå›´0-99å²|
|Country|chr|"CHE"ï¼Œä»£è¡¨ç‘å£«|
|imputed_flag|logi|åŸå§‹æ­»äº¡ç‡ä¸º0ï¼Œç”¨HMDä¸­å…¶ä½™å›½å®¶åŒæ—¥å†å¹´åŒå¹´é¾„çš„å¹³å‡æ­»äº¡ç‡ä»£æ›¿ï¼Œåˆ™è¯¥å˜é‡ä¸ºTRUE|
|mx|num|æ­»äº¡ç‡|
|logmx|num|å¯¹æ•°æ­»äº¡ç‡|

```{r data}
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)
length(unique(all_mort$Age))
length(unique(all_mort$Year))
67*2*100
```

### æ­»äº¡ç‡çƒ­åŠ›å›¾

```{r heatmap}
gender <- "Male"
#gender <- "Female"
m0 <- c(min(all_mort$logmx), max(all_mort$logmx))
# rows are calendar year t, columns are ages x
logmx <- t(matrix(as.matrix(all_mort[which(all_mort$Gender==gender),"logmx"]), nrow=100, ncol=67))
# png("./plots/6/heat.png")
image(z=logmx, useRaster=TRUE,  zlim=m0, col=rev(rainbow(n=60, start=0, end=.72)), xaxt='n', yaxt='n', main=list(paste("Swiss ",gender, " raw log-mortality rates", sep=""), cex=1.5), cex.lab=1.5, ylab="age x", xlab="calendar year t")
axis(1, at=c(0:(2016-1950))/(2016-1950), c(1950:2016))                   
axis(2, at=c(0:49)/50, labels=c(0:49)*2)                   
lines(x=rep((1999-1950+0.5)/(2016-1950), 2), y=c(0:1), lwd=2)
dev.off()
```

å›¾\@ref(fig:heatplot)æ˜¾ç¤ºäº†ç”·å¥³æ€§å¯¹æ•°æ­»äº¡ç‡éšæ—¶é—´çš„æ”¹å–„:

- å·¦å³ä¸¤å¹…å›¾çš„è‰²æ ‡ç›¸åŒï¼Œè“è‰²è¡¨ç¤ºæ­»äº¡ç‡å°ï¼Œçº¢è‰²è¡¨ç¤ºæ­»äº¡ç‡å¤§

- è¯¥å›¾æ˜¾ç¤ºè¿‡å»å‡ åå¹´å…¸å‹çš„æ­»äº¡ç‡æ”¹å–„â€”â€”çƒ­å›¾ä¸­é¢œè‰²å‘ˆç•¥å¾®å‘ä¸Šçš„å¯¹è§’çº¿ç»“æ„

- å¹³å‡è€Œè¨€ï¼Œå¥³æ€§æ­»äº¡ç‡ä½äºç”·æ€§

- å›¾ä¸­ä½äº2000å¹´çš„å‚ç›´é»‘çº¿è¡¨ç¤ºå¯¹äºè®­ç»ƒæ•°æ®$\mathcal{T}$å’ŒéªŒè¯æ•°æ®$\mathcal{V}$çš„åˆ’åˆ†:åç»­æ¨¡å‹å°†ä½¿ç”¨æ—¥å†å¹´$t=1950, \ldots, 1999$ä½œä¸ºè®­ç»ƒæ•°æ®$\mathcal{T}$è¿›è¡Œå­¦ä¹ ï¼Œç”¨$2000, \ldots, 2016$ä½œä¸ºéªŒè¯æ•°æ®$\mathcal{V}$å¯¹æ­»äº¡ç‡åšæ ·æœ¬å¤–éªŒè¯ã€‚

```{r heatplot,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="ç‘å£«ç”·å¥³æ€§æ­»äº¡ç‡çƒ­åŠ›å›¾"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/heat.png")
```

### Lee-Carter æ¨¡å‹

```{r LC}
ObsYear <- 1999
gender <- "Female"
train <- all_mort[Year<=ObsYear][Gender == gender]
min(train$Year)
    
### fit via SVD
train[,ax:= mean(logmx), by = (Age)]
train[,mx_adj:= logmx-ax]  
rates_mat <- as.matrix(train %>% dcast.data.table(Age~Year, value.var = "mx_adj", sum))[,-1]
dim(rates_mat)
svd_fit <- svd(rates_mat)
    
ax <- train[,unique(ax)]
bx <- svd_fit$u[,1]*svd_fit$d[1]
kt <- svd_fit$v[,1]
      
c1 <- mean(kt)
c2 <- sum(bx)
ax <- ax+c1*bx
bx <- bx/c2
kt <- (kt-c1)*c2
    
### extrapolation and forecast
vali  <- all_mort[Year>ObsYear][Gender == gender]    
t_forecast <- vali[,unique(Year)] %>% length()
forecast_kt  =kt %>% forecast::rwf(t_forecast, drift = T)
kt_forecast = forecast_kt$mean
 
# illustration selected drift
plot_data <- c(kt, kt_forecast)
plot(plot_data, pch=20, col="red", cex=2, cex.lab=1.5, xaxt='n', ylab="values k_t", xlab="calendar year t", main=list(paste("estimated process k_t for ",gender, sep=""), cex=1.5)) 
points(kt, col="blue", pch=20, cex=2)
axis(1, at=c(1:length(plot_data)), labels=c(1:length(plot_data))+1949)                   
abline(v=(length(kt)+0.5), lwd=2)
# in-sample and out-of-sample analysis    
fitted = (ax+(bx)%*%t(kt)) %>% melt
train$pred_LC_svd = fitted$value %>% exp
fitted_vali = (ax+(bx)%*%t(kt_forecast)) %>% melt
vali$pred_LC_svd =   fitted_vali$value %>% exp
round(c((mean((train$mx-train$pred_LC_svd)^2)*10^4) , (mean((vali$mx-vali$pred_LC_svd)^2)*10^4)),4)
```

ç”¨å¸¦æ¼‚ç§»é¡¹çš„éšæœºæ¸¸èµ°é¢„æµ‹$t \in \mathcal{V}=\{2000, \ldots, 2016\}$çš„$\hat{k}_{t}$ï¼Œä¸‹å›¾è¯´æ˜äº†ç»“æœã€‚

```{r kt,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="ç‘å£«ç”·å¥³æ€§ktçš„ä¼°è®¡ä¸é¢„æµ‹å€¼"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/kt.png")
```

å›¾\@ref(fig:kt)æ˜¾ç¤ºå¯¹äºå¥³æ€§æ¥è¯´é¢„æµ‹ç»“æœæ˜¯ç›¸å¯¹åˆç†çš„ï¼Œä½†æ˜¯å¯¹äºç”·æ€§è€Œè¨€ï¼Œç”±æ­¤äº§ç”Ÿçš„æ¼‚ç§»å¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„æ¢ç´¢ï¼Œä¸‹é¢**ç”·å¥³æ€§æ ·æœ¬å†…å¤–çš„MSEæŸå¤±**ç»“æœä¹Ÿè¡¨æ˜äº†è¿™ä¸€ç‚¹ï¼šç”·æ€§æ ·æœ¬å¤–æŸå¤±è¾ƒå¤§

$$
\begin{array}{|c|cc|cc|}
\hline & \ {\text { in-sample loss }} & \ {\text { in-sample loss }}  & \ {\text { out-of-sample loss }} & \ {\text { out-of-sample loss }}\\
& \text { female } & \text { male } & \text { female } & \text { male } \\
\hline \text { LC model with SVD } & 3.7573 & 8.8110 & 0.6045 & 1.8152 \\
\hline
\end{array}
$$

### åˆè¯•RNN

1. æ•°æ®è¯´æ˜

- é€‰æ‹©æ€§åˆ«ä¸ºâ€œå¥³æ€§â€ï¼Œæå–$1990, \ldots, 2001$å¹´çš„å¯¹æ•°æ­»äº¡ç‡ï¼Œå¹´é¾„ä¸º$0 \leq x \leq 99$

- è¶…å‚æ•°è®¾ç½®ï¼šå›é¡¾å‘¨æœŸ$T=10$ï¼›$\tau_{0}=3$

- å®šä¹‰è§£é‡Šå˜é‡å’Œå“åº”å˜é‡ï¼š

  å¯¹äº$1 \leq x \leq 98$ï¼Œ$1 \leq t \leq T$ï¼Œæœ‰
  
  **è§£é‡Šå˜é‡**$\boldsymbol{x}_{t, x}=\left(\log \left(M_{1999-(T-t), x-1}\right), \log \left(M_{1999-(T-t), x}\right), \log \left(M_{1999-(T-t), x+1}\right)\right)^{\top} \in \mathbb{R}^{\tau_{0}}$
  
  **å“åº”å˜é‡**$\boldsymbol{Y}_{T, x}=\log(M_{2000,x}) =\log \left(M_{1999-(T-T)+1, x}\right) \in \mathbb{R}_{-}$
  
  åŒæ—¶è€ƒè™‘$(x-1,x,x+1)$ç›®çš„æ˜¯ç”¨é‚»è¿‘çš„å¹´é¾„æ¥å¹³æ»‘è¾“å…¥ã€‚
  
- é€‰æ‹©è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®ï¼š
  
  **è®­ç»ƒæ•°æ®**$\mathcal{T}=\{(\boldsymbol{x}_{1,x}, \ldots,\boldsymbol{x}_{T,x};\boldsymbol{Y}_{T, x});1 \leq x \leq 98\}$
  
  **éªŒè¯æ•°æ®**$\mathcal{V}=\{(\boldsymbol{x}_{2,x}, \ldots,\boldsymbol{x}_{T+1,x};\boldsymbol{Y}_{T+1, x});1 \leq x \leq 98\}$ï¼Œåœ¨è®­ç»ƒæ•°æ®åŸºç¡€ä¸Šæ—¶ç§»äº†ä¸€ä¸ªæ—¥å†å¹´
  
- æ•°æ®å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

```{r,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="RNNåˆè¯•ä¸­é€‰æ‹©çš„æ•°æ®"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/datatoy.png")
```

  é»‘çº¿è¡¨ç¤ºé€‰å®šçš„è§£é‡Šå˜é‡$\boldsymbol{x}_{t, x}$;è“è‰²çš„ç‚¹æ˜¯è®­ç»ƒæ•°æ®ä¸­çš„çš„å“åº”å˜é‡$\boldsymbol{Y}_{T, x}$ï¼›éªŒè¯æ•°æ®ä¸­å“åº”å˜é‡$\boldsymbol{Y}_{T+1, x}=\log(M_{2001,x})$ç”¨çº¢è‰²çš„ç‚¹è¡¨ç¤º
  
2. æ•°æ®é¢„å¤„ç†

- å¯¹è§£é‡Šå˜é‡åº”ç”¨MinMaxScalerè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†

- åˆ‡æ¢å“åº”å˜é‡çš„ç¬¦å·

3. æ¯”è¾ƒLSTMså’ŒGRUs

- åœ¨éªŒè¯é›†$\mathcal{V}$ä¸Šè·Ÿè¸ªè¿‡æ‹Ÿåˆ

- æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•é€‰ç”¨çš„æ˜¯`nadam`

- ä¸‹å›¾æ˜¾ç¤ºäº†5ä¸ªæ¨¡å‹çš„æ”¶æ•›è¡Œä¸º

```{r loss1,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="æ¨¡å‹çš„æ ·æœ¬å†…å¤–æŸå¤±"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/loss1.png")
```

- æ ¹æ®è¿‡æ‹Ÿåˆç¡®å®šçš„åœæ­¢æ—¶é—´çš„æ¨¡å‹æ ¡å‡†ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

$$
\begin{array}{|l|ccc|cc|}
\hline & \# \text { param. } & \text { epochs } & \text { run time } & \text { in-sample loss } & \text { out-of-sample loss } \\
\hline \text { LSTM1 } & 186 & 150 & 8 \mathrm{sec} & 0.0655 & 0.0936 \\
\text { LSTM2 } & 345 & 200 & 15 \mathrm{sec} & 0.0603 & 0.0918 \\
\hline \text { GRU1 } & 141 & 100 & 5 \mathrm{sec} & 0.0671 & 0.0860 \\
\text { GRU2 } & 260 & 200 & 14 \mathrm{sec} & 0.0651 & 0.0958 \\
\hline \text { deep FNN } & 184 & 200 & 5 \mathrm{sec} & 0.0485 & 0.1577 \\
\hline
\end{array}
$$

- è¡¨ä¸­æ‰€ç¤ºæ¨¡å‹çš„è¶…å‚æ•°è®¾ç½®

a. LSTM1å’ŒGRU1è¡¨ç¤ºåªæœ‰ä¸€ä¸ªéšè—å±‚çš„RNNï¼Œè¯¥éšè—å±‚çš„ç¥ç»å…ƒä¸ªæ•°$\tau_{1}=5$

b. LSTM2å’ŒGRU2è¡¨ç¤ºæœ‰ä¸¤ä¸ªéšè—å±‚çš„RNNï¼Œç¬¬ä¸€ä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°$\tau_{1}=5$ï¼›ç¬¬äºŒä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°$\tau_{2}=4$

c. deep FNNè¡¨ç¤ºæœ‰ä¸¤ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œç»“æ„ï¼Œå…¶ä¸­$(q1,q2)=(5,4)$

- ç»“è®º

a. LSTM2ä¸LSTM1æ¨¡å‹é¢„æµ‹è´¨é‡ç›¸å½“ï¼Œä½†LSTM2æœ‰æ›´å¤šå‚æ•°åŠæ›´é•¿çš„è¿è¡Œæ—¶é—´

b. LTSMä¸GRUè¶…å‚æ•°é€‰æ‹©ç›¸åŒæ—¶ï¼Œæ™®éçš„è§‚å¯Ÿç»“æœæ˜¯GRUæ¯”LSTMæ›´å¿«çš„è¿‡æ‹Ÿåˆï¼Œä½†GRUä¸ç¨³å®š

c. å‰é¦ˆç¥ç»ç½‘ç»œä¸RNNç›¸æ¯”æ²¡æœ‰ç«äº‰åŠ›

- åœ¨æœ¬æ–‡å»ºæ¨¡ä¸­**æœªå¼•å…¥å¹´é¾„å˜é‡çš„åŸå› **ï¼š

åå˜é‡æ ‡å‡†åŒ–åˆ°ï¼ˆ-1,1ï¼‰çš„è¿‡ç¨‹æ˜¯åœ¨æ‰€æœ‰å¹´é¾„ä¸ŠåŒæ—¶è¿›è¡Œçš„ï¼Œå› æ­¤åå˜é‡ä¿¡æ¯ä¿ç•™äº†æ­»äº¡ç‡æ°´å¹³ï¼Œè¿™å’Œå¼•å…¥å¹´é¾„å˜é‡å…·æœ‰ç›¸åŒçš„ä¿¡æ¯è´¨é‡ã€‚

4. è¶…å‚æ•°é€‰æ‹©

- åˆ†åˆ«æ”¹å˜$Tã€\tau_{0}ã€\tau_{1}$çš„å€¼ï¼Œå¾—åˆ°ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

$$
\begin{array}{|l|ccc|cc|}
\hline & \text { # param. } & \text { epochs } & \text { run time } & \text { in-sample } & \text { out-of-sample } \\
\hline \text { base case: } & & & & & \\
\text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=5\right) & 186 & 150 & 8 \mathrm{sec} & 0.0655 & 0.0936 \\
\hline \text { LSTM1 }\left(T=10, \tau_{0}=1, \tau_{1}=5\right) & 146 & 100 & 5 \mathrm{sec} & 0.0647 & 0.1195 \\
\text { LSTM1 }\left(T=10, \tau_{0}=5, \tau_{1}=5\right) & 226 & 150 & 15 \mathrm{sec} & 0.0583 & 0.0798 \\
\hline \text { LSTM1 }\left(T=5, \tau_{0}=3, \tau_{1}=5\right) & 186 & 100 & 4 \mathrm{sec} & 0.0753 & 0.1028 \\
\text { LSTM1 }\left(T=20, \tau_{0}=3, \tau_{1}=5\right) & 186 & 200 & 16 \mathrm{sec} & 0.0626 & 0.0968 \\
\hline \text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=3\right) & 88 & 200 & 10 \mathrm{sec} & 0.0694 & 0.0987 \\
\text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=10\right) & 571 & 100 & 5 \mathrm{sec} & 0.0626 & 0.0883 \\
\hline
\end{array}
$$

- ç»“è®º

a. åˆ†åˆ«ä»¤$\tau_{0}=1,3,5$,éœ€è¦æ›´é•¿çš„è¿è¡Œæ—¶é—´å¹¶æä¾›æ›´å¥½çš„æ ·æœ¬å¤–ç»“æœï¼›

b. åˆ†åˆ«ä»¤$T=5,10,20$ï¼Œç»“è®ºåŒä¸Šï¼›

c. åˆ†åˆ«ä»¤$\tau_{1}=3,5,10$ï¼Œå¯¼è‡´æ›´å¿«çš„æ”¶æ•›ï¼Œå› ä¸ºæ¢¯åº¦ä¸‹é™ç®—æ³•æœ‰æ›´å¤šçš„è‡ªç”±åº¦

d. æœ€å¤§çš„å½±å“æ˜¯é€šè¿‡è®¾å®šä¸€ä¸ªæ›´å¤§çš„$\tau_{0}$è€Œäº§ç”Ÿçš„ï¼Œå› æ­¤åé¢RNNç¤ºä¾‹ä¸­è®¾å®š$\tau_{0}=5$

```{r toy example}
# load corresponding data
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)
# LSTMs and GRUs
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R")
T0 <- 10
tau0 <- 3
tau1 <- 5
tau2 <- 4
summary(LSTM1(T0, tau0, tau1, 0, "nadam"))
summary(LSTM2(T0, tau0, tau1, tau2, 0, "nadam"))
summary(LSTM_TD(T0, tau0, tau1, 0, "nadam"))
summary(GRU1(T0, tau0, tau1, 0, "nadam"))
summary(GRU2(T0, tau0, tau1, tau2, 0, "nadam"))
summary(FNN(T0, tau0, tau1, tau2, 0, "nadam"))
# Bringing the data in the right structure for a toy example
gender <- "Female"
ObsYear <- 2000
mort_rates <- all_mort[which(all_mort$Gender==gender), c("Year", "Age", "logmx")] 
mort_rates <- dcast(mort_rates, Year ~ Age, value.var="logmx")
dim(mort_rates)
T0 <- 10     # lookback period
tau0 <- 3    # dimension of x_t (should be odd for our application)
delta0 <- (tau0-1)/2
toy_rates <- as.matrix(mort_rates[which(mort_rates$Year %in% c((ObsYear-T0):(ObsYear+1))),])
dim(toy_rates)
xt <- array(NA, c(2,ncol(toy_rates)-tau0, T0, tau0))
YT <- array(NA, c(2,ncol(toy_rates)-tau0))
for (i in 1:2){for (a0 in 1:(ncol(toy_rates)-tau0)){ 
    xt[i,a0,,] <- toy_rates[c(i:(T0+i-1)),c((a0+1):(a0+tau0))]
    YT[i,a0] <- toy_rates[T0+i,a0+1+delta0]
}}
dim(xt)
dim(YT)
plot(x=toy_rates[1:T0,1], y=toy_rates[1:T0,2], col="white", xlab="calendar years", ylab="raw log-mortality rates", cex.lab=1.5, cex=1.5, main=list("data toy example", cex=1.5), xlim=range(toy_rates[,1]), ylim=range(toy_rates[,-1]), type='l')
for (a0 in 2:ncol(toy_rates)){
  if (a0 %in% (c(1:100)*3)){
    lines(x=toy_rates[1:T0,1], y=toy_rates[1:T0,a0])    
    points(x=toy_rates[(T0+1):(T0+2),1], y=toy_rates[(T0+1):(T0+2),a0], col=c("blue", "red"), pch=20)
    lines(x=toy_rates[(T0):(T0+1),1], y=toy_rates[(T0):(T0+1),a0], col="blue", lty=2)
    lines(x=toy_rates[(T0+1):(T0+2),1], y=toy_rates[(T0+1):(T0+2),a0], col="red", lty=2)
    }}
# LSTMs and GRUs
x.train <- array(2*(xt[1,,,]-min(xt))/(max(xt)-min(xt))-1, c(ncol(toy_rates)-tau0, T0, tau0))
x.vali  <- array(2*(xt[2,,,]-min(xt))/(max(xt)-min(xt))-1, c(ncol(toy_rates)-tau0, T0, tau0))
y.train <- - YT[1,]
(y0 <- mean(y.train))
y.vali  <- - YT[2,]
dim(x.train)
length(y.train);length(y.vali)
# x.age.train<-as.matrix(0:0)
# x.training<-list(x.train,x.age.train)
# x.age.valid<-as.matrix(0:0)
# x.validation<-list(x.vali,x.age.valid)
### examples
tau1 <- 5    # dimension of the outputs z_t^(1) first RNN layer
tau2 <- 4    # dimension of the outputs z_t^(2) second RNN layer
CBs <- callback_model_checkpoint("./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model", monitor = "val_loss", verbose = 0,  save_best_only = TRUE, save_weights_only = TRUE,save_freq = NULL)
model <- LSTM2(T0, tau0, tau1, tau2, y0, "nadam")     
summary(model)
# takes 40 seconds on my laptop
{t1 <- proc.time()
  fit <- model %>% fit(x=x.train, y=y.train, validation_data=list(x.vali, y.vali), batch_size=10, epochs=500, verbose=1, callbacks=CBs)
 proc.time()-t1}
plot(fit[[2]]$val_loss,col="red", ylim=c(0,0.5), main=list("early stopping rule", cex=1.5),xlab="epochs", ylab="MSE loss", cex=1.5, cex.lab=1.5)
lines(fit[[2]]$loss,col="blue")
abline(h=0.1, lty=1, col="black")
legend(x="bottomleft", col=c("blue","red"), lty=c(1,-1), lwd=c(1,-1), pch=c(-1,1), legend=c("in-sample loss", "out-of-sample loss"))
load_model_weights_hdf5(model, "./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model")
Yhat.train1 <- as.vector(model %>% predict(x.train))
Yhat.vali1 <- as.vector(model %>% predict(x.vali))
c(round(mean((Yhat.train1-y.train)^2),4), round(mean((Yhat.vali1-y.vali)^2),4))
```


### RNN

1. æ•°æ®é¢„å¤„ç†

- è§‚å¯Ÿå€¼ï¼š1950-1999å¹´æ•°æ®ï¼›é¢„æµ‹å€¼ï¼š2000-2016å¹´å¯¹æ•°æ­»äº¡ç‡

- åˆ†åˆ«å¯¹ç”·æ€§å’Œå¥³æ€§å»ºç«‹æ¨¡å‹ï¼Œå…ˆé€‰å®šä¸€ä¸ªæ€§åˆ«

- å‚æ•°è®¾ç½®ï¼šå›é¡¾å‘¨æœŸ$T=10$ï¼›$\tau_{0}=5$

- å®šä¹‰è§£é‡Šå˜é‡ï¼ˆéœ€æ‰©å……å¹´é¾„ç•Œé™â€”â€”å¤åˆ¶è¾¹ç¼˜ç‰¹å¾å€¼ï¼‰ï¼š

  å¯¹äº$0 \leq x \leq 99$ï¼Œ$1950 \leq t \leq 1999$ï¼Œæœ‰
  
  **è§£é‡Šå˜é‡**$\boldsymbol{x}_{t, x}=\left(\log \left(M_{t,(x-2) \vee 0}\right), \log \left(M_{t,(x-1) \vee 0}\right), \log \left(M_{t, x}\right), \log \left(M_{t,(x+1) \wedge 99}\right), \log \left(M_{t,(x+2) \wedge 99}\right)\right)^{\top} \in \mathbb{R}^{5}$
  
  è¯¥å¼ä¸­ï¼š$x_{0}\vee x_{1}=\text {max}\{x_{0},x_{1}\}$;$x_{0}\wedge x_{1}=\text {min}\{x_{0},x_{1}\}$
  
- å®šä¹‰è®­ç»ƒæ•°æ®$\mathcal{T}$å’ŒéªŒè¯æ•°æ®$\mathcal{V}$ï¼š
  
  **è®­ç»ƒæ•°æ®**$\mathcal{T}=\{(\boldsymbol{x}_{t-T,x}, \ldots,\boldsymbol{x}_{t-1,x},\boldsymbol{Y}_{t, x});0 \leq x \leq 99\ , 1950+T \leq t \leq 1999\}$
  
  å…¶ä¸­ï¼Œ$\boldsymbol{Y}_{t, x}=\text {log}(M_{t,x})$
  
  **éªŒè¯æ•°æ®**:
  
  $s>1999$çš„ç‰¹å¾å€¼è¦ç”¨ç›¸åº”çš„é¢„æµ‹å€¼æ›¿ä»£
  
$$
\widehat{\boldsymbol{x}}_{s, x}=\left(\log \left(\widehat{M}_{s,(x-2) \vee 0}\right), \log \left(\widehat{M}_{s,(x-1) \vee 0}\right), \log \left(\widehat{M}_{s, x}\right), \log \left(\widehat{M}_{s,(x+1) \wedge 99}\right), \log \left(\widehat{M}_{s,(x+2) \wedge 99}\right)\right)^{\top} \in \mathbb{R}^{5}
$$
 
  å› æ­¤éªŒè¯æ•°æ®$\mathcal{V}=\{(\boldsymbol{x}_{t-T,x}, \ldots,\boldsymbol{x}_{1999,x},\widehat{\boldsymbol{x}}_{2000,x}, \ldots,\widehat{\boldsymbol{x}}_{t-1,x},\boldsymbol{Y}_{t, x});0 \leq x \leq 99\ , 2000 \leq t \leq 2016\}$
  
- åŸºäº**è®­ç»ƒæ•°æ®**æ‰€æœ‰ç‰¹å¾å€¼çš„æœ€å¤§æœ€å°å€¼å¯¹è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®åº”ç”¨`MinMaxScaler`

- åˆ‡æ¢å“åº”å˜é‡ç¬¦å·

2. å»ºç«‹å•ä¸ªæ€§åˆ«çš„RNN

- å°†è®­ç»ƒæ•°æ®$\mathcal T$éšæœºåˆ’åˆ†å­¦ä¹ é›†$\mathcal T_{0}$(åŒ…å«80%æ•°æ®)ä»¥åŠæµ‹è¯•é›†$\mathcal T_{1}$(åŒ…å«20%æ•°æ®)$\mathcal T_{0}$ç”¨äºè¿½è¸ªæ ·æœ¬å†…è¿‡æ‹Ÿåˆï¼›

- å»ºç«‹å…·æœ‰ä¸‰ä¸ªéšè—å±‚çš„LSTM3å’ŒGRU3ï¼›

- è¶…å‚æ•°è®¾ç½®ï¼š$T=10$ï¼›$\tau_{0}=5$ï¼›$\tau_{1}=20$ï¼›$\tau_{2}=15$ï¼›$\tau_{3}=10$ï¼›

- ä¸‹å›¾æ˜¾ç¤ºäº†åˆ†åˆ«å¯¹ç”·æ€§å’Œå¥³æ€§å»ºç«‹ä¸¤ä¸ªæ¨¡å‹çš„æ”¶æ•›è¡Œä¸º

```{r loss2,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="æ¨¡å‹çš„æ ·æœ¬å†…å¤–æŸå¤±"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/loss2.png")
```

è¯¥å›¾æ˜¾ç¤ºï¼šGRUç»“æ„ä¼šå¯¼è‡´æ›´å¿«çš„æ”¶æ•›ï¼Œä½†åç»­ä¼šè¯å®GRUç»“æ„ä¸ç¨³å®šï¼Œå› æ­¤LSTMä¼šæ›´å—æ¬¢è¿
   
- ä¸‹è¡¨ç»™å‡ºä¸‰ä¸ªæ¨¡å‹(LSTM3/GRU3/LC)åˆ†æ€§åˆ«çš„æ ·æœ¬å†…å¤–æŸå¤±

$$
\begin{array}{|l|cc|cc|cc|}
\hline &  {\text { in-sample }} & {\text { in-sample }}  & {\text { out-of-sample }} & {\text { out-of-sample }}&  {\text { run times }}&  {\text { run times }} \\
& \text { female } & \text { male } & \text { female } & \text { male } & \text { female } & \text { male } \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) & 2.5222 & 6.9458 & 0.3566 & 1.3507 & 225 \mathrm{s} & 203 \mathrm{s} \\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) & 2.8370 & 7.0907 & 0.4788 & 1.2435 & 185 \mathrm{s} & 198 \mathrm{s} \\
\hline \hline \text { LC model with SVD } & 3.7573 & 8.8110 & 0.6045 & 1.8152 & - & - \\
\hline
\end{array}
$$

èƒ½å¤Ÿçœ‹åˆ°ï¼Œæ‰€æœ‰è¢«é€‰æ‹©RNNæ¨¡å‹éƒ½ä¼˜äºLCæ¨¡å‹çš„é¢„æµ‹

3. æ¢ç´¢RNNçš„é¢„æµ‹ä¸­éšå«çš„æ¼‚ç§»é¡¹

a. ä¸­å¿ƒåŒ–RNNé¢„æµ‹çš„å¯¹æ•°æ­»äº¡ç‡

$$
\log \left(\widehat{M}_{t, x}^{\circ}\right)=\log(\widehat M_{t,x})-\widehat a_{x}
$$

b. åˆ©ç”¨ä¸‹å¼æ±‚å¾—$2000 \leq t \leq 2016$å¯¹åº”çš„$k_{t}$

$$
\underset{k_{t}}{\arg \min } \sum_{x}\left(\log \left(\widehat{M}_{t, x}^{\circ}\right)-\widehat{b}_{x} k_{t}\right)^{2}
$$

å¼ä¸­ï¼Œ$(\widehat{b}_{x})_{x}$æ˜¯ä»LCæ¨¡å‹ä¼°è®¡å¾—åˆ°çš„ã€‚

c. ä¼°è®¡ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤º

```{r kt2,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="ä¸‰ç§æ¨¡å‹ä¸‹ktçš„ä¼°è®¡ä¸é¢„æµ‹å€¼"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/kt2.png")
```

è¯¥å›¾æ˜¾ç¤ºï¼šå¯¹äºå¥³æ€§ï¼ŒLSTM3çš„é¢„æµ‹ä¸LCçš„é¢„æµ‹åŸºæœ¬ä¸€è‡´ï¼›è€Œå¯¹äºç”·æ€§ï¼ŒLSTM3çš„é¢„æµ‹çš„æ–œç‡ç•¥å¤§äºLCçš„é¢„æµ‹å¹¶æ”¶æ•›ä¸LCçš„é¢„æµ‹ï¼›ä½†æ˜¯GRU3çš„ç»“æœå¹¶ä¸ä»¤äººä¿¡æœï¼Œå¯èƒ½æ˜¯å› ä¸ºä»LCæ¨¡å‹ä¸­å¾—åˆ°çš„ä¸éšæ—¶é—´å˜åŒ–çš„å‚æ•°bxçš„ä¼°è®¡ä¸GRU3æ¨¡å‹äº§ç”Ÿçš„é¢„æµ‹ä¸ç¬¦ã€‚

```{r rnn}
# load corresponding data
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R")
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R")
# choice of parameters
T0 <- 10
tau0 <- 5
gender <- "Female"
ObsYear <- 1999
# training data pre-processing 
data1 <- data.preprocessing.RNNs(all_mort, gender, T0, tau0, ObsYear)
dim(data1[[1]])
dim(data1[[2]])
# validation data pre-processing
all_mort2 <- all_mort[which((all_mort$Year > (ObsYear-10))&(Gender==gender)),]
all_mortV <- all_mort2
vali.Y <- all_mortV[which(all_mortV$Year > ObsYear),]
 
# MinMaxScaler data pre-processing
x.min <- min(data1[[1]])
x.max <- max(data1[[1]])
x.train <- array(2*(data1[[1]]-x.min)/(x.min-x.max)-1, dim(data1[[1]]))
y.train <- - data1[[2]]
y0 <- mean(y.train)
# LSTM architectures
# network architecture deep 3 network
tau1 <- 20
tau2 <- 15
tau3 <- 10
optimizer <- 'adam'
# choose either LSTM or GRU network
RNN.type <- "LSTM"
#RNN.type <- "GRU"
{if (RNN.type=="LSTM"){model <- LSTM3(T0, tau0, tau1, tau2, tau3, y0, optimizer)}else{model <- GRU3(T0, tau0, tau1, tau2, tau3, y0, optimizer)}
 name.model <- paste(RNN.type,"3_", tau0, "_", tau1, "_", tau2, "_", tau3, sep="")
 file.name <- paste("./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_", name.model,"_", gender, sep="")
 summary(model)}
# define callback
CBs <- callback_model_checkpoint(file.name, monitor = "val_loss", verbose = 0,  save_best_only = TRUE, save_weights_only = TRUE, save_freq = NULL)
# gradient descent fitting: takes roughly 200 seconds on my laptop
{t1 <- proc.time()
  fit <- model %>% fit(x=x.train, y=y.train, validation_split=0.2,
                                        batch_size=100, epochs=500, verbose=1, callbacks=CBs)                                        
proc.time()-t1}
# plot loss figures
plot.losses(name.model, gender, fit[[2]]$val_loss, fit[[2]]$loss)
# calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)
load_model_weights_hdf5(model, file.name)
round(10^4*mean((exp(-as.vector(model %>% predict(x.train)))-exp(-y.train))^2),4)
# calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)
pred.result <- recursive.prediction(ObsYear, all_mort2, gender, T0, tau0, x.min, x.max, model)
vali <- pred.result[[1]][which(all_mort2$Year > ObsYear),]
round(10^4*mean((vali$mx-vali.Y$mx)^2),4)
```

### å¼•å…¥æ€§åˆ«åå˜é‡

1. æ•°æ®é¢„å¤„ç†

- æ·»åŠ æ€§åˆ«æŒ‡ç¤ºå˜é‡ï¼š0è¡¨ç¤ºå¥³æ€§ï¼›1è¡¨ç¤ºç”·æ€§

- åœ¨è®­ç»ƒæ•°æ®æ—¶äº¤æ›¿ä½¿ç”¨æ€§åˆ«

- åº”ç”¨MinMaxScaleræ—¶çš„æœ€å¤§æœ€å°å€¼æ˜¯åŒæ—¶è€ƒè™‘ä¸¤ç§æ€§åˆ«æ‰€æœ‰è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å¾—åˆ°çš„ã€‚

- æ¨¡å‹ç»“æ„åŠè¶…å‚æ•°è®¾ç½®ä¸å•ä¸ªæ€§åˆ«RNNç›¸åŒ

2. å»ºç«‹æ¨¡å‹

- åŸºäºä½¿å¾—æµ‹è¯•æŸå¤±æœ€å°çš„LSTM3å’ŒGRUæ¨¡å‹ï¼Œé¢„æµ‹1999å¹´ä¹‹åçš„æ­»äº¡ç‡ï¼Œå¹¶åœ¨ç‰¹å¾å˜é‡ä¸­åŠ å…¥æ€§åˆ«æŒ‡æ ‡ï¼Œä¸‹è¡¨åˆ—å‡ºäº†ç›¸åº”çš„æŸå¤±

$$
\begin{array}{|l|c|cc|c|}
\hline &  {\text { in-sample }} & {\text { out-of-sample }} & {\text { out-of-sample }}&  {\text { run times }}\\
& \text {both genders} & \text { female } & \text { male } & \text {both genders} \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) & 4.7643 & 0.3402 & 1.1346 & 404 \mathrm{s}\\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) & 4.6311 & 0.4646 & 1.2571 &  379 \mathrm{s} \\
\hline \hline \text { LC model with SVD } & 6.2841 & 0.6045 &1.8152 &  - \\
\hline
\end{array}
$$

ä¸LCæ¨¡å‹ç›¸æ¯”ï¼Œå¾—åˆ°äº†ä¸€ä¸ªæœ‰å¾ˆå¤§æ”¹è¿›çš„æ¨¡å‹ï¼Œè‡³å°‘å¯¹æœªæ¥16å¹´çš„é¢„æµ‹æ˜¯è¿™æ ·çš„ï¼›æ­¤å¤–ï¼Œå¼•å…¥æ€§åˆ«åå˜é‡çš„LSTMæ¨¡å‹ä¹Ÿä¼˜äºå•ä¸ªæ€§åˆ«çš„æ¨¡å‹ã€‚

3. éšå«çš„æ¼‚ç§»é¡¹

```{r kt3,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="å¼•å…¥æ€§åˆ«åå˜é‡å»ºæ¨¡çš„ktçš„ä¼°è®¡ä¸é¢„æµ‹å€¼"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/kt3.png")
```

```{r both gender}
# load corresponding data
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R")
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R")
# choice of parameters
T0 <- 10
tau0 <- 5
ObsYear <- 1999
# training data pre-processing 
data1 <- data.preprocessing.RNNs(all_mort, "Female", T0, tau0, ObsYear)
data2 <- data.preprocessing.RNNs(all_mort, "Male", T0, tau0, ObsYear)
xx <- dim(data1[[1]])[1]
x.train <- array(NA, dim=c(2*xx, dim(data1[[1]])[c(2,3)]))
y.train <- array(NA, dim=c(2*xx))
gender.indicator <- rep(c(0,1), xx)
for (l in 1:xx){
   x.train[(l-1)*2+1,,] <- data1[[1]][l,,]
   x.train[(l-1)*2+2,,] <- data2[[1]][l,,]
   y.train[(l-1)*2+1] <- -data1[[2]][l]
   y.train[(l-1)*2+2] <- -data2[[2]][l]
          }
# MinMaxScaler data pre-processing
x.min <- min(x.train)
x.max <- max(x.train)
x.train <- list(array(2*(x.train-x.min)/(x.min-x.max)-1, dim(x.train)), gender.indicator)
y0 <- mean(y.train)
# validation data pre-processing
all_mort2.Female <- all_mort[which((all_mort$Year > (ObsYear-10))&(Gender=="Female")),]
all_mortV.Female <- all_mort2.Female
vali.Y.Female <- all_mortV.Female[which(all_mortV.Female$Year > ObsYear),]
all_mort2.Male <- all_mort[which((all_mort$Year > (ObsYear-10))&(Gender=="Male")),]
all_mortV.Male <- all_mort2.Male
vali.Y.Male <- all_mortV.Male[which(all_mortV.Male$Year > ObsYear),]
# LSTM architectures
# network architecture deep 3 network
tau1 <- 20
tau2 <- 15
tau3 <- 10
optimizer <- 'adam'
# choose either LSTM or GRU network
RNN.type <- "LSTM"
#RNN.type <- "GRU"
{if (RNN.type=="LSTM"){model <- LSTM3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)}else{model <- GRU3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)}
 name.model <- paste(RNN.type,"3_", tau0, "_", tau1, "_", tau2, "_", tau3, sep="")
 #file.name <- paste("./Model_Full_Param/best_model_", name.model, sep="")
 file.name <- paste("./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_", name.model, sep="")
 summary(model)}
# define callback
CBs <- callback_model_checkpoint(file.name, monitor = "val_loss", verbose = 0,  save_best_only = TRUE, save_weights_only = TRUE,save_freq = NULL)
# gradient descent fitting: takes roughly 400 seconds on my laptop
{t1 <- proc.time()
  fit <- model %>% fit(x=x.train, y=y.train, validation_split=0.2,
                                        batch_size=100, epochs=500, verbose=1, callbacks=CBs)                                        
proc.time()-t1}
# plot loss figures
plot.losses(name.model, "Both", fit[[2]]$val_loss, fit[[2]]$loss)
# calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)
load_model_weights_hdf5(model, file.name)
round(10^4*mean((exp(-as.vector(model %>% predict(x.train)))-exp(-y.train))^2),4)
# calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)
# Female
pred.result <- recursive.prediction.Gender(ObsYear, all_mort2.Female, "Female", T0, tau0, x.min, x.max, model)
vali <- pred.result[[1]][which(all_mort2.Female$Year > ObsYear),]
round(10^4*mean((vali$mx-vali.Y.Female$mx)^2),4)
# Male
pred.result <- recursive.prediction.Gender(ObsYear, all_mort2.Male, "Male", T0, tau0, x.min, x.max, model)
vali <- pred.result[[1]][which(all_mort2.Male$Year > ObsYear),]
round(10^4*mean((vali$mx-vali.Y.Male$mx)^2),4)
```

### ç¨³å¥æ€§

ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•çš„æ—©æœŸåœæ­¢è§£å†³æ–¹æ¡ˆçš„ä¸€ä¸ªé—®é¢˜æ˜¯ç”±æ­¤äº§ç”Ÿçš„æ ¡å‡†ä¾èµ–äºç®—æ³•ç§å­ç‚¹ï¼ˆèµ·å§‹å€¼ï¼‰çš„é€‰æ‹©

ä¸‹å›¾å±•ç¤ºä½¿ç”¨ç›¸åŒRNNç»“æ„ã€ç›¸åŒè¶…å‚æ•°å’Œç›¸åŒæ ¡å‡†ç­–ç•¥ï¼Œé’ˆå¯¹100ä¸ªä¸åŒç§å­ç‚¹çš„é€‰æ‹©æ‰€ç”»çš„æŸå¤±çš„ç®±çº¿å›¾

```{r Box,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="100ä¸ªä¸åŒç§å­ä¸‹çš„æŸå¤±ç®±çº¿å›¾"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/Box.png")
```

- çº¢è‰²è¡¨ç¤ºè”åˆæ€§åˆ«çš„LSTMç»“æ„ä¸­çš„é¢„æµ‹ç»“æœï¼›è“è‰²è¡¨ç¤ºè”åˆæ€§åˆ«çš„GRUç»“æ„ä¸­çš„é¢„æµ‹ï¼›æ©™è‰²æ°´å¹³çº¿è¡¨ç¤ºçš„æ˜¯LCçš„é¢„æµ‹

- ç»“è®ºï¼š

a. å·¦ä¾§ç»™å‡ºäº†æ ·æœ¬å†…æŸå¤±ï¼Œä¸LCæ¨¡å‹ç›¸æ¯”ï¼Œä¸¤ç§RNNç»“æ„çš„æ ·æœ¬å†…æŸå¤±éƒ½æœ‰æ˜¾è‘—å‡å°‘ï¼Œå¹³å‡è€Œè¨€ï¼ŒLSTMçš„æŸå¤±æ¯”GRUçš„å°ï¼Œæ³¢åŠ¨æ€§ä¹Ÿæ›´å°ï¼›

b. ä¸­é—´å’Œå³è¾¹åˆ†åˆ«è¡¨ç¤ºå¥³æ€§å’Œç”·æ€§çš„æ ·æœ¬å¤–æŸå¤±ï¼šä¸è®ºç”·æ€§è¿˜æ˜¯å¥³æ€§ï¼ŒLSTMåœ¨å‡ ä¹æ‰€æœ‰çš„100æ¬¡è¿­ä»£ä¸­éƒ½æ¯”LCæ¨¡å‹å¥½ï¼›GRUç»“æ„å°½åœ¨å¤§çº¦ä¸€åŠæ¬¡æ•°çš„è¿­ä»£ä¸­æ¯”LCæ¨¡å‹è¡¨ç°å¥½ã€‚

- **æ”¹è¿›æ–¹æ³•**ï¼šå°†ä¸åŒç§å­ç‚¹ä¸‹å¾—åˆ°çš„é¢„æµ‹å€¼è¿›è¡Œå¹³å‡ï¼Œç»“æœå¦‚ä¸‹:

$$
\begin{array}{|l|c|cc|c|}
\hline &  {\text { in-sample }} & {\text { out-of-sample }} & {\text { out-of-sample }}&  {\text { run times }}\\
& \text {both genders} & \text { female } & \text { male } & \text {both genders} \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) & 4.7643 & 0.3402 & 1.1346 & 404 \mathrm{s}\\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) & 4.6311 & 0.4646 & 1.2571 &  379 \mathrm{s} \\
\hline\hline \text { LSTM3 averaged over 100 different seeds} & - & 0.2451 & 1.2093 & 100 \cdot 404\mathrm{s}\\
\text { GRU3 averaged over 100 different seeds } & - & 0.2341& 1.2746 &  100 \cdot 379 \mathrm{s} \\
\hline \hline \text { LC model with SVD } & 6.2841 & 0.6045 &1.8152 &  - \\
\hline
\end{array}
$$

èƒ½å¤Ÿçœ‹åˆ°ï¼Œå¹³å‡ä¹‹åå¾—åˆ°äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œé¢„æµ‹ç»“æœä¹Ÿå¾ˆå¥½ï¼Œç®±çº¿å›¾ä¸­ç»¿è‰²æ°´å¹³çº¿è¡¨ç¤ºçš„å°±æ˜¯å¹³å‡ä¹‹åçš„é¢„æµ‹æŸå¤±ï¼Œæ˜¾ç¤ºä»…æœ‰æå°‘æ•°çš„åœ¨ç»¿è‰²æ°´å¹³çº¿ä¹‹ä¸‹çš„ç§å­ç‚¹çš„é€‰æ‹©åœ¨å•ç‹¬è¿›è¡Œæ ¡å‡†æ—¶æ•ˆæœä¼šæ›´å¥½ã€‚

### é¢„æµ‹ç»“æœå›¾

```{r,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="å¯¹æ•°æ­»äº¡ç‡çš„è§‚å¯Ÿä¸é¢„æµ‹å€¼"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/mortality.png")
```

ç»“è®ºï¼š

a. 20-40å²ä¹‹é—´LSTMæ–¹æ³•èƒ½å¤Ÿæ›´å¥½çš„æ•æ‰åˆ°æ­»äº¡ç‡çš„æ”¹å–„ï¼Œå·¦è¾¹è§‚å¯Ÿå€¼æ¸…æ¥šçš„è¡¨æ˜LSTMè¿™æ ·çš„æ”¹å–„æ˜¯åˆç†çš„ï¼›

b. å¹´é¾„è¾ƒå°äººç¾¤çš„æ­»äº¡ç‡å®é™…æ”¹å–„æƒ…å†µæ¯”æŒ‰ç…§æœ¬æ–‡æ–¹æ³•é¢„æµ‹çš„å¤§ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®çš„é’å¹´æ­»äº¡ç‡æ”¹å–„æƒ…å†µæ— æ³•ä»£è¡¨2000å¹´ä¹‹åçš„é’å¹´æ­»äº¡ç‡æ”¹å–„æƒ…å†µã€‚