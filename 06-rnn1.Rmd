# å¾ªç¯ç¥ç»ç½‘ç»œä¸æ­»äº¡ç‡é¢„æµ‹ {#rnn}

*æ–¹ç‰æ˜•ã€é²ç‘¶ã€é«˜å…‰è¿œ*

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = F)
```

ğŸ˜· æ–°å† è‚ºç‚æ­»äº¡ç‡æ•°æ®ï¼š<https://mpidr.shinyapps.io/stmortality/>

## Lee-Carter Model

Lee Carteræ¨¡å‹ä¸­ï¼Œæ­»äº¡åŠ›ï¼ˆforce of mortalityï¼‰çš„å®šä¹‰å¦‚ä¸‹ï¼š  
$$\log \left(m_{t, x}\right)=a_{x}+b_{x} k_{t}$$
å…¶ä¸­ï¼Œ  

- $m_{t, x}>0$ æ˜¯ $x$ å²çš„äººåœ¨æ—¥å†å¹´ $t$ çš„æ­»äº¡ç‡ï¼ˆmortality rateï¼‰,   

- $a_{x}$ æ˜¯ $x$ å²çš„äººçš„å¹³å‡å¯¹æ•°æ­»äº¡ç‡,    

- $b_{x}$ æ˜¯æ­»äº¡ç‡å˜åŒ–çš„å¹´é¾„å› ç´ , 

- $\left(k_{t}\right)_{t}$ æ˜¯æ­»äº¡ç‡å˜åŒ–çš„æ—¥å†å¹´å› ç´ .  
  
ç”¨ $M_{t, x}$ è¡¨ç¤ºæŸä¸€æ€§åˆ«æ­»äº¡ç‡çš„è§‚å¯Ÿå€¼ï¼ˆraw mortality ratesï¼‰.  
æˆ‘ä»¬å¯¹å¯¹æ•°æ­»äº¡ç‡ $\log \left(M_{t, x}\right)$  ä¸­å¿ƒåŒ–å¤„ç†ï¼š  
$$\log \left(M_{t, x}^{\circ}\right)=\log \left(M_{t, x}\right)-\widehat{a}_{x}=\log \left(M_{t, x}\right)-\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)$$

å…¶ä¸­ï¼Œ  

- $\mathcal{T}$ ä¸ºè®­ç»ƒé›†ä¸­æ—¥å†å¹´çš„é›†åˆ,  

- $\widehat{a}_{x}=\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)$ æ˜¯å¹³å‡å¯¹æ•°æ­»äº¡ç‡ $a_{x}$ çš„ä¼°è®¡.  
  
å¯¹äº$b_x,k_t$ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ±‚å¦‚ä¸‹æœ€ä¼˜åŒ–é—®é¢˜ï¼š  
$$\underset{\left(b_{x}\right)_{x},\left(k_{t}\right)_{t}}{\arg \min } \sum_{t, x}\left(\log \left(M_{t, x}^{\circ}\right)-b_{x} k_{t}\right)^{2}ã€‚$$

å®šä¹‰çŸ©é˜µ $A=\left(\log \left(M_{t, x}^{\circ}\right)\right)_{x, t}$ã€‚ä¸Šè¿°æœ€ä¼˜åŒ–é—®é¢˜å¯ä»¥é€šè¿‡å¯¹$A$è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è§£å†³$$A=U\Lambda V^\intercal,$$
å…¶ä¸­$U$ç§°ä¸ºå·¦å¥‡å¼‚çŸ©é˜µï¼Œå¯¹è§’çŸ©é˜µ$\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_T)$ä¸­çš„å¯¹è§’å…ƒç´ $\lambda_1\geq\lambda_2\geq\ldots\geq\lambda_T\geq0$ç§°ä¸ºå¥‡å¼‚å€¼ï¼Œ$V$ç§°ä¸ºå³å¥‡å¼‚çŸ©é˜µã€‚

- $A$ çš„ç¬¬ä¸€ä¸ªå·¦å¥‡å¼‚å‘é‡$U_{\cdot,1}$ä¸ç¬¬ä¸€ä¸ªå¥‡å¼‚å€¼$\lambda_1$ç›¸ä¹˜ï¼Œå¯ä»¥å¾—åˆ° $\left(b_{x}\right)_{x}$ çš„ä¸€ä¸ªä¼°è®¡ $\left(\widehat{b}_{x}\right)_{x}$ã€‚

- $A$ çš„ç¬¬ä¸€ä¸ªå³å¥‡å¼‚å‘é‡$V_{\cdot,1}$ç»™å‡ºäº† $\left(k_{t}\right)_{t}$ çš„ä¸€ä¸ªä¼°è®¡ $\left(\widehat{k}_{t}\right)_{t}$ã€‚

ä¸ºäº†æ±‚è§£ç»“æœçš„å”¯ä¸€æ€§ï¼Œå¢åŠ çº¦æŸï¼š  
$$\sum_{x} \hat{b}_{x}=1 \quad \text { and } \quad \sum_{t \in \mathcal{T}} \hat{k}_{t}=0$$
è‡³æ­¤å³å¯è§£å‡ºå”¯ä¸€çš„ $\left(\hat{a}_{x}, \hat{b}_{x}\right)_{x}, \left(\hat{k}_{t}\right)_{t}$ . è¿™å°±æ˜¯ç»å…¸çš„LCæ¨¡å‹æ„å»ºæ–¹æ³•.

## æ™®é€šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆrecurrent neural networkï¼‰

**è¾“å…¥å˜é‡ï¼ˆInputï¼‰** :  $\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$ with components $\boldsymbol{x}_{t} \in \mathbb{R}^{\tau_{0}}$ at times $t=1, \ldots, T$ (in time series structure).

**è¾“å‡ºå˜é‡ï¼ˆOutputï¼‰**: $y \in \mathcal{Y} \subset \mathbb{R}$ .  
  
é¦–å…ˆçœ‹ä¸€ä¸ªå…·æœ‰ $\tau_{1} \in \mathbb{N}$ ä¸ªéšå±‚ç¥ç»å…ƒï¼ˆhidden neuronsï¼‰å’Œå•ä¸€éšå±‚ï¼ˆhidden layerï¼‰çš„RNN. éšå±‚ç”±å¦‚ä¸‹æ˜ å°„ï¼ˆmappingï¼‰å®šä¹‰ï¼š
$$\boldsymbol{z}^{(1)}: \mathbb{R}^{\tau_{0} \times \tau_{1}} \rightarrow \mathbb{R}^{\tau_{1}}, \quad\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) \mapsto \boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right)$$
å…¶ä¸­ä¸‹æ ‡ $t$ è¡¨ç¤ºæ—¶é—´,ä¸Šæ ‡ (1) è¡¨ç¤ºç¬¬ä¸€éšå±‚ï¼ˆæœ¬ä¾‹ä¸­ä¹Ÿæ˜¯å”¯ä¸€éšå±‚ï¼‰.

éšå±‚ç»“æ„æ„é€ å¦‚ä¸‹ï¼š  
$$
\begin{aligned}
\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) =&\left(\phi\left(\left\langle\boldsymbol{w}_{1}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{1}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right),  \ldots, \phi\left(\left\langle\boldsymbol{w}_{\tau_{1}}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{\tau_{1}}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)\right)^{\top} \\
\stackrel{\text { def. }}{=} &\phi\left(\left\langle W^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle U^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)
\end{aligned}
$$
å…¶ä¸­ç¬¬  $1 \leq j \leq \tau_{1}$ ä¸ªç¥ç»å…ƒçš„ç»“æ„ä¸ºï¼š    
$$\phi\left(\left\langle\boldsymbol{w}_{j}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{j}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)=\phi\left(w_{j, 0}^{(1)}+\sum_{l=1}^{\tau_{0}} w_{j, l}^{(1)} x_{t, l}+\sum_{l=1}^{\tau_{1}} u_{j, l}^{(1)} z_{t-1, l}\right)$$

- $\phi: \mathbb{R} \rightarrow \mathbb{R}$ æ˜¯éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰
- ç½‘ç»œå‚æ•°ï¼ˆnetwork parametersï¼‰ä¸º $$W^{(1)}=\left(\boldsymbol{w}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau \times\left(\tau_{0}+1\right)} \text{(including an intercept)}$$  $$U^{(1)}=\left(\boldsymbol{u}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} \text{(excluding an intercept)}$$



é™¤äº†ä¸Šè¿°å•éšå±‚çš„ç»“æ„ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è½»æ¾åœ°è®¾è®¡å¤šéšå±‚çš„RNN.  
  
ä¾‹å¦‚ï¼ŒåŒéšå±‚çš„RNNç»“æ„å¯ä»¥ä¸º:  

- **1st variant** : ä»…å…è®¸åŒçº§éšå±‚ä¹‹é—´çš„å¾ªç¯
$$
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right) \\
\boldsymbol{z}_{t}^{(2)} &=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
$$

- **2nd variant** : å…è®¸è·¨çº§éšå±‚å¾ªç¯
$$
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
$$

- **3rd variant** : å…è®¸äºŒçº§éšå±‚ä¸è¾“å…¥å±‚ $\boldsymbol{x}_{t}$ è¿›è¡Œå¾ªç¯
$$
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &=\boldsymbol{z}^{(2)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
$$


## é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œï¼ˆLong short-term memoryï¼‰

ä»¥ä¸Šplain vanilla RNN æ— æ³•å¤„ç†é•¿è·ç¦»ä¾èµ–å’Œä¸”æœ‰æ¢¯åº¦æ¶ˆæ•£çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼ŒHochreiter-Schmidhuber (1997)æå‡ºäº†é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œ(Long Short Term Memory Network, LSTM)ã€‚  

### æ¿€æ´»å‡½æ•°ï¼ˆActivation functionsï¼‰

LSTM ç”¨åˆ°3ç§ä¸åŒçš„ **æ¿€æ´»å‡½æ•°ï¼ˆactivation functionsï¼‰**:

1. Sigmoidå‡½æ•°ï¼ˆSigmoid functionï¼‰  
$$\phi_{\sigma}(x)=\frac{1}{1+e^{-x}} \in(0,1)$$

2. åŒæ›²æ­£åˆ‡å‡½æ•°ï¼ˆHyberbolic tangent functionï¼‰
$$\phi_{\tanh }(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=2 \phi_{\sigma}(2 x)-1 \in(-1,1)$$
3. ä¸€èˆ¬çš„æ¿€æ´»å‡½æ•°ï¼ˆGeneral activation functionï¼‰
$$\phi: \mathbb{R} \rightarrow \mathbb{R}$$ 


### Gates and cell state 

ä»¤ $\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}$ è¡¨ç¤ºæ—¶é—´ $(t-1)$ æ—¶çš„**æ´»åŒ–çŠ¶æ€ï¼ˆneuron activationsï¼‰**.  æˆ‘ä»¬å®šä¹‰3ä¸­ä¸åŒçš„ **é—¨ï¼ˆgatesï¼‰**, ç”¨æ¥å†³å®šä¼ æ’­åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çš„ä¿¡æ¯é‡ï¼š

- **é—å¿˜é—¨ï¼ˆForget gateï¼‰** (loss of memory rate):
$$\boldsymbol{f}_{t}^{(1)}=\boldsymbol{f}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{f}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{f}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept) $, U_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$ (excluding an intercept $),$ and where the activation function is evaluated element wise.

- **è¾“å…¥é—¨ï¼ˆInput gateï¼‰** (memory update rate):
$$\boldsymbol{i}_{t}^{(1)}=\boldsymbol{i}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{i}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{i}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$.

- **è¾“å‡ºé—¨ï¼ˆOutput gateï¼‰** (release of memory information rate):
$$\boldsymbol{o}_{t}^{(1)}=\boldsymbol{o}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{o}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{o}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept) $, U_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$.

æ³¨æ„ï¼šä»¥ä¸Šä¸‰ç§é—¨çš„åå­—å¹¶ä¸ä»£è¡¨ç€å®ƒä»¬åœ¨å®é™…ä¸­çš„ä½œç”¨ï¼Œå®ƒä»¬çš„ä½œç”¨ç”±ç½‘ç»œå‚æ•°å†³å®šï¼Œè€Œç½‘ç»œå‚æ•°æ˜¯ä»æ•°æ®ä¸­å­¦åˆ°çš„ã€‚

ä»¤ $\left(\boldsymbol{c}_{t}^{(1)}\right)_{t}$ è¡¨ç¤º **ç»†èƒçŠ¶æ€ï¼ˆcell stateï¼‰** , ç”¨ä»¥å‚¨å­˜å·²è·å¾—çš„ç›¸å…³ä¿¡æ¯.  

ç»†èƒçŠ¶æ€çš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š    
$$\begin{aligned}
\boldsymbol{c}_{t}^{(1)}&=\boldsymbol{c}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)\\&=\boldsymbol{f}_{t}^{(1)} \circ \boldsymbol{c}_{t-1}^{(1)}+\boldsymbol{i}_{t}^{(1)} \circ \phi_{\tanh }\left(\left\langle W_{c}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{c}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}
\end{aligned}$$
for network parameters $W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},$ and $\circ$
denotes the Hadamard product (element wise product). 
  
æœ€åï¼Œæˆ‘ä»¬æ›´æ–°æ—¶åˆ» $t$ æ—¶çš„æ´»åŒ–çŠ¶æ€ $\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}$.  
$$\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)=\boldsymbol{o}_{t}^{(1)} \circ \phi\left(\boldsymbol{c}_{t}^{(1)}\right) \in \mathbb{R}^{\tau_{1}}$$

è‡³æ­¤ï¼Œ

- æ¶‰åŠçš„å…¨éƒ¨ç½‘ç»œå‚æ•°æœ‰:  $$W_{f}^{\top}, W_{i}^{\top}, W_{o}^{\top}, W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}ï¼Œ~~ U_{f}^{\top}, U_{i}^{\top}, U_{o}^{\top}, U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} .$$   

- ä¸€ä¸ªLSTMå±‚éœ€è¦ $4\left(\left(\tau_{0}+1\right) \tau_{1}+\tau_{1}^{2}\right)$ ä¸ªç½‘ç»œå‚æ•°ã€‚

- ä»¥ä¸Šå®šä¹‰çš„å¤æ‚æ˜ å°„åœ¨kerasé€šè¿‡å‡½æ•°`layer_lstm()`å³å¯å®ç°ã€‚

- è¿™äº›å‚æ•°å‡ç”±æ¢¯åº¦ä¸‹é™çš„å˜å¼ç®—æ³•ï¼ˆa variant of the gradient descent algorithmï¼‰å­¦ä¹ å¾—.

### Output Function

åŸºäº $\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$ , æˆ‘ä»¬æ¥é¢„æµ‹å®šä¹‰åœ¨ $\mathcal{Y} \subset \mathbb{R}$ çš„éšæœºå˜é‡ $Y_{T}$ .

$$\widehat{Y}_{T}=\widehat{Y}_{T}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{T}^{(1)}\right\rangle \in \mathcal{Y}$$
å…¶ä¸­ï¼Œ   

- $z_{T}^{(1)}$ æ˜¯æœ€æ–°çš„éšå±‚ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ï¼ˆhidden neuron activationï¼‰

- $\boldsymbol{w} \in \mathbb{R}^{\tau_{1}+1}$ æ˜¯è¾“å‡ºæƒé‡(again including an intercept component)

- $\varphi: \mathbb{R} \rightarrow \mathcal{Y}$ æ˜¯ä¸€ä¸ªæ°å½“çš„è¾“å‡ºæ¿€æ´»å‡½æ•°ï¼Œé€‰æ‹©æ—¶éœ€è¦è€ƒè™‘$y$çš„å–å€¼èŒƒå›´ã€‚

### Time-distributed Layer

ä»¥ä¸Šåªè€ƒè™‘äº†æ ¹æ®æœ€æ–°çš„çŠ¶æ€ $\boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$ æ‰€ç¡®å®šçš„å•ä¸€çš„è¾“å‡º $Y_{T}$.    
  
ä½†æ˜¯æˆ‘ä»¬å¯ä»¥è€ƒè™‘ **æ‰€æœ‰** éšå±‚ç¥ç»å…ƒçŠ¶æ€:  
$$\boldsymbol{z}_{1}^{(1)}\left(\boldsymbol{x}_{1}\right), \boldsymbol{z}_{2}^{(1)}\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right), \boldsymbol{z}_{3}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{3}\right), \ldots, \boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)$$
æ¯ä¸€ä¸ªçŠ¶æ€ $\boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)$ éƒ½å¯ä»¥ä½œä¸ºè§£é‡Šå˜é‡ï¼Œç”¨ä»¥ä¼°è®¡ $t$ æ—¶æ‰€å¯¹åº”çš„ $Y_{t}$ :

$$\widehat{Y}_{t}=\widehat{Y}_{t}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\right\rangle=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)\right\rangle$$
å…¶ä¸­è¿‡æ»¤å™¨ï¼ˆfilterï¼‰ $\varphi\langle\boldsymbol{w}, \cdot\rangle$ å¯¹æ‰€æœ‰æ—¶é—´ $t$ å–ç›¸åŒå‡½æ•°.
  
**å°ç»“ï¼šLSTMçš„ä¼˜åŠ¿**

- æ—¶é—´åºåˆ—ç»“æ„å’Œå› æœå…³ç³»éƒ½å¯ä»¥å¾—åˆ°æ­£ç¡®çš„ååº” 

- ç”±äºå‚æ•°ä¸ä¾èµ–æ—¶é—´ï¼ŒLSTMå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‹“å±•åˆ°æœªæ¥æ—¶é—´æ®µ

## é—¨æ§å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆGated Recurrent Unitï¼‰

å¦ä¸€ä¸ªæ¯”è¾ƒçƒ­é—¨çš„RNNç»“æ„æ˜¯ï¼šé—¨æ§å¾ªç¯å•å…ƒï¼ˆgated recurrent unit, GRU), ç”±Cho et al. (2014) æå‡ºï¼Œå®ƒæ¯”LSTMæ›´åŠ ç®€æ´ï¼Œä½†åŒæ ·å¯ä»¥ç¼“è§£plain vanilla RNNä¸­æ¢¯åº¦æ¶ˆæ•£çš„é—®é¢˜ã€‚

### Gates

GRUåªä½¿ç”¨2ä¸ªä¸åŒçš„**é—¨ï¼ˆgatesï¼‰**. ä»¤ $\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}$ è¡¨ç¤º $(t-1)$ æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€.

- **Reset gate**: ç±»ä¼¼äºLSTMä¸­çš„é—å¿˜é—¨
$$\boldsymbol{r}_{t}^{(1)}=\boldsymbol{r}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{r}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{r}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$.

- **Update gate**: ç±»ä¼¼äºLSTMä¸­çš„è¾“å…¥é—¨
$$\boldsymbol{u}_{t}^{(1)}=\boldsymbol{u}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{u}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{u}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}$$
for network parameters $W_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept), $U_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}$

### Neuron Activations

ä»¥ä¸Šé—¨å˜é‡çš„ä½œç”¨æ˜¯ï¼Œå·²çŸ¥ $t-1$ æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ $\boldsymbol{z}_{t-1}^{(1)}$, è®¡ç®— $t$ æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ $\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}$ . æˆ‘ä»¬é€‰ç”¨å¦‚ä¸‹ç»“æ„ï¼š 
$$\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\boldsymbol{r}_{t}^{(1)} \circ \boldsymbol{z}_{t-1}^{(1)}+\left(\mathbf{1}-\boldsymbol{r}_{t}^{(1)}\right) \circ \phi\left(\left\langle W, \boldsymbol{x}_{t}\right\rangle+\boldsymbol{u}_{t} \circ\left\langle U, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}$$
for network parameters $W^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}$ (including an intercept) $, U^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},$ and where $\circ$ denotes the Hadamard product.
  
GRUç½‘ç»œæ¯”LSTMç½‘ç»œçš„ç»“æ„æ›´ç®€æ´ï¼Œè€Œä¸”ä¼šäº§ç”Ÿç›¸è¿‘çš„ç»“æœã€‚
ä½†æ˜¯ï¼ŒGRUåœ¨ç¨³å¥æ€§ä¸Šæœ‰è¾ƒå¤§ç¼ºé™·ï¼Œå› æ­¤ç°é˜¶æ®µLSTMçš„ä½¿ç”¨æ›´ä¸ºå¹¿æ³›. 

## æ¡ˆä¾‹åˆ†æï¼ˆCase studyï¼‰

```{r data}
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)
length(unique(all_mort$Age))
length(unique(all_mort$Year))
67*2*100
```

### æ­»äº¡ç‡çƒ­åŠ›å›¾

```{r heatmap}
gender <- "Male"
#gender <- "Female"
m0 <- c(min(all_mort$logmx), max(all_mort$logmx))
# rows are calendar year t, columns are ages x
logmx <- t(matrix(as.matrix(all_mort[which(all_mort$Gender==gender),"logmx"]), nrow=100, ncol=67))
# png("./plots/6/heat.png")
image(z=logmx, useRaster=TRUE,  zlim=m0, col=rev(rainbow(n=60, start=0, end=.72)), xaxt='n', yaxt='n', main=list(paste("Swiss ",gender, " raw log-mortality rates", sep=""), cex=1.5), cex.lab=1.5, ylab="age x", xlab="calendar year t")
axis(1, at=c(0:(2016-1950))/(2016-1950), c(1950:2016))                   
axis(2, at=c(0:49)/50, labels=c(0:49)*2)                   
lines(x=rep((1999-1950+0.5)/(2016-1950), 2), y=c(0:1), lwd=2)
dev.off()
```

å›¾\@ref(fig:heatplot)æ˜¾ç¤ºäº†ç”·æ€§æ­»äº¡ç‡éšæ—¶é—´çš„æ”¹å–„ã€‚

```{r heatplot,echo=F, eval=T, out.width="60%",fig.align = 'center',fig.cap="ç‘å£«ç”·æ€§æ­»äº¡ç‡çƒ­åŠ›å›¾"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./plots/6/heat.png")
```

### Lee-Carter æ¨¡å‹

```{r LC}
ObsYear <- 1999
gender <- "Female"
train <- all_mort[Year<=ObsYear][Gender == gender]
min(train$Year)
    
### fit via SVD
train[,ax:= mean(logmx), by = (Age)]
train[,mx_adj:= logmx-ax]  
rates_mat <- as.matrix(train %>% dcast.data.table(Age~Year, value.var = "mx_adj", sum))[,-1]
dim(rates_mat)
svd_fit <- svd(rates_mat)
    
ax <- train[,unique(ax)]
bx <- svd_fit$u[,1]*svd_fit$d[1]
kt <- svd_fit$v[,1]
      
c1 <- mean(kt)
c2 <- sum(bx)
ax <- ax+c1*bx
bx <- bx/c2
kt <- (kt-c1)*c2
    
### extrapolation and forecast
vali  <- all_mort[Year>ObsYear][Gender == gender]    
t_forecast <- vali[,unique(Year)] %>% length()
forecast_kt  =kt %>% forecast::rwf(t_forecast, drift = T)
kt_forecast = forecast_kt$mean
 
# illustration selected drift
plot_data <- c(kt, kt_forecast)
plot(plot_data, pch=20, col="red", cex=2, cex.lab=1.5, xaxt='n', ylab="values k_t", xlab="calendar year t", main=list(paste("estimated process k_t for ",gender, sep=""), cex=1.5)) 
points(kt, col="blue", pch=20, cex=2)
axis(1, at=c(1:length(plot_data)), labels=c(1:length(plot_data))+1949)                   
abline(v=(length(kt)+0.5), lwd=2)

# in-sample and out-of-sample analysis    
fitted = (ax+(bx)%*%t(kt)) %>% melt
train$pred_LC_svd = fitted$value %>% exp
fitted_vali = (ax+(bx)%*%t(kt_forecast)) %>% melt
vali$pred_LC_svd =   fitted_vali$value %>% exp
round(c((mean((train$mx-train$pred_LC_svd)^2)*10^4) , (mean((vali$mx-vali$pred_LC_svd)^2)*10^4)),4)
```

### åˆè¯•RNN

```{r toy example}
# load corresponding data
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)

# LSTMs and GRUs
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R")
T0 <- 10
tau0 <- 3
tau1 <- 5
tau2 <- 4
summary(LSTM1(T0, tau0, tau1, 0, "nadam"))
summary(LSTM2(T0, tau0, tau1, tau2, 0, "nadam"))
summary(LSTM_TD(T0, tau0, tau1, 0, "nadam"))
summary(GRU1(T0, tau0, tau1, 0, "nadam"))
summary(GRU2(T0, tau0, tau1, tau2, 0, "nadam"))
summary(FNN(T0, tau0, tau1, tau2, 0, "nadam"))

# Bringing the data in the right structure for a toy example
gender <- "Female"
ObsYear <- 2000
mort_rates <- all_mort[which(all_mort$Gender==gender), c("Year", "Age", "logmx")] 
mort_rates <- dcast(mort_rates, Year ~ Age, value.var="logmx")
dim(mort_rates)

T0 <- 10     # lookback period
tau0 <- 3    # dimension of x_t (should be odd for our application)
delta0 <- (tau0-1)/2

toy_rates <- as.matrix(mort_rates[which(mort_rates$Year %in% c((ObsYear-T0):(ObsYear+1))),])
dim(toy_rates)

xt <- array(NA, c(2,ncol(toy_rates)-tau0, T0, tau0))
YT <- array(NA, c(2,ncol(toy_rates)-tau0))

for (i in 1:2){for (a0 in 1:(ncol(toy_rates)-tau0)){ 
    xt[i,a0,,] <- toy_rates[c(i:(T0+i-1)),c((a0+1):(a0+tau0))]
    YT[i,a0] <- toy_rates[T0+i,a0+1+delta0]
}}
dim(xt)
dim(YT)

plot(x=toy_rates[1:T0,1], y=toy_rates[1:T0,2], col="white", xlab="calendar years", ylab="raw log-mortality rates", cex.lab=1.5, cex=1.5, main=list("data toy example", cex=1.5), xlim=range(toy_rates[,1]), ylim=range(toy_rates[,-1]), type='l')
for (a0 in 2:ncol(toy_rates)){
  if (a0 %in% (c(1:100)*3)){
    lines(x=toy_rates[1:T0,1], y=toy_rates[1:T0,a0])    
    points(x=toy_rates[(T0+1):(T0+2),1], y=toy_rates[(T0+1):(T0+2),a0], col=c("blue", "red"), pch=20)
    lines(x=toy_rates[(T0):(T0+1),1], y=toy_rates[(T0):(T0+1),a0], col="blue", lty=2)
    lines(x=toy_rates[(T0+1):(T0+2),1], y=toy_rates[(T0+1):(T0+2),a0], col="red", lty=2)
    }}

# LSTMs and GRUs
x.train <- array(2*(xt[1,,,]-min(xt))/(max(xt)-min(xt))-1, c(ncol(toy_rates)-tau0, T0, tau0))
x.vali  <- array(2*(xt[2,,,]-min(xt))/(max(xt)-min(xt))-1, c(ncol(toy_rates)-tau0, T0, tau0))
y.train <- - YT[1,]
(y0 <- mean(y.train))
y.vali  <- - YT[2,]
dim(x.train)
length(y.train);length(y.vali)
# x.age.train<-as.matrix(0:0)
# x.training<-list(x.train,x.age.train)
# x.age.valid<-as.matrix(0:0)
# x.validation<-list(x.vali,x.age.valid)
### examples

tau1 <- 5    # dimension of the outputs z_t^(1) first RNN layer
tau2 <- 4    # dimension of the outputs z_t^(2) second RNN layer

CBs <- callback_model_checkpoint("./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model", monitor = "val_loss", verbose = 0,  save_best_only = TRUE, save_weights_only = TRUE,save_freq = NULL)
model <- LSTM2(T0, tau0, tau1, tau2, y0, "nadam")     
summary(model)

# takes 40 seconds on my laptop
{t1 <- proc.time()
  fit <- model %>% fit(x=x.train, y=y.train, validation_data=list(x.vali, y.vali), batch_size=10, epochs=500, verbose=1, callbacks=CBs)
 proc.time()-t1}

plot(fit[[2]]$val_loss,col="red", ylim=c(0,0.5), main=list("early stopping rule", cex=1.5),xlab="epochs", ylab="MSE loss", cex=1.5, cex.lab=1.5)
lines(fit[[2]]$loss,col="blue")
abline(h=0.1, lty=1, col="black")
legend(x="bottomleft", col=c("blue","red"), lty=c(1,-1), lwd=c(1,-1), pch=c(-1,1), legend=c("in-sample loss", "out-of-sample loss"))

load_model_weights_hdf5(model, "./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model")
Yhat.train1 <- as.vector(model %>% predict(x.train))
Yhat.vali1 <- as.vector(model %>% predict(x.vali))
c(round(mean((Yhat.train1-y.train)^2),4), round(mean((Yhat.vali1-y.vali)^2),4))
```


### RNN

```{r rnn}
# load corresponding data
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R")
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R")

# choice of parameters
T0 <- 10
tau0 <- 5
gender <- "Female"
ObsYear <- 1999

# training data pre-processing 
data1 <- data.preprocessing.RNNs(all_mort, gender, T0, tau0, ObsYear)
dim(data1[[1]])
dim(data1[[2]])


# validation data pre-processing
all_mort2 <- all_mort[which((all_mort$Year > (ObsYear-10))&(Gender==gender)),]
all_mortV <- all_mort2
vali.Y <- all_mortV[which(all_mortV$Year > ObsYear),]
 
# MinMaxScaler data pre-processing
x.min <- min(data1[[1]])
x.max <- max(data1[[1]])
x.train <- array(2*(data1[[1]]-x.min)/(x.min-x.max)-1, dim(data1[[1]]))
y.train <- - data1[[2]]
y0 <- mean(y.train)

# LSTM architectures
# network architecture deep 3 network
tau1 <- 20
tau2 <- 15
tau3 <- 10
optimizer <- 'adam'

# choose either LSTM or GRU network
RNN.type <- "LSTM"
#RNN.type <- "GRU"

{if (RNN.type=="LSTM"){model <- LSTM3(T0, tau0, tau1, tau2, tau3, y0, optimizer)}else{model <- GRU3(T0, tau0, tau1, tau2, tau3, y0, optimizer)}
 name.model <- paste(RNN.type,"3_", tau0, "_", tau1, "_", tau2, "_", tau3, sep="")
 file.name <- paste("./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_", name.model,"_", gender, sep="")
 summary(model)}

# define callback
CBs <- callback_model_checkpoint(file.name, monitor = "val_loss", verbose = 0,  save_best_only = TRUE, save_weights_only = TRUE, save_freq = NULL)

# gradient descent fitting: takes roughly 200 seconds on my laptop
{t1 <- proc.time()
  fit <- model %>% fit(x=x.train, y=y.train, validation_split=0.2,
                                        batch_size=100, epochs=500, verbose=1, callbacks=CBs)                                        
proc.time()-t1}

# plot loss figures
plot.losses(name.model, gender, fit[[2]]$val_loss, fit[[2]]$loss)

# calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)
load_model_weights_hdf5(model, file.name)
round(10^4*mean((exp(-as.vector(model %>% predict(x.train)))-exp(-y.train))^2),4)

# calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)
pred.result <- recursive.prediction(ObsYear, all_mort2, gender, T0, tau0, x.min, x.max, model)
vali <- pred.result[[1]][which(all_mort2$Year > ObsYear),]
round(10^4*mean((vali$mx-vali.Y$mx)^2),4)
```

### å¼•å…¥æ€§åˆ«åå˜é‡

```{r both gender}
# load corresponding data
path.data <- "6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv"           # path and name of data file
region <- "CHE"                    # country to be loaded (code is for one selected country)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R")
str(all_mort)
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R")
source(file="6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R")


# choice of parameters
T0 <- 10
tau0 <- 5
ObsYear <- 1999

# training data pre-processing 
data1 <- data.preprocessing.RNNs(all_mort, "Female", T0, tau0, ObsYear)
data2 <- data.preprocessing.RNNs(all_mort, "Male", T0, tau0, ObsYear)

xx <- dim(data1[[1]])[1]
x.train <- array(NA, dim=c(2*xx, dim(data1[[1]])[c(2,3)]))
y.train <- array(NA, dim=c(2*xx))
gender.indicator <- rep(c(0,1), xx)
for (l in 1:xx){
   x.train[(l-1)*2+1,,] <- data1[[1]][l,,]
   x.train[(l-1)*2+2,,] <- data2[[1]][l,,]
   y.train[(l-1)*2+1] <- -data1[[2]][l]
   y.train[(l-1)*2+2] <- -data2[[2]][l]
          }
# MinMaxScaler data pre-processing
x.min <- min(x.train)
x.max <- max(x.train)
x.train <- list(array(2*(x.train-x.min)/(x.min-x.max)-1, dim(x.train)), gender.indicator)
y0 <- mean(y.train)

# validation data pre-processing
all_mort2.Female <- all_mort[which((all_mort$Year > (ObsYear-10))&(Gender=="Female")),]
all_mortV.Female <- all_mort2.Female
vali.Y.Female <- all_mortV.Female[which(all_mortV.Female$Year > ObsYear),]
all_mort2.Male <- all_mort[which((all_mort$Year > (ObsYear-10))&(Gender=="Male")),]
all_mortV.Male <- all_mort2.Male
vali.Y.Male <- all_mortV.Male[which(all_mortV.Male$Year > ObsYear),]

# LSTM architectures
# network architecture deep 3 network
tau1 <- 20
tau2 <- 15
tau3 <- 10
optimizer <- 'adam'

# choose either LSTM or GRU network
RNN.type <- "LSTM"
#RNN.type <- "GRU"

{if (RNN.type=="LSTM"){model <- LSTM3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)}else{model <- GRU3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)}
 name.model <- paste(RNN.type,"3_", tau0, "_", tau1, "_", tau2, "_", tau3, sep="")
 #file.name <- paste("./Model_Full_Param/best_model_", name.model, sep="")
 file.name <- paste("./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_", name.model, sep="")
 summary(model)}

# define callback
CBs <- callback_model_checkpoint(file.name, monitor = "val_loss", verbose = 0,  save_best_only = TRUE, save_weights_only = TRUE,save_freq = NULL)

# gradient descent fitting: takes roughly 400 seconds on my laptop
{t1 <- proc.time()
  fit <- model %>% fit(x=x.train, y=y.train, validation_split=0.2,
                                        batch_size=100, epochs=500, verbose=1, callbacks=CBs)                                        
proc.time()-t1}

# plot loss figures
plot.losses(name.model, "Both", fit[[2]]$val_loss, fit[[2]]$loss)

# calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)
load_model_weights_hdf5(model, file.name)

round(10^4*mean((exp(-as.vector(model %>% predict(x.train)))-exp(-y.train))^2),4)

# calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)
# Female
pred.result <- recursive.prediction.Gender(ObsYear, all_mort2.Female, "Female", T0, tau0, x.min, x.max, model)
vali <- pred.result[[1]][which(all_mort2.Female$Year > ObsYear),]
round(10^4*mean((vali$mx-vali.Y.Female$mx)^2),4)
# Male
pred.result <- recursive.prediction.Gender(ObsYear, all_mort2.Male, "Male", T0, tau0, x.min, x.max, model)
vali <- pred.result[[1]][which(all_mort2.Male$Year > ObsYear),]
round(10^4*mean((vali$mx-vali.Y.Male$mx)^2),4)
```


