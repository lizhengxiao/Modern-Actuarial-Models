<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>现代精算统计模型</title>
  <meta name="description" content="The output format is bookdown::gitbook." />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="现代精算统计模型" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The output format is bookdown::gitbook." />
  <meta name="github-repo" content="sxpyggy/Modern-Actuarial-Models" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="现代精算统计模型" />
  
  <meta name="twitter:description" content="The output format is bookdown::gitbook." />
  

<meta name="author" content="Modern Actuarial Models" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">现代精算统计模型</h1>
<p class="author"><em>Modern Actuarial Models</em></p>
<p class="date"><em>2021-01-10 15:04:13</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#欢迎">👨‍🏫 欢迎</a><ul>
<li><a href="#答疑">🤔 答疑</a></li>
<li><a href="#课程安排">🗓️ 课程安排</a></li>
</ul></li>
<li><a href="#intro">简介</a></li>
<li><a href="#pre"><span class="toc-section-number">1</span> 准备工作</a><ul>
<li><a href="#常用链接"><span class="toc-section-number">1.1</span> 常用链接</a></li>
<li><a href="#克隆代码"><span class="toc-section-number">1.2</span> 克隆代码</a></li>
<li><a href="#r-interface-to-keras"><span class="toc-section-number">1.3</span> R interface to Keras</a><ul>
<li><a href="#r自动安装"><span class="toc-section-number">1.3.1</span> R自动安装</a></li>
<li><a href="#使用reticulate关联conda环境"><span class="toc-section-number">1.3.2</span> 使用reticulate关联conda环境</a></li>
<li><a href="#指定conda安装"><span class="toc-section-number">1.3.3</span> 指定conda安装</a></li>
<li><a href="#使用reticulate安装"><span class="toc-section-number">1.3.4</span> 使用reticulate安装</a></li>
</ul></li>
<li><a href="#r-interface-to-python"><span class="toc-section-number">1.4</span> R interface to Python</a><ul>
<li><a href="#reticulate-常见命令"><span class="toc-section-number">1.4.1</span> reticulate 常见命令</a></li>
<li><a href="#切换r关联的conda环境"><span class="toc-section-number">1.4.2</span> 切换R关联的conda环境</a></li>
</ul></li>
<li><a href="#python"><span class="toc-section-number">1.5</span> Python</a><ul>
<li><a href="#conda环境"><span class="toc-section-number">1.5.1</span> Conda环境</a></li>
<li><a href="#常用的conda命令"><span class="toc-section-number">1.5.2</span> 常用的Conda命令</a></li>
<li><a href="#tensorflowpytorch-gpu-version"><span class="toc-section-number">1.5.3</span> Tensorflow/Pytorch GPU version</a></li>
</ul></li>
</ul></li>
<li><a href="#french"><span class="toc-section-number">2</span> 车险索赔频率预测</a><ul>
<li><a href="#背景介绍"><span class="toc-section-number">2.1</span> 背景介绍</a></li>
<li><a href="#预测模型概述"><span class="toc-section-number">2.2</span> 预测模型概述</a></li>
<li><a href="#特征工程"><span class="toc-section-number">2.3</span> 特征工程</a><ul>
<li><a href="#截断"><span class="toc-section-number">2.3.1</span> 截断</a></li>
<li><a href="#离散化"><span class="toc-section-number">2.3.2</span> 离散化</a></li>
<li><a href="#设定基础水平"><span class="toc-section-number">2.3.3</span> 设定基础水平</a></li>
<li><a href="#协变量变形"><span class="toc-section-number">2.3.4</span> 协变量变形</a></li>
</ul></li>
<li><a href="#训练集-验证集-测试集"><span class="toc-section-number">2.4</span> 训练集-验证集-测试集</a></li>
<li><a href="#泊松偏差损失函数"><span class="toc-section-number">2.5</span> 泊松偏差损失函数</a></li>
<li><a href="#泊松回归模型"><span class="toc-section-number">2.6</span> 泊松回归模型</a></li>
<li><a href="#泊松可加模型"><span class="toc-section-number">2.7</span> 泊松可加模型</a></li>
<li><a href="#泊松回归树"><span class="toc-section-number">2.8</span> 泊松回归树</a></li>
<li><a href="#随机森林"><span class="toc-section-number">2.9</span> 随机森林</a></li>
<li><a href="#泊松提升树"><span class="toc-section-number">2.10</span> 泊松提升树</a></li>
<li><a href="#模型比较"><span class="toc-section-number">2.11</span> 模型比较</a></li>
</ul></li>
<li><a href="#nn"><span class="toc-section-number">3</span> 神经网络</a><ul>
<li><a href="#建立神经网络的一般步骤"><span class="toc-section-number">3.1</span> 建立神经网络的一般步骤</a><ul>
<li><a href="#明确目标和数据类型"><span class="toc-section-number">3.1.1</span> 明确目标和数据类型</a></li>
<li><a href="#数据预处理"><span class="toc-section-number">3.1.2</span> 数据预处理</a></li>
<li><a href="#选取合适的神经网络类型"><span class="toc-section-number">3.1.3</span> 选取合适的神经网络类型</a></li>
<li><a href="#建立神经网络全连接神经网络"><span class="toc-section-number">3.1.4</span> 建立神经网络（全连接神经网络）</a></li>
<li><a href="#训练神经网络"><span class="toc-section-number">3.1.5</span> 训练神经网络</a></li>
<li><a href="#调参"><span class="toc-section-number">3.1.6</span> 调参</a></li>
</ul></li>
<li><a href="#数据预处理-1"><span class="toc-section-number">3.2</span> 数据预处理</a></li>
<li><a href="#神经网络提升模型-combined-actuarial-neural-network"><span class="toc-section-number">3.3</span> 神经网络提升模型 （combined actuarial neural network）</a></li>
<li><a href="#神经网络结构"><span class="toc-section-number">3.4</span> 神经网络结构</a><ul>
<li><a href="#结构参数"><span class="toc-section-number">3.4.1</span> 结构参数</a></li>
<li><a href="#输入层"><span class="toc-section-number">3.4.2</span> 输入层</a></li>
<li><a href="#embedding-layer"><span class="toc-section-number">3.4.3</span> Embedding layer</a></li>
<li><a href="#隐藏层"><span class="toc-section-number">3.4.4</span> 隐藏层</a></li>
<li><a href="#输出层"><span class="toc-section-number">3.4.5</span> 输出层</a></li>
</ul></li>
<li><a href="#训练神经网络-1"><span class="toc-section-number">3.5</span> 训练神经网络</a></li>
<li><a href="#总结"><span class="toc-section-number">3.6</span> 总结</a></li>
<li><a href="#其它模型"><span class="toc-section-number">3.7</span> 其它模型</a></li>
</ul></li>
<li><a href="#boosting"><span class="toc-section-number">4</span> 提升方法 (Boosting)</a><ul>
<li><a href="#adaboost"><span class="toc-section-number">4.1</span> AdaBoost</a></li>
<li><a href="#logit-boost-real-discrete-gentle-adaboost"><span class="toc-section-number">4.2</span> Logit Boost (real, discrete, gentle AdaBoost)</a></li>
<li><a href="#adaboost.m1"><span class="toc-section-number">4.3</span> AdaBoost.M1</a></li>
<li><a href="#samme-stage-wise-additive-modeling-using-a-multi-class-exponential-loss-function"><span class="toc-section-number">4.4</span> SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function)</a></li>
<li><a href="#samme.r-multi-class-real-adaboost"><span class="toc-section-number">4.5</span> SAMME.R (multi-class real AdaBoost)</a></li>
<li><a href="#gradient-boosting"><span class="toc-section-number">4.6</span> Gradient Boosting</a></li>
<li><a href="#newton-boosting"><span class="toc-section-number">4.7</span> Newton Boosting</a></li>
<li><a href="#xgboost"><span class="toc-section-number">4.8</span> XGBoost</a></li>
<li><a href="#case-study"><span class="toc-section-number">4.9</span> Case study</a><ul>
<li><a href="#数据描述"><span class="toc-section-number">4.9.1</span> 数据描述</a></li>
<li><a href="#数据预处理-2"><span class="toc-section-number">4.9.2</span> 数据预处理</a></li>
<li><a href="#特征工程-1"><span class="toc-section-number">4.9.3</span> 特征工程</a></li>
<li><a href="#建模流程"><span class="toc-section-number">4.9.4</span> 建模流程</a></li>
<li><a href="#模型度量gini系数"><span class="toc-section-number">4.9.5</span> 模型度量——Gini系数</a></li>
<li><a href="#建立adaboost模型"><span class="toc-section-number">4.9.6</span> 建立AdaBoost模型</a></li>
<li><a href="#建立xgboost模型"><span class="toc-section-number">4.9.7</span> 建立XGBoost模型</a></li>
<li><a href="#结论"><span class="toc-section-number">4.9.8</span> 结论</a></li>
</ul></li>
<li><a href="#appendix-commonly-used-python-code-for-py-beginners"><span class="toc-section-number">4.10</span> Appendix: Commonly used Python code (for py-beginners)</a><ul>
<li><a href="#python标准数据类型"><span class="toc-section-number">4.10.1</span> Python标准数据类型</a></li>
<li><a href="#python内置函数"><span class="toc-section-number">4.10.2</span> Python内置函数</a></li>
<li><a href="#numpy包"><span class="toc-section-number">4.10.3</span> numpy包</a></li>
<li><a href="#pandas包"><span class="toc-section-number">4.10.4</span> pandas包</a></li>
<li><a href="#matplotlib包"><span class="toc-section-number">4.10.5</span> Matplotlib包</a></li>
<li><a href="#常用教程网址"><span class="toc-section-number">4.10.6</span> 常用教程网址</a></li>
</ul></li>
</ul></li>
<li><a href="#unsupervised-learning"><span class="toc-section-number">5</span> 无监督学习方法</a><ul>
<li><a href="#数据预处理-3"><span class="toc-section-number">5.1</span> 数据预处理</a></li>
<li><a href="#主成分分析"><span class="toc-section-number">5.2</span> 主成分分析</a></li>
<li><a href="#自编码"><span class="toc-section-number">5.3</span> 自编码</a><ul>
<li><a href="#模型训练"><span class="toc-section-number">5.3.1</span> 模型训练</a></li>
</ul></li>
<li><a href="#k-means-clustering"><span class="toc-section-number">5.4</span> K-means clustering</a></li>
<li><a href="#k-medoids-clustering-pam"><span class="toc-section-number">5.5</span> K-medoids clustering (PAM)</a></li>
<li><a href="#gaussian-mixture-modelsgmms"><span class="toc-section-number">5.6</span> Gaussian mixture models(GMMs)</a></li>
<li><a href="#三种聚类方法评价"><span class="toc-section-number">5.7</span> 三种聚类方法评价</a></li>
<li><a href="#t-sne"><span class="toc-section-number">5.8</span> t-SNE</a></li>
<li><a href="#umap"><span class="toc-section-number">5.9</span> UMAP</a></li>
<li><a href="#som"><span class="toc-section-number">5.10</span> SOM</a></li>
</ul></li>
<li><a href="#rnn"><span class="toc-section-number">6</span> 循环神经网络与死亡率预测</a><ul>
<li><a href="#lee-carter-model"><span class="toc-section-number">6.1</span> Lee-Carter Model</a></li>
<li><a href="#普通循环神经网络recurrent-neural-network"><span class="toc-section-number">6.2</span> 普通循环神经网络（recurrent neural network）</a></li>
<li><a href="#长短期记忆神经网络long-short-term-memory"><span class="toc-section-number">6.3</span> 长短期记忆神经网络（Long short-term memory）</a><ul>
<li><a href="#激活函数activation-functions"><span class="toc-section-number">6.3.1</span> 激活函数（Activation functions）</a></li>
<li><a href="#gates-and-cell-state"><span class="toc-section-number">6.3.2</span> Gates and cell state</a></li>
<li><a href="#output-function"><span class="toc-section-number">6.3.3</span> Output Function</a></li>
<li><a href="#time-distributed-layer"><span class="toc-section-number">6.3.4</span> Time-distributed Layer</a></li>
</ul></li>
<li><a href="#门控循环神经网络gated-recurrent-unit"><span class="toc-section-number">6.4</span> 门控循环神经网络（Gated Recurrent Unit）</a><ul>
<li><a href="#gates"><span class="toc-section-number">6.4.1</span> Gates</a></li>
<li><a href="#neuron-activations"><span class="toc-section-number">6.4.2</span> Neuron Activations</a></li>
</ul></li>
<li><a href="#案例分析case-study"><span class="toc-section-number">6.5</span> 案例分析（Case study）</a><ul>
<li><a href="#数据描述-1"><span class="toc-section-number">6.5.1</span> 数据描述</a></li>
<li><a href="#死亡率热力图"><span class="toc-section-number">6.5.2</span> 死亡率热力图</a></li>
<li><a href="#lee-carter-模型"><span class="toc-section-number">6.5.3</span> Lee-Carter 模型</a></li>
<li><a href="#初试rnn"><span class="toc-section-number">6.5.4</span> 初试RNN</a></li>
<li><a href="#rnn-1"><span class="toc-section-number">6.5.5</span> RNN</a></li>
<li><a href="#引入性别协变量"><span class="toc-section-number">6.5.6</span> 引入性别协变量</a></li>
<li><a href="#稳健性"><span class="toc-section-number">6.5.7</span> 稳健性</a></li>
<li><a href="#预测结果图"><span class="toc-section-number">6.5.8</span> 预测结果图</a></li>
</ul></li>
</ul></li>
<li><a href="#nlp"><span class="toc-section-number">7</span> 自然语言处理</a><ul>
<li><a href="#预处理"><span class="toc-section-number">7.1</span> 预处理</a></li>
<li><a href="#bag-of-words"><span class="toc-section-number">7.2</span> Bag of words</a></li>
<li><a href="#bag-of-part-of-speech"><span class="toc-section-number">7.3</span> Bag of part-of-speech</a></li>
<li><a href="#word-embeddings"><span class="toc-section-number">7.4</span> Word embeddings</a><ul>
<li><a href="#pre-trained-word-embeddings"><span class="toc-section-number">7.4.1</span> Pre-trained word embeddings</a></li>
</ul></li>
<li><a href="#机器学习算法"><span class="toc-section-number">7.5</span> 机器学习算法</a></li>
<li><a href="#神经网络"><span class="toc-section-number">7.6</span> 神经网络</a><ul>
<li><a href="#数据预处理-4"><span class="toc-section-number">7.6.1</span> 数据预处理</a></li>
</ul></li>
<li><a href="#case-study-1"><span class="toc-section-number">7.7</span> Case study</a><ul>
<li><a href="#函数说明"><span class="toc-section-number">7.7.1</span> 函数说明</a></li>
<li><a href="#可能遇到的问题"><span class="toc-section-number">7.7.2</span> 可能遇到的问题</a></li>
<li><a href="#结果比较"><span class="toc-section-number">7.7.3</span> 结果比较</a></li>
</ul></li>
<li><a href="#结论-1"><span class="toc-section-number">7.8</span> 结论</a></li>
</ul></li>
<li><a href="#flashlight"><span class="toc-section-number">8</span> 通用模型解释方法</a><ul>
<li><a href="#数据"><span class="toc-section-number">8.1</span> 数据</a></li>
<li><a href="#模型"><span class="toc-section-number">8.2</span> 模型</a><ul>
<li><a href="#glm"><span class="toc-section-number">8.2.1</span> GLM</a></li>
<li><a href="#xgboost-1"><span class="toc-section-number">8.2.2</span> XGBoost</a></li>
<li><a href="#神经网络-1"><span class="toc-section-number">8.2.3</span> 神经网络</a></li>
</ul></li>
<li><a href="#模型整体表现-model-performance"><span class="toc-section-number">8.3</span> 模型整体表现 （model performance）</a></li>
<li><a href="#变量重要性variable-importance"><span class="toc-section-number">8.4</span> 变量重要性（variable importance）</a><ul>
<li><a href="#permutation-importance"><span class="toc-section-number">8.4.1</span> Permutation importance</a></li>
</ul></li>
<li><a href="#边缘效应主效应"><span class="toc-section-number">8.5</span> 边缘效应（主效应）</a><ul>
<li><a href="#individual-conditional-expectationsice"><span class="toc-section-number">8.5.1</span> Individual conditional expectations（ICE）</a></li>
<li><a href="#partial-dependence-profiles"><span class="toc-section-number">8.5.2</span> Partial dependence profiles</a></li>
<li><a href="#accumulated-local-effects-profiles-ale"><span class="toc-section-number">8.5.3</span> Accumulated local effects profiles (ALE)</a></li>
</ul></li>
<li><a href="#交互效应"><span class="toc-section-number">8.6</span> 交互效应</a></li>
<li><a href="#全局代理模型global-surrogate-models"><span class="toc-section-number">8.7</span> 全局代理模型（Global surrogate models）</a></li>
<li><a href="#局部解释样本解释"><span class="toc-section-number">8.8</span> 局部解释（样本解释？）</a><ul>
<li><a href="#lime和live"><span class="toc-section-number">8.8.1</span> LIME和LIVE</a></li>
<li><a href="#shapshapley-additive-explanations"><span class="toc-section-number">8.8.2</span> SHAP(Shapley Additive Explanations)</a></li>
<li><a href="#breakdown-and-approximate-shap"><span class="toc-section-number">8.8.3</span> Breakdown and approximate SHAP</a></li>
<li><a href="#from-local-to-global-properties"><span class="toc-section-number">8.8.4</span> From local to global properties</a></li>
</ul></li>
<li><a href="#improving-the-glm-by-interpretable-machine-learning"><span class="toc-section-number">8.9</span> Improving the GLM by interpretable machine learning</a></li>
<li><a href="#案例分析"><span class="toc-section-number">8.10</span> 案例分析</a><ul>
<li><a href="#导入包"><span class="toc-section-number">8.10.1</span> 导入包</a></li>
<li><a href="#预处理-1"><span class="toc-section-number">8.10.2</span> 预处理</a></li>
<li><a href="#描述性统计"><span class="toc-section-number">8.10.3</span> 描述性统计</a></li>
<li><a href="#建模"><span class="toc-section-number">8.10.4</span> 建模</a></li>
<li><a href="#解释"><span class="toc-section-number">8.10.5</span> 解释</a></li>
<li><a href="#局部性质"><span class="toc-section-number">8.10.6</span> 局部性质</a></li>
<li><a href="#改进glm"><span class="toc-section-number">8.10.7</span> 改进glm</a></li>
</ul></li>
</ul></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">现代精算统计模型</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="欢迎" class="section level1 unnumbered">
<h1>👨‍🏫 欢迎</h1>
<p>《现代精算统计模型》主要讲述如何使用统计学习和机器学习算法，提升传统的精算统计模型或者解决新的精算问题。这门课主要参考瑞士精算师协会发布的<a href="https://actuarialdatascience.org">“精算数据科学”</a>，该教程的主要目的是“为精算师提供一个对数据科学全面且易懂的介绍”，该教程提供了多篇方法性文章并开源代码，这样“读者可以相对容易地把这些数据科学方法用在自己的数据上”。</p>
<p>我们建议大家仔细阅读以下文献，尝试并理解<a href="https://github.com/JSchelldorfer/ActuarialDataScience">所有代码</a>。此网站将作为该课程的辅助，为大家答疑，总结文献，并对文献中的方法做扩展。该网站由授课老师高光远和助教张玮钰管理，欢迎大家反馈意见到助教、微信群、或邮箱 <a href="mailto:guangyuan.gao@ruc.edu.cn" class="email">guangyuan.gao@ruc.edu.cn</a>。</p>
<div id="答疑" class="section level2 unnumbered">
<h2>🤔 答疑</h2>
<p>我定期把同学们的普遍疑问在这里解答，欢迎提问！</p>
<p><strong>👉 Tensorflow for Apple M1</strong> (2020/12/23)</p>
<p>购买Apple M1的同学需要用这个<a href="https://github.com/apple/tensorflow_macos">pre-release tensorflow</a>，从pypi下载的tensorflow暂不支持Apple M1</p>
<p><strong>👉 NLP</strong> (2020/12/18)</p>
<p>数据</p>
<p><img src="./plots/7/data.png" width="50%" style="display: block; margin: auto;" /></p>
<p>这个数据第<span class="math inline">\(i\)</span>行<span class="math inline">\(j\)</span>列表示，在第<span class="math inline">\(i\)</span>个评论中第<span class="math inline">\(j\)</span>个词的排名(依照总出现频率)，所以每一行还保持了句子中词语的先后顺序。每一行都是一个时间序列数据（样本）。</p>
<p>LSTM</p>
<p><img src="./plots/7/lstm.png" width="50%"  style="display: block; margin: auto;" /></p>
<ul>
<li><p><code>input</code>维度是<code>batch size * length * 1</code>，即以上所示的.csv矩阵文档。</p></li>
<li><p><code>embedding_3</code> 作用就是把<code>input</code>的最后一个维度爆炸到256，参数个数为<code>vocab_size* embedding dimension</code>，可以看作把400个高频词映射到256维空间。</p></li>
<li><p><code>embedding_3</code>和<code>lstm_2</code>输出维度中，有两个<code>none</code>,其中第一个表示<code>batch size</code>, 第二个表示<code>sequence length</code>。因为LSTM在时间维度上循环使用参数，所以<code>sequence length</code>不影响参数的个数。</p></li>
<li><p><code>sequence length</code>不影响参数个数，对于不同的句子长度如100或者150，该模型都不需要调整，(应该)可以直接载入数据训练。</p></li>
<li><p><code>lstm_3</code> 只有一个<code>none</code>, 表示<code>batch size</code>, 我们要求<code>lstm_3</code>不返回整个sequence只看最近的状态。</p></li>
</ul>
<p><strong>👉 Reproducible results using Keras</strong> (2020/12/11)</p>
<p>使用Keras复现结果的方法。</p>
<p><a href="https://cran.r-project.org/web/packages/keras/vignettes/faq.html" class="uri">https://cran.r-project.org/web/packages/keras/vignettes/faq.html</a></p>
<p><img src="./plots/reproducible.png" width="50%"  style="display: block; margin: auto;" /></p>
<p><strong>👉 为什么不直接用relu解决vanishing gradient 而设计复杂的lstm gru</strong> (2020/12/11)</p>
<ul>
<li><p>relu值域在0到无穷，不如tanh和sigmoid稳健，后两种可以认为把极大极小值都截断了。</p></li>
<li><p>relu是线性变换，可能描述不了非线性效应。我最常用tanh。</p></li>
<li><p>当然，使用relu会明显提升计算速度，因为relu的导数容易计算。</p></li>
</ul>
<p>另参见<a href="https://datascience.stackexchange.com/questions/61358/relu-for-combating-the-problem-of-vanishing-gradient-in-rnn#:~:text=For%20solving%20the%20problem%20of%20vanishing%20gradients%20in,In%20both%20of%20these%2C%20activation%20function%20is%20tanh.">stackexchange</a></p>
<p><strong>👉 xaringan</strong> (2020/12/06)</p>
<p>html格式的slides：
<a href="https://slides.yihui.org/xaringan/zh-CN.html#1" class="uri">https://slides.yihui.org/xaringan/zh-CN.html#1</a></p>
<p><strong>👉 samme.r</strong> (2020/11/27)</p>
<p>关于samme.r算法, 请参考下面文章中的exponential loss function.
<a href="https://web.stanford.edu/~hastie/Papers/samme.pdf" class="uri">https://web.stanford.edu/~hastie/Papers/samme.pdf</a></p>
<p>算法samme.r仅在以上draft中出现，正式发表时samme.r被删掉了，推测审稿人有异议。正式文章参考
<a href="http://ww.web.stanford.edu/~hastie/Papers/SII-2-3-A8-Zhu.pdf" class="uri">http://ww.web.stanford.edu/~hastie/Papers/SII-2-3-A8-Zhu.pdf</a></p>
<p><strong>👉 随机种子数</strong> (2020/11/20)</p>
<p>输入<code>RNGversion("3.5.0"); set.seed(100)</code>，使得你的随机种子数和paper的相同，模型结果相近。</p>
<p><strong>👉 MAC OS, Linux, WIN</strong> (2020/11/16)</p>
<p>据观察，在MAC OS和Linux系统下安装<code>keras</code>成功的比例较高。WIN系统下，Python各个包的依赖以及和R包的匹配有一定的问题，今天是通过更换镜像源解决了R中无法加载<code>tensorflow.keras</code>模块的问题，推测是TUNA源中WIN包依赖关系没有及时更新。</p>
<p>为了解决镜像源更新延迟、或者tensorflow版本过低的问题，这里共享WIN下经测试的<a href="https://www.jianguoyun.com/p/DcwPgUgQ3cTHBhi1-s0D">conda环境</a>配置。下载该文档，从该文档所在文件夹启动命令行，使用命令<code>conda env create --name &lt;env&gt; --file filename.yaml</code>，安装该conda环境。在R中使用<code>reticulate::use_condaenv("&lt;env&gt;",required=T)</code>关联该环境。</p>
<p>另外，可下载MAC OS系统下经测试的<a href="https://www.jianguoyun.com/p/DYethK4Q3cTHBhjr4s0D">conda环境</a>配置。可通过<code>conda env create --name &lt;env&gt; --file filename.yaml</code>安装。</p>
<p><strong>👉 CASdatasets</strong> (2020/11/13)</p>
<p>源文件在<a href="http://cas.uqam.ca/" class="uri">http://cas.uqam.ca/</a>，但下载速度很慢，我把它放在<a href="https://www.jianguoyun.com/p/DdFyh74Q3cTHBhio2M0D">坚果云共享</a>。下载后选择install from local archive file。</p>
<p><strong>👉 微信群</strong> (2020/11/08)</p>
<p><img src="./plots/wechat.png" width="30%"  style="display: block; margin: auto;" /></p>
</div>
<div id="课程安排" class="section level2 unnumbered">
<h2>🗓️ 课程安排</h2>
<p><img src="./plots/plan.png" width="90%"  style="display: block; margin: auto;" /></p>
<p>以下安排为初步计划，根据大家的需求和背景，我们可能要花更多的时间在某些重要的方法及其在精算上的应用。</p>
<ul>
<li><p>第10周：</p>
<p>准备工作。</p></li>
<li><p>第11周:</p>
<p>1 - French Motor Third-Party Liability Claims</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764</a></p></li>
<li><p>机动</p>
<p>2 - Inisghts from Inside Neural Networks</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3226852" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3226852</a></p>
<p>3 - Nesting Classical Actuarial Models into Neural Networks</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525</a></p></li>
<li><p>第12周：</p>
<p>4 - On Boosting: Theory and Applications</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3402687" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3402687</a></p></li>
<li><p>第13周：</p>
<p>5 - Unsupervised Learning: What is a Sports Car</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3439358" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3439358</a></p></li>
<li><p>第14周：</p>
<p>6 - Lee and Carter go Machine Learning: Recurrent Neural Networks</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3441030" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3441030</a></p></li>
<li><p>第15周：</p>
<p>7 - The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887</a></p></li>
<li><p>第16周：</p>
<p>8 - Peeking into the Black Box: An Actuarial Case Study for Interpretable Machine Learning</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944</a></p></li>
<li><p>第17周：</p>
<p>9 - Convolutional neural network case studies: (1) Anomalies in Mortality Rates (2) Image Recognition</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656210" class="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656210</a></p></li>
</ul>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="intro" class="section level1 unnumbered">
<h1>简介</h1>
<p>保险公司为社会中某些不可预知的经济损失带来了保障。保险公司承担了被保险人的不确定经济损失的风险，被保险人获得了保障，保险公司获得了保费。</p>
<p>通常，保险公司需要在保单的保险期限开始时确定保费，即在保险损失还未发生时确定保费。由于这种定价方式，保险产品和一般消费产品的生产周期相反，服从逆生产周期。</p>
<p>因此，<strong>预测模型</strong>在保险产品的定价中有广泛的应用。</p>
<p>计算机技术的发展带来了计算速度的极大提升和存储能力的极大提高，可以看到当前很多领域的发展都和计算机技术的革新密切相关。</p>
<p><strong>机器学习算法</strong>作为一种预测模型，给传统的<strong>精算回归模型</strong>带来了挑战和机遇。</p>
<ol style="list-style-type: decimal">
<li><p>机器学习算法的预测能力相较于传统回归模型更高;</p></li>
<li><p>机器学习算法的解释性比较差。</p></li>
</ol>
<p>基于机器学习算法的定价模型有助于保险公司<strong>细分风险</strong>，精确定价，减少<em>逆选择</em>风险。</p>
<ul>
<li><p>假设保险公司A在定价中没有考虑到某个重要的风险因子，即对于是否有该类风险的被保险人都收取相同保费；而保险公司B在定价中考虑到该风险因子。</p></li>
<li><p>保险公司B会凭借低保费吸引低风险客户，凭借高保费使得高风险客户留在保险公司A。</p></li>
<li><p>由于被保险人的选择效应，最终保险公司A收取的保费不足以支付被保险人的损失，或者难以获取预期的保险收益。</p></li>
</ul>
<p>另一方面，保险公司承担着风险转移和风险共担的社会角色，过度的细分风险会造成风险个体化，使得保险公司的风险转移和共担的作用消失。
比如，基于被保险人的高风险特征，保险公司会收取极高的保费，使得被保险人的风险转移价值荡然无存，也没有购买保险的动力。
所以，保险公司需要平衡<strong>风险细分和风险共担</strong>，使得保险公司既可以避免逆选择，也能提供有效的风险转移的保险产品。</p>
<p>一般地，保险定价模型受到保险监管机构的严格约束，这些模型在应用到实际中时，必须满足一定的条件。这给机器学习算法在保险定价中的应用带来了很多阻碍。</p>
<ul>
<li><p>在中国，保监会限定了商业车险保费的区间，随着商业车险费率改革，这个区间在不断扩大，保险公司的定价模型发挥的作用越来越大。</p></li>
<li><p>European Union’s General Data Protection Regulation (2018) 建立了algorithmic accountability of decision-making machine algorithms制度，该制度赋予了参与者对于机器学习算法背后逻辑的知情权。</p></li>
</ul>
<p>总之，定价模型必须在一定程度上可以解释给被保险人、保险监管机构等。从被保险人和保险监管的角度出发，他们也希望产品定价和风险管理是建立在一个较透明的模型而不是一个黑盒子，这样有利于维护市场公平、保障被保险人的利益、检测重要风险因子、建立防范风险措施。</p>
<!--chapter:end:00-introduction.Rmd-->
</div>
<div id="pre" class="section level1">
<h1><span class="header-section-number">1</span> 准备工作</h1>
<p>“工欲善其事，必先利其器。”</p>
<p>在以下步骤中，当你发现安装非常慢时，可以尝试4G网络，尝试VPN，尝试改变CRAN的镜像源，或尝试改变conda的镜像源。conda镜像源通过修改用户目录下的<code>.condarc</code>文件使用<a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">TUNA镜像源</a>，但该镜像源可能有更新延迟。</p>
<div id="常用链接" class="section level2">
<h2><span class="header-section-number">1.1</span> 常用链接</h2>
<p>准备工作中常用的链接有</p>
<ul>
<li><p><a href="https://github.com/JSchelldorfer/ActuarialDataScience">GitHub</a></p></li>
<li><p><a href="https://git-scm.com/">Git</a></p></li>
<li><p><a href="https://docs.github.com/cn/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh">SSH key</a></p></li>
<li><p><a href="https://resources.github.com/whitepapers/github-and-rstudio/">GitHub and RStudio</a></p></li>
<li><p><a href="https://jupyter.org/">Jupyter Notebook</a></p></li>
<li><p><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">Anaconda</a></p></li>
<li><p><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/?C=N&amp;O=D">Miniconda</a></p></li>
<li><p><a href="https://docs.conda.io/projects/conda/en/latest/commands.html#">常用Conda命令</a></p></li>
<li><p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">TUNA镜像源</a></p></li>
<li><p><a href="https://keras.rstudio.com/">R interface to Tensorflow and Keras</a></p></li>
<li><p><a href="https://cran.r-project.org/web/packages/reticulate/">reticulate</a></p></li>
<li><p><a href="https://tensorflow.google.cn/">Tensorflow</a></p></li>
<li><p><a href="https://pytorch.apachecn.org/">Pytorch</a></p></li>
<li><p><a href="https://cc.ruc.edu.cn/home">校级计算云</a></p></li>
<li><p><a href="https://developer.nvidia.com/cuda-toolkit-archivE">CUDA</a></p></li>
<li><p><a href="https://developer.nvidia.com/rdp/form/cudnn-download-survey">cuDNN</a></p></li>
</ul>
</div>
<div id="克隆代码" class="section level2">
<h2><span class="header-section-number">1.2</span> 克隆代码</h2>
<p><a href="https://github.com/">GitHub</a>提供了大量开源代码，这门课的代码主要来自<a href="https://github.com/JSchelldorfer/ActuarialDataScience">此链接</a>。通常，使用GitHub开源代码最方便的是<code>fork</code>到自己GitHub账户下，然后<code>clone</code>到本地。具体而言，需要进行以下操作：</p>
<ol style="list-style-type: decimal">
<li><p>注册GitHub账户。</p></li>
<li><p><code>Fork</code><a href="https://github.com/JSchelldorfer/ActuarialDataScience">此链接</a>到自己账户下的新仓库,可重新命名为如<code>Modern-Actuarial-Models</code>或其他名称。</p></li>
<li><p>安装<a href="https://git-scm.com/">git</a>。在命令窗口使用<code>$ git config --global user.name "Your Name"</code> 和 <code>$ git config --global user.email "youremail@yourdomain.com"</code> 配置git的用户名和邮箱分别为GitHub账户的用户名和邮箱。最后可使用<code>$ git config --list</code>查看配置信息。</p></li>
<li><p>(选做)在本地电脑创建ssh public key，并拷贝到GitHub中<code>Setting</code>下<code>SSH and GPG keys</code>。ssh public key一般保存在本人目录下的隐藏文件夹.ssh中，扩展名为.pub。详见<a href="https://docs.github.com/cn/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh">链接</a>。设立SSH可以避免后续<code>push</code>代码到云端时，每次都需要输入密码的麻烦</p></li>
<li><p>电脑连接手机4G热点。一般地，在手机4G网络下克隆的速度比较快。</p></li>
<li><p>在RStudio中创建新的项目，选择Version Control，然后Git，在Repository URL中输入你的GitHub中刚才<code>fork</code>的新仓库地址（在<code>Code</code>下能找到克隆地址，如果第4步完成可以选择SSH地址，如果第4步没完成必须选择HTTPS地址），输入文件夹名称，选择存放位置，点击<code>create project</code>，RStudio开始克隆GitHub上该仓库的所有内容。</p></li>
<li><p>此时，你在GitHub上仓库的内容全部克隆到了本地，且放在了一个R Project中。在该Project中，会多两个文件，.Rproj和.gitignore，第一个文件保存了Project的设置，第二文件告诉git在<code>push</code>本地文件到GitHub时哪些文件被忽略。</p></li>
<li><p>如果你修改了本地文件，可以通过R中内嵌的Git上传到GitHub（先<code>commit</code>再<code>push</code>），这样方便在不同电脑上同步文件。git是代码版本控制工具，在<code>push</code>之前，你可以比较和上个代码版本的差异。GitHub记录了你每次<code>push</code>的详细信息，且存放在本地文件夹.git中。同时，如果GitHub上代码有变化，你可以<code>pull</code>到本地。如果经常在不同电脑上使用本仓库，一般需要先<code>pull</code>成最新版本，然后再编辑修改，最后<code>commit-push</code>到GitHub。</p></li>
<li><p>(选做) 你可以建立新的<code>branch</code>，使自己的修改和源代码分开。具体操作可参考<a href="https://resources.github.com/whitepapers/github-and-rstudio/">链接</a>，或者参考账户建立时自动产生的<code>getting-started</code>仓库。</p></li>
<li><p>(选做) 你可以尝试<a href="https://desktop.github.com/">Github Desktop</a>或者<a href="https://jupyter.org/">Jupyter Lab</a>（加载git extension）管理，但对于这门课，这两种方式不是最优。</p></li>
</ol>
<p>理论上，GitHub上所有仓库都可以采用以上方法在RStudio中管理，当然，RStudio对于R代码仓库管理最有效，因为我们可以直接在RStudio中运行仓库中的代码。</p>
</div>
<div id="r-interface-to-keras" class="section level2">
<h2><span class="header-section-number">1.3</span> R interface to Keras</h2>
<p>这里主要说明<code>keras</code>包的安装和使用。<a href="https://keras.rstudio.com/">Keras</a>是tensorflow的API，在keras中建立的神经网络模型都由tensorflow训练。安装<code>keras</code>包主要是安装Python库tensorflow，并让R与之相关联。</p>
<div id="r自动安装" class="section level3">
<h3><span class="header-section-number">1.3.1</span> R自动安装</h3>
<p>最简单的安装方式如下：</p>
<ol style="list-style-type: decimal">
<li><p>使用<code>install.packages("tensorflow")</code>安装所有相关的包，然后<code>library("tensorflow")</code>。</p></li>
<li><p><code>install_tensorflow()</code></p>
<p>这时大概率会出现</p>
<pre><code>No non-system installation of Python could be found.
Would you like to download and install Miniconda?
Miniconda is an open source environment management system for Python.
See https://docs.conda.io/en/latest/miniconda.html for more details.
Would you like to install Miniconda? [Y/n]:</code></pre>
<p>虽然你可能已经有Anaconda和Python，但R没有“智能”地识别出来，这时仍建议你选<code>Y</code>，让R自己装一下自己能更好识别的<code>Miniconda</code>, 这个命令还会自动建立一个独立conda环境<code>r-reticulate</code>，并在其中装好<code>tensorflow, keras</code>等。</p></li>
<li><p>上步如果正常运行，结束后会自动重启R。这时你运行<code>library(tensorflow)</code>然后<code>tf$constant("Hellow Tensorflow")</code>，如果没报错，那继续<code>install_packages("keras")</code>,<code>library("keras")</code>。</p>
<p>用以下代码验证安装成功</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="kw">layer_flatten</span>(<span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span>, <span class="dv">28</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="kw">layer_dropout</span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="kw">layer_dense</span>(<span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<p>如果出现以下错误</p>
<pre><code>错误: Installation of TensorFlow not found.
Python environments searched for &#39;tensorflow&#39; package:
C:\Users\...\AppData\Local\r-miniconda\envs\r-reticulate\python.exe
You can install TensorFlow using the install_tensorflow() function.</code></pre>
<p>这个错误通常是由于<code>r-reticulate</code>中<code>tensorflow</code>和其他包的依赖关系发生错误，或者<code>tensorflow</code>版本太低，你可以更换镜像源、使用conda/pip install调整该环境中的<code>tensorflow</code>版本和依赖关系。</p>
<p>更好的方式是在conda下安装好指定版本的<code>tensorflow</code>然后关联到R，或者用其他方式让R找到其他方式安装的<code>tensorflow</code>。这时，你先把之前失败的安装<code>C:\Users\...\AppData\Local\r-miniconda</code>，这个文件夹完全删掉。然后参考以下安装步骤。</p></li>
</ol>
</div>
<div id="使用reticulate关联conda环境" class="section level3">
<h3><span class="header-section-number">1.3.2</span> 使用reticulate关联conda环境</h3>
<ol style="list-style-type: decimal">
<li><p>下载并安装<a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">Anaconda</a>或者<a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a>。</p></li>
<li><p>运行<code>Anaconda Prompt</code>或者<code>Anaconda Powershell Prompt</code>，在命令行输入<code>conda create -n r-tensorflow tensorflow=2.1.0</code>，conda会创建一个独立的<code>r-tensorflow</code>环境，并在其中安装<code>tensorflow</code>包。</p></li>
<li><p>继续在命令行运行<code>conda activate r-tensorflow</code>加载刚刚安装的环境，并<code>pip install h5py pyyaml requests Pillow scipy</code>在该环境下安装<code>keras</code>依赖的包。至此，R需要的tensorflow环境已经准备好，接下来让R关联此环境。</p></li>
<li><p>重启R，<code>library("reticulate")</code>然后<code>use_condaenv("r-tensorflow",required=T)</code>,这时R就和上面建立的环境关联好。</p></li>
<li><p><code>library("keras“)</code>。这里假设你已经装好<code>tensorflow</code>和<code>keras</code>包。</p>
<p>用以下代码验证安装成功</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">layer_flatten</span>(<span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span>, <span class="dv">28</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="kw">layer_dropout</span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="kw">layer_dense</span>(<span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="kw">summary</span>(model)</span></code></pre></div></li>
</ol>
</div>
<div id="指定conda安装" class="section level3">
<h3><span class="header-section-number">1.3.3</span> 指定conda安装</h3>
<ol style="list-style-type: decimal">
<li><p>下载并安装<a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">Anaconda</a>或者<a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a>。</p></li>
<li><p>命令行输入<code>which -a python</code>，找到Anaconda中Python的路径记为<code>anapy</code>。</p></li>
<li><p>R中<code>install_packages("tensorflow")</code>，然后</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">install_tensorflow</span>(<span class="dt">method =</span> <span class="st">&quot;conda&quot;</span>, <span class="dt">conda =</span> <span class="st">&quot;anapy&quot;</span>, <span class="dt">envname =</span> <span class="st">&quot;r-tensorflow&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.1.0&quot;</span>)</span></code></pre></div>
<p>此命令会在conda下创建<code>r-tensorflow</code>的环境并装好tensorflow包。</p></li>
<li><p><code>install_packages("keras"); library("keras")</code></p>
<p>用以下代码验证安装成功</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">layer_flatten</span>(<span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span>, <span class="dv">28</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="kw">layer_dropout</span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="kw">layer_dense</span>(<span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="kw">summary</span>(model)</span></code></pre></div></li>
</ol>
</div>
<div id="使用reticulate安装" class="section level3">
<h3><span class="header-section-number">1.3.4</span> 使用reticulate安装</h3>
<ol style="list-style-type: decimal">
<li><p>重启R，<code>library("reticulate")</code>。</p></li>
<li><p><code>options(timeout=300)</code>，防止下载时间过长中断。</p></li>
<li><p><code>install_miniconda()</code>，将会安装<code>miniconda</code>并创建一个<code>r-reticulate</code>conda环境。此环境为R默认调用的Python环境。</p></li>
<li><p>（重启R）<code>library("tensorflow"); install_tensorflow(version="2.1.0")</code>，将会在<code>r-reticulate</code>安装<code>tensorflow</code>。</p></li>
<li><p><code>install_packages("keras"); library("keras")</code></p>
<p>用以下代码验证安装成功</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="kw">layer_flatten</span>(<span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span>, <span class="dv">28</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="kw">layer_dropout</span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="kw">layer_dense</span>(<span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="kw">summary</span>(model)</span></code></pre></div></li>
</ol>
</div>
</div>
<div id="r-interface-to-python" class="section level2">
<h2><span class="header-section-number">1.4</span> R interface to Python</h2>
<p>R包<code>reticulate</code>为<code>tensorflow</code>的依赖包，当你装<code>tensorflow</code>它也被自动安装。它可以建立R与Python的交互。</p>
<div id="reticulate-常见命令" class="section level3">
<h3><span class="header-section-number">1.4.1</span> reticulate 常见命令</h3>
<ul>
<li><p><code>conda_list()</code>列出已安装的conda环境</p></li>
<li><p><code>virtualenv_list()</code>列出已存在的虚拟环境</p></li>
<li><p><code>use_python, use_condaenv, use_virtualenv</code>可以指定与R关联的python。</p></li>
<li><p><code>py_config()</code>可以查看当前Python关联信息。</p></li>
</ul>
<p>很多时候，R会创建一个独立conda环境<code>r-miniconda/envs/r-reticulate</code>。</p>
</div>
<div id="切换r关联的conda环境" class="section level3">
<h3><span class="header-section-number">1.4.2</span> 切换R关联的conda环境</h3>
<p>根据需要，你可以切换R关联的conda环境。具体步骤为</p>
<ol style="list-style-type: decimal">
<li><p>重启R</p></li>
<li><p><code>library("reticulate")</code></p></li>
<li><p><code>conda_list()</code>列出可以关联的环境和路径。</p></li>
<li><p><code>use_condaenv("env-name")</code>。<code>env-name</code>为关联的conda环境。</p></li>
<li><p><code>py_config</code>查看是否关联成功。</p></li>
</ol>
</div>
</div>
<div id="python" class="section level2">
<h2><span class="header-section-number">1.5</span> Python</h2>
<p>一般在每个Python（Conda）环境都需要安装一个Jupyter Notebook (conda install notebook)。</p>
<div id="conda环境" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Conda环境</h3>
<p>Python（conda）环境建立比较简单，在<code>使用reticulate关联conda环境</code>我们已经建立过一个环境<code>r-tensorflow</code>。具体操作如下:</p>
<ol style="list-style-type: decimal">
<li><p>建立独立环境<code>conda create -n env-name python=3.8 tensorflow=2.1.0 notebook</code>。该命令会建立<code>env-name</code>的环境，并在其中安装<code>python=3.8</code>,<code>tensorflow</code>，<code>notebook</code>包及其依赖包。</p></li>
<li><p>激活环境<code>conda activate env-name</code>.</p></li>
<li><p>cd 到你的工作目录。</p></li>
<li><p>启动jupyter notebook <code>jupyter notebook</code>。</p></li>
<li><p>如遇到缺少的包，在该环境<code>env-name</code>下使用<code>conda install ***</code>安装缺少的包。</p></li>
</ol>
</div>
<div id="常用的conda命令" class="section level3">
<h3><span class="header-section-number">1.5.2</span> 常用的Conda命令</h3>
<ul>
<li><p><code>conda create -n env-name2 --clone env-name1</code>:复制环境</p></li>
<li><p><code>conda env list</code>：列出所有环境</p></li>
<li><p><code>conda deactivate</code>：退出当前环境</p></li>
<li><p><code>conda remove -n env-name --all</code>：删除环境<code>env-name</code>中的所有包</p></li>
<li><p><code>conda list -n env-name</code>: 列出环境<code>env-name</code>所安装的包</p></li>
<li><p><code>conda clean -p</code>：删除不使用的包</p></li>
<li><p><code>conda clean -t</code>：删除下载的包</p></li>
<li><p><code>conda clean -a</code>：删除所有不必要的包</p></li>
<li><p><code>pip freeze &gt; pip_pkg.txt</code>, <code>pip install -r pip_pkg.txt</code> 保存当前环境PyPI包版本，从文件安装PyPI包（需同系统）</p></li>
<li><p><code>conda env export &gt; conda_pkg.yaml</code>, <code>conda env export --name env_name &gt; conda_pkg.yaml</code>, <code>conda env create --name env-name2 --file conda_pkg.yaml</code> 保存当前/env-name环境所有包，从文件安装所有包（需同系统）</p></li>
<li><p><code>conda list --explicit &gt; spec-list.txt</code>, <code>conda create --name env-name2 --file spec-list.txt</code> 保存当前环境Conda包下载地址，从文件安装Conda包（需同系统）</p></li>
<li><p><code>conda list --export &gt; spec-list.txt</code>, <code>conda create --name env-name2 --file spec-list.txt</code> 保存当前环境所有包（类似<code>conda env export</code>），从文件安装所有包（需同系统）</p></li>
</ul>
</div>
<div id="tensorflowpytorch-gpu-version" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Tensorflow/Pytorch GPU version</h3>
<p><code>Tensorflow</code>可以综合使用CPU和GPU进行计算，GPU的硬件结构适进行卷积运算，所以适于CNN，RNN等模型的求解。</p>
<p>你可以申请使用<a href="https://cc.ruc.edu.cn/home">校级计算云</a>或者使用学院计算云，它们的服务器都配置了GPU，并装好了可以使用GPU的Tensorflow或者Pytorch。使用<a href="https://cc.ruc.edu.cn/home">校级计算云</a>时，你通常只需要运行Jupyter Notebook就可以使用云端GPU进行计算。使用学院计算云时，你通常需要知道一些常用的<a href="https://www.linuxcool.com/">Linux命令</a>，你也可以安装<a href="https://ubuntu.com/">Ubuntu</a>来熟悉Linux系统。</p>
<p><a href="https://cc.ruc.edu.cn/home">校级计算云</a>和学院计算云有专门的IT人员帮你解决如本页所示的大部分IT问题。</p>
<p>你的机器如果有GPU，可以按如下步骤让GPU发挥它的并行计算能力，关键点是让GPU型号、GPU驱动、CUDA版本、Tensorflow或Pytorch版本彼此匹配，且彼此“相连”。百度或者必应上有很多相关资料可以作为参考。</p>
<ol style="list-style-type: decimal">
<li><p>查看电脑GPU和驱动，以及支持的<a href="https://developer.nvidia.com/cuda-gpus">CUDA版本</a>。 或者在终端执行以下命令：nvidia-smi，查看你的NVIDIA显卡驱动支持的CUDA版本。</p></li>
<li><p>查看各个<a href="https://tensorflow.google.cn/install/source?hl=zh-cn#linux">Tensorflow版本</a>，<a href="https://pytorch.org/get-started/locally/">Pytorch版本</a>对应的CUDA和cuDNN.</p></li>
<li><p>下载并安装正确版本的<a href="https://developer.nvidia.com/cuda-toolkit-archivE">CUDA</a>。注册、下载并安装正确版本的<a href="https://developer.nvidia.com/rdp/form/cudnn-download-survey">cuDNN</a></p></li>
<li><p>配置CUDA和cuDNN.</p></li>
<li><p>安装<a href="https://tensorflow.google.cn/install">Tensorflow</a>或者<a href="https://pytorch.org/get-started/locally/">Pytorch</a>.</p></li>
</ol>
<!--chapter:end:01-env-set-up.Rmd-->
</div>
</div>
</div>
<div id="french" class="section level1">
<h1><span class="header-section-number">2</span> 车险索赔频率预测</h1>
<p>“见多识广、随机应变”</p>
<div id="背景介绍" class="section level2">
<h2><span class="header-section-number">2.1</span> 背景介绍</h2>
<p>车险数据量大，风险特征多，对车险数据分析时可以体现出机器学习算法的优势，即使用算法从大数据中挖掘有用信息、提取特征。</p>
<p>在精算中，常常使用车险保单数据和历史索赔数据进行风险分析、车险定价等。保单数据库是在承保的时候建立的，索赔数据库是在索赔发生时建立的，大部分保单没有发生索赔，所以它们不会在索赔数据库中体现。</p>
<p>保单数据库记录了车险的风险信息，包括：</p>
<ol style="list-style-type: decimal">
<li><p>驾驶员特征：年龄、性别、工作、婚姻、地址等</p></li>
<li><p>车辆特征：品牌、车座数、车龄、价格、马力等</p></li>
<li><p>保单信息：保单编号、承保日期、到期日期</p></li>
<li><p>奖惩系数</p></li>
</ol>
<p>索赔数据库记录了保单的索赔信息，可以得到索赔次数<span class="math inline">\(N\)</span>和每次的索赔金额<span class="math inline">\(Y_l,l=1,\ldots,N\)</span>。理论上，车险的纯保费为以下随机和的期望</p>
<p><span class="math display">\[S=\sum_{l=1}^N Y_l\]</span>
假设索赔次数<span class="math inline">\(N\)</span>和索赔金额<span class="math inline">\(Y_l\)</span>独立且<span class="math inline">\(Y_l\)</span>服从独立同分布，则
<span class="math display">\[\mathbf{E}(S)=\mathbf{E}(N)\times\mathbf{E}(Y)\]</span></p>
<p>所以，车险定价问题很多时候都转化为两个独立模型：索赔次数（频率）模型和索赔金额（强度）模型。对于索赔次数模型，通常假设因变量服从泊松分布，建立泊松回归模型，使用的数据量等于保单数；对于索赔金额模型，通常假设因变量服从伽马分布，建立伽马回归模型，使用的数据量等于发生索赔的保单数。通常，在数据量不大时，索赔金额模型的建立难于索赔次数模型，因为只有发生索赔的保单才能用于索赔金额模型的建立。</p>
<p>记第<span class="math inline">\(i\)</span>个保单的风险信息为<span class="math inline">\(x_i\in\mathcal{X}\)</span>，保险公司定价的目标就是找到两个（最优）回归方程（映射），使之尽可能准确地预测索赔频率和索赔强度:</p>
<p><span class="math display">\[\lambda: \mathcal{X}\rightarrow \mathbf{R}_+, ~~~ x \mapsto \lambda(x_i)\]</span>
<span class="math display">\[\mu: \mathcal{X}\rightarrow \mathbf{R}_+, ~~~ x \mapsto \mu(x_i)\]</span></p>
<p>这里，<span class="math inline">\(\lambda(x_i)\)</span>是对<span class="math inline">\(N\)</span>的期望的估计，<span class="math inline">\(\mu(x_i)\)</span>是对<span class="math inline">\(Y\)</span>的期望的估计。基于这两个模型，纯保费估计为<span class="math inline">\(\lambda(x_i)\mu(x_i)\)</span>。</p>
</div>
<div id="预测模型概述" class="section level2">
<h2><span class="header-section-number">2.2</span> 预测模型概述</h2>
<p>如何得到一个好的预测模型呢？可以从两个方面考虑：</p>
<ol style="list-style-type: decimal">
<li><p>让风险信息空间<span class="math inline">\(\mathcal{X}\)</span>丰富，也称为特征工程，比如包含<span class="math inline">\(x,x^2,\ln x\)</span>、或者加入车联网信息。</p></li>
<li><p>让映射空间<span class="math inline">\(\lambda\in{\Lambda},\mu\in M\)</span>丰富，如GLM只包含线性效应、相加效应，映射空间较小，神经网络包含非线性效应、交互作用，映射空间较大。</p></li>
</ol>
<p>当你选取了映射空间较小的GLM，通常需要进行仔细的特征工程，使得风险信息空间适于GLM；当你选取了映射空间较大的神经网络，通常不需要进行特别仔细的特征工程，神经网络可以自动进行特征工程，发掘风险信息中的有用特征。</p>
<ul>
<li><p>对于传统的统计回归模型，GLM，GAM，MARS，我们使用极大似然方法在映射空间中找到最优的回归方程，在极大似然中使用的数据集称为学习集（learning data set）。为了防止过拟合，我们需要进行协变量选择，可以删掉不显著的协变量，也可以使用逐步回归、最优子集、LASSO等，判断标准为AIC等。</p></li>
<li><p>对于树模型，我们使用 recursive partitioning by binary splits 算法对风险空间进行划分，使得各子空间内的应变量差异最小，差异通常使用偏差损失（deviance loss）度量。为了防止过拟合，通常使用交叉验证对树的深度进行控制。树模型训练使用的数据为学习集。</p></li>
<li><p>树模型的扩展为bootstrap aggregation（bagging）和random forest。第一种算法是对每个bootstrap样本建立树模型，然后平均每个树模型的预测；第二种算法类似第一种，但在建立树模型时，要求只在某些随机选定的协变量上分支。这两种扩展都属于集成学习（ensemble learning）。</p></li>
<li><p>提升算法有多种不同形式，它的核心思想类似逐步回归，区别是每步回归中需要依据上步的预测结果调整各个样本的权重，让上步预测结果差的样本在下步回归中占的权重较大。通常，每步回归使用的模型比较简单，如深度为3的树模型。提升算法也属于集成学习，和前面不同是它的弱学习器不是独立的，而bagging和random forest的弱学习器是彼此独立的。</p></li>
<li><p>对于集成算法，通常需要调整弱学习器的结构参数，如树的深度，也要判断弱学习器的个数，这些称为tuning parameters，通常通过比较在验证集（validation）的损失进行调参，防止过拟合。弱学习器中的参数通过在训练集（training）上训练模型得到。训练集和验证集的并集为学习集。</p></li>
<li><p>前馈神经网络的输入神经元为风险信息，下一层神经元为上一层神经元的线性组合并通过激活函数的非线性变换，最后输出神经元为神经网络对因变量期望的预测，通过减小输出神经元与因变量观察值的差异，训练神经网络中的参数。神经网络含有非常多的参数，很难找到全局最优解，而且最优解必然造成过拟合，所以一般采用梯度下降法对参数进行迭代，使得训练集损失在每次迭代中都有下降趋势。通过比较验证集损失确定迭代次数和神经网络的结构参数，防止过拟合。</p></li>
</ul>
<p>如何评价一个预测模型的好坏呢？通常用样本外损失（test error）评价。对于索赔频率，使用泊松偏差损失，对于索赔强度，使用伽马偏差损失，可以证明这两个损失函数和似然函数成负相关。其中，平均泊松偏差损失为：</p>
<p><span class="math display">\[\mathcal{L}(\mathbf{N},\mathbf{\hat{N}})=\frac{2}{|\mathbf{N}|}\sum_{i}N_i\left[\frac{\hat{N}_i}{N_i}-1-\ln\left(\frac{\hat{N}_i}{N_i}\right)\right]\]</span></p>
<p>Keras中定义的损失函数为</p>
<p><span class="math display">\[\tilde{\mathcal{L}}(\mathbf{N},\mathbf{\hat{N}})=\frac{1}{|\mathbf{N}|}\sum_{i}\left[\hat{N}_i-N_i\ln\left(\hat{N}_i\right)\right]\]</span></p>
</div>
<div id="特征工程" class="section level2">
<h2><span class="header-section-number">2.3</span> 特征工程</h2>
<p>加载包。</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">rm</span>(<span class="dt">list=</span><span class="kw">ls</span>())</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="kw">library</span>(CASdatasets) <span class="co"># data</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># library(keras)  # neural network</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="kw">library</span>(data.table) <span class="co"># fread,fwrite</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="kw">library</span>(glmnet) <span class="co"># lasso </span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="kw">library</span>(plyr) <span class="co"># ddply</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="kw">library</span>(mgcv) <span class="co"># gam</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="kw">library</span>(rpart) <span class="co"># tree</span></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="co"># library(rpart.plot)</span></span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="kw">library</span>(Hmisc) <span class="co"># error bar</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co"># devtools::install_github(&#39;henckr/distRforest&#39;)</span></span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="co"># library(distRforest) </span></span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="kw">library</span>(gbm) <span class="co"># boosting</span></span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="kw">data</span>(freMTPL2freq)</span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="co">#data(freMTPL2sev)</span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="co"># textwidth&lt;-7.3 #inch</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="co"># fwrite(freMTPL2freq,&quot;data/freMTPL2freq.txt&quot;)</span></span>
<span id="cb8-18"><a href="#cb8-18"></a><span class="co"># freMTPL2freq&lt;-fread(&quot;data/freMTPL2freq_mac.txt&quot;)</span></span></code></pre></div>
<pre><code>&#39;data.frame&#39;:   678013 obs. of  12 variables:
 $ IDpol     : num  1 3 5 10 11 13 15 17 18 21 ...
 $ ClaimNb   : &#39;table&#39; num [1:678013(1d)] 1 1 1 1 1 1 1 1 1 1 ...
 $ Exposure  : num  0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ...
 $ VehPower  : int  5 5 6 7 7 6 6 7 7 7 ...
 $ VehAge    : int  0 0 2 0 0 2 2 0 0 0 ...
 $ DrivAge   : int  55 55 52 46 46 38 38 33 33 41 ...
 $ BonusMalus: int  50 50 50 50 50 50 50 68 68 50 ...
 $ VehBrand  : Factor w/ 11 levels &quot;B1&quot;,&quot;B10&quot;,&quot;B11&quot;,..: 4 4 4 4 4 4 4 4 4 4 ...
 $ VehGas    : chr  &quot;Regular&quot; &quot;Regular&quot; &quot;Diesel&quot; &quot;Diesel&quot; ...
 $ Area      : Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 4 4 2 2 2 5 5 3 3 2 ...
 $ Density   : int  1217 1217 54 76 76 3003 3003 137 137 60 ...
 $ Region    : Factor w/ 21 levels &quot;Alsace&quot;,&quot;Aquitaine&quot;,..: 21 21 18 2 2 16 16 13 13 17 ...</code></pre>
<div id="截断" class="section level3">
<h3><span class="header-section-number">2.3.1</span> 截断</h3>
<ul>
<li><p>减少outliers/influential points 的影响</p></li>
<li><p>需根据每个变量的分布确定在哪里截断</p>
<ul>
<li><p>索赔次数在4截断</p></li>
<li><p>风险暴露在1截断</p></li>
<li><p>马力在9截断</p></li>
<li><p>车龄在20截断</p></li>
<li><p>年龄在90截断</p></li>
<li><p>奖惩系数在150截断</p></li>
</ul></li>
</ul>
</div>
<div id="离散化" class="section level3">
<h3><span class="header-section-number">2.3.2</span> 离散化</h3>
<ul>
<li><p>目的是为了刻画非线性效应</p></li>
<li><p>需画出协变量的边缘经验索赔频率判断</p></li>
<li><p>离散化马力、车龄、年龄 <code>VehPowerFac, VehAgeFac，DrivAgeFac</code></p></li>
</ul>
</div>
<div id="设定基础水平" class="section level3">
<h3><span class="header-section-number">2.3.3</span> 设定基础水平</h3>
<ul>
<li><p>方便假设检验</p></li>
<li><p>设定含有最多风险暴露的水平为基准水平</p></li>
</ul>
</div>
<div id="协变量变形" class="section level3">
<h3><span class="header-section-number">2.3.4</span> 协变量变形</h3>
<ul>
<li><p>目的是为了刻画非线性效应</p></li>
<li><p>考虑协变量分布，使之变形后近似服从对称分布</p></li>
<li><p><code>DriveAgeLn/2/3/4, DensityLn</code></p></li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>dat1 &lt;-<span class="st"> </span>freMTPL2freq</span>
<span id="cb10-2"><a href="#cb10-2"></a></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="co"># claim number</span></span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a>dat1<span class="op">$</span>ClaimNb &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>ClaimNb, <span class="dv">4</span>)   </span>
<span id="cb10-6"><a href="#cb10-6"></a></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co"># exposure</span></span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a>dat1<span class="op">$</span>Exposure &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>Exposure, <span class="dv">1</span>) </span>
<span id="cb10-10"><a href="#cb10-10"></a></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="co"># vehicle power</span></span>
<span id="cb10-12"><a href="#cb10-12"></a></span>
<span id="cb10-13"><a href="#cb10-13"></a>dat1<span class="op">$</span>VehPowerFac &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">pmin</span>(dat1<span class="op">$</span>VehPower,<span class="dv">9</span>))</span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="kw">aggregate</span>(dat1<span class="op">$</span>Exposure,<span class="dt">by=</span><span class="kw">list</span>(dat1<span class="op">$</span>VehPowerFac),sum)</span>
<span id="cb10-15"><a href="#cb10-15"></a>dat1[,<span class="st">&quot;VehPowerFac&quot;</span>] &lt;-<span class="kw">relevel</span>(dat1[,<span class="st">&quot;VehPowerFac&quot;</span>], <span class="dt">ref=</span><span class="st">&quot;6&quot;</span>)</span>
<span id="cb10-16"><a href="#cb10-16"></a></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="co"># vehicle age</span></span>
<span id="cb10-18"><a href="#cb10-18"></a></span>
<span id="cb10-19"><a href="#cb10-19"></a>dat1<span class="op">$</span>VehAge &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>VehAge,<span class="dv">20</span>)</span>
<span id="cb10-20"><a href="#cb10-20"></a>VehAgeFac &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">110</span>), <span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">2</span>,<span class="dv">5</span>), <span class="kw">rep</span>(<span class="dv">3</span>,<span class="dv">5</span>),<span class="kw">rep</span>(<span class="dv">4</span>,<span class="dv">5</span>), </span>
<span id="cb10-21"><a href="#cb10-21"></a>                               <span class="kw">rep</span>(<span class="dv">5</span>,<span class="dv">5</span>), <span class="kw">rep</span>(<span class="dv">6</span>,<span class="dv">111-21</span>)))</span>
<span id="cb10-22"><a href="#cb10-22"></a>dat1<span class="op">$</span>VehAgeFac &lt;-<span class="st"> </span><span class="kw">as.factor</span>(VehAgeFac[dat1<span class="op">$</span>VehAge<span class="op">+</span><span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="kw">aggregate</span>(dat1<span class="op">$</span>Exposure,<span class="dt">by=</span><span class="kw">list</span>(dat1<span class="op">$</span>VehAgeFac),sum)</span>
<span id="cb10-24"><a href="#cb10-24"></a>dat1[,<span class="st">&quot;VehAgeFac&quot;</span>] &lt;-<span class="kw">relevel</span>(dat1[,<span class="st">&quot;VehAgeFac&quot;</span>], <span class="dt">ref=</span><span class="st">&quot;2&quot;</span>)</span>
<span id="cb10-25"><a href="#cb10-25"></a></span>
<span id="cb10-26"><a href="#cb10-26"></a><span class="co"># driver age</span></span>
<span id="cb10-27"><a href="#cb10-27"></a></span>
<span id="cb10-28"><a href="#cb10-28"></a>dat1<span class="op">$</span>DrivAge &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>DrivAge,<span class="dv">90</span>)</span>
<span id="cb10-29"><a href="#cb10-29"></a>DrivAgeFac &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">18</span><span class="op">:</span><span class="dv">100</span>), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">21-18</span>), <span class="kw">rep</span>(<span class="dv">2</span>,<span class="dv">26-21</span>), <span class="kw">rep</span>(<span class="dv">3</span>,<span class="dv">31-26</span>), </span>
<span id="cb10-30"><a href="#cb10-30"></a>                  <span class="kw">rep</span>(<span class="dv">4</span>,<span class="dv">41-31</span>), <span class="kw">rep</span>(<span class="dv">5</span>,<span class="dv">51-41</span>), <span class="kw">rep</span>(<span class="dv">6</span>,<span class="dv">71-51</span>), <span class="kw">rep</span>(<span class="dv">7</span>,<span class="dv">101-71</span>)))</span>
<span id="cb10-31"><a href="#cb10-31"></a>dat1<span class="op">$</span>DrivAgeFac &lt;-<span class="st"> </span><span class="kw">as.factor</span>(DrivAgeFac[dat1<span class="op">$</span>DrivAge<span class="dv">-17</span>,<span class="dv">2</span>])</span>
<span id="cb10-32"><a href="#cb10-32"></a><span class="kw">aggregate</span>(dat1<span class="op">$</span>Exposure,<span class="dt">by=</span><span class="kw">list</span>(dat1<span class="op">$</span>DrivAgeFac),sum)</span>
<span id="cb10-33"><a href="#cb10-33"></a>dat1[,<span class="st">&quot;DrivAgeFac&quot;</span>] &lt;-<span class="kw">relevel</span>(dat1[,<span class="st">&quot;DrivAgeFac&quot;</span>], <span class="dt">ref=</span><span class="st">&quot;6&quot;</span>)</span>
<span id="cb10-34"><a href="#cb10-34"></a>dat1<span class="op">$</span>DrivAgeLn&lt;-<span class="kw">log</span>(dat1<span class="op">$</span>DrivAge)</span>
<span id="cb10-35"><a href="#cb10-35"></a>dat1<span class="op">$</span>DrivAge2&lt;-dat1<span class="op">$</span>DrivAge<span class="op">^</span><span class="dv">2</span></span>
<span id="cb10-36"><a href="#cb10-36"></a>dat1<span class="op">$</span>DrivAge3&lt;-dat1<span class="op">$</span>DrivAge<span class="op">^</span><span class="dv">3</span></span>
<span id="cb10-37"><a href="#cb10-37"></a>dat1<span class="op">$</span>DrivAge4&lt;-dat1<span class="op">$</span>DrivAge<span class="op">^</span><span class="dv">4</span></span>
<span id="cb10-38"><a href="#cb10-38"></a></span>
<span id="cb10-39"><a href="#cb10-39"></a><span class="co"># bm</span></span>
<span id="cb10-40"><a href="#cb10-40"></a></span>
<span id="cb10-41"><a href="#cb10-41"></a>dat1<span class="op">$</span>BonusMalus &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">pmin</span>(dat1<span class="op">$</span>BonusMalus, <span class="dv">150</span>))</span>
<span id="cb10-42"><a href="#cb10-42"></a></span>
<span id="cb10-43"><a href="#cb10-43"></a><span class="co"># vehicle brand</span></span>
<span id="cb10-44"><a href="#cb10-44"></a></span>
<span id="cb10-45"><a href="#cb10-45"></a>dat1<span class="op">$</span>VehBrand &lt;-<span class="st"> </span><span class="kw">factor</span>(dat1<span class="op">$</span>VehBrand)      <span class="co"># consider VehGas as categorical</span></span>
<span id="cb10-46"><a href="#cb10-46"></a><span class="kw">aggregate</span>(dat1<span class="op">$</span>Exposure,<span class="dt">by=</span><span class="kw">list</span>(dat1<span class="op">$</span>VehBrand),sum)</span>
<span id="cb10-47"><a href="#cb10-47"></a>dat1[,<span class="st">&quot;VehBrand&quot;</span>] &lt;-<span class="kw">relevel</span>(dat1[,<span class="st">&quot;VehBrand&quot;</span>], <span class="dt">ref=</span><span class="st">&quot;B1&quot;</span>)</span>
<span id="cb10-48"><a href="#cb10-48"></a></span>
<span id="cb10-49"><a href="#cb10-49"></a><span class="co"># vehicle gas</span></span>
<span id="cb10-50"><a href="#cb10-50"></a></span>
<span id="cb10-51"><a href="#cb10-51"></a>dat1<span class="op">$</span>VehGas &lt;-<span class="st"> </span><span class="kw">factor</span>(dat1<span class="op">$</span>VehGas)      <span class="co"># consider VehGas as categorical</span></span>
<span id="cb10-52"><a href="#cb10-52"></a><span class="kw">aggregate</span>(dat1<span class="op">$</span>Exposure,<span class="dt">by=</span><span class="kw">list</span>(dat1<span class="op">$</span>VehGas),sum)</span>
<span id="cb10-53"><a href="#cb10-53"></a>dat1[,<span class="st">&quot;VehGas&quot;</span>] &lt;-<span class="kw">relevel</span>(dat1[,<span class="st">&quot;VehGas&quot;</span>], <span class="dt">ref=</span><span class="st">&quot;Regular&quot;</span>)</span>
<span id="cb10-54"><a href="#cb10-54"></a></span>
<span id="cb10-55"><a href="#cb10-55"></a><span class="co"># area (related to density)</span></span>
<span id="cb10-56"><a href="#cb10-56"></a></span>
<span id="cb10-57"><a href="#cb10-57"></a>dat1<span class="op">$</span>Area &lt;-<span class="st"> </span><span class="kw">as.integer</span>(dat1<span class="op">$</span>Area)</span>
<span id="cb10-58"><a href="#cb10-58"></a></span>
<span id="cb10-59"><a href="#cb10-59"></a><span class="co"># density</span></span>
<span id="cb10-60"><a href="#cb10-60"></a></span>
<span id="cb10-61"><a href="#cb10-61"></a>dat1<span class="op">$</span>DensityLn &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">log</span>(dat1<span class="op">$</span>Density))</span>
<span id="cb10-62"><a href="#cb10-62"></a></span>
<span id="cb10-63"><a href="#cb10-63"></a><span class="co"># region</span></span>
<span id="cb10-64"><a href="#cb10-64"></a></span>
<span id="cb10-65"><a href="#cb10-65"></a><span class="kw">aggregate</span>(dat1<span class="op">$</span>Exposure,<span class="dt">by=</span><span class="kw">list</span>(dat1<span class="op">$</span>Region),sum)[<span class="kw">order</span>(</span>
<span id="cb10-66"><a href="#cb10-66"></a>  <span class="kw">aggregate</span>(dat1<span class="op">$</span>Exposure,<span class="dt">by=</span><span class="kw">list</span>(dat1<span class="op">$</span>Region),sum)<span class="op">$</span>x),]</span>
<span id="cb10-67"><a href="#cb10-67"></a>dat1[,<span class="st">&quot;Region&quot;</span>] &lt;-<span class="kw">relevel</span>(dat1[,<span class="st">&quot;Region&quot;</span>], <span class="dt">ref=</span><span class="st">&quot;Centre&quot;</span>)</span>
<span id="cb10-68"><a href="#cb10-68"></a><span class="kw">str</span>(dat1)</span>
<span id="cb10-69"><a href="#cb10-69"></a></span>
<span id="cb10-70"><a href="#cb10-70"></a><span class="co"># model matrix for GLM</span></span>
<span id="cb10-71"><a href="#cb10-71"></a></span>
<span id="cb10-72"><a href="#cb10-72"></a>design_matrix&lt;-</span>
<span id="cb10-73"><a href="#cb10-73"></a><span class="st">  </span><span class="kw">model.matrix</span>( <span class="op">~</span><span class="st"> </span>ClaimNb <span class="op">+</span><span class="st"> </span>Exposure <span class="op">+</span><span class="st"> </span>VehPowerFac <span class="op">+</span><span class="st"> </span>VehAgeFac <span class="op">+</span><span class="st"> </span></span>
<span id="cb10-74"><a href="#cb10-74"></a><span class="st">                  </span>DrivAge <span class="op">+</span><span class="st"> </span>DrivAgeLn <span class="op">+</span><span class="st"> </span>DrivAge2 <span class="op">+</span><span class="st"> </span>DrivAge3 <span class="op">+</span><span class="st"> </span>DrivAge4 <span class="op">+</span><span class="st"> </span></span>
<span id="cb10-75"><a href="#cb10-75"></a><span class="st">                  </span>BonusMalus <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">+</span><span class="st"> </span>VehGas <span class="op">+</span><span class="st"> </span>Area <span class="op">+</span><span class="st"> </span>DensityLn <span class="op">+</span><span class="st"> </span>Region, <span class="dt">data=</span>dat1)[,<span class="op">-</span><span class="dv">1</span>] </span>
<span id="cb10-76"><a href="#cb10-76"></a><span class="co"># VehPower, VehAge as factor variables</span></span>
<span id="cb10-77"><a href="#cb10-77"></a><span class="co"># design_matrix2&lt;-</span></span>
<span id="cb10-78"><a href="#cb10-78"></a><span class="co"># model.matrix( ~ ClaimNb + Exposure + VehPower + VehAge + DrivAge + </span></span>
<span id="cb10-79"><a href="#cb10-79"></a><span class="co">#                BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] </span></span>
<span id="cb10-80"><a href="#cb10-80"></a><span class="co"># VehPower, VehAge, and DrivAge as continuous variables</span></span>
<span id="cb10-81"><a href="#cb10-81"></a><span class="co"># dim(design_matrix2)</span></span></code></pre></div>
</div>
</div>
<div id="训练集-验证集-测试集" class="section level2">
<h2><span class="header-section-number">2.4</span> 训练集-验证集-测试集</h2>
<ul>
<li><p>比例为<span class="math inline">\(0.6:0.2:0.2\)</span></p></li>
<li><p>根据索赔次数分层抽样</p></li>
<li><p>经验索赔频率约为<span class="math inline">\(10\%\)</span></p></li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>seed_split&lt;-<span class="dv">11</span></span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="co"># claim 0/1 proportions</span></span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a>index_zero&lt;-<span class="kw">which</span>(dat1<span class="op">$</span>ClaimNb<span class="op">==</span><span class="dv">0</span>)</span>
<span id="cb11-6"><a href="#cb11-6"></a>index_one&lt;-<span class="kw">which</span>(dat1<span class="op">$</span>ClaimNb<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb11-7"><a href="#cb11-7"></a>prop_zero&lt;-<span class="kw">round</span>(<span class="kw">length</span>(index_zero)<span class="op">/</span>(<span class="kw">length</span>(index_one)<span class="op">+</span><span class="kw">length</span>(index_zero)),<span class="dv">2</span>)</span>
<span id="cb11-8"><a href="#cb11-8"></a>prop_zero</span>
<span id="cb11-9"><a href="#cb11-9"></a>prop_one&lt;-<span class="kw">round</span>(<span class="kw">length</span>(index_one)<span class="op">/</span>(<span class="kw">length</span>(index_one)<span class="op">+</span><span class="kw">length</span>(index_zero)),<span class="dv">2</span>)</span>
<span id="cb11-10"><a href="#cb11-10"></a>prop_one</span>
<span id="cb11-11"><a href="#cb11-11"></a></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="co"># 0.6:0.2:0.2</span></span>
<span id="cb11-13"><a href="#cb11-13"></a></span>
<span id="cb11-14"><a href="#cb11-14"></a>size_valid&lt;-<span class="kw">round</span>(<span class="kw">nrow</span>(dat1)<span class="op">*</span><span class="fl">0.2</span>,<span class="dv">0</span>)</span>
<span id="cb11-15"><a href="#cb11-15"></a>size_test&lt;-size_valid</span>
<span id="cb11-16"><a href="#cb11-16"></a>size_train&lt;-<span class="kw">nrow</span>(dat1)<span class="op">-</span><span class="dv">2</span><span class="op">*</span>size_valid</span>
<span id="cb11-17"><a href="#cb11-17"></a></span>
<span id="cb11-18"><a href="#cb11-18"></a><span class="co"># stratified sampling</span></span>
<span id="cb11-19"><a href="#cb11-19"></a></span>
<span id="cb11-20"><a href="#cb11-20"></a><span class="kw">set.seed</span>(seed_split)</span>
<span id="cb11-21"><a href="#cb11-21"></a>index_train_<span class="dv">0</span>&lt;-<span class="kw">sample</span>(index_zero,size_train<span class="op">*</span>prop_zero)</span>
<span id="cb11-22"><a href="#cb11-22"></a>index_train_<span class="dv">1</span>&lt;-<span class="kw">sample</span>(index_one, size_train<span class="op">-</span><span class="kw">length</span>(index_train_<span class="dv">0</span>))</span>
<span id="cb11-23"><a href="#cb11-23"></a>index_train&lt;-<span class="kw">union</span>(index_train_<span class="dv">0</span>,index_train_<span class="dv">1</span>)</span>
<span id="cb11-24"><a href="#cb11-24"></a><span class="kw">length</span>(index_train);size_train</span>
<span id="cb11-25"><a href="#cb11-25"></a>index_valid&lt;-<span class="kw">c</span>(<span class="kw">sample</span>(<span class="kw">setdiff</span>(index_zero,index_train_<span class="dv">0</span>),<span class="kw">round</span>(size_valid<span class="op">*</span>prop_zero,<span class="dv">0</span>)),</span>
<span id="cb11-26"><a href="#cb11-26"></a>               <span class="kw">sample</span>(<span class="kw">setdiff</span>(index_one,index_train_<span class="dv">1</span>),size_valid<span class="op">-</span><span class="kw">round</span>(size_valid<span class="op">*</span>prop_zero,<span class="dv">0</span>)))</span>
<span id="cb11-27"><a href="#cb11-27"></a><span class="kw">length</span>(index_valid);size_valid</span>
<span id="cb11-28"><a href="#cb11-28"></a>index_test&lt;-<span class="kw">setdiff</span>(<span class="kw">union</span>(index_zero,index_one),<span class="kw">union</span>(index_train,index_valid))</span>
<span id="cb11-29"><a href="#cb11-29"></a>index_learn&lt;-<span class="kw">union</span>(index_train,index_valid)</span>
<span id="cb11-30"><a href="#cb11-30"></a><span class="kw">length</span>(index_train);<span class="kw">length</span>(index_valid);<span class="kw">length</span>(index_test)</span>
<span id="cb11-31"><a href="#cb11-31"></a></span>
<span id="cb11-32"><a href="#cb11-32"></a><span class="co"># train-validation-test; learn-test</span></span>
<span id="cb11-33"><a href="#cb11-33"></a></span>
<span id="cb11-34"><a href="#cb11-34"></a>dat1_train&lt;-dat1[index_train,]</span>
<span id="cb11-35"><a href="#cb11-35"></a>dat1_valid&lt;-dat1[index_valid,]</span>
<span id="cb11-36"><a href="#cb11-36"></a>dat1_test&lt;-dat1[index_test,]</span>
<span id="cb11-37"><a href="#cb11-37"></a>dat1_learn&lt;-dat1[index_learn,]</span>
<span id="cb11-38"><a href="#cb11-38"></a><span class="kw">sum</span>(dat1_train<span class="op">$</span>ClaimNb)<span class="op">/</span><span class="kw">sum</span>(dat1_train<span class="op">$</span>Exposure)</span>
<span id="cb11-39"><a href="#cb11-39"></a><span class="kw">sum</span>(dat1_valid<span class="op">$</span>ClaimNb)<span class="op">/</span><span class="kw">sum</span>(dat1_valid<span class="op">$</span>Exposure)</span>
<span id="cb11-40"><a href="#cb11-40"></a><span class="kw">sum</span>(dat1_test<span class="op">$</span>ClaimNb)<span class="op">/</span><span class="kw">sum</span>(dat1_test<span class="op">$</span>Exposure)</span>
<span id="cb11-41"><a href="#cb11-41"></a><span class="kw">sum</span>(dat1_learn<span class="op">$</span>ClaimNb)<span class="op">/</span><span class="kw">sum</span>(dat1_learn<span class="op">$</span>Exposure)</span>
<span id="cb11-42"><a href="#cb11-42"></a></span>
<span id="cb11-43"><a href="#cb11-43"></a><span class="co"># glm matrix</span></span>
<span id="cb11-44"><a href="#cb11-44"></a></span>
<span id="cb11-45"><a href="#cb11-45"></a>matrix_train&lt;-design_matrix[index_train,]</span>
<span id="cb11-46"><a href="#cb11-46"></a>matrix_valid&lt;-design_matrix[index_valid,]</span>
<span id="cb11-47"><a href="#cb11-47"></a>matrix_test&lt;-design_matrix[index_test,]</span>
<span id="cb11-48"><a href="#cb11-48"></a>matrix_learn&lt;-design_matrix[index_learn,]</span>
<span id="cb11-49"><a href="#cb11-49"></a></span>
<span id="cb11-50"><a href="#cb11-50"></a><span class="co"># gbm matrix (learn)</span></span>
<span id="cb11-51"><a href="#cb11-51"></a></span>
<span id="cb11-52"><a href="#cb11-52"></a>dat1_learn_gbm&lt;-dat1_learn[,<span class="kw">c</span>(<span class="st">&quot;ClaimNb&quot;</span>, <span class="st">&quot;Exposure&quot;</span>,</span>
<span id="cb11-53"><a href="#cb11-53"></a>                              <span class="st">&quot;VehPower&quot;</span>, <span class="st">&quot;VehAge&quot;</span>, <span class="st">&quot;DrivAge&quot;</span>, <span class="st">&quot;BonusMalus&quot;</span>,</span>
<span id="cb11-54"><a href="#cb11-54"></a>                              <span class="st">&quot;VehBrand&quot;</span>, <span class="st">&quot;VehGas&quot;</span>, <span class="st">&quot;Area&quot;</span>, <span class="st">&quot;DensityLn&quot;</span>, <span class="st">&quot;Region&quot;</span>)]</span>
<span id="cb11-55"><a href="#cb11-55"></a><span class="kw">class</span>(dat1_learn_gbm)</span>
<span id="cb11-56"><a href="#cb11-56"></a>train_pro&lt;-size_train<span class="op">/</span>(size_train<span class="op">+</span>size_valid)</span></code></pre></div>
</div>
<div id="泊松偏差损失函数" class="section level2">
<h2><span class="header-section-number">2.5</span> 泊松偏差损失函数</h2>
<ul>
<li>平均泊松偏差损失</li>
</ul>
<p><span class="math display">\[\mathcal{L}(\mathbf{N},\mathbf{\hat{N}})=\frac{2}{|\mathbf{N}|}\sum_{i}N_i\left[\frac{\hat{N}_i}{N_i}-1-\ln\left(\frac{\hat{N}_i}{N_i}\right)\right]\]</span></p>
<ul>
<li>Keras定义平均泊松偏差损失为</li>
</ul>
<p><span class="math display">\[\tilde{\mathcal{L}}(\mathbf{N},\mathbf{\hat{N}})=\frac{1}{|\mathbf{N}|}\sum_{i}\left[\hat{N}_i-N_i\ln\left(\hat{N}_i\right)\right]\]</span></p>
<ul>
<li>因为对于大部分保单，<span class="math inline">\(N_i-N_i\ln N_i\approx0\)</span>，所以泊松偏差损失函数约为Keras定义的2倍（至少在一个量级）。</li>
</ul>
<p><span class="math display">\[\mathcal{L}(\mathbf{N},\mathbf{\hat{N}})\approx2\tilde{\mathcal{L}}(\mathbf{N},\mathbf{\hat{N}})\]</span></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>Poisson.Deviance &lt;-<span class="st"> </span><span class="cf">function</span>(pred,obs)</span>
<span id="cb12-2"><a href="#cb12-2"></a>  {<span class="dv">200</span><span class="op">*</span>(<span class="kw">sum</span>(pred)<span class="op">-</span><span class="kw">sum</span>(obs)<span class="op">+</span><span class="kw">sum</span>(<span class="kw">log</span>((obs<span class="op">/</span>pred)<span class="op">^</span>(obs))))<span class="op">/</span><span class="kw">length</span>(pred)}</span>
<span id="cb12-3"><a href="#cb12-3"></a>keras_poisson_dev&lt;-<span class="cf">function</span>(y_hat,y_true)</span>
<span id="cb12-4"><a href="#cb12-4"></a>  {<span class="dv">100</span><span class="op">*</span><span class="kw">sum</span>(y_hat<span class="op">-</span>y_true<span class="op">*</span><span class="kw">log</span>(y_hat))<span class="op">/</span><span class="kw">length</span>(y_true)}</span>
<span id="cb12-5"><a href="#cb12-5"></a>f_keras&lt;-<span class="cf">function</span>(x) <span class="dv">100</span><span class="op">*</span>(x<span class="op">-</span>x<span class="op">*</span><span class="kw">log</span>(x))</span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="kw">f_keras</span>(<span class="fl">0.1</span>);<span class="kw">f_keras</span>(<span class="fl">0.2</span>)</span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="co"># png(&quot;./plots/1/poi_dev.png&quot;)</span></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="kw">plot</span>(<span class="kw">seq</span>(<span class="fl">0.05</span>,<span class="fl">0.15</span>,<span class="fl">0.01</span>),<span class="kw">f_keras</span>(<span class="kw">seq</span>(<span class="fl">0.05</span>,<span class="fl">0.15</span>,<span class="fl">0.01</span>)),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,</span>
<span id="cb12-9"><a href="#cb12-9"></a>      <span class="dt">xlab=</span><span class="st">&quot;frequency&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;approximated Poisson deviance&quot;</span>,<span class="dt">main=</span><span class="st">&quot;100(freq - freq * ln freq)&quot;</span>)</span>
<span id="cb12-10"><a href="#cb12-10"></a> <span class="kw">abline</span>(<span class="dt">v=</span><span class="fl">0.1</span>,<span class="dt">lty=</span><span class="dv">2</span>);<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">f_keras</span>((<span class="fl">0.1</span>)),<span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="co"># dev.off()</span></span></code></pre></div>
<p><img src="./plots/1/poi_dev.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="泊松回归模型" class="section level2">
<h2><span class="header-section-number">2.6</span> 泊松回归模型</h2>
<ul>
<li><p>使用极大似然方法在映射空间中找到最优的回归方程</p></li>
<li><p>在极大似然中使用的数据集称为学习集（learning data set）。</p></li>
<li><p>为了防止过拟合，我们需要进行协变量选择，可以删掉不显著的协变量，也可以使用逐步回归、最优子集、LASSO等，判断标准为AIC等。</p></li>
<li><p>同质模型 <span class="math display">\[\mathbf{E}(N)=\beta_0\]</span></p></li>
<li><p>全模型</p>
<p><span class="math display">\[\ln \mathbf{E}(N)=\ln e + \beta_0 + \beta_{\text{VehPowerFac}} + \beta_{\text{VehAgeFac}} \\ + \beta_1\text{DrivAge} + \beta_2\ln\text{DrivAge} + \beta_3\text{DrivAge}^2 + \beta_4\text{DrivAge}^3 + \beta_5\text{DrivAge}^4 \\ \beta_6\text{BM} + \beta_{\text{VehBrand}} + \beta_{\text{VehGas}} + \beta_7\text{Area} + \beta_8\text{DensityLn} + \beta_{\text{Region}}\]</span></p></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># homogeneous model</span></span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a>d.glm0 &lt;-<span class="st"> </span><span class="kw">glm</span>(ClaimNb <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">offset</span>(<span class="kw">log</span> (Exposure)), </span>
<span id="cb13-4"><a href="#cb13-4"></a>              <span class="dt">data=</span><span class="kw">data.frame</span>(matrix_learn), <span class="dt">family=</span><span class="kw">poisson</span>())</span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="co">#summary(d.glm0)</span></span>
<span id="cb13-6"><a href="#cb13-6"></a>dat1_test<span class="op">$</span>fitGLM0 &lt;-<span class="st"> </span></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="st">  </span><span class="kw">predict</span>(d.glm0, <span class="dt">newdata=</span><span class="kw">data.frame</span>(matrix_test), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitGLM0,matrix_test[,<span class="dv">1</span>])</span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitGLM0,matrix_test[,<span class="dv">1</span>])</span>
<span id="cb13-10"><a href="#cb13-10"></a></span>
<span id="cb13-11"><a href="#cb13-11"></a><span class="co"># full GLM</span></span>
<span id="cb13-12"><a href="#cb13-12"></a></span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="kw">names</span>(<span class="kw">data.frame</span>(matrix_learn))</span>
<span id="cb13-14"><a href="#cb13-14"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb13-15"><a href="#cb13-15"></a>d.glm1 &lt;-<span class="st"> </span><span class="kw">glm</span>(ClaimNb <span class="op">~</span><span class="st"> </span>.<span class="op">-</span>Exposure <span class="op">+</span><span class="st"> </span><span class="kw">offset</span>(<span class="kw">log</span>(Exposure)), </span>
<span id="cb13-16"><a href="#cb13-16"></a>              <span class="dt">data=</span><span class="kw">data.frame</span>(matrix_learn), <span class="dt">family=</span><span class="kw">poisson</span>())</span>
<span id="cb13-17"><a href="#cb13-17"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb13-18"><a href="#cb13-18"></a><span class="co"># summary(d.glm1)</span></span>
<span id="cb13-19"><a href="#cb13-19"></a>dat1_train<span class="op">$</span>fitGLM1 &lt;-<span class="st"> </span></span>
<span id="cb13-20"><a href="#cb13-20"></a><span class="st">  </span><span class="kw">predict</span>(d.glm1, <span class="dt">newdata=</span><span class="kw">data.frame</span>(matrix_train), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb13-21"><a href="#cb13-21"></a>dat1_valid<span class="op">$</span>fitGLM1 &lt;-<span class="st"> </span></span>
<span id="cb13-22"><a href="#cb13-22"></a><span class="st">  </span><span class="kw">predict</span>(d.glm1, <span class="dt">newdata=</span><span class="kw">data.frame</span>(matrix_valid), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb13-23"><a href="#cb13-23"></a>dat1_test<span class="op">$</span>fitGLM1 &lt;-<span class="st"> </span></span>
<span id="cb13-24"><a href="#cb13-24"></a><span class="st">  </span><span class="kw">predict</span>(d.glm1, <span class="dt">newdata=</span><span class="kw">data.frame</span>(matrix_test), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb13-25"><a href="#cb13-25"></a>dat1_learn<span class="op">$</span>fitGLM1 &lt;-<span class="st"> </span></span>
<span id="cb13-26"><a href="#cb13-26"></a><span class="st">  </span><span class="kw">predict</span>(d.glm1, <span class="dt">newdata=</span><span class="kw">data.frame</span>(matrix_learn), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb13-27"><a href="#cb13-27"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitGLM1,matrix_test[,<span class="dv">1</span>])</span>
<span id="cb13-28"><a href="#cb13-28"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitGLM1,matrix_test[,<span class="dv">1</span>])</span></code></pre></div>
<p><strong>Step wise、LASSO协变量选择</strong></p>
<ul>
<li><p>逐步回归非常慢，在Linux 8核i7 3.4GHz 16G内存都需要50多分钟。且样本外损失和全模型没有明显减小。</p></li>
<li><p>5折CV Lasso在Linux 8核i7 3.4GHz 16G内存需要5分钟。</p></li>
<li><p>根据5折CV-error选取Lasso正则参数<code>beta=4*10^-5</code>。</p></li>
<li><p>两种方法的样本外损失和全模型没有明显减小，说明没有发生明显过拟合。也说明需要从非线性效应和交互项出发提升模型。</p></li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># step wise selection； this takes a long time (more than 50 minutes!)</span></span>
<span id="cb14-2"><a href="#cb14-2"></a></span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="co"># d.glm00 &lt;- glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + </span></span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="co">#                  DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + </span></span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="co">#                  BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + </span></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="co">#                  offset(log (Exposure)), </span></span>
<span id="cb14-7"><a href="#cb14-7"></a><span class="co">#                data=data.frame(matrix_learn), family=poisson())</span></span>
<span id="cb14-8"><a href="#cb14-8"></a><span class="co"># {t1 &lt;- proc.time()</span></span>
<span id="cb14-9"><a href="#cb14-9"></a><span class="co"># d.glm2&lt;-step(d.glm00,direction=&quot;forward&quot;,trace = 1,</span></span>
<span id="cb14-10"><a href="#cb14-10"></a><span class="co">#              scope =list(lower=formula(d.glm00), upper=formula(d.glm1)))</span></span>
<span id="cb14-11"><a href="#cb14-11"></a><span class="co"># (proc.time()-t1)}</span></span>
<span id="cb14-12"><a href="#cb14-12"></a>d.glm2&lt;-<span class="kw">glm</span>(ClaimNb <span class="op">~</span><span class="st"> </span>VehAgeFac1 <span class="op">+</span><span class="st"> </span>VehAgeFac3 <span class="op">+</span><span class="st"> </span>VehAgeFac4 <span class="op">+</span><span class="st"> </span>VehAgeFac5 <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="st">    </span>DrivAge <span class="op">+</span><span class="st"> </span>DrivAge2 <span class="op">+</span><span class="st"> </span>DrivAge3 <span class="op">+</span><span class="st"> </span>DrivAge4 <span class="op">+</span><span class="st"> </span>DrivAgeLn <span class="op">+</span><span class="st"> </span>BonusMalus <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-14"><a href="#cb14-14"></a><span class="st">    </span>VehBrandB12 <span class="op">+</span><span class="st"> </span>VehGasDiesel <span class="op">+</span><span class="st"> </span>DensityLn <span class="op">+</span><span class="st"> </span>VehPowerFac4 <span class="op">+</span><span class="st"> </span>VehPowerFac8 <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-15"><a href="#cb14-15"></a><span class="st">    </span>RegionNord.Pas.de.Calais <span class="op">+</span><span class="st"> </span>VehPowerFac7 <span class="op">+</span><span class="st"> </span>RegionRhone.Alpes <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-16"><a href="#cb14-16"></a><span class="st">    </span>RegionBretagne <span class="op">+</span><span class="st"> </span>RegionAuvergne <span class="op">+</span><span class="st"> </span>RegionLimousin <span class="op">+</span><span class="st"> </span>RegionLanguedoc.Roussillon <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-17"><a href="#cb14-17"></a><span class="st">    </span>RegionIle.de.France <span class="op">+</span><span class="st"> </span>RegionAquitaine <span class="op">+</span><span class="st"> </span>RegionMidi.Pyrenees <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-18"><a href="#cb14-18"></a><span class="st">    </span>RegionPays.de.la.Loire <span class="op">+</span><span class="st"> </span>RegionProvence.Alpes.Cotes.D.Azur <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-19"><a href="#cb14-19"></a><span class="st">    </span>RegionPoitou.Charentes <span class="op">+</span><span class="st"> </span>RegionHaute.Normandie <span class="op">+</span><span class="st"> </span>VehBrandB5 <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-20"><a href="#cb14-20"></a><span class="st">    </span>VehBrandB11 <span class="op">+</span><span class="st"> </span>RegionBasse.Normandie <span class="op">+</span><span class="st"> </span>VehBrandB14 <span class="op">+</span><span class="st"> </span>RegionCorse <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-21"><a href="#cb14-21"></a><span class="st">    </span><span class="kw">offset</span>(<span class="kw">log</span>(Exposure)), <span class="dt">data=</span><span class="kw">data.frame</span>(matrix_learn), <span class="dt">family=</span><span class="kw">poisson</span>())</span>
<span id="cb14-22"><a href="#cb14-22"></a><span class="kw">summary</span>(d.glm2)</span>
<span id="cb14-23"><a href="#cb14-23"></a>dat1_test<span class="op">$</span>fitGLM2 &lt;-<span class="st"> </span><span class="kw">predict</span>(d.glm2, <span class="dt">newdata=</span><span class="kw">data.frame</span>(matrix_test), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb14-24"><a href="#cb14-24"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitGLM2,<span class="kw">data.frame</span>(matrix_test)<span class="op">$</span>ClaimNb)</span>
<span id="cb14-25"><a href="#cb14-25"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitGLM2,matrix_test[,<span class="dv">1</span>])</span>
<span id="cb14-26"><a href="#cb14-26"></a></span>
<span id="cb14-27"><a href="#cb14-27"></a><span class="co"># lasso regression； this takes a few minutes</span></span>
<span id="cb14-28"><a href="#cb14-28"></a></span>
<span id="cb14-29"><a href="#cb14-29"></a>alpha0=<span class="dv">1</span> <span class="co"># 1 for lasso, 0 for ridge.</span></span>
<span id="cb14-30"><a href="#cb14-30"></a><span class="kw">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb14-31"><a href="#cb14-31"></a><span class="co"># {t1 &lt;- proc.time()</span></span>
<span id="cb14-32"><a href="#cb14-32"></a><span class="co"># cvfit = cv.glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], </span></span>
<span id="cb14-33"><a href="#cb14-33"></a><span class="co">#                   family = &quot;poisson&quot;,offset=log(matrix_learn[,2]),</span></span>
<span id="cb14-34"><a href="#cb14-34"></a><span class="co">#                   alpha = alpha0,nfolds = 5,trace.it = 1)</span></span>
<span id="cb14-35"><a href="#cb14-35"></a><span class="co"># (proc.time()-t1)}</span></span>
<span id="cb14-36"><a href="#cb14-36"></a><span class="co"># cvfit$lambda.min #4*10^-5</span></span>
<span id="cb14-37"><a href="#cb14-37"></a><span class="co"># cvfit$lambda.1se # 0.0016</span></span>
<span id="cb14-38"><a href="#cb14-38"></a><span class="co"># plot(cvfit)</span></span>
<span id="cb14-39"><a href="#cb14-39"></a>d.glm3 =<span class="st"> </span><span class="kw">glmnet</span>(matrix_learn[,<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)], matrix_learn[,<span class="dv">1</span>], </span>
<span id="cb14-40"><a href="#cb14-40"></a>                <span class="dt">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="dt">offset=</span><span class="kw">log</span>(matrix_learn[,<span class="dv">2</span>]), </span>
<span id="cb14-41"><a href="#cb14-41"></a>                <span class="dt">alpha=</span>alpha0, <span class="dt">lambda=</span><span class="fl">4.024746e-05</span>, <span class="dt">trace.it =</span> <span class="dv">1</span>)</span>
<span id="cb14-42"><a href="#cb14-42"></a></span>
<span id="cb14-43"><a href="#cb14-43"></a>dat1_test<span class="op">$</span>fitLasso&lt;-<span class="kw">predict</span>(d.glm3, <span class="dt">newx =</span> matrix_test[,<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)],</span>
<span id="cb14-44"><a href="#cb14-44"></a>                            <span class="dt">newoffset=</span><span class="kw">log</span>(matrix_test[,<span class="dv">2</span>]),<span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb14-45"><a href="#cb14-45"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitLasso, matrix_test[,<span class="dv">1</span>])</span>
<span id="cb14-46"><a href="#cb14-46"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitLasso, matrix_test[,<span class="dv">1</span>])</span></code></pre></div>
</div>
<div id="泊松可加模型" class="section level2">
<h2><span class="header-section-number">2.7</span> 泊松可加模型</h2>
<ul>
<li><p>GAM边缘提升模型</p></li>
<li><p>样本外损失减少，说明非线性效应存在。</p></li>
</ul>
<p><span class="math display">\[\ln \mathbf{E}(N)=\ln\hat{\lambda}^{\text{GLM}}+s_1(\text{VehAge})+s_2(\text{BM})\]</span></p>
<ul>
<li><p><span class="math inline">\(s_1,s_2\)</span>为样条平滑函数。</p></li>
<li><p>使用<code>ddply</code>聚合数据，找到充分统计量，加快模型拟合速度。</p></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co">#  GAM marginals improvement (VehAge and BonusMalus)</span></span>
<span id="cb15-2"><a href="#cb15-2"></a></span>
<span id="cb15-3"><a href="#cb15-3"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb15-4"><a href="#cb15-4"></a>dat.GAM &lt;-<span class="st"> </span><span class="kw">ddply</span>(dat1_learn, .(VehAge, BonusMalus), summarise, </span>
<span id="cb15-5"><a href="#cb15-5"></a>                 <span class="dt">fitGLM1=</span><span class="kw">sum</span>(fitGLM1), <span class="dt">ClaimNb=</span><span class="kw">sum</span>(ClaimNb))</span>
<span id="cb15-6"><a href="#cb15-6"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb15-7"><a href="#cb15-7"></a>d.gam &lt;-<span class="st"> </span><span class="kw">gam</span>(ClaimNb <span class="op">~</span><span class="st"> </span><span class="kw">s</span>(VehAge, <span class="dt">bs=</span><span class="st">&quot;cr&quot;</span>)<span class="op">+</span><span class="kw">s</span>(BonusMalus, <span class="dt">bs=</span><span class="st">&quot;cr&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="st">               </span><span class="kw">offset</span>(<span class="kw">log</span>(fitGLM1)), <span class="dt">data=</span>dat.GAM, <span class="dt">method=</span><span class="st">&quot;GCV.Cp&quot;</span>, <span class="dt">family=</span>poisson)</span>
<span id="cb15-9"><a href="#cb15-9"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb15-10"><a href="#cb15-10"></a><span class="kw">summary</span>(d.gam)</span>
<span id="cb15-11"><a href="#cb15-11"></a>dat1_train<span class="op">$</span>fitGAM1 &lt;-<span class="st"> </span><span class="kw">predict</span>(d.gam, <span class="dt">newdata=</span>dat1_train,<span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb15-12"><a href="#cb15-12"></a>dat1_valid<span class="op">$</span>fitGAM1 &lt;-<span class="st"> </span><span class="kw">predict</span>(d.gam, <span class="dt">newdata=</span>dat1_valid,<span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb15-13"><a href="#cb15-13"></a>dat1_test<span class="op">$</span>fitGAM1 &lt;-<span class="st"> </span><span class="kw">predict</span>(d.gam, <span class="dt">newdata=</span>dat1_test,<span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitGAM1, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb15-15"><a href="#cb15-15"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitGAM1,matrix_test[,<span class="dv">1</span>])</span></code></pre></div>
</div>
<div id="泊松回归树" class="section level2">
<h2><span class="header-section-number">2.8</span> 泊松回归树</h2>
<ul>
<li><p>使用 recursive partitioning by binary splits 算法对风险空间进行划分，使得各子空间内的应变量差异最小。</p></li>
<li><p>为了防止过拟合，使用交叉验证确定cost-complexity parameter。</p></li>
<li><p><code>cp=10^-3.421</code>(1-SD rule)剪枝成<code>split=22</code>的树，或者<code>cp=10^-3.949</code>(min CV rule)剪枝成<code>split=55</code>的树。</p></li>
<li><p><code>split=55</code>(min CV rule)树的样本外损失较小。</p></li>
<li><p>Variable importance (min CV rule)</p>
<pre><code>BonusMalus     VehAge   VehBrand    DrivAge     VehGas   VehPower     Region  DensityLn 
 4675.0231  4396.8667  1389.2909   877.9473   795.6308   715.3584   480.3459   140.5463</code></pre></li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># cross validation using xval in rpart.control</span></span>
<span id="cb17-2"><a href="#cb17-2"></a></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="kw">names</span>(dat1_learn)</span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb17-5"><a href="#cb17-5"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb17-6"><a href="#cb17-6"></a>tree0&lt;-<span class="kw">rpart</span>(<span class="kw">cbind</span>(Exposure, ClaimNb) <span class="op">~</span><span class="st"> </span>VehPower <span class="op">+</span><span class="st"> </span>VehAge <span class="op">+</span><span class="st"> </span>DrivAge <span class="op">+</span><span class="st"> </span></span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="st">               </span>BonusMalus <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">+</span><span class="st"> </span>VehGas <span class="op">+</span><span class="st"> </span>Area <span class="op">+</span><span class="st"> </span>DensityLn <span class="op">+</span><span class="st"> </span>Region, </span>
<span id="cb17-8"><a href="#cb17-8"></a>             <span class="dt">data =</span> dat1_learn, <span class="dt">method =</span> <span class="st">&quot;poisson&quot;</span>, </span>
<span id="cb17-9"><a href="#cb17-9"></a>             <span class="dt">control =</span> <span class="kw">rpart.control</span> (<span class="dt">xval=</span><span class="dv">5</span>, <span class="dt">minbucket=</span><span class="dv">1000</span>, <span class="dt">cp=</span><span class="dv">10</span><span class="op">^-</span><span class="dv">5</span>,</span>
<span id="cb17-10"><a href="#cb17-10"></a>                                      <span class="dt">maxcompete =</span> <span class="dv">0</span>, <span class="dt">maxsurrogate =</span> <span class="dv">0</span>))</span>
<span id="cb17-11"><a href="#cb17-11"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb17-12"><a href="#cb17-12"></a>x0 &lt;-<span class="st"> </span><span class="kw">log10</span>(tree0<span class="op">$</span>cptable[,<span class="dv">1</span>])</span>
<span id="cb17-13"><a href="#cb17-13"></a>err0&lt;-tree0<span class="op">$</span>cptable[,<span class="dv">4</span>]</span>
<span id="cb17-14"><a href="#cb17-14"></a>std0&lt;-tree0<span class="op">$</span>cptable[,<span class="dv">5</span>]</span>
<span id="cb17-15"><a href="#cb17-15"></a>xmain &lt;-<span class="st"> &quot;cross-validation error plot&quot;</span></span>
<span id="cb17-16"><a href="#cb17-16"></a>xlabel &lt;-<span class="st"> &quot;cost-complexity parameter (log-scale)&quot;</span></span>
<span id="cb17-17"><a href="#cb17-17"></a>ylabel &lt;-<span class="st"> &quot;relative CV error&quot;</span></span>
<span id="cb17-18"><a href="#cb17-18"></a></span>
<span id="cb17-19"><a href="#cb17-19"></a>(cp_min&lt;-x0[<span class="kw">which.min</span>(err0)])</span>
<span id="cb17-20"><a href="#cb17-20"></a>(cp_1sd&lt;-x0[<span class="kw">min</span>(<span class="kw">which</span>(err0<span class="op">&lt;</span><span class="kw">min</span>(err0)<span class="op">+</span>std0[<span class="kw">which.min</span>(err0)]))])</span>
<span id="cb17-21"><a href="#cb17-21"></a>(nsplit_min&lt;-tree0<span class="op">$</span>cptable[<span class="kw">which.min</span>(err0),<span class="dv">2</span>])</span>
<span id="cb17-22"><a href="#cb17-22"></a>(nsplit_1sd&lt;-tree0<span class="op">$</span>cptable[<span class="kw">min</span>(<span class="kw">which</span>(err0<span class="op">&lt;</span><span class="kw">min</span>(err0)<span class="op">+</span>std0[<span class="kw">which.min</span>(err0)])),<span class="dv">2</span>])</span>
<span id="cb17-23"><a href="#cb17-23"></a></span>
<span id="cb17-24"><a href="#cb17-24"></a><span class="co"># png(&quot;./plots/1/tree_cv.png&quot;)</span></span>
<span id="cb17-25"><a href="#cb17-25"></a><span class="kw">errbar</span>(<span class="dt">x=</span>x0, <span class="dt">y=</span>err0<span class="op">*</span><span class="dv">100</span>, <span class="dt">yplus=</span>(err0<span class="op">+</span>std0)<span class="op">*</span><span class="dv">100</span>, <span class="dt">yminus=</span>(err0<span class="op">-</span>std0)<span class="op">*</span><span class="dv">100</span>,</span>
<span id="cb17-26"><a href="#cb17-26"></a>      <span class="dt">xlim=</span><span class="kw">rev</span>(<span class="kw">range</span>(x0)), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">main=</span>xmain, <span class="dt">ylab=</span>ylabel, <span class="dt">xlab=</span>xlabel)</span>
<span id="cb17-27"><a href="#cb17-27"></a><span class="kw">lines</span>(<span class="dt">x=</span>x0, <span class="dt">y=</span>err0<span class="op">*</span><span class="dv">100</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb17-28"><a href="#cb17-28"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="kw">min</span>(err0<span class="op">+</span>std0)<span class="op">*</span><span class="dv">100</span>), <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>)</span>
<span id="cb17-29"><a href="#cb17-29"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="kw">min</span>(err0)<span class="op">*</span><span class="dv">100</span>), <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>)</span>
<span id="cb17-30"><a href="#cb17-30"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(cp_1sd,cp_min),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;magenta&quot;</span>))</span>
<span id="cb17-31"><a href="#cb17-31"></a><span class="kw">legend</span>(<span class="dt">x=</span><span class="st">&quot;topright&quot;</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>,<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;magenta&quot;</span>),</span>
<span id="cb17-32"><a href="#cb17-32"></a>      <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb17-33"><a href="#cb17-33"></a>      <span class="kw">c</span>(<span class="st">&quot;tree0&quot;</span>, <span class="st">&quot;1-SD rule&quot;</span>, <span class="st">&quot;min.CV rule&quot;</span>,</span>
<span id="cb17-34"><a href="#cb17-34"></a>        <span class="kw">paste</span>(<span class="st">&quot;log cp = &quot;</span>,<span class="kw">round</span>(cp_1sd,<span class="dv">3</span>)),<span class="kw">paste</span>(<span class="st">&quot;log cp = &quot;</span>, <span class="kw">round</span>(cp_min,<span class="dv">3</span>))))</span>
<span id="cb17-35"><a href="#cb17-35"></a><span class="co"># dev.off()</span></span>
<span id="cb17-36"><a href="#cb17-36"></a>tree1 &lt;-<span class="st"> </span><span class="kw">prune</span>(tree0, <span class="dt">cp=</span><span class="dv">10</span><span class="op">^</span><span class="kw">mean</span>(cp_min,<span class="kw">min</span>(x0[x0<span class="op">&gt;</span>cp_min]))) </span>
<span id="cb17-37"><a href="#cb17-37"></a>tree11&lt;-<span class="st"> </span><span class="kw">prune</span>(tree0, <span class="dt">cp=</span><span class="dv">10</span><span class="op">^</span><span class="kw">mean</span>(cp_1sd,<span class="kw">min</span>(x0[x0<span class="op">&gt;</span>cp_1sd]))) </span>
<span id="cb17-38"><a href="#cb17-38"></a>tree1<span class="op">$</span>cptable[<span class="kw">nrow</span>(tree1<span class="op">$</span>cptable),<span class="dv">2</span>];nsplit_min</span>
<span id="cb17-39"><a href="#cb17-39"></a>tree11<span class="op">$</span>cptable[<span class="kw">nrow</span>(tree11<span class="op">$</span>cptable),<span class="dv">2</span>];nsplit_1sd</span>
<span id="cb17-40"><a href="#cb17-40"></a></span>
<span id="cb17-41"><a href="#cb17-41"></a>dat1_test<span class="op">$</span>fitRT_min &lt;-<span class="st"> </span><span class="kw">predict</span>(tree1, <span class="dt">newdata=</span>dat1_test)<span class="op">*</span>dat1_test<span class="op">$</span>Exposure</span>
<span id="cb17-42"><a href="#cb17-42"></a>dat1_test<span class="op">$</span>fitRT_sd &lt;-<span class="st"> </span><span class="kw">predict</span>(tree11, <span class="dt">newdata=</span>dat1_test)<span class="op">*</span>dat1_test<span class="op">$</span>Exposure</span>
<span id="cb17-43"><a href="#cb17-43"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitRT_min, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb17-44"><a href="#cb17-44"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitRT_sd, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb17-45"><a href="#cb17-45"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitRT_min, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb17-46"><a href="#cb17-46"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitRT_sd, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb17-47"><a href="#cb17-47"></a></span>
<span id="cb17-48"><a href="#cb17-48"></a>tree1<span class="op">$</span>variable.importance</span>
<span id="cb17-49"><a href="#cb17-49"></a>tree11<span class="op">$</span>variable.importance</span></code></pre></div>
<p><img src="./plots/1/tree_cv.png" width="60%"  style="display: block; margin: auto;" /></p>
<ul>
<li><p>交叉验证可使用<code>rpart(..., control=rpart.control(xval= ,...))</code>或者<code>xpred.rpart(tree, group)</code>。</p></li>
<li><p>以上两种方式得到很相近的<code>min CV rule</code>剪枝树55 vs 51，但<code>1-SD rule</code>相差较多22 vs 12。</p></li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># K-fold cross-validation using xpred.rpart</span></span>
<span id="cb18-2"><a href="#cb18-2"></a></span>
<span id="cb18-3"><a href="#cb18-3"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb18-4"><a href="#cb18-4"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb18-5"><a href="#cb18-5"></a>tree00&lt;-<span class="kw">rpart</span>(<span class="kw">cbind</span>(Exposure, ClaimNb) <span class="op">~</span><span class="st"> </span>VehPower <span class="op">+</span><span class="st"> </span>VehAge <span class="op">+</span><span class="st"> </span>DrivAge <span class="op">+</span><span class="st"> </span></span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="st">                </span>BonusMalus <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">+</span><span class="st"> </span>VehGas <span class="op">+</span><span class="st"> </span>Area <span class="op">+</span><span class="st"> </span>DensityLn <span class="op">+</span><span class="st"> </span>Region, </span>
<span id="cb18-7"><a href="#cb18-7"></a>              <span class="dt">data =</span> dat1_learn, <span class="dt">method =</span> <span class="st">&quot;poisson&quot;</span>, </span>
<span id="cb18-8"><a href="#cb18-8"></a>              <span class="dt">control =</span> <span class="kw">rpart.control</span> (<span class="dt">xval=</span><span class="dv">1</span>, <span class="dt">minbucket=</span><span class="dv">1000</span> ,<span class="dt">cp=</span><span class="dv">10</span><span class="op">^-</span><span class="dv">5</span>,</span>
<span id="cb18-9"><a href="#cb18-9"></a>                                       <span class="dt">maxcompete =</span> <span class="dv">0</span>, <span class="dt">maxsurrogate =</span> <span class="dv">0</span>))</span>
<span id="cb18-10"><a href="#cb18-10"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb18-11"><a href="#cb18-11"></a></span>
<span id="cb18-12"><a href="#cb18-12"></a>(n_subtrees &lt;-<span class="st"> </span><span class="kw">dim</span>(tree00<span class="op">$</span>cptable)[<span class="dv">1</span>])</span>
<span id="cb18-13"><a href="#cb18-13"></a>std1&lt;-<span class="st"> </span><span class="kw">numeric</span>(n_subtrees)</span>
<span id="cb18-14"><a href="#cb18-14"></a>err1 &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_subtrees)</span>
<span id="cb18-15"><a href="#cb18-15"></a></span>
<span id="cb18-16"><a href="#cb18-16"></a>K &lt;-<span class="st"> </span><span class="dv">5</span>                  </span>
<span id="cb18-17"><a href="#cb18-17"></a>xgroup &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="dt">length =</span> <span class="kw">nrow</span>(dat1_learn))</span>
<span id="cb18-18"><a href="#cb18-18"></a>xfit &lt;-<span class="st"> </span><span class="kw">xpred.rpart</span>(tree00, xgroup)</span>
<span id="cb18-19"><a href="#cb18-19"></a><span class="kw">dim</span>(xfit);<span class="kw">dim</span>(dat1_learn)</span>
<span id="cb18-20"><a href="#cb18-20"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_subtrees){</span>
<span id="cb18-21"><a href="#cb18-21"></a> err_group&lt;-<span class="kw">rep</span>(<span class="ot">NA</span>,K)</span>
<span id="cb18-22"><a href="#cb18-22"></a> <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K){</span>
<span id="cb18-23"><a href="#cb18-23"></a>  ind_group &lt;-<span class="st"> </span><span class="kw">which</span>(xgroup <span class="op">==</span>k)  </span>
<span id="cb18-24"><a href="#cb18-24"></a>  err_group[k] &lt;-<span class="st"> </span><span class="kw">keras_poisson_dev</span>(dat1_learn[ind_group,<span class="st">&quot;Exposure&quot;</span>]<span class="op">*</span>xfit[ind_group,i],</span>
<span id="cb18-25"><a href="#cb18-25"></a>                                    dat1_learn[ind_group,<span class="st">&quot;ClaimNb&quot;</span>])</span>
<span id="cb18-26"><a href="#cb18-26"></a>  }</span>
<span id="cb18-27"><a href="#cb18-27"></a>  err1[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(err_group)             </span>
<span id="cb18-28"><a href="#cb18-28"></a>  std1[i] &lt;-<span class="st"> </span><span class="kw">sd</span>(err_group)</span>
<span id="cb18-29"><a href="#cb18-29"></a>}</span>
<span id="cb18-30"><a href="#cb18-30"></a></span>
<span id="cb18-31"><a href="#cb18-31"></a>x1 &lt;-<span class="st"> </span><span class="kw">log10</span>(tree00<span class="op">$</span>cptable[,<span class="dv">1</span>])</span>
<span id="cb18-32"><a href="#cb18-32"></a>(cp_min1&lt;-x1[<span class="kw">which.min</span>(err1)])</span>
<span id="cb18-33"><a href="#cb18-33"></a>(cp_1sd1&lt;-x1[<span class="kw">min</span>(<span class="kw">which</span>(err1<span class="op">&lt;</span><span class="kw">min</span>(err1)<span class="op">+</span>std1[<span class="kw">which.min</span>(err1)]))])</span>
<span id="cb18-34"><a href="#cb18-34"></a>(nsplit_min1&lt;-tree00<span class="op">$</span>cptable[<span class="kw">which.min</span>(err1),<span class="dv">2</span>])</span>
<span id="cb18-35"><a href="#cb18-35"></a>(nsplit_1sd1&lt;-tree00<span class="op">$</span>cptable[<span class="kw">min</span>(<span class="kw">which</span>(err1<span class="op">&lt;</span><span class="kw">min</span>(err1)<span class="op">+</span>std1[<span class="kw">which.min</span>(err1)])),<span class="dv">2</span>])</span>
<span id="cb18-36"><a href="#cb18-36"></a></span>
<span id="cb18-37"><a href="#cb18-37"></a>xmain &lt;-<span class="st"> &quot;cross-validation error plot&quot;</span></span>
<span id="cb18-38"><a href="#cb18-38"></a>xlabel &lt;-<span class="st"> &quot;cost-complexity parameter (log-scale)&quot;</span></span>
<span id="cb18-39"><a href="#cb18-39"></a>ylabel &lt;-<span class="st"> &quot;CV error (in 10^(-2))&quot;</span></span>
<span id="cb18-40"><a href="#cb18-40"></a><span class="kw">errbar</span>(<span class="dt">x=</span>x1, <span class="dt">y=</span>err1<span class="op">*</span><span class="dv">100</span>, <span class="dt">yplus=</span>(err1<span class="op">+</span>std1)<span class="op">*</span><span class="dv">100</span>, <span class="dt">yminus=</span>(err1<span class="op">-</span>std1)<span class="op">*</span><span class="dv">100</span>,</span>
<span id="cb18-41"><a href="#cb18-41"></a>       <span class="dt">xlim=</span><span class="kw">rev</span>(<span class="kw">range</span>(x1)), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">main=</span>xmain, <span class="dt">ylab=</span>ylabel, <span class="dt">xlab=</span>xlabel)</span>
<span id="cb18-42"><a href="#cb18-42"></a><span class="kw">lines</span>(<span class="dt">x=</span>x1, <span class="dt">y=</span>err1<span class="op">*</span><span class="dv">100</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb18-43"><a href="#cb18-43"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="kw">min</span>(err1<span class="op">+</span>std1)<span class="op">*</span><span class="dv">100</span>), <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>)</span>
<span id="cb18-44"><a href="#cb18-44"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="kw">min</span>(err1)<span class="op">*</span><span class="dv">100</span>), <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>)</span>
<span id="cb18-45"><a href="#cb18-45"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(cp_1sd1,cp_min1),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;magenta&quot;</span>))</span>
<span id="cb18-46"><a href="#cb18-46"></a><span class="kw">legend</span>(<span class="dt">x=</span><span class="st">&quot;topright&quot;</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>,<span class="st">&quot;orange&quot;</span>,<span class="st">&quot;magenta&quot;</span>),</span>
<span id="cb18-47"><a href="#cb18-47"></a>      <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb18-48"><a href="#cb18-48"></a>      <span class="kw">c</span>(<span class="st">&quot;tree0&quot;</span>, <span class="st">&quot;1-SD rule&quot;</span>, <span class="st">&quot;min.CV rule&quot;</span>,</span>
<span id="cb18-49"><a href="#cb18-49"></a>        <span class="kw">paste</span>(<span class="st">&quot;log cp = &quot;</span>,<span class="kw">round</span>(cp_1sd1,<span class="dv">3</span>)),<span class="kw">paste</span>(<span class="st">&quot;log cp = &quot;</span>, <span class="kw">round</span>(cp_min1,<span class="dv">3</span>))))</span>
<span id="cb18-50"><a href="#cb18-50"></a></span>
<span id="cb18-51"><a href="#cb18-51"></a>tree2 &lt;-<span class="st"> </span><span class="kw">prune</span>(tree00, <span class="dt">cp=</span><span class="dv">10</span><span class="op">^</span><span class="kw">mean</span>(cp_min1,<span class="kw">min</span>(x1[x1<span class="op">&gt;</span>cp_min1]))) </span>
<span id="cb18-52"><a href="#cb18-52"></a>tree22 &lt;-<span class="st"> </span><span class="kw">prune</span>(tree00, <span class="dt">cp=</span><span class="dv">10</span><span class="op">^</span><span class="kw">mean</span>(cp_1sd1,<span class="kw">min</span>(x1[x1<span class="op">&gt;</span>cp_1sd1]))) </span>
<span id="cb18-53"><a href="#cb18-53"></a><span class="kw">printcp</span>(tree2)</span>
<span id="cb18-54"><a href="#cb18-54"></a><span class="kw">printcp</span>(tree22)</span>
<span id="cb18-55"><a href="#cb18-55"></a>dat1_test<span class="op">$</span>fitRT2 &lt;-<span class="st"> </span><span class="kw">predict</span>(tree2, <span class="dt">newdata=</span>dat1_test)<span class="op">*</span>dat1_test<span class="op">$</span>Exposure</span>
<span id="cb18-56"><a href="#cb18-56"></a>dat1_test<span class="op">$</span>fitRT22 &lt;-<span class="st"> </span><span class="kw">predict</span>(tree22, <span class="dt">newdata=</span>dat1_test)<span class="op">*</span>dat1_test<span class="op">$</span>Exposure</span>
<span id="cb18-57"><a href="#cb18-57"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitRT2, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb18-58"><a href="#cb18-58"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitRT22, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb18-59"><a href="#cb18-59"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitRT2, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb18-60"><a href="#cb18-60"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitRT22, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb18-61"><a href="#cb18-61"></a><span class="kw">sum</span>((dat1_test<span class="op">$</span>fitRT2<span class="op">-</span>dat1_test<span class="op">$</span>fitRT_min)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb18-62"><a href="#cb18-62"></a><span class="kw">sum</span>((dat1_test<span class="op">$</span>fitRT22<span class="op">-</span>dat1_test<span class="op">$</span>fitRT_sd)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb18-63"><a href="#cb18-63"></a>tree2<span class="op">$</span>variable.importance</span>
<span id="cb18-64"><a href="#cb18-64"></a>tree1<span class="op">$</span>variable.importance</span>
<span id="cb18-65"><a href="#cb18-65"></a>tree22<span class="op">$</span>variable.importance</span>
<span id="cb18-66"><a href="#cb18-66"></a>tree11<span class="op">$</span>variable.importance</span></code></pre></div>
</div>
<div id="随机森林" class="section level2">
<h2><span class="header-section-number">2.9</span> 随机森林</h2>
<ul>
<li><p>使用<a href="https://github.com/henckr/distRforest" class="uri">https://github.com/henckr/distRforest</a>建立泊松随机森林。</p></li>
<li><p><code>ncand</code>每次分裂考虑的协变量个数；<code>subsample</code>训练每棵树的样本。</p></li>
<li><p>使用验证损失确定树的数量。</p></li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># fit the random forest</span></span>
<span id="cb19-2"><a href="#cb19-2"></a></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="kw">library</span>(distRforest)</span>
<span id="cb19-4"><a href="#cb19-4"></a>ntrees0&lt;-<span class="dv">200</span></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb19-6"><a href="#cb19-6"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb19-7"><a href="#cb19-7"></a>forest1&lt;-<span class="kw">rforest</span>(<span class="kw">cbind</span>(Exposure, ClaimNb) <span class="op">~</span><span class="st">  </span>VehPower <span class="op">+</span><span class="st"> </span>VehAge <span class="op">+</span><span class="st"> </span>DrivAge <span class="op">+</span></span>
<span id="cb19-8"><a href="#cb19-8"></a><span class="st">                   </span>BonusMalus <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">+</span><span class="st"> </span>VehGas <span class="op">+</span><span class="st"> </span>Area <span class="op">+</span><span class="st"> </span>DensityLn <span class="op">+</span><span class="st"> </span>Region,</span>
<span id="cb19-9"><a href="#cb19-9"></a>                 <span class="dt">data =</span> dat1_train, <span class="dt">method =</span> <span class="st">&quot;poisson&quot;</span>, </span>
<span id="cb19-10"><a href="#cb19-10"></a>                 <span class="dt">control =</span> <span class="kw">rpart.control</span> (<span class="dt">xval=</span><span class="dv">0</span>, <span class="dt">minbucket=</span><span class="dv">1000</span> ,<span class="dt">cp=</span><span class="dv">10</span><span class="op">^-</span><span class="dv">4</span>,</span>
<span id="cb19-11"><a href="#cb19-11"></a>                                <span class="dt">maxcompete =</span> <span class="dv">0</span>,<span class="dt">maxsurrogate =</span> <span class="dv">0</span>, <span class="dt">seed=</span><span class="dv">1</span>), </span>
<span id="cb19-12"><a href="#cb19-12"></a>                 <span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">shrink=</span><span class="dv">1</span>), <span class="dt">ncand=</span><span class="dv">5</span>,<span class="dt">ntrees =</span> ntrees0, </span>
<span id="cb19-13"><a href="#cb19-13"></a>                 <span class="dt">subsample =</span> <span class="fl">0.5</span>, <span class="dt">red_mem =</span> T)</span>
<span id="cb19-14"><a href="#cb19-14"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb19-15"><a href="#cb19-15"></a></span>
<span id="cb19-16"><a href="#cb19-16"></a><span class="co"># determine number of trees using validation error</span></span>
<span id="cb19-17"><a href="#cb19-17"></a></span>
<span id="cb19-18"><a href="#cb19-18"></a>fit_valid&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">nrow</span>(dat1_valid))</span>
<span id="cb19-19"><a href="#cb19-19"></a>error_valid&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,ntrees0)</span>
<span id="cb19-20"><a href="#cb19-20"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>ntrees0){</span>
<span id="cb19-21"><a href="#cb19-21"></a>  fit_valid&lt;-fit_valid <span class="op">+</span><span class="st"> </span><span class="kw">predict</span>(forest1<span class="op">$</span>trees[[i]], <span class="dt">newdata=</span>dat1_valid) <span class="op">*</span></span>
<span id="cb19-22"><a href="#cb19-22"></a><span class="st">    </span>dat1_valid<span class="op">$</span>Exposure</span>
<span id="cb19-23"><a href="#cb19-23"></a>  fit_valid_norm &lt;-<span class="st"> </span>fit_valid<span class="op">/</span>i</span>
<span id="cb19-24"><a href="#cb19-24"></a>  error_valid[i]&lt;-<span class="kw">Poisson.Deviance</span>(fit_valid_norm, dat1_valid<span class="op">$</span>ClaimNb)</span>
<span id="cb19-25"><a href="#cb19-25"></a>}</span>
<span id="cb19-26"><a href="#cb19-26"></a><span class="co"># png(&quot;./plots/1/random_forest_error.png&quot;)</span></span>
<span id="cb19-27"><a href="#cb19-27"></a><span class="kw">plot</span>(error_valid,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;number of trees&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;validation error in 10^-2&quot;</span>)</span>
<span id="cb19-28"><a href="#cb19-28"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">which.min</span>(error_valid),<span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb19-29"><a href="#cb19-29"></a><span class="co"># dev.off()</span></span>
<span id="cb19-30"><a href="#cb19-30"></a>(<span class="dt">best.trees=</span><span class="kw">which.min</span>(error_valid))</span>
<span id="cb19-31"><a href="#cb19-31"></a></span>
<span id="cb19-32"><a href="#cb19-32"></a><span class="co"># test error</span></span>
<span id="cb19-33"><a href="#cb19-33"></a></span>
<span id="cb19-34"><a href="#cb19-34"></a>fitRF&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">nrow</span>(dat1_test))</span>
<span id="cb19-35"><a href="#cb19-35"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>best.trees){</span>
<span id="cb19-36"><a href="#cb19-36"></a>  fitRF&lt;-fitRF<span class="op">+</span><span class="kw">predict</span>(forest1<span class="op">$</span>trees[[i]], <span class="dt">newdata=</span>dat1_test)<span class="op">*</span>dat1_test<span class="op">$</span>Exposure</span>
<span id="cb19-37"><a href="#cb19-37"></a>}</span>
<span id="cb19-38"><a href="#cb19-38"></a>dat1_test<span class="op">$</span>fitRF &lt;-<span class="st"> </span>fitRF<span class="op">/</span>best.trees</span>
<span id="cb19-39"><a href="#cb19-39"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitRF, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb19-40"><a href="#cb19-40"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitRF, dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb19-41"><a href="#cb19-41"></a><span class="kw">names</span>(forest1<span class="op">$</span>trees[[<span class="dv">2</span>]]<span class="op">$</span>variable.importance)</span>
<span id="cb19-42"><a href="#cb19-42"></a><span class="kw">sum</span>(forest1<span class="op">$</span>trees[[<span class="dv">3</span>]]<span class="op">$</span>variable.importance)</span></code></pre></div>
<p><img src="./plots/1/random_forest_error.png" width="60%"  style="display: block; margin: auto;" /></p>
</div>
<div id="泊松提升树" class="section level2">
<h2><span class="header-section-number">2.10</span> 泊松提升树</h2>
<ul>
<li><code>n.trees</code> 树的数量；<code>shrinkage</code> 学习步长，和树的数量成反比；<code>interaction.depth</code> 交互项深度；<code>bag.fraction</code> 每棵树使用的数据比例；<code>train.fraction</code> 训练集比例；<code>n.minobsinnode</code>叶子上最少样本量。</li>
</ul>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb20-2"><a href="#cb20-2"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb20-3"><a href="#cb20-3"></a>  gbm1 &lt;-</span>
<span id="cb20-4"><a href="#cb20-4"></a><span class="st">    </span><span class="kw">gbm</span>(</span>
<span id="cb20-5"><a href="#cb20-5"></a>      ClaimNb <span class="op">~</span><span class="st">  </span>VehPower <span class="op">+</span><span class="st"> </span>VehAge <span class="op">+</span><span class="st"> </span>DrivAge <span class="op">+</span><span class="st"> </span>BonusMalus <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">+</span><span class="st"> </span>VehGas <span class="op">+</span></span>
<span id="cb20-6"><a href="#cb20-6"></a><span class="st">        </span>Area <span class="op">+</span><span class="st"> </span>DensityLn <span class="op">+</span><span class="st"> </span>Region <span class="op">+</span><span class="st"> </span><span class="kw">offset</span>(<span class="kw">log</span>(Exposure)),</span>
<span id="cb20-7"><a href="#cb20-7"></a>      <span class="dt">data =</span> dat1_learn_gbm,</span>
<span id="cb20-8"><a href="#cb20-8"></a>      <span class="dt">distribution =</span> <span class="st">&quot;poisson&quot;</span>,</span>
<span id="cb20-9"><a href="#cb20-9"></a>      <span class="dt">n.trees =</span> <span class="dv">500</span>,</span>
<span id="cb20-10"><a href="#cb20-10"></a>      <span class="dt">shrinkage =</span> <span class="fl">0.1</span>,</span>
<span id="cb20-11"><a href="#cb20-11"></a>      <span class="dt">interaction.depth =</span> <span class="dv">5</span>,</span>
<span id="cb20-12"><a href="#cb20-12"></a>      <span class="dt">bag.fraction =</span> <span class="fl">0.5</span>,</span>
<span id="cb20-13"><a href="#cb20-13"></a>      <span class="dt">train.fraction =</span> train_pro,</span>
<span id="cb20-14"><a href="#cb20-14"></a>      <span class="dt">cv.folds =</span> <span class="dv">0</span>,</span>
<span id="cb20-15"><a href="#cb20-15"></a>      <span class="dt">n.minobsinnode =</span> <span class="dv">1000</span>,</span>
<span id="cb20-16"><a href="#cb20-16"></a>      <span class="dt">verbose =</span> T</span>
<span id="cb20-17"><a href="#cb20-17"></a>    )</span>
<span id="cb20-18"><a href="#cb20-18"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb20-19"><a href="#cb20-19"></a></span>
<span id="cb20-20"><a href="#cb20-20"></a><span class="co"># plot the performance</span></span>
<span id="cb20-21"><a href="#cb20-21"></a></span>
<span id="cb20-22"><a href="#cb20-22"></a><span class="co"># png(&quot;./plots/1/gbm_error.png&quot;)</span></span>
<span id="cb20-23"><a href="#cb20-23"></a><span class="kw">gbm.perf</span>(gbm1,<span class="dt">method=</span><span class="st">&quot;test&quot;</span>)</span>
<span id="cb20-24"><a href="#cb20-24"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>),</span>
<span id="cb20-25"><a href="#cb20-25"></a>       <span class="kw">c</span>(<span class="st">&quot;training error&quot;</span>, <span class="st">&quot;validation error&quot;</span>, <span class="st">&quot;best iterations&quot;</span>))</span>
<span id="cb20-26"><a href="#cb20-26"></a><span class="co"># dev.off()</span></span>
<span id="cb20-27"><a href="#cb20-27"></a></span>
<span id="cb20-28"><a href="#cb20-28"></a>best.iter&lt;-<span class="kw">gbm.perf</span>(gbm1,<span class="dt">method=</span><span class="st">&quot;test&quot;</span>)</span>
<span id="cb20-29"><a href="#cb20-29"></a>dat1_test<span class="op">$</span>fitGBM1&lt;-</span>
<span id="cb20-30"><a href="#cb20-30"></a><span class="st">  </span><span class="kw">predict</span>(gbm1, dat1_test,<span class="dt">n.trees=</span>best.iter,<span class="dt">type=</span><span class="st">&quot;response&quot;</span>)<span class="op">*</span>dat1_test<span class="op">$</span>Exposure</span>
<span id="cb20-31"><a href="#cb20-31"></a><span class="kw">keras_poisson_dev</span>(dat1_test<span class="op">$</span>fitGBM1,dat1_test<span class="op">$</span>ClaimNb)</span>
<span id="cb20-32"><a href="#cb20-32"></a><span class="kw">Poisson.Deviance</span>(dat1_test<span class="op">$</span>fitGBM1,dat1_test<span class="op">$</span>ClaimNb)</span></code></pre></div>
<ul>
<li>根据验证集损失确定迭代次数。</li>
</ul>
<p><img src="./plots/1/gbm_error.png" width="60%"  style="display: block; margin: auto;" /></p>
<ul>
<li>Variable importance</li>
</ul>
<pre><code>             rel.inf
BonusMalus 27.687137
VehAge     19.976441
VehBrand   13.515198
Region     13.495375
DrivAge     9.284520
VehGas      7.082648
VehPower    4.583522
DensityLn   4.375159
Area        0.000000</code></pre>
<ul>
<li>重要变量的边缘效应</li>
</ul>
<p><img src="./plots/1/gbm_mar1.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_mar2.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_mar3.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_mar4.png" width="40%"  style="display: block; margin: auto;" /></p>
<ul>
<li>重要变量的交互效应</li>
</ul>
<p><img src="./plots/1/gbm_int1.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_int2.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_int3.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_int4.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_int5.png" width="40%"  style="display: block; margin: auto;" /><img src="./plots/1/gbm_int6.png" width="40%"  style="display: block; margin: auto;" /></p>
</div>
<div id="模型比较" class="section level2">
<h2><span class="header-section-number">2.11</span> 模型比较</h2>
<pre><code>##                       model test_error test_error_keras
## 1                 Intercept    33.5695          21.7647
## 2                       GLM    31.7731          20.8665
## 3                 GLM Lasso    31.8132          20.8866
## 4                       GAM    31.6651          20.8125
## 5             Decision tree    30.9780          20.4690
## 6             Random forest    30.9652          20.4626
## 7 Generalized boosted model    30.8972          20.4286
## 8            Neural network    31.0607          20.5080</code></pre>
<ul>
<li>Boosting &gt; RF &gt; Tree &gt; GAM &gt; GLM &gt; Homo</li>
</ul>
<!--chapter:end:02-french.Rmd-->
</div>
</div>
<div id="nn" class="section level1">
<h1><span class="header-section-number">3</span> 神经网络</h1>
<p>正如计算机速度的提升和MCMC方法给贝叶斯统计带来了生机，计算机运算能力的提升和反向传播算法（back propagation）也给神经网络带来了飞速发展。</p>
<p><code>Tensorflow</code>和<code>Pytorch</code>的更新速度反映了这个领域的热度。</p>
<div id="建立神经网络的一般步骤" class="section level2">
<h2><span class="header-section-number">3.1</span> 建立神经网络的一般步骤</h2>
<div id="明确目标和数据类型" class="section level3">
<h3><span class="header-section-number">3.1.1</span> 明确目标和数据类型</h3>
<ul>
<li><p>神经网络是一种非参非线性回归模型，它可以刻画非线性效应和交互效应。</p></li>
<li><p>在使用神经网络前，需要理解研究目标和数据结构，有一些特殊的layer，如卷积层，专门为某种任务、或某种数据结构而设立，不是可以用在任何的数据上。</p></li>
<li><p>神经网络有大量的参数，全局最优解必然会造成过拟合，通常利用验证集损失来判断梯度下降的次数。</p></li>
</ul>
</div>
<div id="数据预处理" class="section level3">
<h3><span class="header-section-number">3.1.2</span> 数据预处理</h3>
<ul>
<li><p>描述性统计分析</p></li>
<li><p>缺失值、异常值处理</p></li>
<li><p>连续型变量标准化，主要是为了让梯度下降法更有效地工作。常用的标准化方法有<em>MinMaxScaler</em></p>
<p><span class="math display">\[x^*=2\frac{x-\min x}{\max x - \min x}-1\]</span></p></li>
<li><p>分类变量，可以使用dummy coding、one-hot encoding，或者使用神经网络中的embedding layer。第三种办法可以有效地减少参数个数。</p></li>
<li><p>训练-验证-测试数据分割：训练集用于梯度下降法求解参数，验证集用于判断epoch次数、调整模型结构的超参数，测试集用于比较不同模型的样本外预测能力。</p></li>
</ul>
</div>
<div id="选取合适的神经网络类型" class="section level3">
<h3><span class="header-section-number">3.1.3</span> 选取合适的神经网络类型</h3>
<ul>
<li><p>全连接神经网络：适用于一般的回归问题，如索赔频率预测。信息一直向前传递。参数个数较多。可解释性差。</p></li>
<li><p>卷积神经网络：适用于数据有空间结构，且相同的模式可能出现在不同的位置，如图像识别。信息一直向前传递。参数个数较少。有可解释性。</p></li>
<li><p>递归神经网络：适用于数据有时间序列特征，且相同的模式可能出现在不同时间点，如天气预报、语音识别。信息可以返回到前面的神经元。参数个数较多。可解释性差。</p></li>
</ul>
</div>
<div id="建立神经网络全连接神经网络" class="section level3">
<h3><span class="header-section-number">3.1.4</span> 建立神经网络（全连接神经网络）</h3>
<p>在建立神经网络时，需要考虑以下几点：</p>
<ul>
<li><p>输入层数据类型</p></li>
<li><p>隐藏层层数，隐藏层性质，神经元个数，激活函数，正则化，dropout</p></li>
<li><p>输出神经元数据类型，输出神经元激活函数</p></li>
<li><p>损失函数选择</p></li>
</ul>
</div>
<div id="训练神经网络" class="section level3">
<h3><span class="header-section-number">3.1.5</span> 训练神经网络</h3>
<p>在训练神经网络时，需要考虑以下几点</p>
<ul>
<li><p>梯度下降法（optimizer）</p></li>
<li><p>迭代次数（patience）</p></li>
<li><p>遍历次数（epoch），批量大小（batch size）</p></li>
</ul>
</div>
<div id="调参" class="section level3">
<h3><span class="header-section-number">3.1.6</span> 调参</h3>
<p>返回第4步，调整模型结构的超参数（hyper-parameter tuning），观察验证损失的变化，选取最终模型。</p>
</div>
</div>
<div id="数据预处理-1" class="section level2">
<h2><span class="header-section-number">3.2</span> 数据预处理</h2>
<pre><code>  &#39;data.frame&#39;: 678013 obs. of  12 variables:
   $ IDpol     : num  1 3 5 10 11 13 15 17 18 21 ...
   $ ClaimNb   : &#39;table&#39; num [1:678013(1d)] 1 1 1 1 1 1 1 1 1 1 ...
   $ Exposure  : num  0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ...
   $ VehPower  : int  5 5 6 7 7 6 6 7 7 7 ...
   $ VehAge    : int  0 0 2 0 0 2 2 0 0 0 ...
   $ DrivAge   : int  55 55 52 46 46 38 38 33 33 41 ...
   $ BonusMalus: int  50 50 50 50 50 50 50 68 68 50 ...
   $ VehBrand  : Factor w/ 11 levels &quot;B1&quot;,&quot;B10&quot;,&quot;B11&quot;,..: 4 4 4 4 4 4 4 4 4 4 ...
   $ VehGas    : chr  &quot;Regular&quot; &quot;Regular&quot; &quot;Diesel&quot; &quot;Diesel&quot; ...
   $ Area      : Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 4 4 2 2 2 5 5 3 3 2 ...
   $ Density   : int  1217 1217 54 76 76 3003 3003 137 137 60 ...
   $ Region    : Factor w/ 21 levels &quot;Alsace&quot;,&quot;Aquitaine&quot;,..: 21 21 18 2 2 16 16 13 13 17 ...</code></pre>
<p>在进行下面code之前，需要运行上一章的代码直到Tree之前。</p>
<ul>
<li>连续型变量：标准化处理。</li>
</ul>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a>PreProcess.Continuous &lt;-<span class="st"> </span><span class="cf">function</span>(var1, dat1){</span>
<span id="cb24-2"><a href="#cb24-2"></a>   <span class="kw">names</span>(dat1)[<span class="kw">names</span>(dat1) <span class="op">==</span><span class="st"> </span>var1]  &lt;-<span class="st"> &quot;V1&quot;</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>   dat1<span class="op">$</span>X &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(dat1<span class="op">$</span>V1)</span>
<span id="cb24-4"><a href="#cb24-4"></a>   dat1<span class="op">$</span>X &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>(dat1<span class="op">$</span>X<span class="op">-</span><span class="kw">min</span>(dat1<span class="op">$</span>X))<span class="op">/</span>(<span class="kw">max</span>(dat1<span class="op">$</span>X)<span class="op">-</span><span class="kw">min</span>(dat1<span class="op">$</span>X))<span class="op">-</span><span class="dv">1</span></span>
<span id="cb24-5"><a href="#cb24-5"></a>   <span class="kw">names</span>(dat1)[<span class="kw">names</span>(dat1) <span class="op">==</span><span class="st"> &quot;V1&quot;</span>]  &lt;-<span class="st"> </span>var1</span>
<span id="cb24-6"><a href="#cb24-6"></a>   <span class="kw">names</span>(dat1)[<span class="kw">names</span>(dat1) <span class="op">==</span><span class="st"> &quot;X&quot;</span>]  &lt;-<span class="st"> </span><span class="kw">paste</span>(var1,<span class="st">&quot;X&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb24-7"><a href="#cb24-7"></a>   dat1</span>
<span id="cb24-8"><a href="#cb24-8"></a>   }</span>
<span id="cb24-9"><a href="#cb24-9"></a></span>
<span id="cb24-10"><a href="#cb24-10"></a>Features.PreProcess &lt;-<span class="st"> </span><span class="cf">function</span>(dat1){</span>
<span id="cb24-11"><a href="#cb24-11"></a>   dat1<span class="op">$</span>VehPower &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>VehPower,<span class="dv">9</span>)</span>
<span id="cb24-12"><a href="#cb24-12"></a>   dat1 &lt;-<span class="st"> </span><span class="kw">PreProcess.Continuous</span>(<span class="st">&quot;VehPower&quot;</span>, dat1)   </span>
<span id="cb24-13"><a href="#cb24-13"></a>   dat1<span class="op">$</span>VehAge &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>VehAge,<span class="dv">20</span>)</span>
<span id="cb24-14"><a href="#cb24-14"></a>   dat1 &lt;-<span class="st"> </span><span class="kw">PreProcess.Continuous</span>(<span class="st">&quot;VehAge&quot;</span>, dat1)   </span>
<span id="cb24-15"><a href="#cb24-15"></a>   dat1<span class="op">$</span>DrivAge &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>DrivAge,<span class="dv">90</span>)</span>
<span id="cb24-16"><a href="#cb24-16"></a>   dat1 &lt;-<span class="st"> </span><span class="kw">PreProcess.Continuous</span>(<span class="st">&quot;DrivAge&quot;</span>, dat1)   </span>
<span id="cb24-17"><a href="#cb24-17"></a>   dat1<span class="op">$</span>BonusMalus &lt;-<span class="st"> </span><span class="kw">pmin</span>(dat1<span class="op">$</span>BonusMalus,<span class="dv">150</span>)</span>
<span id="cb24-18"><a href="#cb24-18"></a>   dat1 &lt;-<span class="st"> </span><span class="kw">PreProcess.Continuous</span>(<span class="st">&quot;BonusMalus&quot;</span>, dat1)   </span>
<span id="cb24-19"><a href="#cb24-19"></a>   dat1<span class="op">$</span>VehBrandX &lt;-<span class="st"> </span><span class="kw">as.integer</span>(dat1<span class="op">$</span>VehBrand)<span class="op">-</span><span class="dv">1</span>  <span class="co"># categorical variable</span></span>
<span id="cb24-20"><a href="#cb24-20"></a>   dat1<span class="op">$</span>VehGas &lt;-<span class="st"> </span><span class="kw">as.factor</span>(dat1<span class="op">$</span>VehGas)</span>
<span id="cb24-21"><a href="#cb24-21"></a>   dat1<span class="op">$</span>VehGasX &lt;-<span class="st"> </span><span class="kw">as.integer</span>(dat1<span class="op">$</span>VehGas) <span class="op">-</span><span class="st"> </span><span class="fl">1.5</span> <span class="co"># binary: continuous or categorical</span></span>
<span id="cb24-22"><a href="#cb24-22"></a>   dat1 &lt;-<span class="st"> </span><span class="kw">PreProcess.Continuous</span>(<span class="st">&quot;Area&quot;</span>, dat1)   </span>
<span id="cb24-23"><a href="#cb24-23"></a>   dat1 &lt;-<span class="st"> </span><span class="kw">PreProcess.Continuous</span>(<span class="st">&quot;Density&quot;</span>, dat1)   </span>
<span id="cb24-24"><a href="#cb24-24"></a>   dat1<span class="op">$</span>RegionX &lt;-<span class="st"> </span><span class="kw">as.integer</span>(dat1<span class="op">$</span>Region) <span class="op">-</span><span class="st"> </span><span class="dv">1</span> <span class="co"># categorical</span></span>
<span id="cb24-25"><a href="#cb24-25"></a>   dat1</span>
<span id="cb24-26"><a href="#cb24-26"></a>}</span>
<span id="cb24-27"><a href="#cb24-27"></a></span>
<span id="cb24-28"><a href="#cb24-28"></a>dat2 &lt;-<span class="st"> </span><span class="kw">Features.PreProcess</span>(freMTPL2freq)   </span>
<span id="cb24-29"><a href="#cb24-29"></a><span class="kw">names</span>(dat2)</span>
<span id="cb24-30"><a href="#cb24-30"></a>dat2_train&lt;-dat2[index_train,]</span>
<span id="cb24-31"><a href="#cb24-31"></a>dat2_valid&lt;-dat2[index_valid,]</span>
<span id="cb24-32"><a href="#cb24-32"></a>dat2_test&lt;-dat2[index_test,]</span>
<span id="cb24-33"><a href="#cb24-33"></a>dat2_learn&lt;-dat2[index_learn,]</span></code></pre></div>
<ul>
<li><p>分类变量： 采用embedding layer。</p></li>
<li><p>训练集-验证集-测试集：分层抽样。</p></li>
<li><p>调整数据结构，使其匹配神经网络的输入层及其维度。输入数据集包括<code>Xtrain, Brtain, Retrain, Vtrain</code>，因变量数据集为<code>Ytrain</code>。</p></li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a>lambda.hom &lt;-<span class="st"> </span><span class="kw">sum</span>(dat2_train<span class="op">$</span>ClaimNb)<span class="op">/</span><span class="kw">sum</span>(dat2_train<span class="op">$</span>Exposure);lambda.hom</span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="kw">names</span>(dat2)</span>
<span id="cb25-3"><a href="#cb25-3"></a></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="co"># index of continous variables (non-categorical)</span></span>
<span id="cb25-5"><a href="#cb25-5"></a></span>
<span id="cb25-6"><a href="#cb25-6"></a>features &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">13</span><span class="op">:</span><span class="dv">16</span>, <span class="dv">18</span><span class="op">:</span><span class="dv">20</span>)</span>
<span id="cb25-7"><a href="#cb25-7"></a><span class="kw">names</span>(dat2_learn)[features]</span>
<span id="cb25-8"><a href="#cb25-8"></a>(q0 &lt;-<span class="st"> </span><span class="kw">length</span>(features))</span>
<span id="cb25-9"><a href="#cb25-9"></a></span>
<span id="cb25-10"><a href="#cb25-10"></a><span class="co"># training data</span></span>
<span id="cb25-11"><a href="#cb25-11"></a></span>
<span id="cb25-12"><a href="#cb25-12"></a>Xtrain&lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_train[, features])  <span class="co"># design matrix learning sample</span></span>
<span id="cb25-13"><a href="#cb25-13"></a>Brtrain &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_train<span class="op">$</span>VehBrandX)</span>
<span id="cb25-14"><a href="#cb25-14"></a>Retrain &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_train<span class="op">$</span>RegionX)</span>
<span id="cb25-15"><a href="#cb25-15"></a>Ytrain&lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_train<span class="op">$</span>ClaimNb)</span>
<span id="cb25-16"><a href="#cb25-16"></a>Vtrain&lt;-<span class="kw">as.matrix</span>(<span class="kw">log</span>(dat2_train<span class="op">$</span>Exposure<span class="op">*</span>lambda.hom))</span>
<span id="cb25-17"><a href="#cb25-17"></a></span>
<span id="cb25-18"><a href="#cb25-18"></a><span class="co"># validation data </span></span>
<span id="cb25-19"><a href="#cb25-19"></a></span>
<span id="cb25-20"><a href="#cb25-20"></a>Xvalid&lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_valid[, features])  <span class="co"># design matrix learning sample</span></span>
<span id="cb25-21"><a href="#cb25-21"></a>Brvalid &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_valid<span class="op">$</span>VehBrandX)</span>
<span id="cb25-22"><a href="#cb25-22"></a>Revalid &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_valid<span class="op">$</span>RegionX)</span>
<span id="cb25-23"><a href="#cb25-23"></a>Yvalid&lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_valid<span class="op">$</span>ClaimNb)</span>
<span id="cb25-24"><a href="#cb25-24"></a>Vvalid&lt;-<span class="kw">as.matrix</span>(<span class="kw">log</span>(dat2_valid<span class="op">$</span>Exposure<span class="op">*</span>lambda.hom))</span>
<span id="cb25-25"><a href="#cb25-25"></a>xxvalid&lt;-<span class="kw">list</span>(Xvalid,Brvalid,Revalid,Vvalid)</span>
<span id="cb25-26"><a href="#cb25-26"></a></span>
<span id="cb25-27"><a href="#cb25-27"></a><span class="co"># testing data</span></span>
<span id="cb25-28"><a href="#cb25-28"></a></span>
<span id="cb25-29"><a href="#cb25-29"></a>Xtest &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_test[, features])    <span class="co"># design matrix test sample</span></span>
<span id="cb25-30"><a href="#cb25-30"></a>Brtest &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_test<span class="op">$</span>VehBrandX)</span>
<span id="cb25-31"><a href="#cb25-31"></a>Retest &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_test<span class="op">$</span>RegionX)</span>
<span id="cb25-32"><a href="#cb25-32"></a>Ytest &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dat2_test<span class="op">$</span>ClaimNb)</span>
<span id="cb25-33"><a href="#cb25-33"></a>Vtest &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">log</span>(dat2_test<span class="op">$</span>Exposure<span class="op">*</span>lambda.hom))</span></code></pre></div>
</div>
<div id="神经网络提升模型-combined-actuarial-neural-network" class="section level2">
<h2><span class="header-section-number">3.3</span> 神经网络提升模型 （combined actuarial neural network）</h2>
<p>基本结构</p>
<p><span class="math display">\[\ln \lambda(\mathbf{x})= e\hat{\lambda}^{\text{GAM}}(\mathbf{x})\hat{\lambda}^{\text{NN}}(\mathbf{x})\]</span></p>
<p>其中，<span class="math inline">\(\hat{\lambda}^{\text{GAM}}\)</span>为广义可加边缘提升模型的索赔频率估计值（参见上一章），<span class="math inline">\(\hat{\lambda}^{\text{NN}}\)</span>为神经网络索赔频率的估计值，第一项在模型训练中保持不变。</p>
<p>使用上述模型的优点：</p>
<ul>
<li><p><span class="math inline">\(\hat{\lambda}^{\text{GAM}}\)</span>的部分可解释性。</p></li>
<li><p>神经网络从一个相对“较好”的初始状态<span class="math inline">\(e\hat{\lambda}^{\text{GAM}}(\mathbf{x})\)</span>开始训练，很快收敛。</p></li>
</ul>
<p>在代码实现中，可以把<span class="math inline">\(e\hat{\lambda}^{\text{GAM}}\)</span>当作伪风险暴露数。</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a>CANN &lt;-<span class="st"> </span><span class="dv">1</span>  <span class="co"># 0 = normal NN, 1=CANN</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="cf">if</span> (CANN<span class="op">==</span><span class="dv">1</span>){</span>
<span id="cb26-3"><a href="#cb26-3"></a>     Vtrain &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">log</span>(dat1_train<span class="op">$</span>fitGAM1))</span>
<span id="cb26-4"><a href="#cb26-4"></a>     Vvalid&lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">log</span>(dat1_valid<span class="op">$</span>fitGAM1))</span>
<span id="cb26-5"><a href="#cb26-5"></a>     Vtest &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">log</span>(dat1_test<span class="op">$</span>fitGAM1))</span>
<span id="cb26-6"><a href="#cb26-6"></a>}</span></code></pre></div>
</div>
<div id="神经网络结构" class="section level2">
<h2><span class="header-section-number">3.4</span> 神经网络结构</h2>
<p><img src="./plots/1/nn-structure.png" width="60%" style="display: block; margin: auto;" /></p>
<p>在构建神经网络时，需要注意以下几点：</p>
<div id="结构参数" class="section level3">
<h3><span class="header-section-number">3.4.1</span> 结构参数</h3>
<ol style="list-style-type: decimal">
<li>神经网络结构的超参数选择，<code>BrLabel</code>为车型个数，<code>ReLabel</code>为地区个数，<code>q1-q4</code>为四个隐藏层中神经元个数，<code>d</code>为embedding layer中神经元个数。</li>
</ol>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># hyperparameters of the neural network architecture</span></span>
<span id="cb27-2"><a href="#cb27-2"></a></span>
<span id="cb27-3"><a href="#cb27-3"></a>(BrLabel &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">unique</span>(dat2_train<span class="op">$</span>VehBrandX)))</span>
<span id="cb27-4"><a href="#cb27-4"></a>(ReLabel &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">unique</span>(dat2_train<span class="op">$</span>RegionX)))</span>
<span id="cb27-5"><a href="#cb27-5"></a>q1 &lt;-<span class="st"> </span><span class="dv">20</span>   </span>
<span id="cb27-6"><a href="#cb27-6"></a>q2 &lt;-<span class="st"> </span><span class="dv">15</span></span>
<span id="cb27-7"><a href="#cb27-7"></a>q3 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb27-8"><a href="#cb27-8"></a>q4 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb27-9"><a href="#cb27-9"></a>d &lt;-<span class="st"> </span><span class="dv">1</span>        <span class="co"># dimensions embedding layers for categorical features</span></span></code></pre></div>
</div>
<div id="输入层" class="section level3">
<h3><span class="header-section-number">3.4.2</span> 输入层</h3>
<ol start="2" style="list-style-type: decimal">
<li><p>输入层包括<code>Design, VehBrand, Region, LogVol</code>，其中<code>Design</code>为连续型协变量的输入层，<code>VehBrand, Region</code>为分类变量的输入层，<code>LogVol</code>直接连接到模型输出神经元。</p></li>
<li><p><code>shape</code>表示输入（输出）维度（神经元个数）,<code>dtype</code>表示数据类型，<code>name</code>表示层名。</p></li>
<li><p><code>shape=(None, 7)</code> 中<code>None</code>表示样本大小，因为还没有数据进入神经网络，故此时不确定。</p>
<p>Tensor(“Design_1:0”, shape=(None, 7), dtype=float32)</p></li>
</ol>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># input layer</span></span>
<span id="cb28-2"><a href="#cb28-2"></a></span>
<span id="cb28-3"><a href="#cb28-3"></a>(Design   &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(q0),  <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Design&#39;</span>))</span>
<span id="cb28-4"><a href="#cb28-4"></a>(VehBrand &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">1</span>),   <span class="dt">dtype =</span> <span class="st">&#39;int32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;VehBrand&#39;</span>))</span>
<span id="cb28-5"><a href="#cb28-5"></a>(Region   &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">1</span>),   <span class="dt">dtype =</span> <span class="st">&#39;int32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Region&#39;</span>))</span>
<span id="cb28-6"><a href="#cb28-6"></a>(LogVol   &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">1</span>),   <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;LogVol&#39;</span>))</span></code></pre></div>
</div>
<div id="embedding-layer" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Embedding layer</h3>
<ol start="5" style="list-style-type: decimal">
<li><p>建立一个layer，需要明确输入神经元个数<code>input_dim</code>和输出神经元个数<code>output_dim</code>，通常需要指定输出神经元个数，而输入神经元个数由它的上层输出神经元个数决定。</p></li>
<li><p>把分类变量用<code>layer_embedding</code>处理，这两个<code>layer_embedding</code>的输出神经元个数为<span class="math inline">\(d\)</span>，即每个水平通过<code>layer_embedding</code>输出<span class="math inline">\(d\)</span>个连续型变量，当<span class="math inline">\(d=1\)</span>，<code>layer_embedding</code>类似于GLM对分类变量的处理。<code>input_length</code>主要用于时间序列数据，如每个样本为多个词组成的一句话，这里一个样本只有一个水平，故<code>input_length = 1</code>。</p></li>
<li><p><code>layer_flatten</code>用于调整维度，<code>layer_flatten</code>的输入维度是<span class="math inline">\(n\times 1\times d\)</span>，输出维度是<span class="math inline">\(n\times d\)</span>，该层没有参数。该输出维度是<code>layer_dense</code>要求的输入维度。建立神经网络需要注意层间维度匹配。</p></li>
<li><p><code>BrandEmb</code>建立的映射为<span class="math inline">\(\{1,\ldots,11\}\rightarrow 1\times\mathbf{R}^d\rightarrow\mathbf{R}^d\)</span>. <code>RegionEmb</code>建立的映射为<span class="math inline">\(\{1,\ldots,21\}\rightarrow 1\times\mathbf{R}^d\rightarrow\mathbf{R}^d\)</span></p></li>
</ol>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="co"># embedding layer</span></span>
<span id="cb29-2"><a href="#cb29-2"></a></span>
<span id="cb29-3"><a href="#cb29-3"></a>(<span class="dt">BrandEmb =</span> VehBrand <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="st">    </span><span class="kw">layer_embedding</span>(<span class="dt">input_dim =</span> BrLabel, <span class="dt">output_dim =</span> d, </span>
<span id="cb29-5"><a href="#cb29-5"></a>                    <span class="dt">input_length =</span> <span class="dv">1</span>, <span class="dt">name =</span> <span class="st">&#39;BrandEmb&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb29-6"><a href="#cb29-6"></a><span class="st">   </span><span class="kw">layer_flatten</span>(<span class="dt">name=</span><span class="st">&#39;Brand_flat&#39;</span>))</span>
<span id="cb29-7"><a href="#cb29-7"></a><span class="co"># input_dim is the size of vocabulary; input_length is the length of input sequences</span></span>
<span id="cb29-8"><a href="#cb29-8"></a>    </span>
<span id="cb29-9"><a href="#cb29-9"></a>(<span class="dt">RegionEmb =</span> Region <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb29-10"><a href="#cb29-10"></a><span class="st">      </span><span class="kw">layer_embedding</span>(<span class="dt">input_dim =</span> ReLabel, <span class="dt">output_dim =</span> d, </span>
<span id="cb29-11"><a href="#cb29-11"></a>                      <span class="dt">input_length =</span> <span class="dv">1</span>, <span class="dt">name =</span> <span class="st">&#39;RegionEmb&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb29-12"><a href="#cb29-12"></a><span class="st">      </span><span class="kw">layer_flatten</span>(<span class="dt">name=</span><span class="st">&#39;Region_flat&#39;</span>))</span></code></pre></div>
</div>
<div id="隐藏层" class="section level3">
<h3><span class="header-section-number">3.4.4</span> 隐藏层</h3>
<p>9 <code>Network</code>建立的映射为<span class="math display">\[[-1,1]^{q0}\times\mathbf{R}^d\times\mathbf{R}^d\rightarrow (-1,1)^{q1}\rightarrow (-1,1)^{q2}\\ \rightarrow (-1,1)^{q3}\rightarrow (-1,1)^{q4}\rightarrow\mathbf{R}\]</span></p>
<ol start="10" style="list-style-type: decimal">
<li><p><code>layer_concatenate</code>把三个输入层连起来，<code>layer_dropout</code>为防止过拟合，<code>layer_batch_normalization</code>为防止vanishing gradient problem，这三种层内无参数，且不会改变上层的维度。<code>layer_dropout</code>令一定比例的上层神经元为0，正则化方法还包括在<code>layer_dense</code>中使用<span class="math inline">\(L^2\)</span>范数正则化<code>kernel_regularizer = regularizer_l2</code>。<code>layer_batch_normalization</code>把输出神经元映射到<span class="math inline">\((-1,1)\)</span>，通常在激活函数为<code>relu</code>更有用。</p></li>
<li><p>常用的激活函数为<code>tanh, relu, linear, exponential, softmax, sigmoid</code>。其中，<code>sigmoid, softmax</code>适用于二分类和多分类的输出神经元，<code>exponential</code>适用于因变量为正，如此时的索赔频率预测。此外<code>sigmoid</code>和<code>tanh</code>有线性关系，可以只考虑其中一个。</p></li>
<li><p><code>layer_dense</code>的映射为<code>output = activation (dot (input, kernal) + bias)</code>，所以每个输出神经元都含有输入神经元的信息。如果考虑多个全连接层，可以刻画协变量的交互效应现。激活函数如果取非线性函数，则可以刻画协变量的非线性效应。</p></li>
<li><p><code>Network</code>中最后一层的参数设定为0，使得<code>Network</code>初始值为0，这样神经网络初始状态为GAM，梯度下降将从GAM开始。</p></li>
</ol>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a>Network =<span class="st"> </span><span class="kw">list</span>(Design, BrandEmb, RegionEmb) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">layer_concatenate</span>(<span class="dt">name=</span><span class="st">&#39;concate&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q1, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden1&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-3"><a href="#cb30-3"></a><span class="st">          </span><span class="kw">layer_batch_normalization</span>()<span class="op">%&gt;%</span></span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="st">          </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span><span class="fl">0.05</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q2, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden2&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-6"><a href="#cb30-6"></a><span class="st">          </span><span class="kw">layer_batch_normalization</span>()<span class="op">%&gt;%</span></span>
<span id="cb30-7"><a href="#cb30-7"></a><span class="st">          </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span><span class="fl">0.05</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-8"><a href="#cb30-8"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q3, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden3&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-9"><a href="#cb30-9"></a><span class="st">          </span><span class="kw">layer_batch_normalization</span>()<span class="op">%&gt;%</span></span>
<span id="cb30-10"><a href="#cb30-10"></a><span class="st">          </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span><span class="fl">0.05</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-11"><a href="#cb30-11"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q4, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden4&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-12"><a href="#cb30-12"></a><span class="st">          </span><span class="kw">layer_batch_normalization</span>()<span class="op">%&gt;%</span></span>
<span id="cb30-13"><a href="#cb30-13"></a><span class="st">          </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span><span class="fl">0.05</span>) <span class="op">%&gt;%</span></span>
<span id="cb30-14"><a href="#cb30-14"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span><span class="dv">1</span>, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">name=</span><span class="st">&#39;Network&#39;</span>,</span>
<span id="cb30-15"><a href="#cb30-15"></a>                      <span class="dt">weights =</span> <span class="kw">list</span>(<span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(q4,<span class="dv">1</span>)), <span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>))))</span></code></pre></div>
</div>
<div id="输出层" class="section level3">
<h3><span class="header-section-number">3.4.5</span> 输出层</h3>
<ol start="14" style="list-style-type: decimal">
<li><p><code>Response</code>建立的映射为<span class="math inline">\(\mathbf{R}\times \mathbf{R}\rightarrow \mathbf{R}^+\)</span>，且要求该映射中的参数不参加梯度下降法。可以看到<code>Network</code>的输出神经元为<span class="math inline">\(\ln \hat{\lambda}^{\text{NN}}(\mathbf{x})\)</span>，输入层<code>LogVol</code>为<span class="math inline">\(\ln e\hat{\lambda}^{\text{GAM}}(\mathbf{x})\)</span>，<code>Response</code>的输出神经元为<span class="math display">\[\exp\left(\ln \hat{\lambda}^{\text{NN}}(\mathbf{x}) + \ln e\hat{\lambda}^{\text{GAM}}(\mathbf{x})\right)=e\hat{\lambda}^{\text{GAM}}(\mathbf{x})\hat{\lambda}^{\text{NN}}(\mathbf{x}).\]</span></p></li>
<li><p>通过梯度下降法使得输出神经元<span class="math inline">\(e\hat{\lambda}^{\text{GAM}}(\mathbf{x})\hat{\lambda}^{\text{NN}}(\mathbf{x})\)</span>与观察值<span class="math inline">\(N\)</span>最接近（用泊松偏差损失度量），进而训练神经网络<span class="math inline">\(\hat{\lambda}^{\text{NN}}(\mathbf{x})\)</span>中的参数。</p></li>
<li><p>Keras定义平均泊松偏差损失为</p></li>
</ol>
<p><span class="math display">\[\tilde{\mathcal{L}}(\mathbf{N},\mathbf{\hat{N}})=\frac{1}{|\mathbf{N}|}\sum_{i}\left[\hat{N}_i-N_i\ln\left(\hat{N}_i\right)\right]\]</span></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a>Response =<span class="st"> </span><span class="kw">list</span>(Network, LogVol) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">layer_add</span>(<span class="dt">name=</span><span class="st">&#39;Add&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="st">           </span><span class="kw">layer_dense</span>(<span class="dt">units=</span><span class="dv">1</span>, <span class="dt">activation=</span>k_exp, <span class="dt">name =</span> <span class="st">&#39;Response&#39;</span>, <span class="dt">trainable=</span><span class="ot">FALSE</span>,</span>
<span id="cb31-3"><a href="#cb31-3"></a>                        <span class="dt">weights=</span><span class="kw">list</span>(<span class="kw">array</span>(<span class="dv">1</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)), <span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>))))</span>
<span id="cb31-4"><a href="#cb31-4"></a></span>
<span id="cb31-5"><a href="#cb31-5"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> <span class="kw">c</span>(Design, VehBrand, Region, LogVol), <span class="dt">outputs =</span> <span class="kw">c</span>(Response))</span>
<span id="cb31-6"><a href="#cb31-6"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;poisson&#39;</span>)</span>
<span id="cb31-7"><a href="#cb31-7"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<ol start="17" style="list-style-type: decimal">
<li>下表列出了神经网络的结构，包括层的名称、(层的特性)、输出神经元个数、参数个数、上层的名称。</li>
</ol>
<pre><code>Model: &quot;model&quot;
________________________________________________________________________________________________________________________________
Layer (type)                              Output Shape                Param #        Connected to                               
================================================================================================================================
VehBrand (InputLayer)                     [(None, 1)]                 0                                                         
________________________________________________________________________________________________________________________________
Region (InputLayer)                       [(None, 1)]                 0                                                         
________________________________________________________________________________________________________________________________
BrandEmb (Embedding)                      (None, 1, 1)                11             VehBrand[0][0]                             
________________________________________________________________________________________________________________________________
RegionEmb (Embedding)                     (None, 1, 1)                21             Region[0][0]                               
________________________________________________________________________________________________________________________________
Design (InputLayer)                       [(None, 7)]                 0                                                         
________________________________________________________________________________________________________________________________
Brand_flat (Flatten)                      (None, 1)                   0              BrandEmb[0][0]                             
________________________________________________________________________________________________________________________________
Region_flat (Flatten)                     (None, 1)                   0              RegionEmb[0][0]                            
________________________________________________________________________________________________________________________________
concate (Concatenate)                     (None, 9)                   0              Design[0][0]                               
                                                                                     Brand_flat[0][0]                           
                                                                                     Region_flat[0][0]                          
________________________________________________________________________________________________________________________________
hidden1 (Dense)                           (None, 20)                  200            concate[0][0]                              
________________________________________________________________________________________________________________________________
batch_normalization_1 (BatchNormalization (None, 20)                  80             hidden1[0][0]                              
________________________________________________________________________________________________________________________________
dropout (Dropout)                         (None, 20)                  0              batch_normalization_1[0][0]                
________________________________________________________________________________________________________________________________
hidden2 (Dense)                           (None, 15)                  315            dropout[0][0]                              
________________________________________________________________________________________________________________________________
batch_normalization_2 (BatchNormalization (None, 15)                  60             hidden2[0][0]                              
________________________________________________________________________________________________________________________________
dropout_1 (Dropout)                       (None, 15)                  0              batch_normalization_2[0][0]                
________________________________________________________________________________________________________________________________
hidden3 (Dense)                           (None, 10)                  160            dropout_1[0][0]                            
________________________________________________________________________________________________________________________________
batch_normalization_3 (BatchNormalization (None, 10)                  40             hidden3[0][0]                              
________________________________________________________________________________________________________________________________
dropout_2 (Dropout)                       (None, 10)                  0              batch_normalization_3[0][0]                
________________________________________________________________________________________________________________________________
hidden4 (Dense)                           (None, 5)                   55             dropout_2[0][0]                            
________________________________________________________________________________________________________________________________
batch_normalization_4 (BatchNormalization (None, 5)                   20             hidden4[0][0]                              
________________________________________________________________________________________________________________________________
dropout_3 (Dropout)                       (None, 5)                   0              batch_normalization_4[0][0]                
________________________________________________________________________________________________________________________________
Network (Dense)                           (None, 1)                   6              dropout_3[0][0]                            
________________________________________________________________________________________________________________________________
LogVol (InputLayer)                       [(None, 1)]                 0                                                         
________________________________________________________________________________________________________________________________
Add (Add)                                 (None, 1)                   0              Network[0][0]                              
                                                                                     LogVol[0][0]                               
________________________________________________________________________________________________________________________________
Response (Dense)                          (None, 1)                   2              Add[0][0]                                  
================================================================================================================================
Total params: 970
Trainable params: 868
Non-trainable params: 102
________________________________________________________________________________________________________________________________</code></pre>
</div>
</div>
<div id="训练神经网络-1" class="section level2">
<h2><span class="header-section-number">3.5</span> 训练神经网络</h2>
<p>训练神经网络需要注意以下几点：</p>
<ol style="list-style-type: decimal">
<li><p>初始化神经网络，将从GAM开始训练神经网络，且GAM预测部分保持不变。</p></li>
<li><p>当<code>batch_size</code>为全体训练集时，为<em>steepest gradient decent method</em>，参数在一个<code>epoch</code>只迭代一次。</p></li>
<li><p>当<code>batch_size</code>比全体训练集小时，为<em>stochastic gradient decent method</em>，参数在一个<code>epoch</code>迭代次数约为<code>training size / batch size</code>。</p></li>
<li><p>梯度下降法常引入<em>momentum</em>，进而提升优化效率，如<code>adam, nadam,rmsprop</code>等，这些算法自动选择<code>learning rate, momentum parameters</code>等。</p></li>
<li><p><code>callback_early_stopping (monitor = "val_loss", patience =10)</code>表示如果验证集损失在10次内没有提升，那么停止训练，由此可以控制迭代次数。</p></li>
<li><p>使用<code>predict</code>在测试集上预测。</p></li>
</ol>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># fitting the neural network</span></span>
<span id="cb33-2"><a href="#cb33-2"></a>early_stop &lt;-<span class="st"> </span><span class="kw">callback_early_stopping</span>(<span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">patience =</span><span class="dv">10</span>)</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="co"># print_dot_callback &lt;- callback_lambda(</span></span>
<span id="cb33-4"><a href="#cb33-4"></a><span class="co">#   on_epoch_end = function(epoch, logs) {</span></span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="co">#     if (epoch %% 50 == 0) cat(&quot;\n&quot;)</span></span>
<span id="cb33-6"><a href="#cb33-6"></a><span class="co">#     cat(&quot;.&quot;)</span></span>
<span id="cb33-7"><a href="#cb33-7"></a><span class="co">#   }</span></span>
<span id="cb33-8"><a href="#cb33-8"></a><span class="co"># )  </span></span>
<span id="cb33-9"><a href="#cb33-9"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>();</span>
<span id="cb33-10"><a href="#cb33-10"></a>fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">list</span>(Xtrain, Brtrain, Retrain, Vtrain), Ytrain,</span>
<span id="cb33-11"><a href="#cb33-11"></a>                     <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">batch_size=</span><span class="dv">5000</span>, <span class="dt">verbose=</span><span class="dv">1</span>,</span>
<span id="cb33-12"><a href="#cb33-12"></a>                     <span class="dt">validation_data=</span><span class="kw">list</span>(xxvalid,Yvalid),</span>
<span id="cb33-13"><a href="#cb33-13"></a>                     <span class="dt">callbacks=</span><span class="kw">list</span>(early_stop));</span>
<span id="cb33-14"><a href="#cb33-14"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb33-15"><a href="#cb33-15"></a></span>
<span id="cb33-16"><a href="#cb33-16"></a><span class="co"># png(&quot;./plots/1/nn.png&quot;)</span></span>
<span id="cb33-17"><a href="#cb33-17"></a><span class="kw">matplot</span>(<span class="kw">cbind</span>(fit<span class="op">$</span>metrics<span class="op">$</span>loss,fit<span class="op">$</span>metrics<span class="op">$</span>val_loss), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;epoch&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Keras Poisson Loss&quot;</span>)</span>
<span id="cb33-18"><a href="#cb33-18"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="kw">c</span>(<span class="st">&quot;training loss&quot;</span>,<span class="st">&quot;validation loss&quot;</span>),<span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span>
<span id="cb33-19"><a href="#cb33-19"></a><span class="co"># dev.off()</span></span>
<span id="cb33-20"><a href="#cb33-20"></a></span>
<span id="cb33-21"><a href="#cb33-21"></a><span class="co"># calculating the predictions</span></span>
<span id="cb33-22"><a href="#cb33-22"></a>dat2_test<span class="op">$</span>fitNN &lt;-<span class="st"> </span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">list</span>(Xtest, Brtest, Retest, Vtest)))</span>
<span id="cb33-23"><a href="#cb33-23"></a><span class="kw">keras_poisson_dev</span>(dat2_test<span class="op">$</span>fitNN, dat2_test<span class="op">$</span>ClaimNb)</span>
<span id="cb33-24"><a href="#cb33-24"></a><span class="kw">Poisson.Deviance</span>(dat2_test<span class="op">$</span>fitNN, dat2_test<span class="op">$</span>ClaimNb)</span></code></pre></div>
<p><img src="./plots/1/nn.png" width="60%"  style="display: block; margin: auto;" /></p>
</div>
<div id="总结" class="section level2">
<h2><span class="header-section-number">3.6</span> 总结</h2>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a>dev_sum&lt;-<span class="kw">fread</span>(<span class="st">&quot;./plots/1/dev_sum.csv&quot;</span>)[,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb34-2"><a href="#cb34-2"></a>AD&lt;-<span class="kw">data.frame</span>(<span class="dt">model=</span><span class="st">&quot;Neural network&quot;</span>,<span class="dt">test_error=</span><span class="dv">0</span>,<span class="dt">test_error_keras=</span><span class="dv">0</span>)</span>
<span id="cb34-3"><a href="#cb34-3"></a>dev_sum&lt;-<span class="kw">rbind</span>(dev_sum,AD)</span>
<span id="cb34-4"><a href="#cb34-4"></a>dev_sum<span class="op">$</span>test_error[<span class="dv">8</span>]&lt;-<span class="kw">round</span>(<span class="kw">Poisson.Deviance</span>(dat2_test<span class="op">$</span>fitNN, dat2_test<span class="op">$</span>ClaimNb),<span class="dv">4</span>)</span>
<span id="cb34-5"><a href="#cb34-5"></a>dev_sum<span class="op">$</span>test_error_keras[<span class="dv">8</span>]&lt;-<span class="kw">round</span>(<span class="kw">keras_poisson_dev</span>(dat2_test<span class="op">$</span>fitNN, dat2_test<span class="op">$</span>ClaimNb),<span class="dv">4</span>)</span>
<span id="cb34-6"><a href="#cb34-6"></a><span class="co"># write.csv(dev_sum,&quot;./plots/1/dev_sum.csv&quot;)</span></span></code></pre></div>
<p><img src="03-nn_files/figure-html/sum output-1.png" width="672"  /></p>
<ul>
<li><p>Boosting &gt; RF &gt; Tree &gt; NN &gt; GAM &gt; GLM &gt; Homo</p></li>
<li><p>Boosting, RF, Tree, NN相较于GAM的提升主要在于交互作用；GAM相较于GLM的提升不大，原因是在GLM中进行了合适的特征工程，可以刻画非线性效应。</p></li>
</ul>
</div>
<div id="其它模型" class="section level2">
<h2><span class="header-section-number">3.7</span> 其它模型</h2>
<p>通过尝试发现，主要存在两个交互作用：<code>VehPower, VehAge, VehGas, VehBrand</code>和<code>DriAge, BonusMalus</code>，可设立如下简化的神经网络提升模型。</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a>train.x &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">as.matrix</span>(dat2_train[,<span class="kw">c</span>(<span class="st">&quot;VehPowerX&quot;</span>, <span class="st">&quot;VehAgeX&quot;</span>, <span class="st">&quot;VehGasX&quot;</span>)]),</span>
<span id="cb35-2"><a href="#cb35-2"></a>                <span class="kw">as.matrix</span>(dat2_train[,<span class="st">&quot;VehBrandX&quot;</span>]),</span>
<span id="cb35-3"><a href="#cb35-3"></a>                <span class="kw">as.matrix</span>(dat2_train[,<span class="kw">c</span>(<span class="st">&quot;DrivAgeX&quot;</span>, <span class="st">&quot;BonusMalus&quot;</span>)]),</span>
<span id="cb35-4"><a href="#cb35-4"></a>                <span class="kw">as.matrix</span>(<span class="kw">log</span>(dat1_train<span class="op">$</span>fitGAM1)) )</span>
<span id="cb35-5"><a href="#cb35-5"></a>valid.x &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">as.matrix</span>(dat2_valid[,<span class="kw">c</span>(<span class="st">&quot;VehPowerX&quot;</span>, <span class="st">&quot;VehAgeX&quot;</span>, <span class="st">&quot;VehGasX&quot;</span>)]),</span>
<span id="cb35-6"><a href="#cb35-6"></a>                <span class="kw">as.matrix</span>(dat2_valid[,<span class="st">&quot;VehBrandX&quot;</span>]),</span>
<span id="cb35-7"><a href="#cb35-7"></a>                <span class="kw">as.matrix</span>(dat2_valid[,<span class="kw">c</span>(<span class="st">&quot;DrivAgeX&quot;</span>, <span class="st">&quot;BonusMalus&quot;</span>)]),</span>
<span id="cb35-8"><a href="#cb35-8"></a>                <span class="kw">as.matrix</span>(<span class="kw">log</span>(dat1_valid<span class="op">$</span>fitGAM1)) )</span>
<span id="cb35-9"><a href="#cb35-9"></a>test.x &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">as.matrix</span>(dat2_test[,<span class="kw">c</span>(<span class="st">&quot;VehPowerX&quot;</span>, <span class="st">&quot;VehAgeX&quot;</span>, <span class="st">&quot;VehGasX&quot;</span>)]),</span>
<span id="cb35-10"><a href="#cb35-10"></a>                <span class="kw">as.matrix</span>(dat2_test[,<span class="st">&quot;VehBrandX&quot;</span>]),</span>
<span id="cb35-11"><a href="#cb35-11"></a>                <span class="kw">as.matrix</span>(dat2_test[,<span class="kw">c</span>(<span class="st">&quot;DrivAgeX&quot;</span>, <span class="st">&quot;BonusMalus&quot;</span>)]),</span>
<span id="cb35-12"><a href="#cb35-12"></a>                <span class="kw">as.matrix</span>(<span class="kw">log</span>(dat1_test<span class="op">$</span>fitGAM1)) )</span>
<span id="cb35-13"><a href="#cb35-13"></a></span>
<span id="cb35-14"><a href="#cb35-14"></a>neurons &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">10</span>,<span class="dv">5</span>)</span>
<span id="cb35-15"><a href="#cb35-15"></a></span>
<span id="cb35-16"><a href="#cb35-16"></a>model<span class="fl">.2</span>IA &lt;-<span class="st"> </span><span class="cf">function</span>(Brlabel){</span>
<span id="cb35-17"><a href="#cb35-17"></a>   Cont1        &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">3</span>), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name=</span><span class="st">&#39;Cont1&#39;</span>)</span>
<span id="cb35-18"><a href="#cb35-18"></a>   Cat1         &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">1</span>), <span class="dt">dtype =</span> <span class="st">&#39;int32&#39;</span>,   <span class="dt">name=</span><span class="st">&#39;Cat1&#39;</span>)</span>
<span id="cb35-19"><a href="#cb35-19"></a>   Cont2        &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">2</span>), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name=</span><span class="st">&#39;Cont2&#39;</span>)</span>
<span id="cb35-20"><a href="#cb35-20"></a>   LogExposure  &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">1</span>), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;LogExposure&#39;</span>) </span>
<span id="cb35-21"><a href="#cb35-21"></a>   </span>
<span id="cb35-22"><a href="#cb35-22"></a>   x.input &lt;-<span class="st"> </span><span class="kw">c</span>(Cont1, Cat1, Cont2, LogExposure)</span>
<span id="cb35-23"><a href="#cb35-23"></a>   <span class="co">#</span></span>
<span id="cb35-24"><a href="#cb35-24"></a>   Cat1_embed =<span class="st"> </span>Cat1 <span class="op">%&gt;%</span><span class="st">  </span></span>
<span id="cb35-25"><a href="#cb35-25"></a><span class="st">            </span><span class="kw">layer_embedding</span>(<span class="dt">input_dim =</span> Brlabel, <span class="dt">output_dim =</span> <span class="dv">1</span>, <span class="dt">trainable=</span><span class="ot">TRUE</span>, </span>
<span id="cb35-26"><a href="#cb35-26"></a>                    <span class="dt">input_length =</span> <span class="dv">1</span>, <span class="dt">name =</span> <span class="st">&#39;Cat1_embed&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-27"><a href="#cb35-27"></a><span class="st">                    </span><span class="kw">layer_flatten</span>(<span class="dt">name=</span><span class="st">&#39;Cat1_flat&#39;</span>)</span>
<span id="cb35-28"><a href="#cb35-28"></a>   <span class="co">#</span></span>
<span id="cb35-29"><a href="#cb35-29"></a>   NNetwork1 =<span class="st"> </span><span class="kw">list</span>(Cont1, Cat1_embed) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">layer_concatenate</span>(<span class="dt">name=</span><span class="st">&#39;cont&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-30"><a href="#cb35-30"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>neurons[<span class="dv">1</span>], <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden1&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-31"><a href="#cb35-31"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>neurons[<span class="dv">2</span>], <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden2&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-32"><a href="#cb35-32"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>neurons[<span class="dv">3</span>], <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden3&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-33"><a href="#cb35-33"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span><span class="dv">1</span>, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">name=</span><span class="st">&#39;NNetwork1&#39;</span>, </span>
<span id="cb35-34"><a href="#cb35-34"></a>                    <span class="dt">weights=</span><span class="kw">list</span>(<span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(neurons[<span class="dv">3</span>],<span class="dv">1</span>)), <span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>))))</span>
<span id="cb35-35"><a href="#cb35-35"></a>   <span class="co">#</span></span>
<span id="cb35-36"><a href="#cb35-36"></a>   NNetwork2 =<span class="st"> </span>Cont2 <span class="op">%&gt;%</span></span>
<span id="cb35-37"><a href="#cb35-37"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>neurons[<span class="dv">1</span>], <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden4&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-38"><a href="#cb35-38"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>neurons[<span class="dv">2</span>], <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden5&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-39"><a href="#cb35-39"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>neurons[<span class="dv">3</span>], <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>, <span class="dt">name=</span><span class="st">&#39;hidden6&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-40"><a href="#cb35-40"></a><span class="st">            </span><span class="kw">layer_dense</span>(<span class="dt">units=</span><span class="dv">1</span>, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">name=</span><span class="st">&#39;NNetwork2&#39;</span>, </span>
<span id="cb35-41"><a href="#cb35-41"></a>                    <span class="dt">weights=</span><span class="kw">list</span>(<span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(neurons[<span class="dv">3</span>],<span class="dv">1</span>)), <span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>))))</span>
<span id="cb35-42"><a href="#cb35-42"></a></span>
<span id="cb35-43"><a href="#cb35-43"></a>   <span class="co">#</span></span>
<span id="cb35-44"><a href="#cb35-44"></a>   NNoutput =<span class="st"> </span><span class="kw">list</span>(NNetwork1, NNetwork2, LogExposure) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">layer_add</span>(<span class="dt">name=</span><span class="st">&#39;Add&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-45"><a href="#cb35-45"></a><span class="st">                 </span><span class="kw">layer_dense</span>(<span class="dt">units=</span><span class="dv">1</span>, <span class="dt">activation=</span>k_exp, <span class="dt">name =</span> <span class="st">&#39;NNoutput&#39;</span>, <span class="dt">trainable=</span><span class="ot">FALSE</span>,</span>
<span id="cb35-46"><a href="#cb35-46"></a>                       <span class="dt">weights=</span><span class="kw">list</span>(<span class="kw">array</span>(<span class="kw">c</span>(<span class="dv">1</span>), <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)), <span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>))))</span>
<span id="cb35-47"><a href="#cb35-47"></a></span>
<span id="cb35-48"><a href="#cb35-48"></a>   model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> x.input, <span class="dt">outputs =</span> <span class="kw">c</span>(NNoutput))</span>
<span id="cb35-49"><a href="#cb35-49"></a>   model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;poisson&#39;</span>)        </span>
<span id="cb35-50"><a href="#cb35-50"></a>   model</span>
<span id="cb35-51"><a href="#cb35-51"></a>   }</span>
<span id="cb35-52"><a href="#cb35-52"></a></span>
<span id="cb35-53"><a href="#cb35-53"></a>model &lt;-<span class="st"> </span><span class="kw">model.2IA</span>(BrLabel)</span>
<span id="cb35-54"><a href="#cb35-54"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a>early_stop &lt;-<span class="st"> </span><span class="kw">callback_early_stopping</span>(<span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">patience =</span><span class="dv">10</span>)</span>
<span id="cb36-2"><a href="#cb36-2"></a><span class="co"># print_dot_callback &lt;- callback_lambda(</span></span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="co">#   on_epoch_end = function(epoch, logs) {</span></span>
<span id="cb36-4"><a href="#cb36-4"></a><span class="co">#     if (epoch %% 50 == 0) cat(&quot;\n&quot;)</span></span>
<span id="cb36-5"><a href="#cb36-5"></a><span class="co">#     cat(&quot;.&quot;)</span></span>
<span id="cb36-6"><a href="#cb36-6"></a><span class="co">#   }</span></span>
<span id="cb36-7"><a href="#cb36-7"></a><span class="co"># )  </span></span>
<span id="cb36-8"><a href="#cb36-8"></a><span class="co"># may take a couple of minutes if epochs is more than 100</span></span>
<span id="cb36-9"><a href="#cb36-9"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb36-10"><a href="#cb36-10"></a>     fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(train.x, <span class="kw">as.matrix</span>(dat2_train<span class="op">$</span>ClaimNb), </span>
<span id="cb36-11"><a href="#cb36-11"></a>                          <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">batch_size=</span><span class="dv">10000</span>, <span class="dt">verbose=</span><span class="dv">1</span>,</span>
<span id="cb36-12"><a href="#cb36-12"></a>                                       <span class="dt">validation_data=</span><span class="kw">list</span>(valid.x,dat2_valid<span class="op">$</span>ClaimNb),</span>
<span id="cb36-13"><a href="#cb36-13"></a>                          <span class="dt">callback=</span><span class="kw">list</span>(early_stop))</span>
<span id="cb36-14"><a href="#cb36-14"></a>(<span class="kw">proc.time</span>()<span class="op">-</span>t1)}</span>
<span id="cb36-15"><a href="#cb36-15"></a></span>
<span id="cb36-16"><a href="#cb36-16"></a><span class="kw">matplot</span>(<span class="kw">cbind</span>(fit<span class="op">$</span>metrics<span class="op">$</span>loss,fit<span class="op">$</span>metrics<span class="op">$</span>val_loss), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</span>
<span id="cb36-17"><a href="#cb36-17"></a></span>
<span id="cb36-18"><a href="#cb36-18"></a>dat2_test<span class="op">$</span>fitGAMPlus &lt;-<span class="st"> </span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(test.x))</span>
<span id="cb36-19"><a href="#cb36-19"></a><span class="kw">Poisson.Deviance</span>(dat2_test<span class="op">$</span>fitGAMPlus, dat2_test<span class="op">$</span>ClaimNb)</span>
<span id="cb36-20"><a href="#cb36-20"></a><span class="kw">keras_poisson_dev</span>(dat2_test<span class="op">$</span>fitGAMPlus, dat2_test<span class="op">$</span>ClaimNb)</span></code></pre></div>
<!--chapter:end:03-nn.Rmd-->
</div>
</div>
<div id="boosting" class="section level1">
<h1><span class="header-section-number">4</span> 提升方法 (Boosting)</h1>
<p><em>范雨文、毛雪婷、高光远</em></p>
<blockquote>
<p>Breiman called <strong>AdaBoost</strong> the <strong>‘best off-the-shelf classifier in the world’</strong> (NIPS Workshop 1996).</p>
</blockquote>
<blockquote>
<p>On the data science competition platform Kaggle, among <strong>29</strong> challenge winning solutions in 2015, <strong>17</strong> used <strong>XGBoost</strong>, a boosting algorithm introduced by Chen and Guestrin.</p>
</blockquote>
<p><strong>AdaBoost及相近算法</strong></p>
<p>AdaBoost是一种迭代算法，其核心思想是训练不同的分类器(弱分类器<span class="math inline">\(T\)</span>)，然后把这些弱分类器线性组合起来，构成一个更强的最终分类器（强分类器<span class="math inline">\(C\)</span>）。</p>
<p>该算法是一个简单的弱分类算法提升过程，这个过程通过不断的训练，可以提高对数据的分类能力。整个过程如下所示：</p>
<ol style="list-style-type: decimal">
<li><p>通过对训练样本<span class="math inline">\((\mathcal{D},\mathbb{\omega})\)</span>的学习得到第<span class="math inline">\(m-1\)</span>个弱分类器<code>WeakClassifier m-1</code>, <span class="math inline">\(T^{(m-1)}\)</span>；</p></li>
<li><p>计算得出其分类错误率<span class="math inline">\(\epsilon^{(m-1)}\)</span>，以此计算出其弱分类器权重<span class="math inline">\(\alpha^{(m-1)}\)</span>与数据权重<span class="math inline">\(\omega^{(m-1)}_i\)</span>;</p></li>
<li><p>用权重为<span class="math inline">\(\omega^{(m-1)}_i\)</span>的数据集训练得到训练弱分类器<code>WeakClassifier m</code>, <span class="math inline">\(T^{(m)}\)</span>;</p></li>
<li><p>重复以上不断迭代的过程;</p></li>
<li><p>最终结果通过加权投票表决的方法，让所有弱分类器<span class="math inline">\(T^{(m)}\)</span>进行权重为<span class="math inline">\(\alpha^{(m)}\)</span>的投票表决的方法得到最终预测输出。</p></li>
</ol>
<p><img src="./plots/4/overview.png" width="60%" style="display: block; margin: auto;" /></p>
<ul>
<li><p>AdaBoost: Schapire and Freund (1997, 2012)</p></li>
<li><p>LogitBoost: Friedman, Hastie, Tibshirani (1998)</p></li>
<li><p>AdaBoost.M1: Schapire and Freund (1996, 1997)</p></li>
<li><p>SAMME: Zhu, Zou, Rosset et al. (2006)</p></li>
<li><p>SAMME.R: Zhu, Zou, Rosset et al. (2006)</p></li>
</ul>
<p><strong>梯度提升算法</strong></p>
<p>核心思想: Gradient descent method and Newton’s method.</p>
<p><em>Gradient descent method</em></p>
<p>Minimize the following approximation over <span class="math inline">\(y\)</span>,
<span class="math display">\[f(y)\approx f(x)+\nabla f(x)^T(y-x) +\frac{1}{2t}||y-x||^2 \]</span>
we have <span class="math inline">\(y=x^+=x-t\nabla f(x)\)</span>.</p>
<p><em>Newton’s method</em></p>
<p>Minimize the following approximation over <span class="math inline">\(y\)</span>,
<span class="math display">\[f(y)\approx f(x)+\nabla f(x)^T(y-x) +\frac{1}{2}(y-x)^T\nabla^2f(x)(y-x)\]</span>
we have <span class="math inline">\(y=x^+=x-\frac{\nabla f(x)}{\nabla ^2f(x)}\)</span>.</p>
<ul>
<li><p>Gradient Boost: Friedman (2001)</p></li>
<li><p>Newton Boosting: Nielsen (2016)</p></li>
<li><p>XGBoost: Chen and Guestrin (2016)</p></li>
</ul>
<div id="adaboost" class="section level2">
<h2><span class="header-section-number">4.1</span> AdaBoost</h2>
<p><span class="math inline">\(Y\in\{0,1\}\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始权重 <span class="math inline">\(\omega^{(0)}_i=\frac{1}{n}\)</span>.</p></li>
<li><p>使用<span class="math inline">\((\mathcal{D},\mathbf{\omega}^{(m-1)})\)</span>，训练弱学习机<span class="math inline">\(T^{(m-1)}\)</span>.</p></li>
<li><p>计算加权分类错误 <span class="math display">\[\epsilon^{(m-1)}=\sum_{i=1}^n\omega^{(m-1)}_i \mathbb{I}(y_i \neq T^{(m-1)}(\mathbf{x}_i))\]</span></p></li>
<li><p>计算模型权重 <span class="math inline">\(\alpha^{(m-1)}=\ln\beta^{(m-1)}\)</span>, 其中<span class="math display">\[\beta^{(m-1)}=\frac{1-\epsilon^{(m-1)}}{\epsilon^{(m-1)}}\]</span></p></li>
<li><p>计算样本权重<span class="math display">\[\omega^{(m)}_i=\omega^{(m-1)}_i\exp\left( \alpha^{(m-1)}\mathbb{I}(y_i \neq T^{(m-1)}(\mathbf{x}_i)) \right)/w^{(m)}\]</span>, 其中<span class="math inline">\(w^{(m)}\)</span>为标准化常数。</p></li>
<li><p>最终预测结果为 <span class="math display">\[C(\mathbf{x})= \underset{k}{\arg \max} \sum_{m=1}^M\alpha^{(m)}\mathbb{I}(T^{(m)}(\mathbf{x})=k)\]</span></p></li>
</ol>
<p><strong>另外一种等价算法</strong></p>
<p><span class="math inline">\(Y\in\{-1,1\}\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始权重 <span class="math inline">\(\omega^{(0)}_i=\frac{1}{n}\)</span>.</p></li>
<li><p>使用<span class="math inline">\((\mathcal{D},\mathbf{\omega}^{(m-1)})\)</span>，训练弱学习机<span class="math inline">\(T^{(m-1)}\)</span>.</p></li>
<li><p>计算加权分类错误 <span class="math display">\[\epsilon^{(m-1)}=\sum_{i=1}^n\omega^{(m-1)}_i \mathbb{I}(y_i \neq T^{(m-1)}(\mathbf{x}_i))\]</span></p></li>
<li><p>计算模型权重 <span class="math inline">\(\alpha^{(m-1)}=\frac{1}{2}\ln\beta^{(m-1)}\)</span>, 其中<span class="math display">\[\beta^{(m-1)}=\frac{1-\epsilon^{(m-1)}}{\epsilon^{(m-1)}}.\]</span></p></li>
<li><p>计算样本权重<span class="math display">\[\omega^{(m)}_i=\omega^{(m-1)}_i\exp\left(-\alpha^{(m-1)}y_i T^{(m-1)}(\mathbf{x}_i) \right)/w^{(m)},\]</span> 其中<span class="math inline">\(w^{(m)}\)</span>为标准化常数。</p></li>
<li><p>最终预测结果为<span class="math display">\[C(\mathbf{x})= \underset{k}{\arg \max} \sum_{m=1}^M\alpha^{(m)}\mathbb{I}(T^{(m)}(\mathbf{x})=k)\]</span></p></li>
</ol>
</div>
<div id="logit-boost-real-discrete-gentle-adaboost" class="section level2">
<h2><span class="header-section-number">4.2</span> Logit Boost (real, discrete, gentle AdaBoost)</h2>
<p><span class="math inline">\(Y\in\{-1,1\}\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始弱学习机 <span class="math inline">\(T^{(0)}=0, C^{(0)}=0\)</span>.</p></li>
<li><p>计算预测概率 <span class="math display">\[p^{(m-1)}(Y_i|\mathbf{x_i})=\frac{1}{1+\exp(-Y_iT^{(m-1)}(\mathbf{x_i}))}\]</span> 注：<span class="math display">\[p^{(m-1)}(Y_i=1|\mathbf{x_i})+p^{(m-1)}(Y_i=-1|\mathbf{x_i})=1\]</span></p></li>
<li><p>计算样本权重 <span class="math display">\[\omega^{(m-1)}_i=p^{(m-1)}(Y_i=y_i|\mathbf{x_i})(1-p^{(m-1)}(Y_i=y_i|\mathbf{x_i}))\]</span></p></li>
<li><p>计算工作因变量 <span class="math display">\[z^{(m)}_i = y_i(1+\exp(-y_i C^{(m-1)}(\mathbf{x_i})))\]</span></p></li>
<li><p>训练弱学习机<span class="math inline">\(T^{(m)}\)</span>，使之最小化如下加权损失函数 <span class="math display">\[\sum_{i=1}^N \omega_i^{(m-1)}(T^{(m)}(\mathbf{x_i})-z^{(m-1)}_i)^2\]</span></p></li>
<li><p>令<span class="math inline">\(C^{(m)}=C^{(m-1)}+T^{(m)}\)</span></p></li>
<li><p>最终预测概率为<span class="math display">\[\Pr(Y=y|\mathbf{x})= \frac{1}{1+\exp(-yC^{(m)}(\mathbf{x_i}))}\]</span></p></li>
</ol>
</div>
<div id="adaboost.m1" class="section level2">
<h2><span class="header-section-number">4.3</span> AdaBoost.M1</h2>
<p><span class="math inline">\(Y\in\{1,\ldots,K\}\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始权重 <span class="math inline">\(\omega^{(0)}_i=\frac{1}{n}\)</span>.</p></li>
<li><p>使用<span class="math inline">\((\mathcal{D},\mathbf{\omega}^{(m-1)})\)</span>，训练弱学习机<span class="math inline">\(T^{(m-1)}\)</span>.</p></li>
<li><p>计算加权分类错误 <span class="math display">\[\epsilon^{(m-1)}=\sum_{i=1}^n\omega^{(m-1)}_i \mathbb{I}(y_i \neq T^{(m-1)}(\mathbf{x}_i))\]</span></p></li>
<li><p>计算模型权重 <span class="math inline">\(\alpha^{(m-1)}=\ln\beta^{(m-1)}\)</span>, 其中<span class="math display">\[\beta^{(m-1)}=\frac{1-\epsilon^{(m-1)}}{\epsilon^{(m-1)}}\]</span></p></li>
<li><p>计算样本权重<span class="math display">\[\omega^{(m)}_i=\omega^{(m-1)}_i\exp\left( \alpha^{(m-1)}\mathbb{I}(y_i \neq T^{(m-1)}(\mathbf{x}_i)) \right)/w^{(m)},\]</span> 其中<span class="math inline">\(w^{(m)}\)</span>为标准化常数。</p></li>
<li><p>最终预测结果为 <span class="math display">\[C(\mathbf{x})= \underset{k}{\arg \max} \sum_{m=1}^M\alpha^{(m)}\mathbb{I}(T^{(m)}(\mathbf{x})=k)\]</span></p></li>
</ol>
</div>
<div id="samme-stage-wise-additive-modeling-using-a-multi-class-exponential-loss-function" class="section level2">
<h2><span class="header-section-number">4.4</span> SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function)</h2>
<p><span class="math inline">\(Y\in \{1,\ldots,K\}\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始权重 <span class="math inline">\(\omega^{(0)}_i=\frac{1}{n}\)</span>.</p></li>
<li><p>使用<span class="math inline">\((\mathcal{D},\mathbf{\omega}^{(m-1)})\)</span>，训练弱学习机<span class="math inline">\(T^{(m-1)}\)</span>.</p></li>
<li><p>计算加权分类错误 <span class="math display">\[\epsilon^{(m-1)}=\sum_{i=1}^n\omega^{(m-1)}_i \mathbb{I}(y_i \neq T^{(m-1)}(\mathbf{x}_i))\]</span></p></li>
<li><p>计算模型权重 <span class="math display">\[\alpha^{(m-1)}=\eta\left(\ln\beta^{(m-1)}+\ln (k-1)\right),\]</span> 其中<span class="math display">\[\beta^{(m-1)}=\frac{1-\epsilon^{(m-1)}}{\epsilon^{(m-1)}}\]</span></p></li>
<li><p>计算样本权重<span class="math display">\[\omega^{(m)}_i=\omega^{(m-1)}_i\exp\left( \alpha^{(m-1)}\mathbb{I}(y_i \neq T^{(m-1)}(\mathbf{x}_i)) \right)/w^{(m)},\]</span> 其中<span class="math inline">\(w^{(m)}\)</span>为标准化常数。</p></li>
<li><p>最终预测结果为 <span class="math display">\[C(\mathbf{x})= \underset{k}{\arg \max} \sum_{m=1}^M\alpha^{(m)}\mathbb{I}(T^{(m)}(\mathbf{x})=k)\]</span></p></li>
</ol>
</div>
<div id="samme.r-multi-class-real-adaboost" class="section level2">
<h2><span class="header-section-number">4.5</span> SAMME.R (multi-class real AdaBoost)</h2>
<p>见<a href="https://web.stanford.edu/~hastie/Papers/samme.pdf" class="uri">https://web.stanford.edu/~hastie/Papers/samme.pdf</a></p>
<p><span class="math inline">\(Y\in \{1,\ldots,K\},\mathbf{Z}=(Z_1,\ldots,Z_k)&#39;\in\{1,-1/(K-1)\}^K\)</span>, 建立映射<span class="math inline">\(\{1,\ldots,K\}\rightarrow \{1,-1/(K-1)\}^K, Y\mapsto \mathbf{Z}(Y)\)</span>, 其中<span class="math inline">\(Z_k(k)=1\)</span>, 且<span class="math inline">\(Z_{k&#39;}(k)=-1/(K-1), k&#39;\neq k\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始权重 <span class="math inline">\(\omega^{(0)}_i=\frac{1}{n}\)</span>.</p></li>
<li><p>使用<span class="math inline">\((\mathcal{D},\mathbf{\omega}^{(m-1)})\)</span>，训练弱学习机<span class="math inline">\(T^{(m-1)}\)</span>.</p></li>
<li><p>根据<span class="math inline">\(T^{(m-1)}\)</span>计算(加权)预测频率 <span class="math display">\[p_k^{(m-1)}(\mathbf{x})=\Pr(y=k|\mathbf{x}),\]</span> 令<span class="math inline">\(\mathbf{p}^{(m-1)}(\mathbf{x})=(p_1^{(m-1)}(\mathbf{x}), \ldots,p_K^{(m-1)}(\mathbf{x}))&#39;\)</span></p></li>
<li><p>计算模型(预测为<span class="math inline">\(k\)</span>)权重 <span class="math display">\[h^{(m-1)}_k(\mathbf{x})=(K-1)\left(\ln p^{(m-1)}_k(\mathbf{x})-\frac{1}{K}\sum_{k&#39;\neq k} \ln p_{k&#39;}^{(m-1)}(\mathbf{x})\right)\]</span></p></li>
<li><p>计算样本权重<span class="math display">\[\omega^{(m)}_i=\omega^{(m-1)}_i\exp\left( -\frac{K-1}{K}\mathbf{Z}(y_i)^{T}\ln p^{(m-1)}(x_i) \right)/w^{(m)},\]</span> 其中<span class="math inline">\(w^{(m)}\)</span>为标准化常数。</p></li>
<li><p>最终预测结果为 <span class="math display">\[C(\mathbf{x})= \underset{k}{\arg \max} \sum_{m=1}^M h_k^{(m)}(\mathbf{x})\]</span></p></li>
</ol>
<p>参考<code>Multiclass exponential loss</code> <span class="math display">\[L(Z,f)=\exp\left(-\frac{1}{K}Z^Tf\right)\]</span></p>
</div>
<div id="gradient-boosting" class="section level2">
<h2><span class="header-section-number">4.6</span> Gradient Boosting</h2>
<p><span class="math inline">\(Y\in\{-1,1\}\)</span>，设定学习率<span class="math inline">\(\eta\in(0,1]\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始弱学习器
<span class="math display">\[f_0(\mathbf{x})= \underset{\theta\in\mathbb{R}}{\arg \min} \sum_{i=1}^n L(Y_i, \theta)\]</span></p></li>
<li><p>计算第<span class="math inline">\(m\)</span>次迭代中,损失函数的负梯度<span class="math display">\[g_m(\mathbf{x_i}) = - \frac{\partial L(Y_i, f_i)}{\partial f_{m-1,i}}\]</span></p></li>
<li><p>求解弱学习器<span class="math inline">\(T^m\)</span>参数 <span class="math display">\[h_m^{*} = \underset{h\in\mathcal{F},\beta}{\arg \min} \sum_{i=1}^n(g_m(\mathbf{x_i}) - \beta h(\mathbf{x_i}))^2 \]</span></p></li>
<li><p>通过线搜索得到步长 <span class="math display">\[\rho_m = \underset{\rho&gt;0}{\arg \min} \sum_{i=1}^n L(Y_i,f_{m-1}(\mathbf{x_i}) + \rho h_m^{*}(\mathbf{x_i}))\]</span></p></li>
<li><p>shrinkage，乘以提前给定的学习率 <span class="math display">\[f_m^{*} = \eta\rho_m h_m^{*}\]</span></p></li>
<li><p>更新，前<span class="math inline">\(m\)</span>个弱学习器的线性组合 <span class="math display">\[f_m(\mathbf{x}) = f_{m-1}(\mathbf{x}) + f_{m}^*(\mathbf{x}) \]</span></p></li>
<li><p>最终预测结果为 <span class="math inline">\(f_M(\mathbf{x})\)</span></p></li>
</ol>
</div>
<div id="newton-boosting" class="section level2">
<h2><span class="header-section-number">4.7</span> Newton Boosting</h2>
<p><span class="math inline">\(Y\in\{-1,1\}\)</span>，设定学习率<span class="math inline">\(\eta\in(0,1]\)</span></p>
<ol style="list-style-type: decimal">
<li><p>初始弱学习器<span class="math display">\[f_0(\mathbf{x})= \underset{\theta\in\mathbb{R}}{\arg \min} \sum_{i=1}^n L(Y_i, \theta)\]</span></p></li>
<li><p>计算第<span class="math inline">\(m\)</span>次迭代中的负梯度<span class="math display">\[g_m(\mathbf{x_i}) = - \frac{\partial L(Y_i, \theta)}{\partial f_{m-1,i}}\]</span></p></li>
<li><p>计算Hessian Matrix <span class="math display">\[H_m(\mathbf{x_i}) = (\nabla^2\mathcal{L}(\mathbb{f_{m-1}}))_{ii}\]</span></p></li>
<li><p>求解弱学习器<span class="math inline">\(T^m\)</span>参数<span class="math display">\[h_m^{*} = \underset{h\in\mathcal{F}}{\arg \min} \sum_{i=1}^n(\frac{g_m(\mathbf{x_i})}{H_m(\mathbf{x_i})} + \frac{1}{2} H_m(\mathbf{x_i})h(\mathbf{x_i}))^2 \\ 
= \underset{h\in\mathcal{F}}{\arg \min} \sum_{i=1}^n\frac{1}{2} H_m(\mathbf{x_i})(-\frac{g_m(\mathbf{x_i})}{H_m(\mathbf{x_i})} - h(\mathbf{x_i}))^2 \]</span></p></li>
<li><p>shrinkage，乘以提前给定的学习率<span class="math display">\[f_m^{*} = \eta h_m^{*}\]</span></p></li>
<li><p>更新，前<span class="math inline">\(m\)</span>个弱学习器的线性组合 <span class="math display">\[f_m(\mathbf{x}) = f_{m-1}(\mathbf{x}) + f_{m}^*(\mathbf{x}) \]</span></p></li>
<li><p>最终预测结果为 <span class="math inline">\(f_M(\mathbf{x})\)</span></p></li>
</ol>
</div>
<div id="xgboost" class="section level2">
<h2><span class="header-section-number">4.8</span> XGBoost</h2>
<ol style="list-style-type: decimal">
<li><p>objective function:
<span class="math display">\[\mathcal{L} =\sum_{i=1}^n L(\hat{y_i},y_i) + \sum_{m=1}^M\Omega(f_m)\]</span> where <span class="math inline">\(L(\hat{y_i},y_i)\)</span> is a differential convex loss function and <span class="math inline">\(\Omega(f_m) = \gamma T + \frac{1}{2}\lambda||\omega||^2\)</span> is a regularization term to penalize model complexity (including number of leaves <span class="math inline">\(T\)</span> and <span class="math inline">\(L^2\)</span>-norm of leaf scores <span class="math inline">\(\omega\)</span>)</p></li>
<li><p>Newton approximation of objective function:
<span class="math display">\[\tilde{\mathcal{L}}^m = \sum_{i=1}^n \lbrack L(\hat{y_i}^{m-1},y_i) + g_if_m(\mathbf{x_i}) + \frac{1}{2}h_if_m^2(\mathbf{x_i}) \rbrack+\Omega(f_m) \]</span> where <span class="math inline">\(g_i = \frac{\partial L}{\partial\hat{y_i}^{m-1}}|_{(\hat{y_i}^{m-1}, y_i)}\)</span> and <span class="math inline">\(h_i= \frac{\partial^2 L}{\partial\hat{y_i}^{m-1}\partial\hat{y_i}^{m-1}}|_{(\hat{y_i}^{m-1}, y_i)}\)</span></p></li>
<li><p>最小化上述式子，得到the optimal score (or weight) <span class="math inline">\(\omega_j^*\)</span> of leaf <span class="math inline">\(j\in\{1,\dots,T\}\)</span>is:<span class="math display">\[ \omega_j^*=-\frac{\sum_{i=1}^n g_i \mathbb{I}[q(\mathbf{x_i}) = j]}{\lambda + \sum_{i=1}^nh_i\mathbb{I}[q(\mathbf{x_i}) = j]} \]</span></p></li>
</ol>
<p><img src="./plots/4/summary.png" width="60%"  style="display: block; margin: auto;" /></p>
</div>
<div id="case-study" class="section level2">
<h2><span class="header-section-number">4.9</span> Case study</h2>
<p>本案例的数据来源于Kaggle上的比赛“Porto Seguro’s Safe Driver Prediction Competition”。</p>
<p>比赛的目标是预测未来一年司机是否会发生交通事故，是一个<strong>二分类</strong>问题。此案例中，所有的数据都经过了匿名化处理。</p>
<div id="数据描述" class="section level3">
<h3><span class="header-section-number">4.9.1</span> 数据描述</h3>
<p>数据包含595212条记录，59个变量。</p>
<p>变量包含三类：</p>
<ul>
<li>唯一编码（1个）：<code>id</code></li>
<li>目标变量（1个）：<code>target</code>，取值为<span class="math inline">\(\{0, 1\}\)</span></li>
<li>解释变量（57个）：<code>ps_*</code>，包括四种，分别是二分类变量（binary），多分类变量（categorical），连续型变量（continuous），定序变量（ordinal）。</li>
</ul>
<p>此案例中，缺失值用-1表示。</p>
<p><img src="./plots/4/数据描述.png" width="60%"  style="display: block; margin: auto;" /></p>
</div>
<div id="数据预处理-2" class="section level3">
<h3><span class="header-section-number">4.9.2</span> 数据预处理</h3>
<ol style="list-style-type: decimal">
<li>统计缺失值</li>
</ol>
<p>数据中，<code>ps_car_03_cat</code>和<code>ps_car_05_cat</code>的缺失值较多，缺失值分别占69.09%和44.78%，之后将进行缺失值处理。</p>
<p><img src="./plots/4/缺失值.png" width="60%"  style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li>单变量分析</li>
</ol>
<p>对于不同类型的解释变量，我们将依照不同的方法进行处理。</p>
<ul>
<li><p>分类变量，统计各个类别的占比</p></li>
<li><p>定距和定序变量，作条形图</p></li>
<li><p>数值型变量，作直方图</p></li>
</ul>
<p>对于目标变量，我们发现它的取值非常不平衡。</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="co"># levels for the target variable </span></span>
<span id="cb37-2"><a href="#cb37-2"></a>lev_target <span class="op">=</span> ( pd.crosstab(index <span class="op">=</span> data[<span class="st">&#39;target&#39;</span>], columns <span class="op">=</span> <span class="st">&#39;Frequency&#39;</span>) <span class="op">/</span> data.shape[<span class="dv">0</span>] ) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb37-3"><a href="#cb37-3"></a>lev_target.<span class="bu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<p>通常来说，处理<strong>不平衡</strong>的分类数据，我们可以采取如下方法：</p>
<ul>
<li><p>SMOTE（Synthetic Minority Oversampling Technique）</p></li>
<li><p>上采样（Over-sampling）</p></li>
<li><p>下采样（Under-sampling）</p></li>
<li><p>加权抽样（Sampling Weighting）</p></li>
<li><p>成本敏感训练（Cost-sensitive Training）</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>多变量分析</li>
</ol>
<p>在不同的解释变量之间，我们可以作相关系数矩阵热图和散点图矩阵。</p>
<ul>
<li>相关系数矩阵热图</li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># Pearson correlation matrix: computation and visualization</span></span>
<span id="cb38-2"><a href="#cb38-2"></a></span>
<span id="cb38-3"><a href="#cb38-3"></a><span class="co"># use method=&#39;pearson&#39; resp. method=&#39;spearman&#39; to compute Pearson resp. Spearman correlations</span></span>
<span id="cb38-4"><a href="#cb38-4"></a><span class="kw">def</span> corr_heatmap(v):</span>
<span id="cb38-5"><a href="#cb38-5"></a>    correlations <span class="op">=</span> data[v].corr(method<span class="op">=</span><span class="st">&#39;pearson&#39;</span>)</span>
<span id="cb38-6"><a href="#cb38-6"></a>    fig <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb38-7"><a href="#cb38-7"></a></span>
<span id="cb38-8"><a href="#cb38-8"></a>    sns.heatmap(correlations,  center<span class="op">=</span><span class="dv">0</span>, fmt<span class="op">=</span><span class="st">&#39;.2f&#39;</span>, cbar<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb38-9"><a href="#cb38-9"></a>                square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="dv">1</span>, annot<span class="op">=</span><span class="va">True</span>,  cmap<span class="op">=</span><span class="st">&quot;YlGnBu&quot;</span>)</span>
<span id="cb38-10"><a href="#cb38-10"></a>    plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>) </span>
<span id="cb38-11"><a href="#cb38-11"></a>    plt.yticks(rotation<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb38-12"><a href="#cb38-12"></a>    plt.show()</span>
<span id="cb38-13"><a href="#cb38-13"></a></span>
<span id="cb38-14"><a href="#cb38-14"></a><span class="co"># one applies the corr_heatmap function on the interval features    </span></span>
<span id="cb38-15"><a href="#cb38-15"></a>v <span class="op">=</span> meta[(meta.level <span class="op">==</span> <span class="st">&#39;interval&#39;</span>) <span class="op">&amp;</span> (meta.keep)].index</span>
<span id="cb38-16"><a href="#cb38-16"></a>corr_heatmap(v)</span></code></pre></div>
<p><img src="./plots/4/相关系数矩阵热图.png" width="60%"  style="display: block; margin: auto;" /></p>
<ul>
<li>散点图矩阵</li>
</ul>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># scatterplot high correlation interval variables</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="im">import</span> seaborn</span>
<span id="cb39-3"><a href="#cb39-3"></a>high <span class="op">=</span> pd.Index([<span class="st">&#39;ps_reg_01&#39;</span>, <span class="st">&#39;ps_reg_02&#39;</span>, <span class="st">&#39;ps_reg_03&#39;</span>, <span class="st">&#39;ps_car_12&#39;</span>, <span class="st">&#39;ps_car_13&#39;</span>, <span class="st">&#39;ps_car_15&#39;</span>])</span>
<span id="cb39-4"><a href="#cb39-4"></a>pd.plotting.scatter_matrix(data[high], alpha <span class="op">=</span> <span class="fl">0.2</span>, figsize <span class="op">=</span> (<span class="dv">40</span>, <span class="dv">40</span>), diagonal <span class="op">=</span> <span class="st">&#39;kde&#39;</span>)</span></code></pre></div>
<p><img src="./plots/4/散点图矩阵.png" width="80%"  style="display: block; margin: auto;" /></p>
<p>在解释变量和目标变量之间，我们可以作散点图、箱线图、条形图等。</p>
<p><img src="./plots/4/target_vs_features.png" width="80%"  style="display: block; margin: auto;" /></p>
</div>
<div id="特征工程-1" class="section level3">
<h3><span class="header-section-number">4.9.3</span> 特征工程</h3>
<ol style="list-style-type: decimal">
<li>删除变量</li>
</ol>
<p>为了简化分析、提升计算速度，删除14个定距变量（interval）和定序变量（ordinal）
<code>ps_calc_01</code>，<code>ps_calc_02</code>，<code>ps_calc_03</code>，<code>ps_calc_04</code>，<code>ps_calc_05</code>，<code>ps_calc_06</code>，<code>ps_calc_07</code>，<code>ps_calc_08</code>，<code>ps_calc_09</code>，<code>ps_calc_10</code>，<code>ps_calc_11</code>，<code>ps_calc_12</code>，<code>ps_calc_13</code>，<code>ps_calc_14</code>。</p>
<ol start="2" style="list-style-type: decimal">
<li>缺失值处理</li>
</ol>
<ul>
<li><p>删除缺失值较多的变量<code>ps_car_03_cat</code>和<code>ps_car_05_cat</code></p></li>
<li><p>均值插补连续型变量<code>ps_reg_03</code>，<code>ps_car_12</code>和<code>ps_car_14</code></p></li>
<li><p>众数插补分类变量<code>ps_car_11</code></p></li>
</ul>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># dropping &#39;ps_car_03_cat&#39;, &#39;ps_car_05_cat&#39; and updating meta information</span></span>
<span id="cb40-2"><a href="#cb40-2"></a>vars_to_drop <span class="op">=</span> [<span class="st">&#39;ps_car_03_cat&#39;</span>, <span class="st">&#39;ps_car_05_cat&#39;</span>]</span>
<span id="cb40-3"><a href="#cb40-3"></a>data.drop(vars_to_drop, inplace <span class="op">=</span> <span class="va">True</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb40-4"><a href="#cb40-4"></a>meta.loc[(vars_to_drop), <span class="st">&#39;keep&#39;</span>] <span class="op">=</span> <span class="va">False</span>  </span>
<span id="cb40-5"><a href="#cb40-5"></a></span>
<span id="cb40-6"><a href="#cb40-6"></a><span class="co"># imputing with the mean or mode using Imputer from sklearn.preprocessing</span></span>
<span id="cb40-7"><a href="#cb40-7"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> Imputer</span>
<span id="cb40-8"><a href="#cb40-8"></a></span>
<span id="cb40-9"><a href="#cb40-9"></a>mean_imp <span class="op">=</span> Imputer(missing_values <span class="op">=</span> <span class="dv">-1</span>, strategy <span class="op">=</span> <span class="st">&#39;mean&#39;</span>, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb40-10"><a href="#cb40-10"></a>mode_imp <span class="op">=</span> Imputer(missing_values <span class="op">=</span> <span class="dv">-1</span>, strategy <span class="op">=</span> <span class="st">&#39;most_frequent&#39;</span>, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb40-11"><a href="#cb40-11"></a></span>
<span id="cb40-12"><a href="#cb40-12"></a>data[<span class="st">&#39;ps_reg_03&#39;</span>] <span class="op">=</span> mean_imp.fit_transform(data[[<span class="st">&#39;ps_reg_03&#39;</span>]]).ravel()</span>
<span id="cb40-13"><a href="#cb40-13"></a>data[<span class="st">&#39;ps_car_12&#39;</span>] <span class="op">=</span> mean_imp.fit_transform(data[[<span class="st">&#39;ps_car_12&#39;</span>]]).ravel()</span>
<span id="cb40-14"><a href="#cb40-14"></a>data[<span class="st">&#39;ps_car_14&#39;</span>] <span class="op">=</span> mean_imp.fit_transform(data[[<span class="st">&#39;ps_car_14&#39;</span>]]).ravel()</span>
<span id="cb40-15"><a href="#cb40-15"></a>data[<span class="st">&#39;ps_car_11&#39;</span>] <span class="op">=</span> mode_imp.fit_transform(data[[<span class="st">&#39;ps_car_11&#39;</span>]]).ravel()</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>生成哑变量</li>
</ol>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="co"># creating dummy variables</span></span>
<span id="cb41-2"><a href="#cb41-2"></a>data <span class="op">=</span> pd.get_dummies(data, columns <span class="op">=</span> v, drop_first <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb41-3"><a href="#cb41-3"></a><span class="bu">print</span>(<span class="st">&#39;After dummification we have </span><span class="sc">{}</span><span class="st"> variables in data&#39;</span>.<span class="bu">format</span>(data.shape[<span class="dv">1</span>]))</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>划分学习集和测试集</li>
</ol>
<p>以80:20的比例划分学习集和测试集，分别记作<code>(X_train, y_train)</code>和<code>(X_test, y_test)</code>。</p>
<p>经过划分，学习集有476169条记录，测试集有119043条记录。模型将会在学习集上训练，然后在测试集上分析其表现。</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(data.drop([<span class="st">&#39;id&#39;</span>, <span class="st">&#39;target&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>), </span>
<span id="cb42-2"><a href="#cb42-2"></a>                                                    data[<span class="st">&#39;target&#39;</span>], </span>
<span id="cb42-3"><a href="#cb42-3"></a>                                                    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb42-4"><a href="#cb42-4"></a>                                                    random_state<span class="op">=</span>random_state</span>
<span id="cb42-5"><a href="#cb42-5"></a>                                                   )</span></code></pre></div>
</div>
<div id="建模流程" class="section level3">
<h3><span class="header-section-number">4.9.4</span> 建模流程</h3>
<table>
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="60%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th align="center">模型编号</th>
<th>建模过程</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>基准模型 Baseline modeling</td>
<td align="center">ST0</td>
<td>用默认参数训练模型 (no cv, no tunning)</td>
</tr>
<tr class="even">
<td>深入建模 In-depth modeling</td>
<td align="center">ST1, ST2</td>
<td>包括参数调整(<code>iteration</code>, <code>depth</code>, <code>learning rate</code>)、交叉验证、外样本测试<br><br>整体步骤为：<br>1.选择若干套参数 (hyper parameter tuning)<br>2.对于每套参数，在<code>X_train</code>上进行训练和交叉验证，计算每一折的表现<br>(先用1-4折训练，计算第5折的cv error；再用第1-3和5折训练，计算第4折的cv error；依次类推)<br>3.计算每套参数的平均表现(GINI，AUC，accuracy，logit loss function)<br>4.选择表现最好的一套参数在<code>X_train</code>上训练，作为最优模型<br>5.在<code>X_test</code>上对最优模型进行测试</td>
</tr>
</tbody>
</table>
</div>
<div id="模型度量gini系数" class="section level3">
<h3><span class="header-section-number">4.9.5</span> 模型度量——Gini系数</h3>
<ol style="list-style-type: decimal">
<li>Gini系数</li>
</ol>
<p>Gini系数是度量模型表现的一个指标，它的计算公式为：</p>
<p><span class="math display">\[Gini_{CAP} = \frac {a_R} {a_P}\]</span></p>
<p>其中，<span class="math inline">\(a_R\)</span>是某一模型CAP曲线和随机猜模型CAP曲线间的面积，<span class="math inline">\(a_P\)</span>是完美模型CAP曲线和随机猜模型CAP曲线间的面积。</p>
<ol start="2" style="list-style-type: decimal">
<li>Gini系数案例</li>
</ol>
<p><a href="https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation">Gini Coefficient - An Intuitive Explanation</a></p>
<p><img src="./plots/4/gini.png" width="80%"  style="display: block; margin: auto;" /></p>
<p><span class="math display">\[Gini = \frac {0.189} {0.3} = 0.630\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Gini系数代码</li>
</ol>
<p>在python中，我们可以用如下代码来定义Gini系数。</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> make_scorer</span>
<span id="cb43-2"><a href="#cb43-2"></a></span>
<span id="cb43-3"><a href="#cb43-3"></a><span class="co"># Gini coefficient</span></span>
<span id="cb43-4"><a href="#cb43-4"></a><span class="kw">def</span> gini(actual, pred):</span>
<span id="cb43-5"><a href="#cb43-5"></a>    </span>
<span id="cb43-6"><a href="#cb43-6"></a>    <span class="co"># a structural check</span></span>
<span id="cb43-7"><a href="#cb43-7"></a>    <span class="cf">assert</span> (<span class="bu">len</span>(actual) <span class="op">==</span> <span class="bu">len</span>(pred))</span>
<span id="cb43-8"><a href="#cb43-8"></a>    </span>
<span id="cb43-9"><a href="#cb43-9"></a>    <span class="co"># introducing an array called all</span></span>
<span id="cb43-10"><a href="#cb43-10"></a>    <span class="bu">all</span> <span class="op">=</span> np.asarray(np.c_[actual, pred, np.arange(<span class="bu">len</span>(actual))], dtype<span class="op">=</span>np.<span class="bu">float</span>)  <span class="co">#slicing along second axis</span></span>
<span id="cb43-11"><a href="#cb43-11"></a>    </span>
<span id="cb43-12"><a href="#cb43-12"></a>    <span class="co"># sorting the array along predicted probabilities (descending order) and along the index axis all[:, 2] in case of ties</span></span>
<span id="cb43-13"><a href="#cb43-13"></a>    <span class="bu">all</span> <span class="op">=</span> <span class="bu">all</span>[np.lexsort((<span class="bu">all</span>[:, <span class="dv">2</span>], <span class="dv">-1</span> <span class="op">*</span> <span class="bu">all</span>[:, <span class="dv">1</span>]))]                             <span class="co">#</span></span>
<span id="cb43-14"><a href="#cb43-14"></a></span>
<span id="cb43-15"><a href="#cb43-15"></a>    <span class="co"># towards the Gini coefficient</span></span>
<span id="cb43-16"><a href="#cb43-16"></a>    totalLosses <span class="op">=</span> <span class="bu">all</span>[:, <span class="dv">0</span>].<span class="bu">sum</span>()</span>
<span id="cb43-17"><a href="#cb43-17"></a>    giniSum <span class="op">=</span> <span class="bu">all</span>[:, <span class="dv">0</span>].cumsum().<span class="bu">sum</span>() <span class="op">/</span> totalLosses</span>
<span id="cb43-18"><a href="#cb43-18"></a></span>
<span id="cb43-19"><a href="#cb43-19"></a>    giniSum <span class="op">-=</span> (<span class="bu">len</span>(actual) <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="fl">2.</span></span>
<span id="cb43-20"><a href="#cb43-20"></a>    <span class="cf">return</span> giniSum <span class="op">/</span> <span class="bu">len</span>(actual)</span>
<span id="cb43-21"><a href="#cb43-21"></a></span>
<span id="cb43-22"><a href="#cb43-22"></a><span class="co"># normalized Gini coefficient</span></span>
<span id="cb43-23"><a href="#cb43-23"></a><span class="kw">def</span> gini_normalized_score(actual, pred):</span>
<span id="cb43-24"><a href="#cb43-24"></a>    <span class="cf">return</span> gini(actual, pred) <span class="op">/</span> gini(actual, actual)</span>
<span id="cb43-25"><a href="#cb43-25"></a></span>
<span id="cb43-26"><a href="#cb43-26"></a><span class="co"># score using the normalized Gini</span></span>
<span id="cb43-27"><a href="#cb43-27"></a>score_gini <span class="op">=</span> make_scorer(gini_normalized_score, greater_is_better<span class="op">=</span><span class="va">True</span>, needs_threshold <span class="op">=</span> <span class="va">True</span>)</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Gini系数与AUC</li>
</ol>
<p>Gini系数与AUC之间存在如下等式关系：</p>
<p><span class="math display">\[Gini = 2 \times AUC - 1\]</span></p>
<p>在此例中，Gini系数将用于计算交叉验证中模型在验证集上的表现。</p>
</div>
<div id="建立adaboost模型" class="section level3">
<h3><span class="header-section-number">4.9.6</span> 建立AdaBoost模型</h3>
<div id="st0" class="section level4">
<h4><span class="header-section-number">4.9.6.1</span> ST0</h4>
<p>选取tree作为基模型，构建SAMME和SAMME.R模型。</p>
<p>默认参数：<code>n_estimators = 50</code>，<code>learning_rate = 1</code></p>
<p><img src="./plots/4/ada_st0.png" width="80%"  style="display: block; margin: auto;" /></p>
<p>SAMME.R模型的表现优于SAMME模型。</p>
</div>
<div id="st1" class="section level4">
<h4><span class="header-section-number">4.9.6.2</span> ST1</h4>
<p>构建SAMME.R模型，选取参数<code>n_estimators = 500</code>，<code>learning_rate = 1</code>，观察不同<code>max_depth</code>下外样本测试的AUC。</p>
<p>左图<code>max_depth = 1</code>，右图<code>max_depth = 3</code>。</p>
<p><img src="./plots/4/ada_st1.png" width="80%"  style="display: block; margin: auto;" /></p>
<p>当<code>max_depth = 1</code>时，测试集上，SAMME.R的最大AUC为<strong>0.639</strong>，在迭代267次时取得；</p>
<p>当<code>max_depth = 3</code>时，测试集上，SAMME.R的最大AUC为0.624，在迭代8次时取得，出现了过拟合；</p>
<p>当<code>max_depth = 5</code>，迭代极少的次数就出现了过拟合。</p>
<p>因此，当增加树的最大深度时，容易出现过拟合，这时需要降低学习率以避免过拟合。</p>
</div>
<div id="st2" class="section level4">
<h4><span class="header-section-number">4.9.6.3</span> ST2</h4>
<p>接下来，设置不同的<code>max_depth</code>、<code>learning_rate</code>、<code>n_estimators</code>，构建SAMME.R模型，搜寻外样本测试上AUC最大的模型。</p>
<p><img src="./plots/4/ada_st2.png" width="80%"  style="display: block; margin: auto;" /></p>
<p>可以看出，最佳模型的参数是<code>max_depth = 1</code>，<code>learning rate = 0.1</code>，<code>n_estimators = 400</code>，此时AUC为<strong>0.637</strong>。</p>
</div>
</div>
<div id="建立xgboost模型" class="section level3">
<h3><span class="header-section-number">4.9.7</span> 建立XGBoost模型</h3>
<div id="st0-1" class="section level4">
<h4><span class="header-section-number">4.9.7.1</span> ST0</h4>
<p>采用默认参数：<code>learning rate = 0.1</code>，<code>max_depth = 3</code>，<code>n_estimators = 100</code>，构建XGBoost模型。</p>
<p>得出out-of-sample AUC为<strong>0.638</strong>，这已经好于AdaBoost的ST0（0.635）和ST2（0.637），稍差于ST1（0.639）。</p>
</div>
<div id="st1-1" class="section level4">
<h4><span class="header-section-number">4.9.7.2</span> ST1</h4>
<p>设置不同的<code>max_depth</code>、<code>learning_rate</code>、<code>n_estimators</code>，构建XGBoost模型，搜寻外样本测试上AUC最大的模型。</p>
<p><img src="./plots/4/xg_st1.png" width="60%"  style="display: block; margin: auto;" /></p>
<p>可以看到，在<code>max_depth = 2</code>，<code>learning_rate = 0.1</code>，<code>n_estimators = 500</code>和<code>max_depth = 3</code>，<code>learning_rate = 0.1</code>，<code>n_estimators =  300</code>，AUC都取到了最大为<strong>0.643</strong>。</p>
<p>特征重要性排序如下图。</p>
<p><img src="./plots/4/feature_impo.png" width="80%"  style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="结论" class="section level3">
<h3><span class="header-section-number">4.9.8</span> 结论</h3>
<p>总的来说，在此数据集上:</p>
<p>SAMME.R优于SAMME，SAMME.R在单层树、适度的学习率和较大的迭代次数上表现较好；</p>
<p>XGBoost优于AdaBoost，XGBoost在较浅的树、适度的学习率和较大的迭代次数上表现较好。</p>
</div>
</div>
<div id="appendix-commonly-used-python-code-for-py-beginners" class="section level2">
<h2><span class="header-section-number">4.10</span> Appendix: Commonly used Python code (for py-beginners)</h2>
<div id="python标准数据类型" class="section level3">
<h3><span class="header-section-number">4.10.1</span> Python标准数据类型</h3>
<ul>
<li><p>Numbers（数字）：用于存储数值，包括int，long，float和complex。</p></li>
<li><p>String（字符串）：由数字、字母、下划线组成的一串字符。</p></li>
<li><p>List（列表）：Python中使用最频繁的数据类型，可以完成大多数集合类的数据结构实现，它支持数字、字符串甚至可以包含列表（即嵌套）。</p></li>
<li><p>Tuple（元组）：元组不能二次赋值，相当于“只读”列表。</p></li>
<li><p>Dictionary（字典）：除列表以外python之中最灵活的内置数据结构类型，与列表的区别在于——列表是有序的对象集合，字典是无序的对象集合，字典当中的元素是通过键来存取的。</p></li>
</ul>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>n <span class="op">=</span> <span class="fl">3.6</span>  <span class="co"># 数字</span></span>
<span id="cb44-2"><a href="#cb44-2"></a>s <span class="op">=</span> <span class="st">&#39;Hello, python!&#39;</span>  <span class="co"># 字符串</span></span>
<span id="cb44-3"><a href="#cb44-3"></a>L <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="st">&#39;a&#39;</span>]  <span class="co"># 列表</span></span>
<span id="cb44-4"><a href="#cb44-4"></a>t <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">2</span>, <span class="st">&#39;a&#39;</span>)  <span class="co"># 元组</span></span>
<span id="cb44-5"><a href="#cb44-5"></a>d <span class="op">=</span> {<span class="st">&#39;a&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;b&#39;</span>:<span class="dv">2</span>}  <span class="co"># 字典</span></span>
<span id="cb44-6"><a href="#cb44-6"></a><span class="bu">print</span>(n, s, L, t, d, sep <span class="op">=</span> <span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">&#39;</span>)</span></code></pre></div>
</div>
<div id="python内置函数" class="section level3">
<h3><span class="header-section-number">4.10.2</span> Python内置函数</h3>
<ol style="list-style-type: decimal">
<li>输入输出</li>
</ol>
<ul>
<li><p><code>print()</code>将对象输出至控制台</p></li>
<li><p><code>open()</code>打开文件并返回文件对象</p></li>
<li><p><code>input()</code>获取控制台输入</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>迭代相关</li>
</ol>
<ul>
<li><p><code>enumerate()</code>返回元素的序号与对应值</p></li>
<li><p><code>zip()</code>将多个序列中的元素配对，产生新的元组列表</p></li>
<li><p><code>all()</code>如果给定的可迭代参数中的所有元素都为True则返回True，否则返回False</p></li>
<li><p><code>any()</code>如果给定的可迭代参数中的任一元素为True则返回True，否则返回False</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>序列属性</li>
</ol>
<ul>
<li><p><code>max()</code>序列最大值</p></li>
<li><p><code>min()</code>序列最小值</p></li>
<li><p><code>sum()</code>序列的和</p></li>
<li><p><code>len()</code>序列长度</p></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>序列操作</li>
</ol>
<ul>
<li><p><code>range()</code>生成序列</p></li>
<li><p><code>reversed()</code>将序列逆置</p></li>
<li><p><code>sorted()</code>对序列进行排序</p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>对象属性</li>
</ol>
<ul>
<li><p><code>dir()</code>返回属性列表</p></li>
<li><p><code>id()</code>返回对象地址</p></li>
<li><p><code>isinstance()</code>判断对象的类型</p></li>
<li><p><code>type</code>返回对象的类型</p></li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>映射类型</li>
</ol>
<ul>
<li><p><code>eval()</code>去除字符串的单引号，从而获取引号内部内容</p></li>
<li><p><code>map()</code>将传进来的函数应用于序列中的每一个元素，并返回迭代器</p></li>
<li><p><code>slice()</code>生成切片</p></li>
</ul>
</div>
<div id="numpy包" class="section level3">
<h3><span class="header-section-number">4.10.3</span> numpy包</h3>
<p>NumPy(Numerical Python)是Python的一个扩展程序库，支持大量的维度数组与矩阵运算，它也针对数组运算提供大量的数学函数库。</p>
<ol style="list-style-type: decimal">
<li>创建ndarray数组</li>
</ol>
<p>ndarray是一种多维数组对象，其中的所有元素必须是相同类型的。</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb45-2"><a href="#cb45-2"></a>a1 <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])  <span class="co"># 创建数组</span></span>
<span id="cb45-3"><a href="#cb45-3"></a><span class="bu">print</span>(a1)</span>
<span id="cb45-4"><a href="#cb45-4"></a><span class="bu">print</span>(a1.ndim)  <span class="co"># 数组的维度</span></span>
<span id="cb45-5"><a href="#cb45-5"></a><span class="bu">print</span>(a1.shape)  <span class="co"># 数组的形状</span></span>
<span id="cb45-6"><a href="#cb45-6"></a><span class="bu">print</span>(a1.dtype)  <span class="co"># 数组的元素类型</span></span>
<span id="cb45-7"><a href="#cb45-7"></a><span class="bu">print</span>(a1.itemsize)  <span class="co"># 每个元素的字节单位长度</span></span>
<span id="cb45-8"><a href="#cb45-8"></a></span>
<span id="cb45-9"><a href="#cb45-9"></a><span class="co"># 其他创建数组的方法</span></span>
<span id="cb45-10"><a href="#cb45-10"></a>a2 <span class="op">=</span> np.zeros(shape <span class="op">=</span> (<span class="dv">2</span>,<span class="dv">2</span>), dtype <span class="op">=</span> <span class="bu">float</span>)  <span class="co"># 创建元素全是0的数组</span></span>
<span id="cb45-11"><a href="#cb45-11"></a>a3 <span class="op">=</span> np.ones(shape <span class="op">=</span> (<span class="dv">2</span>,<span class="dv">2</span>), dtype <span class="op">=</span> <span class="bu">int</span>)  <span class="co"># 创建元素全是1的数组</span></span>
<span id="cb45-12"><a href="#cb45-12"></a>a4 <span class="op">=</span> np.arange(start <span class="op">=</span> <span class="dv">10</span>, stop <span class="op">=</span> <span class="dv">20</span>, step <span class="op">=</span> <span class="dv">2</span>)  <span class="co"># 创建指定数据范围的数组</span></span>
<span id="cb45-13"><a href="#cb45-13"></a><span class="bu">print</span>(a1, a2, a3, a4, sep <span class="op">=</span> <span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>ndarray对象的切片和索引</li>
</ol>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-2"><a href="#cb46-2"></a>a <span class="op">=</span> np.arange(<span class="dv">24</span>).reshape((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>))  <span class="co"># 创建2维、3行、4列的数组，元素从0-23填充</span></span>
<span id="cb46-3"><a href="#cb46-3"></a><span class="bu">print</span>(a[<span class="dv">0</span>, <span class="dv">0</span>:<span class="dv">2</span>, <span class="dv">1</span>:<span class="dv">3</span>])  <span class="co"># 索引第1个数组第1-2行第2-3列</span></span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-2"><a href="#cb47-2"></a>arr <span class="op">=</span> np.arange(<span class="dv">10</span>)  <span class="co"># 创建元素为0-9的一维数组</span></span>
<span id="cb47-3"><a href="#cb47-3"></a>arr_s <span class="op">=</span> arr[<span class="dv">3</span>:<span class="dv">5</span>]  <span class="co"># 切片，提出数组的第4、5个元素</span></span>
<span id="cb47-4"><a href="#cb47-4"></a>arr_s[:] <span class="op">=</span> <span class="dv">99</span>  <span class="co"># 将99赋值给切片arr_s中的所有元素</span></span>
<span id="cb47-5"><a href="#cb47-5"></a><span class="bu">print</span>(arr_s, arr, sep <span class="op">=</span> <span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">&#39;</span>)  <span class="co"># 修改会直接反映到源数组上</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>数学运算</li>
</ol>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb48-2"><a href="#cb48-2"></a>a <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">5.55</span>, <span class="dv">123</span>, <span class="fl">0.567</span>, <span class="fl">25.532</span>])  </span>
<span id="cb48-3"><a href="#cb48-3"></a>b <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">1</span>)</span>
<span id="cb48-4"><a href="#cb48-4"></a></span>
<span id="cb48-5"><a href="#cb48-5"></a><span class="bu">print</span>(np.around(a, decimals <span class="op">=</span> <span class="dv">1</span>))  <span class="co"># 四舍五入至1位小数</span></span>
<span id="cb48-6"><a href="#cb48-6"></a><span class="bu">print</span>(np.floor(a))  <span class="co"># 向下取整</span></span>
<span id="cb48-7"><a href="#cb48-7"></a><span class="bu">print</span>(np.ceil(a))  <span class="co"># 向上取整</span></span>
<span id="cb48-8"><a href="#cb48-8"></a></span>
<span id="cb48-9"><a href="#cb48-9"></a><span class="bu">print</span>(np.sqrt(a))  <span class="co"># 开根号</span></span>
<span id="cb48-10"><a href="#cb48-10"></a><span class="bu">print</span>(np.square(a))  <span class="co"># 平方</span></span>
<span id="cb48-11"><a href="#cb48-11"></a><span class="bu">print</span>(np.log(a))  <span class="co"># 取对数</span></span>
<span id="cb48-12"><a href="#cb48-12"></a><span class="bu">print</span>(np.exp(a))  <span class="co"># 取指数</span></span>
<span id="cb48-13"><a href="#cb48-13"></a><span class="bu">print</span>(np.sign(a))  <span class="co"># 取符号函数</span></span>
<span id="cb48-14"><a href="#cb48-14"></a></span>
<span id="cb48-15"><a href="#cb48-15"></a><span class="bu">print</span>(np.add(a, b))  <span class="co"># 两个数组相加</span></span>
<span id="cb48-16"><a href="#cb48-16"></a><span class="bu">print</span>(np.subtract(a, b))  <span class="co"># 两个数组相减</span></span>
<span id="cb48-17"><a href="#cb48-17"></a><span class="bu">print</span>(np.multiply(a, b))  <span class="co"># 两个数组相乘</span></span>
<span id="cb48-18"><a href="#cb48-18"></a><span class="bu">print</span>(np.divide(a, b))  <span class="co"># 两个数组相除</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>统计运算</li>
</ol>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb49-2"><a href="#cb49-2"></a>a <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">5</span>], [<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">3</span>], [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">9</span>]])  </span>
<span id="cb49-3"><a href="#cb49-3"></a></span>
<span id="cb49-4"><a href="#cb49-4"></a><span class="bu">print</span>(np.<span class="bu">min</span>(a, axis <span class="op">=</span> <span class="dv">0</span>))  <span class="co"># 沿纵轴的最小值</span></span>
<span id="cb49-5"><a href="#cb49-5"></a><span class="bu">print</span>(np.<span class="bu">max</span>(a, axis <span class="op">=</span> <span class="dv">1</span>))  <span class="co"># 沿横轴的最大值        # 以下函数均可以通过参数axis选择纵轴（axis=0）或横轴（axis=1）</span></span>
<span id="cb49-6"><a href="#cb49-6"></a><span class="bu">print</span>(np.ptp(a))  <span class="co"># 数组中元素最大值与最小值的差</span></span>
<span id="cb49-7"><a href="#cb49-7"></a><span class="bu">print</span>(np.percentile(a, q <span class="op">=</span> <span class="dv">70</span>, axis <span class="op">=</span> <span class="dv">0</span>))  <span class="co"># 百分位数</span></span>
<span id="cb49-8"><a href="#cb49-8"></a><span class="bu">print</span>(np.<span class="bu">sum</span>(a))  <span class="co"># 求和</span></span>
<span id="cb49-9"><a href="#cb49-9"></a><span class="bu">print</span>(np.median(a))  <span class="co"># 中位数</span></span>
<span id="cb49-10"><a href="#cb49-10"></a><span class="bu">print</span>(np.mean(a))  <span class="co"># 均值</span></span>
<span id="cb49-11"><a href="#cb49-11"></a><span class="bu">print</span>(np.average(a, axis <span class="op">=</span> <span class="dv">0</span>, weights <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>], returned <span class="op">=</span> <span class="va">True</span>))  <span class="co"># 加权平均数</span></span>
<span id="cb49-12"><a href="#cb49-12"></a><span class="bu">print</span>(np.std(a))  <span class="co"># 标准差</span></span>
<span id="cb49-13"><a href="#cb49-13"></a><span class="bu">print</span>(np.var(a))  <span class="co"># 方差</span></span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>排序</li>
</ol>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb50-2"><a href="#cb50-2"></a>a <span class="op">=</span> np.array([(<span class="st">&quot;raju&quot;</span>,<span class="dv">21</span>), (<span class="st">&quot;anil&quot;</span>,<span class="dv">25</span>), (<span class="st">&quot;ravi&quot;</span>,<span class="dv">17</span>), (<span class="st">&quot;amar&quot;</span>,<span class="dv">27</span>)], dtype <span class="op">=</span> np.dtype([(<span class="st">&#39;name&#39;</span>,<span class="st">&#39;S10&#39;</span>), (<span class="st">&#39;age&#39;</span>,<span class="bu">int</span>)]))</span>
<span id="cb50-3"><a href="#cb50-3"></a></span>
<span id="cb50-4"><a href="#cb50-4"></a><span class="bu">print</span>(a)</span>
<span id="cb50-5"><a href="#cb50-5"></a><span class="bu">print</span>(np.sort(a, order <span class="op">=</span> <span class="st">&#39;age&#39;</span>))  <span class="co"># 从小到大排序</span></span>
<span id="cb50-6"><a href="#cb50-6"></a><span class="bu">print</span>(np.argsort(a, order <span class="op">=</span> <span class="st">&#39;age&#39;</span>))  <span class="co"># 从小到大排序的索引</span></span></code></pre></div>
<ol start="6" style="list-style-type: decimal">
<li>线性代数</li>
</ol>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb51-2"><a href="#cb51-2"></a>a <span class="op">=</span> np.arange(<span class="dv">6</span>).reshape(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb51-3"><a href="#cb51-3"></a>b <span class="op">=</span> np.arange(<span class="dv">6</span>).reshape(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb51-4"><a href="#cb51-4"></a>c <span class="op">=</span> a.copy()  <span class="co"># 复制</span></span>
<span id="cb51-5"><a href="#cb51-5"></a></span>
<span id="cb51-6"><a href="#cb51-6"></a><span class="bu">print</span>(a)</span>
<span id="cb51-7"><a href="#cb51-7"></a><span class="bu">print</span>(a.T)  <span class="co"># 转置</span></span>
<span id="cb51-8"><a href="#cb51-8"></a><span class="bu">print</span>(np.dot(a,b))  <span class="co"># 数组点积</span></span>
<span id="cb51-9"><a href="#cb51-9"></a><span class="bu">print</span>(np.vdot(a,b))  <span class="co"># 向量点积, 多维数组会被展开</span></span>
<span id="cb51-10"><a href="#cb51-10"></a><span class="bu">print</span>(np.inner(a,c))  <span class="co"># 向量内积，对于更高的维度，它返回最后一个轴上的和的乘积</span></span></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb52-2"><a href="#cb52-2"></a>a <span class="op">=</span> np.arange(<span class="dv">4</span>).reshape(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb52-3"><a href="#cb52-3"></a><span class="bu">print</span>(a)</span>
<span id="cb52-4"><a href="#cb52-4"></a><span class="bu">print</span>(np.diag(a))  <span class="co"># 对角阵</span></span>
<span id="cb52-5"><a href="#cb52-5"></a><span class="bu">print</span>(np.linalg.inv(a))  <span class="co"># 逆</span></span>
<span id="cb52-6"><a href="#cb52-6"></a><span class="bu">print</span>(np.linalg.det(a))  <span class="co"># 行列式</span></span>
<span id="cb52-7"><a href="#cb52-7"></a><span class="bu">print</span>(np.linalg.eig(a))  <span class="co"># 特征值与特征向量</span></span>
<span id="cb52-8"><a href="#cb52-8"></a><span class="bu">print</span>(np.linalg.svd(a))  <span class="co"># 奇异值分解</span></span></code></pre></div>
<ol start="7" style="list-style-type: decimal">
<li>随机数</li>
</ol>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-2"><a href="#cb53-2"></a>np.random.seed(<span class="dv">123</span>)  <span class="co"># 随机数种子</span></span>
<span id="cb53-3"><a href="#cb53-3"></a></span>
<span id="cb53-4"><a href="#cb53-4"></a><span class="bu">print</span>(np.random.rand(<span class="dv">2</span>, <span class="dv">2</span>))  <span class="co"># 均匀分布</span></span>
<span id="cb53-5"><a href="#cb53-5"></a><span class="bu">print</span>(np.random.randn(<span class="dv">2</span>, <span class="dv">3</span>)) <span class="co"># 标准正态分布</span></span>
<span id="cb53-6"><a href="#cb53-6"></a><span class="bu">print</span>(np.random.randint(low <span class="op">=</span> <span class="dv">0</span>, high <span class="op">=</span> <span class="dv">100</span>, size <span class="op">=</span> (<span class="dv">2</span>, <span class="dv">2</span>)))  <span class="co"># 随机整数</span></span>
<span id="cb53-7"><a href="#cb53-7"></a></span>
<span id="cb53-8"><a href="#cb53-8"></a><span class="co"># 分布</span></span>
<span id="cb53-9"><a href="#cb53-9"></a><span class="bu">print</span>(np.random.normal(loc <span class="op">=</span> <span class="dv">3</span>, scale <span class="op">=</span> <span class="dv">9</span>, size <span class="op">=</span> <span class="dv">2</span>))  <span class="co"># 正态</span></span>
<span id="cb53-10"><a href="#cb53-10"></a><span class="bu">print</span>(np.random.poisson(lam <span class="op">=</span> <span class="dv">10</span>, size <span class="op">=</span> <span class="dv">6</span>))  <span class="co"># 泊松</span></span>
<span id="cb53-11"><a href="#cb53-11"></a><span class="bu">print</span>(np.random.binomial(n <span class="op">=</span> <span class="dv">10</span>, p <span class="op">=</span> <span class="fl">0.1</span>, size <span class="op">=</span> (<span class="dv">2</span>, <span class="dv">2</span>)))  <span class="co"># 二项</span></span>
<span id="cb53-12"><a href="#cb53-12"></a><span class="bu">print</span>(np.random.negative_binomial(n <span class="op">=</span> <span class="dv">10</span>, p <span class="op">=</span> <span class="fl">0.1</span>, size <span class="op">=</span> <span class="dv">1</span>))  <span class="co"># 负二项</span></span>
<span id="cb53-13"><a href="#cb53-13"></a><span class="bu">print</span>(np.random.gamma(shape <span class="op">=</span> <span class="dv">3</span>, scale <span class="op">=</span> <span class="dv">2</span>, size <span class="op">=</span> <span class="dv">10</span>))  <span class="co"># 伽马</span></span></code></pre></div>
</div>
<div id="pandas包" class="section level3">
<h3><span class="header-section-number">4.10.4</span> pandas包</h3>
<p>pandas是基于NumPy的一个为解决数据分析任务而创建的包，提供了大量能使我们快速便捷地处理数据的函数和方法。</p>
<ol style="list-style-type: decimal">
<li>创建DataFrame</li>
</ol>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb54-2"><a href="#cb54-2"></a></span>
<span id="cb54-3"><a href="#cb54-3"></a>data1 <span class="op">=</span> pd.read_csv(<span class="st">&#39;file.csv&#39;</span>, encoding <span class="op">=</span> <span class="st">&#39;gbk&#39;</span>)  <span class="co"># 从外部读入csv文件</span></span>
<span id="cb54-4"><a href="#cb54-4"></a></span>
<span id="cb54-5"><a href="#cb54-5"></a>data2 <span class="op">=</span> {<span class="st">&#39;state&#39;</span>: [<span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Nevada&#39;</span>, <span class="st">&#39;Nevada&#39;</span>],  <span class="co"># 先创建字典</span></span>
<span id="cb54-6"><a href="#cb54-6"></a>        <span class="st">&#39;year&#39;</span>: [<span class="dv">2000</span>, <span class="dv">2001</span>, <span class="dv">2002</span>, <span class="dv">2001</span>, <span class="dv">2002</span>],</span>
<span id="cb54-7"><a href="#cb54-7"></a>        <span class="st">&#39;pop&#39;</span>: [<span class="fl">1.5</span>, <span class="fl">1.7</span>, <span class="fl">3.6</span>, <span class="fl">2.4</span>, <span class="fl">2.9</span>]}</span>
<span id="cb54-8"><a href="#cb54-8"></a>data2 <span class="op">=</span> pd.DataFrame(data2, columns <span class="op">=</span> [<span class="st">&#39;year&#39;</span>, <span class="st">&#39;state&#39;</span>, <span class="st">&#39;pop&#39;</span>])  <span class="co"># 基于字典创建DataFrame</span></span>
<span id="cb54-9"><a href="#cb54-9"></a>data2[<span class="st">&#39;debt&#39;</span>] <span class="op">=</span> <span class="fl">16.5</span>  <span class="co"># 新增一列debt</span></span>
<span id="cb54-10"><a href="#cb54-10"></a></span>
<span id="cb54-11"><a href="#cb54-11"></a><span class="bu">print</span>(data1, data2, sep <span class="op">=</span> <span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">&#39;</span>)</span>
<span id="cb54-12"><a href="#cb54-12"></a></span>
<span id="cb54-13"><a href="#cb54-13"></a><span class="bu">print</span>(data2.dtypes)  <span class="co"># 元素类型</span></span>
<span id="cb54-14"><a href="#cb54-14"></a><span class="bu">print</span>(data2.columns)  <span class="co"># 列名</span></span>
<span id="cb54-15"><a href="#cb54-15"></a><span class="bu">print</span>(data2.shape)  <span class="co"># 形状</span></span>
<span id="cb54-16"><a href="#cb54-16"></a><span class="bu">print</span>(data2.head(<span class="dv">10</span>))  <span class="co"># 看前10条记录</span></span>
<span id="cb54-17"><a href="#cb54-17"></a><span class="bu">print</span>(data2.tail(<span class="dv">5</span>))  <span class="co"># 看后5条记录</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>索引</li>
</ol>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb55-2"><a href="#cb55-2"></a>data <span class="op">=</span> {<span class="st">&#39;state&#39;</span>: [<span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Nevada&#39;</span>, <span class="st">&#39;Nevada&#39;</span>],</span>
<span id="cb55-3"><a href="#cb55-3"></a>        <span class="st">&#39;year&#39;</span>: [<span class="dv">2000</span>, <span class="dv">2001</span>, <span class="dv">2002</span>, <span class="dv">2001</span>, <span class="dv">2002</span>],</span>
<span id="cb55-4"><a href="#cb55-4"></a>        <span class="st">&#39;pop&#39;</span>: [<span class="fl">1.5</span>, <span class="fl">1.7</span>, <span class="fl">3.6</span>, <span class="fl">2.4</span>, <span class="fl">2.9</span>]}</span>
<span id="cb55-5"><a href="#cb55-5"></a>data <span class="op">=</span> pd.DataFrame(data, columns <span class="op">=</span> [<span class="st">&#39;year&#39;</span>, <span class="st">&#39;state&#39;</span>, <span class="st">&#39;pop&#39;</span>])</span>
<span id="cb55-6"><a href="#cb55-6"></a>data[<span class="st">&#39;debt&#39;</span>] <span class="op">=</span> <span class="fl">16.5</span></span>
<span id="cb55-7"><a href="#cb55-7"></a></span>
<span id="cb55-8"><a href="#cb55-8"></a><span class="bu">print</span>(data)</span>
<span id="cb55-9"><a href="#cb55-9"></a><span class="bu">print</span>(data[<span class="dv">0</span>:<span class="dv">2</span>])  <span class="co"># 索引第1-2行</span></span>
<span id="cb55-10"><a href="#cb55-10"></a><span class="bu">print</span>(data.iloc[<span class="dv">0</span>:<span class="dv">2</span>])  <span class="co"># 索引第1-2行</span></span>
<span id="cb55-11"><a href="#cb55-11"></a><span class="bu">print</span>(data.loc[<span class="dv">0</span>:<span class="dv">2</span>])  <span class="co"># 索引index为0-2的行</span></span>
<span id="cb55-12"><a href="#cb55-12"></a><span class="bu">print</span>(data[<span class="st">&#39;year&#39;</span>])  <span class="co"># 索引名为year的列</span></span>
<span id="cb55-13"><a href="#cb55-13"></a><span class="bu">print</span>(data.loc[<span class="dv">0</span>,<span class="st">&#39;year&#39;</span>])</span>
<span id="cb55-14"><a href="#cb55-14"></a><span class="bu">print</span>(data.iloc[<span class="dv">0</span>:<span class="dv">2</span>, <span class="dv">0</span>:<span class="dv">2</span>])  <span class="co"># 索引第1-2行、第1-2列</span></span>
<span id="cb55-15"><a href="#cb55-15"></a></span>
<span id="cb55-16"><a href="#cb55-16"></a><span class="bu">print</span>(data[data[<span class="st">&#39;pop&#39;</span>]<span class="op">&gt;</span><span class="dv">2</span>])  <span class="co"># 索引pop&gt;2的行</span></span>
<span id="cb55-17"><a href="#cb55-17"></a><span class="bu">print</span>(data[(data[<span class="st">&#39;pop&#39;</span>]<span class="op">&gt;</span><span class="dv">2</span>) <span class="op">&amp;</span> (data[<span class="st">&#39;state&#39;</span>] <span class="op">==</span> <span class="st">&#39;Ohio&#39;</span>)])  <span class="co"># 索引pop&gt;2且state是Ohio的行</span></span>
<span id="cb55-18"><a href="#cb55-18"></a><span class="bu">print</span>(data[(data[<span class="st">&#39;pop&#39;</span>]<span class="op">&gt;</span><span class="dv">2</span>) <span class="op">&amp;</span> (data[<span class="st">&#39;state&#39;</span>] <span class="op">==</span> <span class="st">&#39;Ohio&#39;</span>)][[<span class="st">&#39;year&#39;</span>, <span class="st">&#39;debt&#39;</span>]])  <span class="co"># 索引pop&gt;2且state是Ohio的行、名为year和debt的列</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>数据预处理</li>
</ol>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-2"><a href="#cb56-2"></a>data <span class="op">=</span> {<span class="st">&#39;state&#39;</span>: [<span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Nevada&#39;</span>, <span class="st">&#39;Nevada&#39;</span>],</span>
<span id="cb56-3"><a href="#cb56-3"></a>        <span class="st">&#39;year&#39;</span>: [<span class="dv">2000</span>, <span class="dv">2001</span>, <span class="dv">2001</span>, <span class="dv">2001</span>, <span class="dv">2002</span>],</span>
<span id="cb56-4"><a href="#cb56-4"></a>        <span class="st">&#39;pop&#39;</span>: [<span class="fl">1.5</span>, <span class="fl">1.7</span>, <span class="fl">1.7</span>, <span class="fl">2.4</span>, <span class="va">None</span>]}</span>
<span id="cb56-5"><a href="#cb56-5"></a>data <span class="op">=</span> pd.DataFrame(data, columns <span class="op">=</span> [<span class="st">&#39;year&#39;</span>, <span class="st">&#39;state&#39;</span>, <span class="st">&#39;pop&#39;</span>])</span>
<span id="cb56-6"><a href="#cb56-6"></a>data[<span class="st">&#39;debt&#39;</span>] <span class="op">=</span> <span class="fl">16.5</span></span>
<span id="cb56-7"><a href="#cb56-7"></a><span class="bu">print</span>(data)</span>
<span id="cb56-8"><a href="#cb56-8"></a><span class="bu">print</span>(data.drop_duplicates())  <span class="co"># 删除重复行</span></span>
<span id="cb56-9"><a href="#cb56-9"></a><span class="bu">print</span>(data.dropna(axis <span class="op">=</span> <span class="dv">0</span>, how <span class="op">=</span> <span class="st">&quot;any&quot;</span>))  <span class="co"># 删除有缺失值的行</span></span>
<span id="cb56-10"><a href="#cb56-10"></a><span class="bu">print</span>(data.drop([<span class="st">&#39;debt&#39;</span>], axis <span class="op">=</span> <span class="dv">1</span>))  <span class="co"># 删除列debt</span></span>
<span id="cb56-11"><a href="#cb56-11"></a></span>
<span id="cb56-12"><a href="#cb56-12"></a><span class="bu">print</span>(pd.get_dummies(data, drop_first <span class="op">=</span> <span class="va">True</span>))  <span class="co"># 生成哑变量</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>排序</li>
</ol>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb57-2"><a href="#cb57-2"></a>data <span class="op">=</span> {<span class="st">&#39;state&#39;</span>: [<span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Nevada&#39;</span>, <span class="st">&#39;Nevada&#39;</span>],</span>
<span id="cb57-3"><a href="#cb57-3"></a>        <span class="st">&#39;year&#39;</span>: [<span class="dv">2000</span>, <span class="dv">2001</span>, <span class="dv">2002</span>, <span class="dv">2001</span>, <span class="dv">2002</span>],</span>
<span id="cb57-4"><a href="#cb57-4"></a>        <span class="st">&#39;pop&#39;</span>: [<span class="fl">1.5</span>, <span class="fl">1.7</span>, <span class="fl">3.6</span>, <span class="fl">2.4</span>, <span class="fl">2.9</span>]}</span>
<span id="cb57-5"><a href="#cb57-5"></a>data <span class="op">=</span> pd.DataFrame(data, columns <span class="op">=</span> [<span class="st">&#39;year&#39;</span>, <span class="st">&#39;state&#39;</span>, <span class="st">&#39;pop&#39;</span>])</span>
<span id="cb57-6"><a href="#cb57-6"></a>data[<span class="st">&#39;debt&#39;</span>] <span class="op">=</span> <span class="fl">16.5</span></span>
<span id="cb57-7"><a href="#cb57-7"></a></span>
<span id="cb57-8"><a href="#cb57-8"></a><span class="bu">print</span>(data.sort_values(by <span class="op">=</span> <span class="st">&#39;year&#39;</span>, ascending <span class="op">=</span> <span class="va">True</span>))  <span class="co"># 按照year的值升序</span></span>
<span id="cb57-9"><a href="#cb57-9"></a><span class="bu">print</span>(data.sort_index(axis <span class="op">=</span> <span class="dv">1</span>))  <span class="co"># 按照列索引升序</span></span>
<span id="cb57-10"><a href="#cb57-10"></a><span class="bu">print</span>(data.rank())  <span class="co"># 求秩</span></span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>统计分析</li>
</ol>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb58-2"><a href="#cb58-2"></a>data <span class="op">=</span> {<span class="st">&#39;state&#39;</span>: [<span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Ohio&#39;</span>, <span class="st">&#39;Nevada&#39;</span>, <span class="st">&#39;Nevada&#39;</span>],</span>
<span id="cb58-3"><a href="#cb58-3"></a>        <span class="st">&#39;year&#39;</span>: [<span class="dv">2000</span>, <span class="dv">2001</span>, <span class="dv">2002</span>, <span class="dv">2001</span>, <span class="dv">2002</span>],</span>
<span id="cb58-4"><a href="#cb58-4"></a>        <span class="st">&#39;pop&#39;</span>: [<span class="fl">1.5</span>, <span class="fl">1.7</span>, <span class="fl">3.6</span>, <span class="fl">2.4</span>, <span class="fl">2.9</span>]}</span>
<span id="cb58-5"><a href="#cb58-5"></a>data <span class="op">=</span> pd.DataFrame(data, columns <span class="op">=</span> [<span class="st">&#39;year&#39;</span>, <span class="st">&#39;state&#39;</span>, <span class="st">&#39;pop&#39;</span>])</span>
<span id="cb58-6"><a href="#cb58-6"></a>data[<span class="st">&#39;debt&#39;</span>] <span class="op">=</span> <span class="fl">16.5</span></span>
<span id="cb58-7"><a href="#cb58-7"></a></span>
<span id="cb58-8"><a href="#cb58-8"></a><span class="bu">print</span>(data.describe())  <span class="co"># 对每列计算基本统计量</span></span>
<span id="cb58-9"><a href="#cb58-9"></a><span class="bu">print</span>(data.count())  <span class="co"># 计数</span></span>
<span id="cb58-10"><a href="#cb58-10"></a><span class="bu">print</span>(data.<span class="bu">max</span>())  <span class="co"># 最大值</span></span>
<span id="cb58-11"><a href="#cb58-11"></a><span class="bu">print</span>(data.<span class="bu">min</span>())  <span class="co"># 最小值</span></span>
<span id="cb58-12"><a href="#cb58-12"></a><span class="bu">print</span>(data.<span class="bu">sum</span>())  <span class="co"># 和</span></span>
<span id="cb58-13"><a href="#cb58-13"></a><span class="bu">print</span>(data.mean())  <span class="co"># 均值</span></span>
<span id="cb58-14"><a href="#cb58-14"></a><span class="bu">print</span>(data.median())  <span class="co"># 中位数</span></span>
<span id="cb58-15"><a href="#cb58-15"></a><span class="bu">print</span>(data.var())  <span class="co"># 方差</span></span>
<span id="cb58-16"><a href="#cb58-16"></a><span class="bu">print</span>(data.std())  <span class="co"># 标准差</span></span>
<span id="cb58-17"><a href="#cb58-17"></a><span class="bu">print</span>(data.cov())  <span class="co"># 协方差</span></span>
<span id="cb58-18"><a href="#cb58-18"></a><span class="bu">print</span>(data.corr())  <span class="co"># 相关系数</span></span></code></pre></div>
</div>
<div id="matplotlib包" class="section level3">
<h3><span class="header-section-number">4.10.5</span> Matplotlib包</h3>
<p>Matplotlib是一个Python 的2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。</p>
<ol style="list-style-type: decimal">
<li>折线图</li>
</ol>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb59-2"><a href="#cb59-2"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt </span>
<span id="cb59-3"><a href="#cb59-3"></a> </span>
<span id="cb59-4"><a href="#cb59-4"></a>x <span class="op">=</span> np.arange(<span class="dv">1</span>,<span class="dv">11</span>) </span>
<span id="cb59-5"><a href="#cb59-5"></a>y <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">5</span> </span>
<span id="cb59-6"><a href="#cb59-6"></a></span>
<span id="cb59-7"><a href="#cb59-7"></a>plt.title(<span class="st">&#39;Matplotlib demo&#39;</span>) </span>
<span id="cb59-8"><a href="#cb59-8"></a>plt.xlabel(<span class="st">&#39;x&#39;</span>) </span>
<span id="cb59-9"><a href="#cb59-9"></a>plt.ylabel(<span class="st">&#39;y&#39;</span>) </span>
<span id="cb59-10"><a href="#cb59-10"></a>plt.plot(x, y, ls <span class="op">=</span> <span class="st">&#39;--&#39;</span>, marker <span class="op">=</span> <span class="st">&#39;+&#39;</span>, color <span class="op">=</span> <span class="st">&#39;lightblue&#39;</span>)  <span class="co"># ls为线型，marker为标记类型</span></span>
<span id="cb59-11"><a href="#cb59-11"></a>plt.show()</span></code></pre></div>
<p><img src="./plots/4/折线图.png" width="50%"  style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li>散点图</li>
</ol>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb60-2"><a href="#cb60-2"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt </span>
<span id="cb60-3"><a href="#cb60-3"></a></span>
<span id="cb60-4"><a href="#cb60-4"></a>x <span class="op">=</span> np.random.random(<span class="dv">100</span>)</span>
<span id="cb60-5"><a href="#cb60-5"></a>y <span class="op">=</span> np.random.random(<span class="dv">100</span>)</span>
<span id="cb60-6"><a href="#cb60-6"></a></span>
<span id="cb60-7"><a href="#cb60-7"></a>plt.scatter(x, y, s<span class="op">=</span>x<span class="op">*</span><span class="dv">1000</span>, color<span class="op">=</span><span class="st">&#39;pink&#39;</span>, marker<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">1</span>), alpha<span class="op">=</span><span class="fl">0.5</span>, lw<span class="op">=</span><span class="dv">2</span>)  <span class="co"># s为图像大小，lw为图像边框宽度</span></span>
<span id="cb60-8"><a href="#cb60-8"></a>plt.show()</span></code></pre></div>
<p><img src="./plots/4/散点图.png" width="50%"  style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li>箱线图</li>
</ol>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-2"><a href="#cb61-2"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb61-3"><a href="#cb61-3"></a>x <span class="op">=</span> np.random.gamma(shape <span class="op">=</span> <span class="dv">3</span>, scale <span class="op">=</span> <span class="dv">2</span>, size <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb61-4"><a href="#cb61-4"></a></span>
<span id="cb61-5"><a href="#cb61-5"></a>plt.boxplot(x, vert<span class="op">=</span><span class="va">True</span>)  <span class="co"># vert控制方向</span></span>
<span id="cb61-6"><a href="#cb61-6"></a>plt.show()</span></code></pre></div>
<p><img src="./plots/4/箱线图.png" width="50%"  style="display: block; margin: auto;" /></p>
<ol start="4" style="list-style-type: decimal">
<li>条形图</li>
</ol>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb62-2"><a href="#cb62-2"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb62-3"><a href="#cb62-3"></a></span>
<span id="cb62-4"><a href="#cb62-4"></a>x_index <span class="op">=</span> np.arange(<span class="dv">5</span>)   <span class="co">#柱的索引</span></span>
<span id="cb62-5"><a href="#cb62-5"></a>x_data <span class="op">=</span> [<span class="st">&#39;A&#39;</span>, <span class="st">&#39;B&#39;</span>, <span class="st">&#39;C&#39;</span>, <span class="st">&#39;D&#39;</span>, <span class="st">&#39;E&#39;</span>]</span>
<span id="cb62-6"><a href="#cb62-6"></a>y1_data <span class="op">=</span> [<span class="dv">20</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">35</span>, <span class="dv">27</span>]</span>
<span id="cb62-7"><a href="#cb62-7"></a>y2_data <span class="op">=</span> [<span class="dv">25</span>, <span class="dv">32</span>, <span class="dv">34</span>, <span class="dv">20</span>, <span class="dv">25</span>]</span>
<span id="cb62-8"><a href="#cb62-8"></a></span>
<span id="cb62-9"><a href="#cb62-9"></a>plt.bar(x_index, y1_data, width<span class="op">=</span><span class="fl">0.35</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">&#39;lightblue&#39;</span>, label<span class="op">=</span><span class="st">&#39;y1&#39;</span>)  <span class="co"># 参数：左偏移、高度、柱宽、透明度、颜色、图例</span></span>
<span id="cb62-10"><a href="#cb62-10"></a>plt.bar(x_index <span class="op">+</span> <span class="fl">0.35</span>, y2_data, width<span class="op">=</span><span class="fl">0.35</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">&#39;pink&#39;</span>, label<span class="op">=</span><span class="st">&#39;y2&#39;</span>)</span>
<span id="cb62-11"><a href="#cb62-11"></a></span>
<span id="cb62-12"><a href="#cb62-12"></a>plt.xticks(x_index <span class="op">+</span> bar_width<span class="op">/</span><span class="dv">2</span>, x_data)  <span class="co"># x轴刻度线</span></span>
<span id="cb62-13"><a href="#cb62-13"></a>plt.legend()  <span class="co"># 显示图例</span></span>
<span id="cb62-14"><a href="#cb62-14"></a>plt.show()</span></code></pre></div>
<p><img src="./plots/4/条形图.png" width="50%"  style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>直方图</li>
</ol>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb63-2"><a href="#cb63-2"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb63-3"><a href="#cb63-3"></a></span>
<span id="cb63-4"><a href="#cb63-4"></a>x <span class="op">=</span> np.random.randn(<span class="dv">10000</span>)</span>
<span id="cb63-5"><a href="#cb63-5"></a></span>
<span id="cb63-6"><a href="#cb63-6"></a>plt.hist(x, bins<span class="op">=</span><span class="dv">40</span>, density<span class="op">=</span><span class="va">True</span>, histtype<span class="op">=</span><span class="st">&#39;bar&#39;</span>, color<span class="op">=</span><span class="st">&#39;lightblue&#39;</span>)</span>
<span id="cb63-7"><a href="#cb63-7"></a>plt.show()</span></code></pre></div>
<p><img src="./plots/4/直方图.png" width="50%"  style="display: block; margin: auto;" /></p>
<ol start="6" style="list-style-type: decimal">
<li>饼图</li>
</ol>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb64-2"><a href="#cb64-2"></a></span>
<span id="cb64-3"><a href="#cb64-3"></a>labels <span class="op">=</span> [<span class="st">&#39;A&#39;</span>, <span class="st">&#39;B&#39;</span>, <span class="st">&#39;C&#39;</span>, <span class="st">&#39;D&#39;</span>]</span>
<span id="cb64-4"><a href="#cb64-4"></a>x <span class="op">=</span> [<span class="dv">15</span>, <span class="dv">30</span>, <span class="dv">45</span>, <span class="dv">10</span>]</span>
<span id="cb64-5"><a href="#cb64-5"></a>explode <span class="op">=</span> (<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb64-6"><a href="#cb64-6"></a>colors <span class="op">=</span> [<span class="st">&#39;pink&#39;</span>, <span class="st">&#39;tomato&#39;</span>, <span class="st">&#39;lightblue&#39;</span>, <span class="st">&#39;lightyellow&#39;</span>]</span>
<span id="cb64-7"><a href="#cb64-7"></a>    </span>
<span id="cb64-8"><a href="#cb64-8"></a>plt.pie(x, labels<span class="op">=</span>labels, autopct<span class="op">=</span><span class="st">&#39;</span><span class="sc">%1.1f%%</span><span class="st">&#39;</span>, shadow<span class="op">=</span><span class="va">False</span>, explode<span class="op">=</span>explode, startangle<span class="op">=</span><span class="dv">90</span>, colors<span class="op">=</span>colors)</span>
<span id="cb64-9"><a href="#cb64-9"></a>plt.axis(<span class="st">&#39;equal&#39;</span>)</span>
<span id="cb64-10"><a href="#cb64-10"></a>plt.legend(labels<span class="op">=</span>labels, loc<span class="op">=</span><span class="st">&#39;right&#39;</span>)</span>
<span id="cb64-11"><a href="#cb64-11"></a>plt.show()</span></code></pre></div>
<p><img src="./plots/4/饼图.png" width="50%"  style="display: block; margin: auto;" /></p>
</div>
<div id="常用教程网址" class="section level3">
<h3><span class="header-section-number">4.10.6</span> 常用教程网址</h3>
<ul>
<li><p><a href="https://www.runoob.com/python/python-tutorial.html">Python基础教程</a></p></li>
<li><p><a href="https://docs.python.org/3/">Python3说明文档</a></p></li>
<li><p><a href="https://matplotlib.org/">matplotlib官网</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/">sklearn学习</a></p></li>
</ul>
<!--chapter:end:04-boosting.Rmd-->
</div>
</div>
</div>
<div id="unsupervised-learning" class="section level1">
<h1><span class="header-section-number">5</span> 无监督学习方法</h1>
<p><em>梁译中、方明慧、王晗、高光远</em></p>
<p>大学及以后生活中最常用的学习方法。</p>
<p>在无监督学习中， 我们可以降低数据（协变量，特征）维度、根据特征的相似度对样本进行聚类、设计可视化工具揭示高维数据的特性。无监督学习不考虑响应变量，仅考虑特征的相似性。</p>
<p>本章将考虑以下几种方法：</p>
<ul>
<li><p><strong>降维</strong>：</p>
<ul>
<li><p>主成分分析（PCA）</p></li>
<li><p>自编码，瓶颈神经网络（BNN）</p></li>
</ul></li>
<li><p><strong>聚类</strong>：</p>
<ul>
<li><p>分层聚类：不需事先指定聚类个数</p>
<ul>
<li><p>自下而上：初始<span class="math inline">\(n\)</span>类，再将相距最近的两类合并，建立一个新的类，直到最后合并成<span class="math inline">\(1\)</span>类；</p></li>
<li><p>自上而下：初始<span class="math inline">\(1\)</span>类，再将相距最远的样本分裂成两类，直到最后分裂成<span class="math inline">\(n\)</span>个类。</p></li>
</ul></li>
<li><p>基于质心的聚类: K-means, K-medoids</p></li>
<li><p>基于分布的聚类: Gaussian mixture models (GMMs)</p></li>
</ul></li>
<li><p><strong>可视化高维数据</strong>：</p>
<ul>
<li><p>变分自动编码器（VAE）</p></li>
<li><p><span class="math inline">\(t\)</span>分布随机邻近嵌入（<span class="math inline">\(t\)</span>-SNE），</p></li>
<li><p>统一流形逼近和投影（UMAP），</p></li>
<li><p>自组织映射（SOM）</p></li>
<li><p>Kohonen图。</p></li>
</ul></li>
</ul>
<div id="数据预处理-3" class="section level2">
<h2><span class="header-section-number">5.1</span> 数据预处理</h2>
<p>数据中各个变量的说明如下：</p>
<table>
<thead>
<tr class="header">
<th align="center">变量</th>
<th align="center">类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">brand</td>
<td align="center">factor</td>
<td>43个汽车品牌</td>
</tr>
<tr class="even">
<td align="center">type</td>
<td align="center">factor</td>
<td>96个水平</td>
</tr>
<tr class="odd">
<td align="center">model</td>
<td align="center">factor</td>
<td>113个水平</td>
</tr>
<tr class="even">
<td align="center">seats</td>
<td align="center">int</td>
<td>座位数</td>
</tr>
<tr class="odd">
<td align="center">max_power</td>
<td align="center">int</td>
<td>发动机最大功率(kW),取对数</td>
</tr>
<tr class="even">
<td align="center">max_torque</td>
<td align="center">num</td>
<td>最大转矩(Nm),取对数</td>
</tr>
<tr class="odd">
<td align="center">cubic_capacity</td>
<td align="center">int</td>
<td>容量(cm<span class="math inline">\(^3\)</span>),取对数</td>
</tr>
<tr class="even">
<td align="center">weight</td>
<td align="center">int</td>
<td>车重(kg)，取对数</td>
</tr>
<tr class="odd">
<td align="center">max_engine_speed</td>
<td align="center">int</td>
<td>发动机最大转速(rpm)</td>
</tr>
<tr class="even">
<td align="center">seconds_to_100</td>
<td align="center">int</td>
<td>达到100km/h所需要秒数</td>
</tr>
<tr class="odd">
<td align="center">top_speed</td>
<td align="center">int</td>
<td>最大行驶速度(km/h)</td>
</tr>
<tr class="even">
<td align="center">sports_car</td>
<td align="center">int</td>
<td>跑车</td>
</tr>
<tr class="odd">
<td align="center">tau</td>
<td align="center">num</td>
<td>专家评分</td>
</tr>
</tbody>
</table>
<p>Figure @ref(fig:pairs) 显示了各个变量（取对数后）的散点图，Q-Q图，及相关系数。</p>
<div class="figure" style="text-align: center">
<img src="./plots/5/log.png" alt="散点图" width="40%" />
<p class="caption">
(#fig:pairs1)散点图
</p>
</div>
<p><span class="math inline">\(\tau\)</span>为专家提出的评分方程，据此评分可以大概判断该车是否为跑车：</p>
<p><span class="math display">\[\tau=\frac{\text{weight}}{\frac{\text{max_power}}{0.735499}}\text{seats}^{\frac{1}{3}}\left(\frac{\text{cubic_capacity}}{1000}\right)^{\frac{1}{4}}\]</span>
如果把常数项提出，可得到如下等价的评分<span class="math inline">\(\tau^+\)</span>:</p>
<p><span class="math display">\[\tau^+=\frac{\text{weight}}{\text{max_power}}\text{seats}^{\frac{1}{3}}\text{cubic_capacity}^{\frac{1}{4}}\]</span></p>
<p>专家把<span class="math inline">\(\tau&lt;17\)</span>或<span class="math inline">\(\tau^+&lt;129.9773\)</span>的汽车定义为跑车。</p>
</div>
<div id="主成分分析" class="section level2">
<h2><span class="header-section-number">5.2</span> 主成分分析</h2>
<p>Ingenbleek-Lemaire (1988) 的目标是利用主成分分析来对数据进行降维，根据选取的主成分来区分跑车和普通车，并尝试达到和专家选择一样的效果。</p>
<p>PCA适用于高斯分布，若变量显著不符合高斯分布，需要对数据进行预处理（比如取对数或其他方法）。
Ingenbleek-Lemaire (1988) 构造了以下5个近似服从高斯分布的变量以便进行后续分析。</p>
<p><span class="math display">\[x_1^*=\ln\left(\frac{\text{weight}}{\text{max_power}}\right)\]</span>
<span class="math display">\[x_2^*=\ln\left(\frac{\text{max_power}}{\text{cubic_capacity}}\right)\]</span>
<span class="math display">\[x_3^*=\ln\left(\text{max_torque}\right)\]</span>
<span class="math display">\[x_4^*=\ln\left(\text{max_engine_speed}\right)\]</span> <span class="math display">\[x_5^*=log\left(\text{cubic_capacity}\right)\]</span></p>
<p>Figure (fig:pairs2) 展示了以上5个变量的相关性。</p>
<div class="figure" style="text-align: center">
<img src="./plots/5/x1-x5scatter.png" alt="散点图" width="40%"  />
<p class="caption">
(#fig:pairs2)散点图
</p>
</div>
<p>主成分分析可以降低高维数据的维数，使相对于原始数据的重构误差最小。如果应用成功，它减少了特征空间的维数，并且它对于(精算)回归建模特别有用，因为它提供了少量的不相关的解释变量。</p>
<p>假设样本量为<span class="math inline">\(n\)</span>的样本有<span class="math inline">\(q\)</span>个特征<span class="math inline">\(\mathbf{x}_1^*,\ldots,\mathbf{x}^*_n\in\mathbb{R}^q\)</span>。其设计矩阵为
<span class="math display">\[\mathbf{X}^*=(\mathbf{x}_1^*,\ldots,\mathbf{x}^*_n)^\intercal\in\mathbb{R}^{n\times q}.\]</span>
把设计矩阵的每列进行标准化，得到
<span class="math display">\[\mathbf{X}=(x_{i,j})_{1\le i \le n,1\le j\le q}\in\mathbb{R}^{n\times q}.\]</span>
其中，第<span class="math inline">\(i\)</span>行是样本<span class="math inline">\(i\)</span>的特征<span class="math inline">\(\mathbf{x}_i\in\mathbb{R}^q, 1\le i\le n\)</span>, 第<span class="math inline">\(j\)</span>列是第<span class="math inline">\(j\)</span>个特征<span class="math inline">\(x_j\in\mathbb{R}^n\)</span>.</p>
<p>矩阵<span class="math inline">\(\mathbf{X}\)</span>的秩为<span class="math inline">\(q\le n\)</span>，可以找到<span class="math inline">\(q\)</span>个正交的<span class="math inline">\(q\)</span>维基向量<span class="math inline">\(\mathbf{v}_1,\ldots,\mathbf{v}_q\in\mathbb{R}^q\)</span>, 使得<span class="math inline">\(\mathbf{v}_1\)</span>为<span class="math inline">\(\mathbf{X}\)</span>波动最大的方向，<span class="math inline">\(\mathbf{v}_2\)</span>为与<span class="math inline">\(\mathbf{v}_1\)</span>正交方向上的<span class="math inline">\(\mathbf{X}\)</span>波动最大的方向，依次类推。</p>
<p>用数学公式表示如下：
<span class="math display">\[\mathbf{v}_1=\underset{||\omega||_2=1}{\arg \max}||\mathbf{X}\omega||_2^2=\underset{\omega^\intercal\omega=1}{\arg \max} (\omega^\intercal\mathbf{X}^\intercal\mathbf{X}\omega)\]</span></p>
<p><span class="math display">\[\mathbf{v}_2=\underset{||\omega||_2=1}{\arg \max}||\mathbf{X}\omega||_2^2 ~~~\text{ subject to } \mathbf{v}_1^\intercal\omega=0.\]</span></p>
<p><span class="math display">\[\ldots\]</span></p>
<p>主成分分析可通过以下两种方式实现</p>
<ul>
<li><p>求<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>或者<span class="math inline">\(\mathbf{X}\)</span>的协方差矩阵<span class="math inline">\(\mathbf{\Sigma}\)</span>的特征向量和特征值。易知<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}=n\times\mathbf{\Sigma}\)</span>，所以它们的特征向量相同。第一个特征向量即为<span class="math inline">\(\mathbf{v}_1\)</span>，第二个特征向量为<span class="math inline">\(\mathbf{v}_2\)</span>。前两个主成分为<span class="math inline">\(\mathbf{X}\mathbf{v}_1,\mathbf{X}\mathbf{v}_2\)</span></p></li>
<li><p>对<span class="math inline">\(\mathbf{X}\)</span>进行奇异值（singular value decomposition）分解:<span class="math display">\[\mathbf{X}=U\Lambda V^\intercal.\]</span>其中，对角矩阵<span class="math inline">\(\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_q)\)</span>的元素为<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>的特征值，<span class="math inline">\(V\)</span>为<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>的特征向量。主成分可以通过<span class="math inline">\(\mathbf{X}V\)</span>求得。</p></li>
</ul>
<p>利用前<span class="math inline">\(p\)</span>个主成分可以重构设计矩阵的近似值<span class="math display">\[\mathbf{X}_p=U\text{diag}(\lambda_1,\ldots,\lambda_p,0,\ldots,0)V^{\intercal}.\]</span></p>
<p>该近似值为以下极值问题的根<span class="math display">\[\underset{B\in\mathbb{R}^{n\times q}}{\arg \min}||\mathbf{X}-B||^2 ~~\text{subject to rank}(B)\le q,\]</span></p>
<p>即矩阵<span class="math inline">\(\mathbf{X}_p\)</span>是所有秩为<span class="math inline">\(p\)</span>的矩阵中，与原始设计矩阵<span class="math inline">\(\mathbf{X}\)</span>重组平方误差(F范数)最小的矩阵。</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1"></a><span class="co"># standardize matrix</span></span>
<span id="cb65-2"><a href="#cb65-2"></a>X &lt;-<span class="st"> </span>X01<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">colMeans</span>(X01<span class="op">^</span><span class="dv">2</span>))[<span class="kw">col</span>(X01)]</span>
<span id="cb65-3"><a href="#cb65-3"></a></span>
<span id="cb65-4"><a href="#cb65-4"></a><span class="co"># eigenvectors and eigenvalues</span></span>
<span id="cb65-5"><a href="#cb65-5"></a>X1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X)</span>
<span id="cb65-6"><a href="#cb65-6"></a><span class="kw">nrow</span>(X1)</span>
<span id="cb65-7"><a href="#cb65-7"></a>A &lt;-<span class="st">  </span><span class="kw">t</span>(X1) <span class="op">%*%</span><span class="st"> </span>X1</span>
<span id="cb65-8"><a href="#cb65-8"></a>A</span>
<span id="cb65-9"><a href="#cb65-9"></a><span class="kw">sum</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value)<span class="op">/</span><span class="dv">5</span></span>
<span id="cb65-10"><a href="#cb65-10"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value)      <span class="co"># singular values</span></span>
<span id="cb65-11"><a href="#cb65-11"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value<span class="op">/</span><span class="kw">nrow</span>(X1))   <span class="co"># scaled eigenvalues</span></span>
<span id="cb65-12"><a href="#cb65-12"></a><span class="kw">eigen</span>(A)<span class="op">$</span>vector</span>
<span id="cb65-13"><a href="#cb65-13"></a>A1&lt;-<span class="kw">cor</span>(X1)</span>
<span id="cb65-14"><a href="#cb65-14"></a>A1<span class="op">*</span><span class="kw">nrow</span>(X1)</span>
<span id="cb65-15"><a href="#cb65-15"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A1)<span class="op">$</span>value)  </span>
<span id="cb65-16"><a href="#cb65-16"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>vector</span>
<span id="cb65-17"><a href="#cb65-17"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>value</span>
<span id="cb65-18"><a href="#cb65-18"></a></span>
<span id="cb65-19"><a href="#cb65-19"></a><span class="co"># singular value decomposition</span></span>
<span id="cb65-20"><a href="#cb65-20"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(X1)</span>
<span id="cb65-21"><a href="#cb65-21"></a>SVD<span class="op">$</span>d                       <span class="co"># singular values</span></span>
<span id="cb65-22"><a href="#cb65-22"></a><span class="kw">rbind</span>(SVD<span class="op">$</span>v[,<span class="dv">1</span>],SVD<span class="op">$</span>v[,<span class="dv">2</span>])  <span class="co"># first two right singular vectors</span></span>
<span id="cb65-23"><a href="#cb65-23"></a></span>
<span id="cb65-24"><a href="#cb65-24"></a><span class="co"># PCA with package PCA</span></span>
<span id="cb65-25"><a href="#cb65-25"></a>t.pca &lt;-<span class="st"> </span><span class="kw">princomp</span>(X1,<span class="dt">cor=</span><span class="ot">TRUE</span>)</span>
<span id="cb65-26"><a href="#cb65-26"></a>t.pca<span class="op">$</span>loadings          </span>
<span id="cb65-27"><a href="#cb65-27"></a><span class="kw">summary</span>(t.pca)</span>
<span id="cb65-28"><a href="#cb65-28"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>value<span class="op">/</span><span class="kw">sum</span>(<span class="kw">eigen</span>(A1)<span class="op">$</span>value)</span></code></pre></div>
<p>通过<code>summary(t.pca)</code>我们可以得到前两个主成分的累计贡献度已经到<span class="math inline">\(92%\)</span>，前两个主成分提取了原始数据的绝大部分信息，所以我们选择2个主成分应该可以较好地重构原始数据。</p>
<p>以第一主成分为例说明第一主成分和<span class="math inline">\(x_1^*,\ldots,x_5^*\)</span>的关系</p>
<pre class="{r，eval=f}"><code># PCA Sports Cars weights
alpha &lt;- SVD$v[,1]/sds
(alpha_star &lt;- c(alpha[1],alpha[2]-alpha[1], alpha[3], alpha[4], alpha[5]-alpha[2])/alpha[1])</code></pre>
<p><span class="math inline">\(y_1=\left&lt;\mathbf{v_1},\mathbf{x}\right&gt;=-0.558x_1+0.412x_2+0.539x_3+0.126x_4+0.461x_5\)</span>因为此处的<span class="math inline">\(x_1,\ldots,x_5\)</span>来自标准设计矩阵，所以我们做逆变换<span class="math inline">\(\alpha_lx_l^*=\alpha_l\left(\hat{\sigma_l}\frac{x_l^*-\hat{\mu_l}}{\hat{\sigma_l}}+\hat{\mu_l}\right)=\alpha_l\hat{\sigma_l}x_l+\alpha_l\hat{\mu_l}\)</span>,其中<span class="math inline">\(\alpha_l\hat{\sigma_l}=\mathbf{v_{1,l}}\)</span>,由此可得原始方程为<span class="math inline">\(\frac{y^*}{\alpha_1}=log(\text{weight})-1.93log(\text{max_power})-0.65log(\text{max_torque})-0.64log(\text{max_engine_speed})+0.25log(\text{cubic_capacity})\)</span>将样本带入计算我们即可得到第一主成分的得分。第二主成分计算同上。</p>
<pre class="{r，eval=f}"><code># scatter plot
switch_sign &lt;- -1           # switch sign of the first component to make svd and princomp compatible
tt.pca &lt;- t.pca$scores
tt.pca[,1] &lt;- switch_sign *tt.pca[,1]
pairs(tt.pca,diag.panel=panel.qq,upper.panel=panel.cor)</code></pre>
<div class="figure" style="text-align: center">
<img src="./plots/5/pcascatter.png" alt="主成分散点图" width="40%"  />
<p class="caption">
(#fig:pairs)主成分散点图
</p>
</div>
<p>Figure 5.3:对角线为Q-Q图，左下部分散点图，右上部分相关系数图（各个主成分之间相互独立，所以相关系数为0）</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1"></a><span class="co"># plot first two principal components</span></span>
<span id="cb68-2"><a href="#cb68-2"></a>dat3 &lt;-<span class="st"> </span>d.data </span>
<span id="cb68-3"><a href="#cb68-3"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span>X1 <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">1</span>]</span>
<span id="cb68-4"><a href="#cb68-4"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span>X1 <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">2</span>]</span>
<span id="cb68-5"><a href="#cb68-5"></a></span>
<span id="cb68-6"><a href="#cb68-6"></a><span class="co"># png(&quot;./plots/5/pca.png&quot;)</span></span>
<span id="cb68-7"><a href="#cb68-7"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">ylab=</span><span class="st">&quot;2nd principal component&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;1st principal component&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;principal components analysis&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb68-8"><a href="#cb68-8"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(dat3<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb68-9"><a href="#cb68-9"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb68-10"><a href="#cb68-10"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(dat3<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb68-11"><a href="#cb68-11"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb68-12"><a href="#cb68-12"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb68-13"><a href="#cb68-13"></a><span class="co">#dev.off()</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/pca.png" alt="主成分得分图" width="40%"  />
<p class="caption">
(#fig:2pcas)主成分得分图
</p>
</div>
<p>Figure 5.4:样本(n=475)的主成分得分图，其中蓝色点和红色点之间有一个超平面将跑车和普通车很好的区分开了。</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1"></a><span class="co"># reconstruction error</span></span>
<span id="cb69-2"><a href="#cb69-2"></a>reconstruction.PCA &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(<span class="dv">5</span>))</span>
<span id="cb69-3"><a href="#cb69-3"></a></span>
<span id="cb69-4"><a href="#cb69-4"></a><span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>){</span>
<span id="cb69-5"><a href="#cb69-5"></a>  Xp &lt;-<span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>p] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(SVD<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>p]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X)</span>
<span id="cb69-6"><a href="#cb69-6"></a>  Xp &lt;-<span class="st"> </span><span class="kw">t</span>(Xp)</span>
<span id="cb69-7"><a href="#cb69-7"></a>  reconstruction.PCA[p] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(<span class="kw">as.matrix</span>((X<span class="op">-</span>Xp)<span class="op">^</span><span class="dv">2</span>))<span class="op">/</span><span class="kw">nrow</span>(X))</span>
<span id="cb69-8"><a href="#cb69-8"></a>               }</span>
<span id="cb69-9"><a href="#cb69-9"></a><span class="kw">round</span>(reconstruction.PCA,<span class="dv">2</span>)               </span>
<span id="cb69-10"><a href="#cb69-10"></a></span>
<span id="cb69-11"><a href="#cb69-11"></a><span class="co"># biplot</span></span>
<span id="cb69-12"><a href="#cb69-12"></a>tt.pca &lt;-<span class="st"> </span>t.pca</span>
<span id="cb69-13"><a href="#cb69-13"></a>tt.pca<span class="op">$</span>scores[,<span class="dv">1</span>] &lt;-<span class="st">  </span>switch_sign <span class="op">*</span><span class="st"> </span>tt.pca<span class="op">$</span>scores[,<span class="dv">1</span>]</span>
<span id="cb69-14"><a href="#cb69-14"></a>tt.pca<span class="op">$</span>loadings[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span>] &lt;-<span class="st"> </span>switch_sign <span class="op">*</span><span class="st"> </span>tt.pca<span class="op">$</span>loadings[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span>] </span>
<span id="cb69-15"><a href="#cb69-15"></a><span class="kw">biplot</span>(tt.pca,<span class="dt">choices=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dt">scale=</span><span class="dv">0</span>, <span class="dt">expand=</span><span class="dv">2</span>, <span class="dt">xlab=</span><span class="st">&quot;1st principal component&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;2nd principal component&quot;</span>, <span class="dt">cex=</span><span class="kw">c</span>(<span class="fl">0.4</span>,<span class="fl">1.5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>))</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/biplot.png" alt="矢量分解图" width="40%"  />
<p class="caption">
(#fig:2pcas-2)矢量分解图
</p>
</div>
<p>Figure 5.5:黑点之间的距离表示相似性。红色矢量是前两个标准正交权值向量<span class="math inline">\(\mathbf{v_1}\)</span>和<span class="math inline">\(\mathbf{v_2}\)</span>的分量。长度反映了变量的标准差，夹角的余弦值给出了相应的相关性。</p>
</div>
<div id="自编码" class="section level2">
<h2><span class="header-section-number">5.3</span> 自编码</h2>
<p>PCA对异常值很敏感，也有稳健的PCA版本。例如，Croux等人给出了一个基于中值绝对偏差(MADs)的算法，R包：pcaPP。
主成分分析可以看作是一个自动编码器。接下来，我们将更一般地介绍自动编码器，例如BNN。</p>
<p>自编码包含编码和解码两个镜面对称的映射：</p>
<ul>
<li><p>编码: <span class="math inline">\(\varphi:\mathbb{R}^q\rightarrow\mathbb{R}^p\)</span></p></li>
<li><p>解码: <span class="math inline">\(\psi:\mathbb{R}^p\rightarrow\mathbb{R}^q\)</span>
其中<span class="math inline">\(p \le q\)</span>，我们选择一个度量差异的函数<span class="math inline">\(d(·,·)\)</span>，当且仅当<span class="math inline">\(\mathbf{x}=\mathbf{y}\)</span>时<span class="math inline">\(d(x,y)=0\)</span>，自编码就是找到一对<span class="math inline">\((\varphi,\psi)\)</span>使得有<span class="math inline">\(\pi=\psi\circ\varphi\)</span>满足<span class="math inline">\(d(\pi(\mathbf{x}),\mathbf{x})\)</span>最小。</p></li>
</ul>
<p>在PCA的例子中：</p>
<p><span class="math display">\[\varphi:\mathbb{R}^q\rightarrow\mathbb{R}^p,\mathbf{x}\mapsto\mathbf{y}=\varphi(\mathbf{x})=(\left&lt;\mathbf{v_1},\mathbf{x}\right&gt;,\ldots,\left&lt;\mathbf{v_p},\mathbf{x}\right&gt;)^T=(\mathbf{v_1},\ldots,\mathbf{v_p})^T\mathbf{x}.\]</span></p>
<p><span class="math display">\[\psi:\mathbb{R}^p\rightarrow\mathbb{R}^q,\mathbf{y}\mapsto\psi(\mathbf{y})=(\mathbf{v_1},\ldots,\mathbf{v_p})\mathbf{y}.\]</span></p>
<p><span class="math display">\[\pi(\mathbf{X}^T)=\psi\circ\varphi(\mathbf{x}^T)=(\mathbf{v_1},\ldots,\mathbf{v_p})(\mathbf{v_1},\ldots,\mathbf{v_p})^T\mathbf{X}^T=\mathbf{X_p}^T.\]</span></p>
<p><span class="math display">\[\sum_{i=1}^nd(\pi(\mathbf{x}_i),\mathbf{x}_i)=\sum_{i=1}^n\|\pi(\mathbf{x}_i-\mathbf{x}_i)\|_2^2=\|\mathbf{X}_p-\mathbf{X}\|_F^2.\]</span></p>
<p>此处衡量重构误差的为针对矩阵的欧氏距离，即F范数。</p>
<p>作为非线性自编码器的一个例子，我们考虑瓶颈神经网络(BNN)。为了成功校准一个BNN，它的隐藏层数应该是奇数<span class="math inline">\(d\)</span> (<span class="math inline">\(d\)</span>称为神经网络的深度)，并且中心隐藏层应该是低维的，有<span class="math inline">\(p\)</span>个隐藏神经元，所有剩余的隐藏层应该是围绕这个中心隐藏层对称的。因此对于深度<span class="math inline">\(d = 3\)</span>的BNN，我们可以选择图@ref(fig:bnn-structure)展示的神经网络结构</p>
<div class="figure" style="text-align: center">
<img src="./plots/5/bnn.png" alt="自编码 q=5,p=2" width="40%"  />
<p class="caption">
(#fig:bnn-structure)自编码 q=5,p=2
</p>
</div>
<p>一般的神经网络有如下结构：</p>
<p><span class="math display">\[\pi:\mathbb{R}^{q_0}\rightarrow\mathbb{R}^{q_{d+1}=q_0},\mathbf{x}\mapsto\pi(\mathbf{x})=(\mathbf{z}^\left(d+1\right)\circ\mathbf{z}^\left(d\right)\circ\ldots\circ\mathbf{z}^\left(1\right))(\mathbf{x})\]</span></p>
<p><span class="math display">\[\mathbf{z}^\left(m\right):\mathbb{R}^{q_{m-1}}\rightarrow\mathbb{R}^{q_m},\mathbf{z}\mapsto\mathbf{z}^\left(m\right)(\mathbf{z})=\left(\phi\left(\left&lt;\mathbf{w}_1^{\left(m\right)},\mathbf{z}\right&gt;\right),\ldots,\phi\left(\left&lt;\mathbf{w}_{q_m}^{\left(m\right)},\mathbf{z}\right&gt;\right)\right)^T\]</span></p>
<p>其中<span class="math inline">\(\mathbf{w}_l^{\left(m\right)}\in\mathbb{R}^{q_{m-1}},1 \le l\le q_m\)</span>为权重，<span class="math inline">\(\phi:\mathbb{R}\rightarrow\mathbb{R}\)</span>为激活函数。</p>
<p><span class="math display">\[\mathbf{z}^\left(d+1\right):\mathbb{R}^{q_{d}}\rightarrow\mathbb{R}^{q_{d+1}},\mathbf{z}\mapsto\mathbf{z}^\left(d+1\right)(\mathbf{z})=\left(\left&lt;\mathbf{w}_1^{\left(d+1\right)},\mathbf{z}\right&gt;,\ldots,\left&lt;\mathbf{w}_{q_{d+1}}^{\left(d+1\right)},\mathbf{z}\right&gt;\right)^T\]</span></p>
<p>总参数个数为<span class="math inline">\(r=\sum_{m=1}^{d+1}q_mq_{m-1}\)</span>。</p>
<ul>
<li><p>与经典的前馈神经网络相比，我们这里没有截距项，因为特征$_i已经被标准化。这略微降低了网络参数的维数。</p></li>
<li><p>选择输出激活为线性激活，是因为x的所有成份都在实数领域中。下面我们还将应用其他输出激活函数。</p></li>
<li><p>作为隐层的激活函数，我们通常选用双曲正切函数。如果一个BNN只有线性激活函数，那么他是等价于PCA的。</p></li>
</ul>
<div id="模型训练" class="section level3">
<h3><span class="header-section-number">5.3.1</span> 模型训练</h3>
<p>具体过程如下：</p>
<p>在正式进入神经网之前，我们要先进行权重预训练。</p>
<div class="figure" style="text-align: center">
<img src="./plots/5/bnn_train.png" alt="自编码训练过程" width="60%"  />
<p class="caption">
(#fig:bnn-train)自编码训练过程
</p>
</div>
<ul>
<li><p>首先我们先将中间的隐藏层折叠得到如图@ref(fig:bnn-train)中间结构的神经网络，据此我们得到权重：<span class="math inline">\(\mathbf{w}_1^{(1)}\ldots,\mathbf{w}_{q_1=7}^{(1)}\in\mathbb{R}^{q_0=5}\)</span>
和<span class="math inline">\(\mathbf{w}_1^{(d+1)}\ldots,\mathbf{w}_{q_{d+1}=5}^{(d+1)}\in\mathbb{R}^{q_d=7}\)</span></p></li>
<li><p>再训练隐藏层即图@ref(fig:bnn-train)中右图的权重，得到：<span class="math inline">\(\mathbf{w}_1^{(2)}\ldots,\mathbf{w}_{q_2=2}^{(2)}\in\mathbb{R}^{q_1=7}\)</span>
和<span class="math inline">\(\mathbf{w}_1^{(3)}\ldots,\mathbf{w}_{q_{3}=7}^{(3)}\in\mathbb{R}^{q_2=2}\)</span></p></li>
</ul>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1"></a>bottleneck<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="cf">function</span>(q00, q22){</span>
<span id="cb70-2"><a href="#cb70-2"></a>   Input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(q00), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Input&#39;</span>)</span>
<span id="cb70-3"><a href="#cb70-3"></a>   </span>
<span id="cb70-4"><a href="#cb70-4"></a>   Output =<span class="st"> </span>Input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-5"><a href="#cb70-5"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q22, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Bottleneck&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-6"><a href="#cb70-6"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q00, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Output&#39;</span>)</span>
<span id="cb70-7"><a href="#cb70-7"></a></span>
<span id="cb70-8"><a href="#cb70-8"></a>   model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> Input, <span class="dt">outputs =</span> Output)</span>
<span id="cb70-9"><a href="#cb70-9"></a>   </span>
<span id="cb70-10"><a href="#cb70-10"></a>   model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb70-11"><a href="#cb70-11"></a>   model</span>
<span id="cb70-12"><a href="#cb70-12"></a>   }</span>
<span id="cb70-13"><a href="#cb70-13"></a></span>
<span id="cb70-14"><a href="#cb70-14"></a>bottleneck<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="cf">function</span>(q00, q11, q22){   </span>
<span id="cb70-15"><a href="#cb70-15"></a>   Input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(q00), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Input&#39;</span>)</span>
<span id="cb70-16"><a href="#cb70-16"></a>   </span>
<span id="cb70-17"><a href="#cb70-17"></a>   Encoder =<span class="st"> </span>Input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-18"><a href="#cb70-18"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q11, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Layer1&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb70-19"><a href="#cb70-19"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q22, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Bottleneck&#39;</span>) </span>
<span id="cb70-20"><a href="#cb70-20"></a></span>
<span id="cb70-21"><a href="#cb70-21"></a>   Decoder =<span class="st"> </span>Encoder <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-22"><a href="#cb70-22"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q11, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Layer3&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-23"><a href="#cb70-23"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q00, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Output&#39;</span>)</span>
<span id="cb70-24"><a href="#cb70-24"></a></span>
<span id="cb70-25"><a href="#cb70-25"></a>   model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> Input, <span class="dt">outputs =</span> Decoder)</span>
<span id="cb70-26"><a href="#cb70-26"></a>   model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb70-27"><a href="#cb70-27"></a>   model</span>
<span id="cb70-28"><a href="#cb70-28"></a>   }</span></code></pre></div>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1"></a><span class="co"># bottleneck architecture</span></span>
<span id="cb71-2"><a href="#cb71-2"></a>q1 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb71-3"><a href="#cb71-3"></a>q2 &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb71-4"><a href="#cb71-4"></a>q0 &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)</span>
<span id="cb71-5"><a href="#cb71-5"></a></span>
<span id="cb71-6"><a href="#cb71-6"></a><span class="co"># pre-training 1: merging layers 1 and 3 (skipping bottleneck)</span></span>
<span id="cb71-7"><a href="#cb71-7"></a>model<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.1</span>(q0, q1)</span>
<span id="cb71-8"><a href="#cb71-8"></a>model<span class="fl">.1</span></span>
<span id="cb71-9"><a href="#cb71-9"></a>epochs &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb71-10"><a href="#cb71-10"></a>batch_size &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb71-11"><a href="#cb71-11"></a></span>
<span id="cb71-12"><a href="#cb71-12"></a><span class="co"># fit the merged model</span></span>
<span id="cb71-13"><a href="#cb71-13"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb71-14"><a href="#cb71-14"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.1</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(X), <span class="kw">as.matrix</span>(X), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb71-15"><a href="#cb71-15"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb71-16"><a href="#cb71-16"></a></span>
<span id="cb71-17"><a href="#cb71-17"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0),  <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="st">&quot;gradient descent algorithm&quot;</span>) </span>
<span id="cb71-18"><a href="#cb71-18"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.6124</span>), <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>)</span>
<span id="cb71-19"><a href="#cb71-19"></a> </span>
<span id="cb71-20"><a href="#cb71-20"></a><span class="co"># neuron activations in the central layer </span></span>
<span id="cb71-21"><a href="#cb71-21"></a>zz &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs=</span>model<span class="fl">.1</span><span class="op">$</span>input, <span class="dt">outputs=</span><span class="kw">get_layer</span>(model<span class="fl">.1</span>, <span class="st">&#39;Bottleneck&#39;</span>)<span class="op">$</span>output)</span>
<span id="cb71-22"><a href="#cb71-22"></a>yy &lt;-<span class="st"> </span>zz <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb71-23"><a href="#cb71-23"></a></span>
<span id="cb71-24"><a href="#cb71-24"></a><span class="co"># pre-training 2: middlepart</span></span>
<span id="cb71-25"><a href="#cb71-25"></a>model<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.1</span>(q1, q2)</span>
<span id="cb71-26"><a href="#cb71-26"></a>model<span class="fl">.2</span></span>
<span id="cb71-27"><a href="#cb71-27"></a>epochs &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb71-28"><a href="#cb71-28"></a></span>
<span id="cb71-29"><a href="#cb71-29"></a><span class="co"># fit the merged model</span></span>
<span id="cb71-30"><a href="#cb71-30"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb71-31"><a href="#cb71-31"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.2</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(yy), <span class="kw">as.matrix</span>(yy), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb71-32"><a href="#cb71-32"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb71-33"><a href="#cb71-33"></a></span>
<span id="cb71-34"><a href="#cb71-34"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0),  <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="st">&quot;gradient descent algorithm&quot;</span>) </span></code></pre></div>
<p>预训练结束后，用这些预先训练好的权值对整个BNN进行重构，我们会得到一个重构误差（如下）在此案例中，此处的重构误差大于使用PCA时取两个主成分的重构误差。</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1"></a><span class="co"># fitting the full model</span></span>
<span id="cb72-2"><a href="#cb72-2"></a>model<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.3</span>(q0, q1, q2)</span>
<span id="cb72-3"><a href="#cb72-3"></a>model<span class="fl">.3</span></span>
<span id="cb72-4"><a href="#cb72-4"></a></span>
<span id="cb72-5"><a href="#cb72-5"></a><span class="co"># set weights</span></span>
<span id="cb72-6"><a href="#cb72-6"></a>weight<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.3</span>)</span>
<span id="cb72-7"><a href="#cb72-7"></a>weight<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.1</span>)</span>
<span id="cb72-8"><a href="#cb72-8"></a>weight<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.2</span>)</span>
<span id="cb72-9"><a href="#cb72-9"></a>weight<span class="fl">.3</span>[[<span class="dv">1</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.1</span>[[<span class="dv">1</span>]]</span>
<span id="cb72-10"><a href="#cb72-10"></a>weight<span class="fl">.3</span>[[<span class="dv">4</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.1</span>[[<span class="dv">2</span>]]</span>
<span id="cb72-11"><a href="#cb72-11"></a>weight<span class="fl">.3</span>[[<span class="dv">2</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.2</span>[[<span class="dv">1</span>]]</span>
<span id="cb72-12"><a href="#cb72-12"></a>weight<span class="fl">.3</span>[[<span class="dv">3</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.2</span>[[<span class="dv">2</span>]]</span>
<span id="cb72-13"><a href="#cb72-13"></a><span class="kw">set_weights</span>(model<span class="fl">.3</span>, weight<span class="fl">.3</span>)</span>
<span id="cb72-14"><a href="#cb72-14"></a>fit0 &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb72-15"><a href="#cb72-15"></a></span>
<span id="cb72-16"><a href="#cb72-16"></a><span class="co"># reconstruction error of the pre-calibrated network</span></span>
<span id="cb72-17"><a href="#cb72-17"></a><span class="co"># note that this error may differ from the tutorial because we did not set a seed</span></span>
<span id="cb72-18"><a href="#cb72-18"></a><span class="kw">round</span>(<span class="kw">Frobenius.loss</span>(X,fit0),<span class="dv">4</span>)</span></code></pre></div>
<p>使用这些预先训练好的权值作为初始化，将梯度下降算法应用于整个BNN，以获得BNN降维。</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1"></a><span class="co"># calibrate full bottleneck network</span></span>
<span id="cb73-2"><a href="#cb73-2"></a>epochs &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb73-3"><a href="#cb73-3"></a>batch_size &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb73-4"><a href="#cb73-4"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb73-5"><a href="#cb73-5"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(X), <span class="kw">as.matrix</span>(X), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb73-6"><a href="#cb73-6"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb73-7"><a href="#cb73-7"></a></span>
<span id="cb73-8"><a href="#cb73-8"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;gradient descent algorithm&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>) </span>
<span id="cb73-9"><a href="#cb73-9"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.6124</span>), <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>) </span>
<span id="cb73-10"><a href="#cb73-10"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;decrease GDM&quot;</span>, <span class="st">&quot;PCA(p=2)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;orange&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb73-11"><a href="#cb73-11"></a></span>
<span id="cb73-12"><a href="#cb73-12"></a><span class="co"># reconstruction error (slightly differs from 0.5611 because of missing seed)</span></span>
<span id="cb73-13"><a href="#cb73-13"></a>fit0 &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb73-14"><a href="#cb73-14"></a><span class="kw">round</span>(<span class="kw">Frobenius.loss</span>(X,fit0),<span class="dv">4</span>)</span>
<span id="cb73-15"><a href="#cb73-15"></a></span>
<span id="cb73-16"><a href="#cb73-16"></a><span class="co"># read off the bottleneck activations</span></span>
<span id="cb73-17"><a href="#cb73-17"></a>encoder &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs=</span>model<span class="fl">.3</span><span class="op">$</span>input, <span class="dt">outputs=</span><span class="kw">get_layer</span>(model<span class="fl">.3</span>, <span class="st">&#39;Bottleneck&#39;</span>)<span class="op">$</span>output)</span>
<span id="cb73-18"><a href="#cb73-18"></a>y&lt;-<span class="st"> </span><span class="kw">predict</span>(encoder,<span class="kw">as.matrix</span>(X))</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/gda.png" alt="重构误差图" width="60%"  />
<p class="caption">
(#fig:gda)重构误差图
</p>
</div>
<p>我们说明了<span class="math inline">\(\mathbf{F}\)</span>范数损失函数在超过10,000次迭代时的下降过程。在大约2000次迭代后，损失低于<span class="math inline">\(p=2\)</span>个主成分的PCA(图@ref(fig:gda)中橙色线在水平0.6124处)。10,000次迭代后的最终重构误差为0.5428。</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1"></a><span class="co"># note that we may need sign switches to make it comparable to PCA</span></span>
<span id="cb74-2"><a href="#cb74-2"></a>y0 &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(y))<span class="op">*</span><span class="fl">1.1</span></span>
<span id="cb74-3"><a href="#cb74-3"></a><span class="kw">plot</span>(<span class="dt">x=</span>y[,<span class="dv">1</span>], <span class="dt">y=</span>y[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>y0,y0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>y0,y0), <span class="dt">ylab=</span><span class="st">&quot;2nd bottleneck neuron&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;1st bottleneck neuron&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;bottleneck neural network autoencoder&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb74-4"><a href="#cb74-4"></a>dat0 &lt;-<span class="st"> </span>y[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb74-5"><a href="#cb74-5"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0[,<span class="dv">1</span>], <span class="dt">y=</span>dat0[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb74-6"><a href="#cb74-6"></a>dat0 &lt;-<span class="st"> </span>y[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb74-7"><a href="#cb74-7"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0[,<span class="dv">1</span>], <span class="dt">y=</span>dat0[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb74-8"><a href="#cb74-8"></a><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/bnns.png" alt="BNN" width="60%"  />
<p class="caption">
(#fig:bnns)BNN
</p>
</div>
<p>图@ref(fig:bnns)为对样本<span class="math inline">\((n=475)\)</span>的分类效果，与PCA（旋转）很像。</p>
<div class="figure" style="text-align: center">
<img src="./plots/5/BPcamparison.png" alt="BNN与PCA重构误差对比图" width="60%"  />
<p class="caption">
(#fig:BPcamparison)BNN与PCA重构误差对比图
</p>
</div>
<p>由图@ref(fig:BPcamparison)得出结论：两种方法都得到了相似的结果。但在一般情况下，BNN的重构误差较小。在少数情况下，BNN可以得到更好的重建结果(右下角的橙色点)。
针对这个例子而言，属于低维数的问题，主成分分析通常就可以了，因为非线性的部分并没有发挥关键作用。</p>
</div>
</div>
<div id="k-means-clustering" class="section level2">
<h2><span class="header-section-number">5.4</span> K-means clustering</h2>
<p>K-means聚类是一种基于质心（centroid-based）的聚类方法，它将<span class="math inline">\(n\)</span>个样本点<span class="math inline">\(\mathbf{x}_i\in\mathcal{X}\subset\mathbb{R}^q\)</span>划分为<span class="math inline">\(K\)</span>个不相交的类:<span class="math display">\[\mathcal{C}_K:\mathbb{R}^q\rightarrow\mathcal{K}=\{1,\ldots,K\},~~\mathbf{x}\mapsto\mathcal{C}_K(\mathbf{x})\]</span>，以上给出了对特征空间<span class="math inline">\(\mathcal{X}\)</span>的一个分割<span class="math inline">\((C_1,\ldots,C_K)\)</span>，其中<span class="math display">\[C_k=\{\mathbf{x}\in\mathcal{X};\mathcal{C}_K(\mathbf{x})=k\}\]</span></p>
<p>确定<span class="math inline">\(\mathcal{C}_K\)</span>的原则是使总类内差异最小，这可以转化为计算使类内离差平方和总和最小的一个分割，所构造的目标函数为 ：
<span class="math display">\[\underset{(C_1,\ldots,C_K)}{\arg \min}\sum_{k=1}^K\sum_{\mathbf{x}_i\in C_k\cap\mathcal{X}}d(\mathbf{\mu}_k,\mathbf{x}_i)=\underset{(C_1,\ldots,C_K)}{\arg \min}\sum_{k=1}^K\sum_{\mathbf{x}_i\in C_k\cap\mathcal{X}}||\mathbf{\mu}_k-\mathbf{x}_i||_2^2\]</span></p>
<p>其中<span class="math inline">\(\mathbf{\mu}_k\)</span>为类均值向量，因此目标函数衡量了类内样本点围绕类均值向量的紧密程度，其值越小意味着类内样本相似度越高，聚类效果越好。但是上述目标函数并不容易找到最优解，这需要考虑<span class="math inline">\(n\)</span>个样本点所有可能的类划分，因此K-means算法采用了贪心策略，通过迭代优化来近似求解上述目标函数。</p>
<p>K-means算法:</p>
<ol style="list-style-type: decimal">
<li><p>选择初始聚类中心<span class="math inline">\(\mathbf{\mu}_k^{(0)}\)</span>和聚类个数<span class="math inline">\(K\)</span>；</p></li>
<li><p>迭代(终止条件：除非类均值向量<span class="math inline">\(\mathbf{\mu}_k^{(t-1)}\)</span>不再更新或达到最大迭代次数)</p>
<p>(1)计算<span class="math inline">\(n\)</span>个样本点<span class="math inline">\(x_i\)</span>与前一轮均值向量<span class="math inline">\(\mathbf{\mu}_k^{(t)}\)</span>的距离，根据距离最近原则重新分配所有样本点，即<span class="math display">\[C_k^{(t)}(x_i)=\underset{\mathbf{k}\in\mathcal{K}}{\arg\min}||\mathbf{\mu}_k^{(t-1)}-\mathbf{x}_i||_2^2\]</span>
(2)基于<span class="math inline">\(C_k^{(t)}\)</span>更新类均值向量<span class="math inline">\(\mathbf{\mu}_k^{(t)}\)</span></p></li>
</ol>
<p>K-means中存在的问题</p>
<ol style="list-style-type: decimal">
<li><p>对初始聚类中心敏感。选择不同的聚类中心会产生不同的聚类结果和不同的准确率；若随机指定初始聚类中心，当初始指定的两个聚类中心在同一类中或很接近，聚类结果很难区分。此处可以使用一种优化算法：先指定k=2执行k-means(初始聚类中心随机指定)，结果得到两个聚类中心，把它们作为k=3时的其中两个初始聚类中心，剩下一个初始聚类中心随机指定(可以使用列均值)，以此类推。</p></li>
<li><p>K-means聚类由于使用类均值向量作为聚类中心，因此对离群点非常敏感</p></li>
<li><p>因为K-means算法主要采用欧式距离函数度量类间相似度，并且采用误差平方和作为目标函数，因此通常只能发现数据分布比较均匀的球状类。</p></li>
</ol>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1"></a><span class="co"># initialize</span></span>
<span id="cb75-2"><a href="#cb75-2"></a>Kaverage &lt;-<span class="st"> </span><span class="kw">colMeans</span>(X)</span>
<span id="cb75-3"><a href="#cb75-3"></a>K0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb75-4"><a href="#cb75-4"></a>TWCD &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(K0))  <span class="co"># total within-cluster dissimilarity</span></span>
<span id="cb75-5"><a href="#cb75-5"></a>Classifier &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span>, <span class="kw">c</span>(K0, <span class="kw">nrow</span>(X)))</span>
<span id="cb75-6"><a href="#cb75-6"></a>(TWCD[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">colSums</span>(<span class="kw">as.matrix</span>(X<span class="op">^</span><span class="dv">2</span>))))</span>
<span id="cb75-7"><a href="#cb75-7"></a></span>
<span id="cb75-8"><a href="#cb75-8"></a><span class="co"># run K-means algorithm</span></span>
<span id="cb75-9"><a href="#cb75-9"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb75-10"><a href="#cb75-10"></a><span class="cf">for</span> (K <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>K0){ </span>
<span id="cb75-11"><a href="#cb75-11"></a>   <span class="cf">if</span> (K<span class="op">==</span><span class="dv">2</span>){(K_res &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X,K) )}</span>
<span id="cb75-12"><a href="#cb75-12"></a>   <span class="cf">if</span> (K<span class="op">&gt;</span><span class="dv">2</span>){(K_res  &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X,K_centers) )}</span>
<span id="cb75-13"><a href="#cb75-13"></a>   TWCD[K] &lt;-<span class="st"> </span><span class="kw">sum</span>(K_res<span class="op">$</span>withins)</span>
<span id="cb75-14"><a href="#cb75-14"></a>   Classifier[K,] &lt;-<span class="st"> </span>K_res<span class="op">$</span>cluster</span>
<span id="cb75-15"><a href="#cb75-15"></a>   K_centers &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(K<span class="op">+</span><span class="dv">1</span>, <span class="kw">ncol</span>(X)))</span>
<span id="cb75-16"><a href="#cb75-16"></a>   K_centers[K<span class="op">+</span><span class="dv">1</span>,] &lt;-<span class="st"> </span>Kaverage</span>
<span id="cb75-17"><a href="#cb75-17"></a>   K_centers[<span class="dv">1</span><span class="op">:</span>K,] &lt;-<span class="st"> </span>K_res<span class="op">$</span>centers </span>
<span id="cb75-18"><a href="#cb75-18"></a>                }</span>
<span id="cb75-19"><a href="#cb75-19"></a></span>
<span id="cb75-20"><a href="#cb75-20"></a><span class="co"># plot losses                </span></span>
<span id="cb75-21"><a href="#cb75-21"></a>xtitle &lt;-<span class="st"> &quot;decrease in total within-cluster dissimilarity &quot;</span></span>
<span id="cb75-22"><a href="#cb75-22"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>K0), <span class="dt">y=</span>TWCD, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">max</span>(TWCD)), <span class="dt">main=</span><span class="kw">list</span>(xtitle, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylab=</span><span class="st">&quot;total within-cluster dissimilarity&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;hyperparameter K&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb75-23"><a href="#cb75-23"></a><span class="kw">lines</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>K0), <span class="dt">y=</span>TWCD, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span>
<span id="cb75-24"><a href="#cb75-24"></a></span>
<span id="cb75-25"><a href="#cb75-25"></a><span class="co"># singular value decomposition</span></span>
<span id="cb75-26"><a href="#cb75-26"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb75-27"><a href="#cb75-27"></a>pca &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb75-28"><a href="#cb75-28"></a>dat3 &lt;-<span class="st"> </span>d.data</span>
<span id="cb75-29"><a href="#cb75-29"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]]</span>
<span id="cb75-30"><a href="#cb75-30"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]]</span>
<span id="cb75-31"><a href="#cb75-31"></a></span>
<span id="cb75-32"><a href="#cb75-32"></a>lim0 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb75-33"><a href="#cb75-33"></a></span>
<span id="cb75-34"><a href="#cb75-34"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;K-means vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb75-35"><a href="#cb75-35"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">4</span>),]</span>
<span id="cb75-36"><a href="#cb75-36"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb75-37"><a href="#cb75-37"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">1</span>),]</span>
<span id="cb75-38"><a href="#cb75-38"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb75-39"><a href="#cb75-39"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb75-40"><a href="#cb75-40"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb75-41"><a href="#cb75-41"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="k-medoids-clustering-pam" class="section level2">
<h2><span class="header-section-number">5.5</span> K-medoids clustering (PAM)</h2>
<p>K-means算法使用类均值向量作为聚类中心，因此对离群点非常敏感，如果具有极大值，可能大幅度地扭曲数据的分布。K-mediods算法使用样本点作为聚类中心，修正聚类中心是计算当前类非聚类中心点到其他所有点的最小值来更新聚类中心，同时可以使用Manhattan距离，可以有效削弱离群点的影响。本例中K-medoids聚类使用Manhattan距离,(<span class="math inline">\(L1\)</span>范式对离群点的惩罚权重比欧式距离小)，结果显示其聚类中心要比K-means聚类更加聚集一些。</p>
<p>PAM算法：</p>
<ol style="list-style-type: decimal">
<li><p>选择初始聚类中心<span class="math inline">\(c_1,c_2,\ldots,c_k\in\mathcal{X}\)</span>和聚类个数<span class="math inline">\(K\)</span>，计算<span class="math inline">\(TWCD\)</span>；
<span class="math display">\[TWCD=\sum_{k=1}^K\sum_{x_i\in{C_k}\cap\mathcal{X}}d(c_k),x_i\]</span></p></li>
<li><p>迭代(终止条件：<span class="math inline">\(TWCD\)</span>不再减少)</p>
<p>(1)遍历每个非聚类中心<span class="math inline">\(x_i\)</span>替换聚类中心点<span class="math inline">\(c_k^{t-1}\)</span>，根据距离最近原则重新分配各样本点，计算<span class="math inline">\(TWCD\)</span>;</p>
<p>(2)根据<span class="math inline">\(TWCD\)</span>最小原则更新聚类中心<span class="math inline">\(c_k^{t}\)</span>。</p></li>
</ol>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb76-2"><a href="#cb76-2"></a>(K_res &lt;-<span class="st"> </span><span class="kw">pam</span>(X, <span class="dt">k=</span><span class="dv">4</span>, <span class="dt">metric=</span><span class="st">&quot;manhattan&quot;</span>, <span class="dt">diss=</span><span class="ot">FALSE</span>))</span>
<span id="cb76-3"><a href="#cb76-3"></a></span>
<span id="cb76-4"><a href="#cb76-4"></a><span class="co"># plot K-medoids versus PCA</span></span>
<span id="cb76-5"><a href="#cb76-5"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;K-medoids vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb76-6"><a href="#cb76-6"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">4</span>),]</span>
<span id="cb76-7"><a href="#cb76-7"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb76-8"><a href="#cb76-8"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb76-9"><a href="#cb76-9"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb76-10"><a href="#cb76-10"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">2</span>),]</span>
<span id="cb76-11"><a href="#cb76-11"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb76-12"><a href="#cb76-12"></a><span class="kw">points</span>(<span class="dt">x=</span>dat3[K_res<span class="op">$</span>id.med,<span class="st">&quot;v1&quot;</span>],<span class="dt">y=</span>dat3[K_res<span class="op">$</span>id.med,<span class="st">&quot;v2&quot;</span>], <span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb76-13"><a href="#cb76-13"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="gaussian-mixture-modelsgmms" class="section level2">
<h2><span class="header-section-number">5.6</span> Gaussian mixture models(GMMs)</h2>
<p>K-means假设数据点是球状的，GMMs假设数据点是呈高斯分布，提供了更多的可能性。对<span class="math inline">\(n\)</span>维样本空间<span class="math inline">\(\mathcal{X}\)</span>中的随机向量<span class="math inline">\(x\)</span>，若<span class="math inline">\(x\)</span>服从高斯分布，<span class="math inline">\(\mu\)</span>是<span class="math inline">\(n\)</span>维均值向量，<span class="math inline">\(\Sigma\)</span>是<span class="math inline">\(n\times n\)</span> 的协方差矩阵，其概率密度函数为：
<span class="math display">\[p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)\Sigma^{-1}(x-\mu)}\]</span></p>
<p>由此可定义高斯混合分布，该分布由<span class="math inline">\(k\)</span>个混合成分组成，每个混合成分对应一个高斯分布，其中<span class="math inline">\(\mu_i\)</span>与<span class="math inline">\(\Sigma_i\)</span>是第<span class="math inline">\(i\)</span>个高斯混合成分的参数，<span class="math inline">\(\alpha_i\)</span>为相应的混合系数(<span class="math inline">\(\alpha_i&gt;0,\sum_{i=1}^k\alpha_i=1\)</span>)
<span class="math display">\[p_{\mathcal{M}}=\sum_{i=1}^k\alpha_i\times p(x|\mu_i,\Sigma_i)\]</span></p>
<p>若训练集<span class="math inline">\(D={x_1,x_2,\dots,x_m}\)</span>由上述过程生成，令随机变量<span class="math inline">\(z_j\in{1,2,\dots,k}\)</span>表示生成样本<span class="math inline">\(x_j\)</span>的高斯混合成分，换句话说，样本<span class="math inline">\(x_j\)</span>属于第<span class="math inline">\(z_j\)</span>个高斯分布，<span class="math inline">\(z_j\)</span>的先验概率<span class="math inline">\(p(x_j=i)=\alpha_i(i=1,2,\dots,k)\)</span>。根据贝叶斯定理，则<span class="math inline">\(z_j\)</span>的后验分布(表示样本<span class="math inline">\(x_j\)</span>由第<span class="math inline">\(i\)</span>个高斯混合成分生成的后验概率<span class="math inline">\(\gamma_ji\)</span>)为：
<span class="math display">\[\gamma_{ji}=p_{\mathcal{M}}(z_j=i|x_j)=\frac{p(z_j=i)\times p_{\mathcal{M}}(x_j|z_j=i)}{p_{\mathcal{M}}(x_j)}=\frac{a_i\times p(x_j|\mu_i,\Sigma_i)}{\sum_{i=1}^k\alpha_l\times p(x_j|\mu_l,\Sigma_l)}\]</span></p>
<p>当高斯混合成不已知时，高斯混合聚类将把样本集<span class="math inline">\(D\)</span>划分为<span class="math inline">\(k\)</span>个类<span class="math inline">\(C=\{C_1,C_2,\dots,C_k\}\)</span>，则每个样本<span class="math inline">\(x_j\)</span>的类标记<span class="math inline">\(\gamma_j\)</span>作如下确定：
<span class="math display">\[\gamma_j=\underset{i\in\{1,2,\dots,k\}}{\arg \min}\gamma_{ji}\]</span></p>
<p>对给定样本集<span class="math inline">\(D\)</span>，可采用极大似然估计求解模型参数<span class="math inline">\(\{(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k\}\)</span>，似然函数
<span class="math display">\[LL(D)=ln\left(\prod_{j=1}^m p_{\mathcal{M}(x_j)}\right)=\sum_{j=1}^m ln\left(\sum_{i=1}^k \alpha_i \times p(x_j|\mu_i,\Sigma_i) \right)\]</span></p>
<p>令<span class="math inline">\(\frac{\partial{LL(D)}}{\partial(\mu_i)}=0\)</span>，得到<span class="math inline">\(\mu_i=\frac{\sum_{j=1}^{m}\gamma_{ji}x_j}{\sum_{j=1}^m\gamma_{ji}}\)</span>
令<span class="math inline">\(\frac{\partial{LL(D)}}{\partial(\Sigma_i)}=0\)</span>，得到<span class="math inline">\(\Sigma_i=\frac{\sum_{j=1}^{m}\gamma_{ji}(x_j-\mu_i)(x_j-\mu_i)^T}{\sum_{j=1}^m\gamma_{ji}}\)</span>
可以看到各混合成分的均值<span class="math inline">\(\mu_i\)</span>和协方差阵<span class="math inline">\(\Sigma_i\)</span>可通过样本加权平均来估计，样本权重是每个样本属于该成分的后验概率。</p>
<p>对于混合系数<span class="math inline">\(\alpha_i\)</span>，除了要最大化<span class="math inline">\(LL(D)\)</span>，还需要满足<span class="math inline">\(\alpha_i&gt;0,\sum_{i=1}^k\alpha_i=1\)</span>，可以使用拉格朗日条件极值求解，其中<span class="math inline">\(\lambda\)</span>为拉格朗日乘子。
<span class="math display">\[LLF(D)=LL(D)=\lambda\left(\sum_{i=1}^k\alpha_i-1\right)\]</span></p>
<p>令<span class="math inline">\(\frac{\partial{LLF(D)}}{\partial(\alpha_i)}=0\)</span>，得到<span class="math inline">\(\alpha_i=\frac{\sum_{j=1}^{m}\gamma_{ji}}{m}\)</span>
可以看到各高斯成分<span class="math inline">\(\alpha_i\)</span>的混合系数由样本属于该成分的平均后验概率确定。</p>
<p>因为<span class="math inline">\(z_j\in\{1,2\dots,k\}\)</span>是隐变量(LatentVariable)，无法观测，一般采用EM算法进行迭代优化。EM算法：</p>
<ol style="list-style-type: decimal">
<li><p>确定初始高斯混合分布的模型参数<span class="math inline">\(\{(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k\}\)</span>和聚类个数<span class="math inline">\(K\)</span>.</p></li>
<li><p>迭代(终止条件：似然函数<span class="math inline">\(LL(D)\)</span>增长很少甚至不再增长或达到最大迭代次数)</p>
<p>(1)根据参数<span class="math inline">\(\Theta^{t-1}=\{(\alpha_i^{t-1},\mu_i^{t-1},\Sigma_i^{t-1})|1 \le i \le k\}\)</span>确定<span class="math inline">\(\gamma_{ji}\)</span>;</p>
<p>(2)根据<span class="math inline">\(\gamma_{ji}\)</span>更新参数<span class="math inline">\(\Theta^{t}=\{(\alpha_i^t,\mu_i^t,\Sigma_i^t)|1 \le i \le k\}\)</span></p></li>
<li><p>最后根据<span class="math inline">\(\gamma_j=\underset{i\in\{1,2,\dots,k\}}{\arg \min}\gamma_{ji}\)</span>确定各样本<span class="math inline">\(x_j\)</span>所属类</p></li>
</ol>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1"></a>seed &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb77-2"><a href="#cb77-2"></a><span class="kw">set.seed</span>(seed)</span>
<span id="cb77-3"><a href="#cb77-3"></a>K_res &lt;-<span class="st"> </span><span class="kw">GMM</span>(X, <span class="dt">gaussian_comps=</span><span class="dv">4</span>, <span class="dt">dist_mode=</span><span class="st">&quot;eucl_dist&quot;</span>, <span class="dt">seed_mode=</span><span class="st">&quot;random_subset&quot;</span>, <span class="dt">em_iter=</span><span class="dv">5</span>, <span class="dt">seed=</span>seed)</span>
<span id="cb77-4"><a href="#cb77-4"></a><span class="kw">summary</span>(K_res)</span>
<span id="cb77-5"><a href="#cb77-5"></a>clust &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_res<span class="op">$</span>centroids, K_res<span class="op">$</span>covariance_matrices, K_res<span class="op">$</span>weights)<span class="op">$</span>cluster_labels</span>
<span id="cb77-6"><a href="#cb77-6"></a></span>
<span id="cb77-7"><a href="#cb77-7"></a>pred &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_res<span class="op">$</span>centroids, K_res<span class="op">$</span>covariance_matrices, K_res<span class="op">$</span>weights)</span>
<span id="cb77-8"><a href="#cb77-8"></a><span class="kw">names</span>(pred)</span>
<span id="cb77-9"><a href="#cb77-9"></a></span>
<span id="cb77-10"><a href="#cb77-10"></a>pred<span class="op">$</span>cluster_labels[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</span>
<span id="cb77-11"><a href="#cb77-11"></a>pred<span class="op">$</span>cluster_proba[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</span>
<span id="cb77-12"><a href="#cb77-12"></a></span>
<span id="cb77-13"><a href="#cb77-13"></a>K_res<span class="op">$</span>centroids</span>
<span id="cb77-14"><a href="#cb77-14"></a></span>
<span id="cb77-15"><a href="#cb77-15"></a><span class="co"># singular value decomposition</span></span>
<span id="cb77-16"><a href="#cb77-16"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb77-17"><a href="#cb77-17"></a></span>
<span id="cb77-18"><a href="#cb77-18"></a>pca &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb77-19"><a href="#cb77-19"></a>dat3 &lt;-<span class="st"> </span>d.data</span>
<span id="cb77-20"><a href="#cb77-20"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]]</span>
<span id="cb77-21"><a href="#cb77-21"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]]</span>
<span id="cb77-22"><a href="#cb77-22"></a></span>
<span id="cb77-23"><a href="#cb77-23"></a>(kk1 &lt;-<span class="st"> </span>K_res<span class="op">$</span>centroids <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]])</span>
<span id="cb77-24"><a href="#cb77-24"></a>(kk2 &lt;-<span class="st"> </span>K_res<span class="op">$</span>centroids <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]])</span>
<span id="cb77-25"><a href="#cb77-25"></a></span>
<span id="cb77-26"><a href="#cb77-26"></a>lim0 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb77-27"><a href="#cb77-27"></a></span>
<span id="cb77-28"><a href="#cb77-28"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;GMM(diagonal) vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb77-29"><a href="#cb77-29"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">0</span>),]</span>
<span id="cb77-30"><a href="#cb77-30"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb77-31"><a href="#cb77-31"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb77-32"><a href="#cb77-32"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb77-33"><a href="#cb77-33"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">1</span>),]</span>
<span id="cb77-34"><a href="#cb77-34"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb77-35"><a href="#cb77-35"></a><span class="kw">points</span>(<span class="dt">x=</span>kk1,<span class="dt">y=</span>kk2, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb77-36"><a href="#cb77-36"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="三种聚类方法评价" class="section level2">
<h2><span class="header-section-number">5.7</span> 三种聚类方法评价</h2>
<p>图@ref(fig:cluster-results)展示了聚类的结果，GMMs最好。</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb78-2"><a href="#cb78-2"></a><span class="co">#K-means聚类(欧式距离)</span></span>
<span id="cb78-3"><a href="#cb78-3"></a>K_means &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dv">4</span>)</span>
<span id="cb78-4"><a href="#cb78-4"></a><span class="co">#的K-medoids聚类(曼哈顿距离)</span></span>
<span id="cb78-5"><a href="#cb78-5"></a>K_medoids &lt;-<span class="st"> </span><span class="kw">pam</span>(X, <span class="dt">k =</span> <span class="dv">4</span>, <span class="dt">metric =</span> <span class="st">&quot;manhattan&quot;</span>)</span>
<span id="cb78-6"><a href="#cb78-6"></a><span class="co">#GMM聚类(欧式距离)</span></span>
<span id="cb78-7"><a href="#cb78-7"></a>K_gmm &lt;-<span class="st"> </span><span class="kw">GMM</span>(X, <span class="dt">gaussian_comps =</span> <span class="dv">4</span>, <span class="dt">dist_mode =</span> <span class="st">&quot;eucl_dist&quot;</span>, </span>
<span id="cb78-8"><a href="#cb78-8"></a>             <span class="dt">seed_mode =</span> <span class="st">&quot;random_subset&quot;</span>, </span>
<span id="cb78-9"><a href="#cb78-9"></a>             <span class="dt">em_iter=</span> <span class="dv">5</span>,<span class="dt">seed =</span> <span class="dv">100</span>)</span>
<span id="cb78-10"><a href="#cb78-10"></a>clust &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_gmm<span class="op">$</span>centroids, </span>
<span id="cb78-11"><a href="#cb78-11"></a>                     K_gmm<span class="op">$</span>covariance_matrices, K_gmm<span class="op">$</span>weights)<span class="op">$</span>cluster_labels</span>
<span id="cb78-12"><a href="#cb78-12"></a><span class="co">#k-means</span></span>
<span id="cb78-13"><a href="#cb78-13"></a>cluster &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;cluster1&#39;</span>,<span class="st">&#39;cluster2&#39;</span>,<span class="st">&#39;cluster3&#39;</span>,<span class="st">&#39;cluster4&#39;</span>)</span>
<span id="cb78-14"><a href="#cb78-14"></a>cars &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) <span class="kw">nrow</span>(dat3[<span class="kw">which</span>(y <span class="op">==</span><span class="st"> </span>x),])</span>
<span id="cb78-15"><a href="#cb78-15"></a>sports &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) <span class="kw">nrow</span>(dat3[<span class="kw">which</span>(y <span class="op">==</span><span class="st"> </span>x <span class="op">&amp;</span><span class="st"> </span>dat3<span class="op">$</span>sports_car <span class="op">==</span><span class="st"> </span><span class="dv">1</span>),])</span>
<span id="cb78-16"><a href="#cb78-16"></a>K_means &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(cluster,</span>
<span id="cb78-17"><a href="#cb78-17"></a>                            <span class="st">&quot;cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">2</span>),</span>
<span id="cb78-18"><a href="#cb78-18"></a>                                     <span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">4</span>),</span>
<span id="cb78-19"><a href="#cb78-19"></a>                                     <span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">3</span>),</span>
<span id="cb78-20"><a href="#cb78-20"></a>                                     <span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">1</span>)),</span>
<span id="cb78-21"><a href="#cb78-21"></a>                            <span class="st">&quot;sports cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">2</span>),</span>
<span id="cb78-22"><a href="#cb78-22"></a>                                            <span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">4</span>),</span>
<span id="cb78-23"><a href="#cb78-23"></a>                                            <span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">3</span>),</span>
<span id="cb78-24"><a href="#cb78-24"></a>                                            <span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">1</span>)))</span>
<span id="cb78-25"><a href="#cb78-25"></a>K_medoids &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(cluster,</span>
<span id="cb78-26"><a href="#cb78-26"></a>                              <span class="st">&quot;cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">4</span>),</span>
<span id="cb78-27"><a href="#cb78-27"></a>                                       <span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">4</span>),</span>
<span id="cb78-28"><a href="#cb78-28"></a>                                       <span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">2</span>),</span>
<span id="cb78-29"><a href="#cb78-29"></a>                                       <span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">1</span>)),</span>
<span id="cb78-30"><a href="#cb78-30"></a>                              <span class="st">&quot;sports cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">4</span>),</span>
<span id="cb78-31"><a href="#cb78-31"></a>                                              <span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">3</span>),</span>
<span id="cb78-32"><a href="#cb78-32"></a>                                              <span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">2</span>),</span>
<span id="cb78-33"><a href="#cb78-33"></a>                                              <span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">1</span>)))</span>
<span id="cb78-34"><a href="#cb78-34"></a>GMMs &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(cluster,</span>
<span id="cb78-35"><a href="#cb78-35"></a>                         <span class="st">&quot;cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cars</span>(clust,<span class="dv">0</span>),<span class="kw">cars</span>(clust,<span class="dv">3</span>),</span>
<span id="cb78-36"><a href="#cb78-36"></a>                                  <span class="kw">cars</span>(clust,<span class="dv">1</span>),<span class="kw">cars</span>(clust,<span class="dv">2</span>)),</span>
<span id="cb78-37"><a href="#cb78-37"></a>                         <span class="st">&quot;sports cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">sports</span>(clust,<span class="dv">0</span>),<span class="kw">sports</span>(clust,<span class="dv">3</span>),</span>
<span id="cb78-38"><a href="#cb78-38"></a>                                         <span class="kw">sports</span>(clust,<span class="dv">1</span>),<span class="kw">sports</span>(clust,<span class="dv">2</span>)))</span>
<span id="cb78-39"><a href="#cb78-39"></a>k_means &lt;-<span class="st"> </span>reshape2<span class="op">::</span><span class="kw">melt</span>(K_means, <span class="dt">id.vars =</span> <span class="st">&quot;cluster&quot;</span>,</span>
<span id="cb78-40"><a href="#cb78-40"></a>                          <span class="dt">variable.name =</span> <span class="st">&quot;k_means&quot;</span>)</span>
<span id="cb78-41"><a href="#cb78-41"></a>k_medoids &lt;-<span class="st"> </span>reshape2<span class="op">::</span><span class="kw">melt</span>(K_medoids, <span class="dt">id =</span> <span class="st">&quot;cluster&quot;</span>, </span>
<span id="cb78-42"><a href="#cb78-42"></a>                            <span class="dt">variable.name =</span> <span class="st">&quot;k_medoids&quot;</span>)</span>
<span id="cb78-43"><a href="#cb78-43"></a>GMMs &lt;-<span class="st"> </span>reshape2<span class="op">::</span><span class="kw">melt</span>(GMMs, <span class="dt">id =</span> <span class="st">&quot;cluster&quot;</span>, <span class="dt">variable.name =</span> <span class="st">&quot;GMMs&quot;</span>)</span>
<span id="cb78-44"><a href="#cb78-44"></a><span class="co">#绘制三种聚类方法聚类结果中跑车样本与真实跑车样本对比图</span></span>
<span id="cb78-45"><a href="#cb78-45"></a>myggplot &lt;-<span class="st"> </span><span class="cf">function</span>(mydf,myxcol,myycol,myfill) {</span>
<span id="cb78-46"><a href="#cb78-46"></a>   ggplot2<span class="op">::</span><span class="kw">ggplot</span>(<span class="dt">data=</span>mydf,<span class="kw">aes</span>(<span class="dt">x =</span> {{myxcol}},</span>
<span id="cb78-47"><a href="#cb78-47"></a>                                 <span class="dt">y =</span> {{myycol}},</span>
<span id="cb78-48"><a href="#cb78-48"></a>                                 <span class="dt">fill =</span> {{myfill}}))<span class="op">+</span></span>
<span id="cb78-49"><a href="#cb78-49"></a><span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>,<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>)<span class="op">+</span></span>
<span id="cb78-50"><a href="#cb78-50"></a><span class="st">      </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> value),</span>
<span id="cb78-51"><a href="#cb78-51"></a>                <span class="dt">position =</span> <span class="kw">position_dodge</span>((<span class="dv">1</span>)),</span>
<span id="cb78-52"><a href="#cb78-52"></a>                <span class="dt">size =</span> <span class="dv">3</span>,</span>
<span id="cb78-53"><a href="#cb78-53"></a>                <span class="dt">vjust =</span> <span class="fl">-0.5</span>)<span class="op">+</span></span>
<span id="cb78-54"><a href="#cb78-54"></a><span class="st">      </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>),</span>
<span id="cb78-55"><a href="#cb78-55"></a>            <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb78-56"><a href="#cb78-56"></a>            <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb78-57"><a href="#cb78-57"></a>            <span class="dt">panel.background =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb78-58"><a href="#cb78-58"></a>            <span class="dt">axis.title.x =</span> <span class="kw">element_blank</span>())<span class="op">+</span></span>
<span id="cb78-59"><a href="#cb78-59"></a><span class="st">      </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;number&quot;</span>)<span class="op">+</span></span>
<span id="cb78-60"><a href="#cb78-60"></a><span class="st">      </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">250</span>)</span>
<span id="cb78-61"><a href="#cb78-61"></a>   </span>
<span id="cb78-62"><a href="#cb78-62"></a>}</span>
<span id="cb78-63"><a href="#cb78-63"></a><span class="co">#组合图形</span></span>
<span id="cb78-64"><a href="#cb78-64"></a><span class="kw">multiplot</span>(<span class="kw">myggplot</span>(k_means,cluster,value,k_means),</span>
<span id="cb78-65"><a href="#cb78-65"></a>          <span class="kw">myggplot</span>(k_medoids,cluster,value,k_medoids),</span>
<span id="cb78-66"><a href="#cb78-66"></a>          <span class="kw">myggplot</span>(GMMs,cluster,value,GMMs))</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/cluster.png" alt="聚类比较" width="60%"  />
<p class="caption">
(#fig:cluster-results)聚类比较
</p>
</div>
</div>
<div id="t-sne" class="section level2">
<h2><span class="header-section-number">5.8</span> t-SNE</h2>
<p><strong>简介</strong></p>
<p><strong>t分布-随机邻近嵌入</strong>(t-SNE, t-distributed stochastic neighbor embedding)由 Laurens van der Maaten和Geoffrey Hinton在2008年提出。t-SNE本质是一种嵌入模型，能够将高维空间中的数据映射到低维空间中，并保留数据集的局部特性。</p>
<p><strong>基本原理</strong></p>
<p>t-SNE将数据点之间的<strong>相似度转化为条件概率</strong>，<strong>原始空间</strong>中数据点的相似度由<strong>高斯联合分布</strong>表示，<strong>嵌入空间</strong>中数据点的相似度由<strong>t分布</strong>表示。</p>
<p>将原始空间和嵌入空间的联合概率分布的<strong>KL散度</strong>作为损失函数(loss function)，评估嵌入效果的好坏。通过<strong>梯度下降算法</strong>最小化损失函数，最终获得收敛结果。</p>
<p><strong>具体过程</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>原始空间</strong><br />
构建一个高维对象间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。<span class="math inline">\(q\)</span>维空间中给定一组数据<span class="math inline">\(x_1,…,x_n\)</span>。<br />
定义条件概率：
<span class="math display">\[
q_{j|i}=\frac{exp\left\{-\frac{1}{2\sigma_i^2}||x_i-x_j||_2^2\right\}}{\sum_{k\ne i}exp\left\{-\frac{1}{2\sigma_i^2}||x_i-x_j||_2^2\right\}},\ for\ i\ne j
\]</span>
定义联合概率分布：
<span class="math display">\[
q_{i,j}=\frac{1}{2n}(q_{j|i}+q_{i|j}),\ for\ i\ne j
\]</span>
此式既保证了对称性，又使得<span class="math inline">\(\sum_jq_{i,j}&gt;\frac{1}{2n}\)</span>for all <span class="math inline">\(i\)</span><br />
<strong>困惑度(Perplexity)</strong><br />
<span class="math inline">\(\sigma_i\)</span>的选择必须满足：在数据密集的地方要小，数据稀疏的地方要大。<br />
对于<span class="math inline">\(\sigma_i\)</span>，一个好的分配应使得困惑度为常数。
<span class="math display">\[
Perp(q_{.|i})=exp\left\{H(q_{.|i})\right\}=exp\left\{-\sum_{j\ne i}q_{j|i}log_2(q_{j|i})\right\} 
\]</span>
困惑度可理解为对每个点邻居数量的猜测，对最终成图有着复杂的影响。低困惑度对应的是局部视角；高困惑度对应的是全局视角。</p></li>
<li><p><strong>嵌入空间</strong><br />
在<span class="math inline">\(p\)</span>维空间中(<span class="math inline">\(p&lt;q\)</span>)找到一组数据点<span class="math inline">\(y_1,…,y_n\)</span>，使得这组数据点构建的联合概率分布<span class="math inline">\(p\)</span>尽可能地与高维空间中的联合概率分布<span class="math inline">\(q\)</span>相似。
<span class="math display">\[
p_{i,j}=\frac{(1+||y_i-y_j||_2^2)^{-1}}{\sum_{k\ne l}(1+||y_k-y_l||_2^2)^{-1}},\ for\ i\ne j
\]</span>
<strong>t(1)分布的选择</strong>：解决拥挤问题（高维空间中分离的簇，在低维中被分的不明显）</p>
<div class="figure" style="text-align: center">
<img src="./plots/5/t-distribution.png" alt="t-distribution" width="60%"  />
<p class="caption">
(#fig:tdist)t-distribution
</p>
</div>
<p>图@ref(fig:tdist)展示了不同自由度下的t分布的密度函数图像，可以看出，自由度越小，t分布的尾部越厚。
<span class="math display">\[
p_{i,j}\approx||y_i-y_j||^{-2}_2\ for\ ||y_i-y_j||_2\rightarrow\infty 
\]</span>
降维的效果用两分布间的KL散度(Kullback-Leibler divergences)度量
<span class="math display">\[
D_{KL}(q||p)=\sum_{j=1}^Jq_jlog\frac{q_j}{p_j}
\]</span></p></li>
</ol>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1"></a><span class="kw">library</span>(tsne)</span>
<span id="cb79-2"><a href="#cb79-2"></a>perp =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">30</span>,<span class="dv">40</span>,<span class="dv">50</span>,<span class="dv">60</span>) <span class="co">#困惑度</span></span>
<span id="cb79-3"><a href="#cb79-3"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb79-4"><a href="#cb79-4"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>){</span>
<span id="cb79-5"><a href="#cb79-5"></a>    <span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb79-6"><a href="#cb79-6"></a>    tsne =<span class="st"> </span><span class="kw">tsne</span>(X, <span class="dt">k=</span><span class="dv">2</span>, <span class="dt">initial_dim=</span><span class="kw">ncol</span>(X), <span class="dt">perplexity=</span>perp[i])</span>
<span id="cb79-7"><a href="#cb79-7"></a>    tsne1 =<span class="st"> </span>tsne[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>)]</span>
<span id="cb79-8"><a href="#cb79-8"></a>    <span class="kw">plot</span>(tsne1,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>,</span>
<span id="cb79-9"><a href="#cb79-9"></a>         <span class="dt">ylab=</span><span class="st">&quot;component 1&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;component 2&quot;</span>,</span>
<span id="cb79-10"><a href="#cb79-10"></a>         <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;t-SNE with perplexity &quot;</span>, perp[i], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)))</span>
<span id="cb79-11"><a href="#cb79-11"></a>    <span class="kw">points</span>(tsne1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),], <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb79-12"><a href="#cb79-12"></a>    <span class="kw">points</span>(tsne1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb79-13"><a href="#cb79-13"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/t-SNE.png" alt="t-SNE" width="80%"  />
<p class="caption">
(#fig:t-SNE)t-SNE
</p>
</div>
<p>图@ref(fig:t-SNE)展示了困惑度（10-60）对降维结果的影响，可以看出，困惑度为30时，降维的效果最佳。</p>
</div>
<div id="umap" class="section level2">
<h2><span class="header-section-number">5.9</span> UMAP</h2>
<p><strong>简介</strong></p>
<strong>统一流形逼近与投影</strong>(UMAP, Uniform Manifold Approximation and Projection)是建立在黎曼几何和代数拓扑理论框架上的新的降维<strong>流形学习技术</strong>。在可视化质量方面，UMAP算法与t-SNE具有竞争优势，但是它保留了更多全局结构、具有优越的运行性能、更好的可扩展性。
<div class="figure" style="text-align: center">
<img src="./plots/5/manifold.png" alt="manifold" width="60%"  />
<p class="caption">
(#fig:manifold)manifold
</p>
</div>
<p>图@ref(fig:manifold)中两个黑点，若考虑直线距离，那么这两个黑点之间距离很相近；如果放到流形学上，那么这两个点距离就得沿着图中曲线绕两圈。</p>
<p><strong>基本原理</strong></p>
<ol style="list-style-type: decimal">
<li><p>计算<strong>高维</strong>的<strong>流形结构特征</strong>，确定高维空间中各个点之间的距离，从而构造高维的数据分布结构。</p></li>
<li><p>将它们<strong>投影到低维空间</strong>，根据高维空间点与点之间的相对关系，提取特征值，在低维空间中<strong>重构</strong>这种距离关系，并计算低维空间中各个点之间的距离。</p></li>
<li><p>使用<strong>随机梯度下降</strong>来最小化这些距离之间的差异。</p></li>
</ol>
<p><strong>具体过程</strong></p>
<ol style="list-style-type: decimal">
<li><p>构建<strong>高维空间</strong>的模糊拓扑表示<br />
<span class="math inline">\(q\)</span>维空间中给定一组数据<span class="math inline">\(x_1,…,x_n\)</span>。定义：
<span class="math display">\[
d:dissimilarity\ measure\\
X_i=\left\{x_{i_1},…,x_{i_k}\right\}:k\ nearest\ neighbors\ of\ x_i
\]</span>
对于每个<span class="math inline">\(x_i\)</span>，确定<span class="math inline">\(\rho_i\)</span>和<span class="math inline">\(\sigma_i\)</span>
<span class="math display">\[
\rho_i=min\ d(x_i,x_{i_j}),1\le j\le k\\
\sum_{j=1}^kexp\left\{−\frac{d(x_i,x_{i_j})-\rho_i}{\sigma_i}\right\} =log_2k
\]</span>
<span class="math inline">\(\rho_i\)</span>控制嵌入的紧密程度，值越小点越聚集；<span class="math inline">\(\sigma_i\)</span>控制有效的嵌入降维范围。<br />
分布计算：
<span class="math display">\[
q_{i|j}=exp\left\{−\frac{d(x_i,x_{i_j})-\rho_i}{\sigma_i}\right\}\\
q_{i,j}=q_{i|j}+q_{j|i}-q_{i|j}q_{j|i}
\]</span></p></li>
<li><p>简单地优化低维表示，使其具有尽可能接近的模糊拓扑表示，并用交叉熵来度量。<br />
低维空间的分布<br />
<span class="math display">\[
p_{ij}=(1+a(y_i-y_j)^{2b})^{−1}
\]</span>
where <span class="math inline">\(a\approx1.93\)</span> and <span class="math inline">\(b\approx0.79\)</span> for default UMAP hyperparameters<br />
交叉熵作为代价函数<br />
<span class="math display">\[
CE(X,Y)=\sum_i\sum_j\left\{q_{ij}(X)ln\frac{q_{ij}(X)}{p_{ij}(Y)}+(1-q_{ij}(X))ln\frac{1-q_{ij}(X)}{1-p_{ij}(Y)}\right\}
\]</span></p></li>
</ol>
<p><strong>参数说明</strong></p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>umap configuration parameters</th>
<th>value</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>n_neighbors</strong></td>
<td><strong>15</strong></td>
<td><strong>确定相邻点的数量，通常取2-100</strong></td>
</tr>
<tr class="even">
<td><strong>n_components</strong></td>
<td><strong>2</strong></td>
<td><strong>降维的维数，默认是2</strong></td>
</tr>
<tr class="odd">
<td><strong>metric</strong></td>
<td><strong>euclidean</strong></td>
<td><strong>距离的计算方法，可选：euclidean,manhattan,chebyshev,minkowski,correlation,hamming等</strong></td>
</tr>
<tr class="even">
<td><strong>n_epochs</strong></td>
<td><strong>200</strong></td>
<td><strong>模型训练迭代次数。数据量大时200，小时500</strong></td>
</tr>
<tr class="odd">
<td>input</td>
<td>data</td>
<td>数据类型，如果是data就会按照数据计算；如果是dist就会按距离矩阵计算</td>
</tr>
<tr class="even">
<td>init</td>
<td>spectral</td>
<td>初始化，有三种方式：spectral,random,自定义</td>
</tr>
<tr class="odd">
<td><strong>min_dist</strong></td>
<td><strong>0.1</strong></td>
<td><strong>控制嵌入的紧密程度，值越小点越聚集，默认0.1</strong></td>
</tr>
<tr class="even">
<td>set_op_mix_ratio</td>
<td>1</td>
<td>降维过程中特征的结合方式，取0-1。0代表取交集，1代表取合集；中间就是比例</td>
</tr>
<tr class="odd">
<td>local_connectivity</td>
<td>1</td>
<td>局部连接的点之间值，默认1，其值越大局部连接越多，导致的结果就是超越固有的流形维数出现改变</td>
</tr>
<tr class="even">
<td>bandwidth</td>
<td>1</td>
<td>用于构造子集参数</td>
</tr>
<tr class="odd">
<td>alpha</td>
<td>1</td>
<td>学习率</td>
</tr>
<tr class="even">
<td>gamma</td>
<td>1</td>
<td>布局最优的学习率</td>
</tr>
<tr class="odd">
<td>negative_sample_rate</td>
<td>5</td>
<td>每一个阳性样本导致的阴性率。其值越大导致高的优化也就是过拟合，预测准确度下降。默认是5</td>
</tr>
<tr class="even">
<td>a</td>
<td>NA</td>
<td></td>
</tr>
<tr class="odd">
<td>b</td>
<td>NA</td>
<td></td>
</tr>
<tr class="even">
<td><strong>spread</strong></td>
<td><strong>1</strong></td>
<td><strong>控制有效的嵌入降维范围，与min_dist联合使用</strong></td>
</tr>
<tr class="odd">
<td><strong>random_state</strong></td>
<td><strong>NA</strong></td>
<td><strong>随机种子，确保模型的可重复性</strong></td>
</tr>
<tr class="even">
<td>transform_state</td>
<td>NA</td>
<td>用于数值转换操作。默认值42</td>
</tr>
<tr class="odd">
<td>knn</td>
<td>NA</td>
<td></td>
</tr>
<tr class="even">
<td>knn_repeats</td>
<td>1</td>
<td></td>
</tr>
<tr class="odd">
<td>verbose</td>
<td>FALSE</td>
<td>控制工作日志，防止存储过多</td>
</tr>
<tr class="even">
<td>umap_learn_args</td>
<td>NA</td>
<td>调用python基于umap-learn训练好的参数</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1"></a><span class="kw">library</span>(umap)</span>
<span id="cb80-2"><a href="#cb80-2"></a>min_dist =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.5</span>,<span class="fl">0.9</span>)</span>
<span id="cb80-3"><a href="#cb80-3"></a>k =<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">50</span>,<span class="dv">75</span>,<span class="dv">100</span>)</span>
<span id="cb80-4"><a href="#cb80-4"></a>sign =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>),<span class="dv">4</span>,<span class="dv">2</span>,<span class="dt">byrow =</span> F)</span>
<span id="cb80-5"><a href="#cb80-5"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">4</span>))</span>
<span id="cb80-6"><a href="#cb80-6"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>){</span>
<span id="cb80-7"><a href="#cb80-7"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){</span>
<span id="cb80-8"><a href="#cb80-8"></a>        umap.param =<span class="st"> </span>umap.defaults</span>
<span id="cb80-9"><a href="#cb80-9"></a>        umap.param<span class="op">$</span>n_components =<span class="st"> </span><span class="dv">2</span></span>
<span id="cb80-10"><a href="#cb80-10"></a>        umap.param<span class="op">$</span>random_state =<span class="st"> </span><span class="dv">100</span></span>
<span id="cb80-11"><a href="#cb80-11"></a>        umap.param<span class="op">$</span>min_dist =<span class="st"> </span>min_dist[i]</span>
<span id="cb80-12"><a href="#cb80-12"></a>        umap.param<span class="op">$</span>n_neighbors =<span class="st"> </span>k[j]</span>
<span id="cb80-13"><a href="#cb80-13"></a>        umap =<span class="st"> </span><span class="kw">umap</span>(X, <span class="dt">config=</span>umap.param, <span class="dt">method=</span><span class="st">&quot;naive&quot;</span>)</span>
<span id="cb80-14"><a href="#cb80-14"></a>        umap1 =<span class="st"> </span><span class="kw">matrix</span>()</span>
<span id="cb80-15"><a href="#cb80-15"></a>        umap<span class="op">$</span>layout[,<span class="dv">1</span>] =<span class="st"> </span>sign[j,<span class="dv">1</span>]<span class="op">*</span>umap<span class="op">$</span>layout[,<span class="dv">1</span>]</span>
<span id="cb80-16"><a href="#cb80-16"></a>        umap<span class="op">$</span>layout[,<span class="dv">2</span>] =<span class="st"> </span>sign[j,<span class="dv">2</span>]<span class="op">*</span>umap<span class="op">$</span>layout[,<span class="dv">2</span>]</span>
<span id="cb80-17"><a href="#cb80-17"></a>        umap1 =<span class="st"> </span>umap<span class="op">$</span>layout[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>)]</span>
<span id="cb80-18"><a href="#cb80-18"></a>        <span class="kw">plot</span>(umap1,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>,</span>
<span id="cb80-19"><a href="#cb80-19"></a>             <span class="dt">ylab=</span><span class="st">&quot;component 1&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;component 2&quot;</span>,</span>
<span id="cb80-20"><a href="#cb80-20"></a>             <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;UMAP (k=&quot;</span>,k[j],<span class="st">&quot;, min_dist= &quot;</span>,min_dist[i], <span class="st">&quot;)&quot;</span>,<span class="dt">sep=</span><span class="st">&quot;&quot;</span>)))</span>
<span id="cb80-21"><a href="#cb80-21"></a>        <span class="kw">points</span>(umap1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),], <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb80-22"><a href="#cb80-22"></a>        <span class="kw">points</span>(umap1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb80-23"><a href="#cb80-23"></a>    }</span>
<span id="cb80-24"><a href="#cb80-24"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/UMAP.png" alt="UMAP" width="80%"  />
<p class="caption">
(#fig:UMAP)UMAP
</p>
</div>
<p>图@ref(fig:UMAP)展示了降维结果以及两参数<code>k</code>和<code>min_dist</code>对降维结果的影响<br />
- 竖看：<code>min_dist</code>越大，图形中的点分散地越均匀<br />
- 横看：<code>k</code>越小，数据的流行结构显示地越明显</p>
</div>
<div id="som" class="section level2">
<h2><span class="header-section-number">5.10</span> SOM</h2>
<p><strong>自组织映射</strong>(Self-organizing map, SOM)是一种竞争型神经网络，由输入层和竞争层（常见2维）构成。</p>
图@ref(fig:som-ill)展示了SOM的结构。
<div class="figure" style="text-align: center">
<img src="./plots/5/som.png" alt="SOM" width="60%"  />
<p class="caption">
(#fig:som-ill)SOM
</p>
</div>
<ul>
<li><p><strong>输入层</strong>神经元的数量由输入向量的维度决定，一个神经元对应一个特征</p></li>
<li><p><strong>竞争层</strong>的常见结构：矩形(Rectangular)、六边形(Hexagonal)，参见图@ref(fig:SOM-com)。</p>
<div class="figure" style="text-align: center">
<img src="./plots/5/som-com.png" alt="SOM-com" width="80%"  />
<p class="caption">
(#fig:SOM-com)SOM-com
</p>
</div>
<p>竞争层神经元的数量决定了最终模型的粒度与规模，对最终模型的准确性与泛化能力影响很大。<br />
经验公式：<span class="math inline">\(N\ge5\sqrt{m}\)</span>，<span class="math inline">\(m\)</span>为训练样本数</p></li>
</ul>
<p><strong>基本原理</strong></p>
<p>运用竞争学习(competitive learning)策略，竞争层各神经元竞争对输入层响应的机会，最后仅有一个神经元获胜，代表对输入层的分类。如此迭代，逐步优化网络。</p>
<p><strong>具体过程</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>初始化</strong>：初始化连接权重<span class="math inline">\(\omega_{j}^{(0)}=\left(\omega_{j 1}, \ldots, \omega_{j n}\right), j=1, \ldots, N\)</span></p></li>
<li><p><strong>竞争</strong>：对于输入样本<span class="math inline">\(x_i\)</span>，遍历竞争层中每一个神经元，计算<span class="math inline">\(x_i\)</span>与每个神经元的连接权重<span class="math inline">\(\omega_j\)</span>之间的相似度（通常使用欧式距离或平方欧式距离）。距离最小的神经元节点胜出，称为BMN(best matching neuron)</p></li>
</ol>
<p><span class="math display">\[
j^{*}=j^{*}(i)=\underset{j \in \mathcal{J}}{\arg \min } d\left(\boldsymbol{w}_{j}, \boldsymbol{x}_{i}\right)
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p><strong>合作</strong>：获胜神经元决定了兴奋神经元拓扑邻域的空间位置，从而为相邻神经元之间的合作提供了基础。
神经生物学的研究表明，一组兴奋神经元内存在横向的相互作用。因此当一个神经元被激活时，近邻节点往往比远离的节点更兴奋。定义邻域函数<span class="math inline">\(\theta\)</span>，表示获胜神经元对近邻神经元的影响强弱，也即优胜邻域中每个神经元的更新幅度。常见的选择是高斯函数：
<span class="math display">\[
\theta\left(j^{*}(i), j ; t\right)=\exp \left\{-\frac{1}{2 \sigma(t)^{2}}\left\|j-j^{*}(i)\right\|_{2}^{2} / J^{2}\right\}
\]</span>
邻域半径<span class="math inline">\(\sigma(t)\)</span>随着时间的推移而减少。<br />
越靠近优胜神经元，更新幅度越大；越远离优胜神经元，更新幅度越小</p></li>
<li><p><strong>适应/学习</strong>：更新优胜邻域内神经元的连接权重
<span class="math display">\[
\boldsymbol{w}_{j}^{(t)}=\boldsymbol{w}_{j}^{(t-1)}+\theta\left(j^{*}(i), j ; t\right) \alpha(t)\left(\boldsymbol{x}_{i}-\boldsymbol{w}_{j}^{(t-1)}\right)
\]</span>
学习率<span class="math inline">\(\alpha(t)\)</span>随着时间的推移而减少。</p></li>
<li><p><strong>迭代</strong>：返回第二步，迭代至收敛或达到设定次数</p></li>
</ol>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1"></a><span class="kw">library</span>(kohonen)</span>
<span id="cb81-2"><a href="#cb81-2"></a>p =<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">sqrt</span>(<span class="dv">5</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">nrow</span>(X)))) <span class="co">#根据经验公式确定正方形的边长</span></span>
<span id="cb81-3"><a href="#cb81-3"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb81-4"><a href="#cb81-4"></a>som1 =<span class="st"> </span><span class="kw">som</span>(<span class="kw">as.matrix</span>(X),<span class="dt">grid=</span><span class="kw">somgrid</span>(<span class="dt">xdim=</span>p,<span class="dt">ydim=</span>p,<span class="dt">topo=</span><span class="st">&quot;rectangular&quot;</span>),<span class="dt">rlen=</span><span class="dv">500</span>,<span class="dt">dist.fcts=</span><span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb81-5"><a href="#cb81-5"></a><span class="kw">summary</span>(som1)</span>
<span id="cb81-6"><a href="#cb81-6"></a><span class="kw">head</span>(som1<span class="op">$</span>unit.classif,<span class="dv">100</span>) <span class="co">#各数据点的获胜神经元</span></span>
<span id="cb81-7"><a href="#cb81-7"></a><span class="kw">tail</span>(som1<span class="op">$</span>codes[[<span class="dv">1</span>]]) <span class="co">#连接权重向量</span></span>
<span id="cb81-8"><a href="#cb81-8"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb81-9"><a href="#cb81-9"></a>som2 =<span class="st"> </span><span class="kw">som</span>(<span class="kw">as.matrix</span>(X),<span class="dt">grid=</span><span class="kw">somgrid</span>(<span class="dt">xdim=</span>p,<span class="dt">ydim=</span>p,<span class="dt">topo=</span><span class="st">&quot;hexagonal&quot;</span>),<span class="dt">rlen=</span><span class="dv">500</span>,<span class="dt">dist.fcts=</span><span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb81-10"><a href="#cb81-10"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb81-11"><a href="#cb81-11"></a><span class="kw">plot</span>(som1,<span class="kw">c</span>(<span class="st">&quot;changes&quot;</span>))</span>
<span id="cb81-12"><a href="#cb81-12"></a><span class="kw">plot</span>(som1,<span class="kw">c</span>(<span class="st">&quot;counts&quot;</span>), <span class="dt">main=</span><span class="st">&quot;allocation counts to neurons&quot;</span>)</span>
<span id="cb81-13"><a href="#cb81-13"></a>d.data<span class="op">$</span>tau2 =<span class="st"> </span>d.data<span class="op">$</span>sports_car<span class="op">+</span><span class="kw">as.integer</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb81-14"><a href="#cb81-14"></a><span class="kw">plot</span>(som1,<span class="kw">c</span>(<span class="st">&quot;mapping&quot;</span>),<span class="dt">classif=</span><span class="kw">predict</span>(som1),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;red&quot;</span>)[d.data<span class="op">$</span>tau2], <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">main=</span><span class="st">&quot;allocation of cases to neurons&quot;</span>)</span>
<span id="cb81-15"><a href="#cb81-15"></a><span class="kw">plot</span>(som2,<span class="kw">c</span>(<span class="st">&quot;changes&quot;</span>))</span>
<span id="cb81-16"><a href="#cb81-16"></a><span class="kw">plot</span>(som2,<span class="kw">c</span>(<span class="st">&quot;counts&quot;</span>), <span class="dt">main=</span><span class="st">&quot;allocation counts to neurons&quot;</span>)</span>
<span id="cb81-17"><a href="#cb81-17"></a>d.data<span class="op">$</span>tau2 =<span class="st"> </span>d.data<span class="op">$</span>sports_car<span class="op">+</span><span class="kw">as.integer</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb81-18"><a href="#cb81-18"></a><span class="kw">plot</span>(som2,<span class="kw">c</span>(<span class="st">&quot;mapping&quot;</span>),<span class="dt">classif=</span><span class="kw">predict</span>(som2),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;red&quot;</span>)[d.data<span class="op">$</span>tau2], <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">main=</span><span class="st">&quot;allocation of cases to neurons&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/5/som2.png" alt="SOM" width="80%"  />
<p class="caption">
(#fig:SOM)SOM
</p>
</div>
<p>图@ref(fig:SOM)的第一行展示的是矩形竞争层输出的降维结果，第二行展示的是六边形竞争层输出的降维结果，sports cars（红色）可以被较为明显地区分开来。</p>
<!--chapter:end:05-unsupervised.Rmd-->
</div>
</div>
<div id="rnn" class="section level1">
<h1><span class="header-section-number">6</span> 循环神经网络与死亡率预测</h1>
<p><em>方玉昕、鲁瑶、高光远</em></p>
<p>😷 新冠肺炎死亡率数据：<a href="https://mpidr.shinyapps.io/stmortality/" class="uri">https://mpidr.shinyapps.io/stmortality/</a></p>
<div id="lee-carter-model" class="section level2">
<h2><span class="header-section-number">6.1</span> Lee-Carter Model</h2>
<p>Lee Carter模型中，死亡力（force of mortality）的定义如下：<br />
<span class="math display">\[\log \left(m_{t, x}\right)=a_{x}+b_{x} k_{t}\]</span>
其中，</p>
<ul>
<li><p><span class="math inline">\(m_{t, x}&gt;0\)</span> 是 <span class="math inline">\(x\)</span> 岁的人在日历年 <span class="math inline">\(t\)</span> 的死亡率（mortality rate）,</p></li>
<li><p><span class="math inline">\(a_{x}\)</span> 是 <span class="math inline">\(x\)</span> 岁的人的平均对数死亡率,</p></li>
<li><p><span class="math inline">\(b_{x}\)</span> 是死亡率变化的年龄因素,</p></li>
<li><p><span class="math inline">\(\left(k_{t}\right)_{t}\)</span> 是死亡率变化的日历年因素.</p></li>
</ul>
<p>用 <span class="math inline">\(M_{t, x}\)</span> 表示某一性别死亡率的观察值（raw mortality rates）.<br />
我们对对数死亡率 <span class="math inline">\(\log \left(M_{t, x}\right)\)</span> 中心化处理：<br />
<span class="math display">\[\log \left(M_{t, x}^{\circ}\right)=\log \left(M_{t, x}\right)-\widehat{a}_{x}=\log \left(M_{t, x}\right)-\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)\]</span></p>
<p>其中，</p>
<ul>
<li><p><span class="math inline">\(\mathcal{T}\)</span> 为训练集中日历年的集合,</p></li>
<li><p><span class="math inline">\(\widehat{a}_{x}=\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)\)</span> 是平均对数死亡率 <span class="math inline">\(a_{x}\)</span> 的估计.</p></li>
</ul>
<p>对于<span class="math inline">\(b_x,k_t\)</span> 我们的目标是求如下最优化问题：<br />
<span class="math display">\[\underset{\left(b_{x}\right)_{x},\left(k_{t}\right)_{t}}{\arg \min } \sum_{t, x}\left(\log \left(M_{t, x}^{\circ}\right)-b_{x} k_{t}\right)^{2}。\]</span></p>
<p>定义矩阵 <span class="math inline">\(A=\left(\log \left(M_{t, x}^{\circ}\right)\right)_{x, t}\)</span>。上述最优化问题可以通过对<span class="math inline">\(A\)</span>进行奇异值分解（SVD）解决<span class="math display">\[A=U\Lambda V^\intercal,\]</span>
其中<span class="math inline">\(U\)</span>称为左奇异矩阵，对角矩阵<span class="math inline">\(\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_T)\)</span>中的对角元素<span class="math inline">\(\lambda_1\geq\lambda_2\geq\ldots\geq\lambda_T\geq0\)</span>称为奇异值，<span class="math inline">\(V\)</span>称为右奇异矩阵。</p>
<ul>
<li><p><span class="math inline">\(A\)</span> 的第一个左奇异向量<span class="math inline">\(U_{\cdot,1}\)</span>与第一个奇异值<span class="math inline">\(\lambda_1\)</span>相乘，可以得到 <span class="math inline">\(\left(b_{x}\right)_{x}\)</span> 的一个估计 <span class="math inline">\(\left(\widehat{b}_{x}\right)_{x}\)</span>。</p></li>
<li><p><span class="math inline">\(A\)</span> 的第一个右奇异向量<span class="math inline">\(V_{\cdot,1}\)</span>给出了 <span class="math inline">\(\left(k_{t}\right)_{t}\)</span> 的一个估计 <span class="math inline">\(\left(\widehat{k}_{t}\right)_{t}\)</span>。</p></li>
</ul>
<p>为了求解结果的唯一性，增加约束：<br />
<span class="math display">\[\sum_{x} \hat{b}_{x}=1 \quad \text { and } \quad \sum_{t \in \mathcal{T}} \hat{k}_{t}=0\]</span>
至此即可解出唯一的 <span class="math inline">\(\left(\hat{a}_{x}, \hat{b}_{x}\right)_{x}, \left(\hat{k}_{t}\right)_{t}\)</span> . 这就是经典的LC模型构建方法.</p>
</div>
<div id="普通循环神经网络recurrent-neural-network" class="section level2">
<h2><span class="header-section-number">6.2</span> 普通循环神经网络（recurrent neural network）</h2>
<p><strong>输入变量（Input）</strong> : <span class="math inline">\(\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\)</span> with components <span class="math inline">\(\boldsymbol{x}_{t} \in \mathbb{R}^{\tau_{0}}\)</span> at times <span class="math inline">\(t=1, \ldots, T\)</span> (in time series structure).</p>
<p><strong>输出变量（Output）</strong>: <span class="math inline">\(y \in \mathcal{Y} \subset \mathbb{R}\)</span> .</p>
<p>首先看一个具有 <span class="math inline">\(\tau_{1} \in \mathbb{N}\)</span> 个隐层神经元（hidden neurons）和单一隐层（hidden layer）的RNN. 隐层由如下映射（mapping）定义：
<span class="math display">\[\boldsymbol{z}^{(1)}: \mathbb{R}^{\tau_{0} \times \tau_{1}} \rightarrow \mathbb{R}^{\tau_{1}}, \quad\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) \mapsto \boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right)\]</span>
其中下标 <span class="math inline">\(t\)</span> 表示时间,上标 (1) 表示第一隐层（本例中也是唯一隐层）.</p>
<p>隐层结构构造如下：<br />
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) =&amp;\left(\phi\left(\left\langle\boldsymbol{w}_{1}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{1}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right),  \ldots, \phi\left(\left\langle\boldsymbol{w}_{\tau_{1}}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{\tau_{1}}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)\right)^{\top} \\
\stackrel{\text { def. }}{=} &amp;\phi\left(\left\langle W^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle U^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)
\end{aligned}
\]</span>
其中第 <span class="math inline">\(1 \leq j \leq \tau_{1}\)</span> 个神经元的结构为：<br />
<span class="math display">\[\phi\left(\left\langle\boldsymbol{w}_{j}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{j}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)=\phi\left(w_{j, 0}^{(1)}+\sum_{l=1}^{\tau_{0}} w_{j, l}^{(1)} x_{t, l}+\sum_{l=1}^{\tau_{1}} u_{j, l}^{(1)} z_{t-1, l}\right)\]</span></p>
<ul>
<li><span class="math inline">\(\phi: \mathbb{R} \rightarrow \mathbb{R}\)</span> 是非线性激活函数（activation function）</li>
<li>网络参数（network parameters）为 <span class="math display">\[W^{(1)}=\left(\boldsymbol{w}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau \times\left(\tau_{0}+1\right)} \text{(including an intercept)}\]</span> <span class="math display">\[U^{(1)}=\left(\boldsymbol{u}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} \text{(excluding an intercept)}\]</span></li>
</ul>
<p>除了上述单隐层的结构，我们还可以轻松地设计多隐层的RNN.</p>
<p>例如，双隐层的RNN结构可以为:</p>
<ul>
<li><p><strong>1st variant</strong> : 仅允许同级隐层之间的循环
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &amp;=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right) \\
\boldsymbol{z}_{t}^{(2)} &amp;=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
\]</span></p></li>
<li><p><strong>2nd variant</strong> : 允许跨级隐层循环
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &amp;=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &amp;=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
\]</span></p></li>
<li><p><strong>3rd variant</strong> : 允许二级隐层与输入层 <span class="math inline">\(\boldsymbol{x}_{t}\)</span> 进行循环
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &amp;=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &amp;=\boldsymbol{z}^{(2)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
\]</span></p></li>
</ul>
</div>
<div id="长短期记忆神经网络long-short-term-memory" class="section level2">
<h2><span class="header-section-number">6.3</span> 长短期记忆神经网络（Long short-term memory）</h2>
<p>以上plain vanilla RNN 无法处理长距离依赖和且有梯度消散的问题。为此，Hochreiter-Schmidhuber (1997)提出了长短期记忆神经网络(Long Short Term Memory Network, LSTM)。</p>
<div id="激活函数activation-functions" class="section level3">
<h3><span class="header-section-number">6.3.1</span> 激活函数（Activation functions）</h3>
<p>LSTM 用到3种不同的 <strong>激活函数（activation functions）</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>Sigmoid函数（Sigmoid function）<br />
<span class="math display">\[\phi_{\sigma}(x)=\frac{1}{1+e^{-x}} \in(0,1)\]</span></p></li>
<li><p>双曲正切函数（Hyberbolic tangent function）
<span class="math display">\[\phi_{\tanh }(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=2 \phi_{\sigma}(2 x)-1 \in(-1,1)\]</span></p></li>
<li><p>一般的激活函数（General activation function）
<span class="math display">\[\phi: \mathbb{R} \rightarrow \mathbb{R}\]</span></p></li>
</ol>
</div>
<div id="gates-and-cell-state" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Gates and cell state</h3>
<p>令 <span class="math inline">\(\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span> 表示时间 <span class="math inline">\((t-1)\)</span> 时的<strong>活化状态（neuron activations）</strong>. 我们定义3中不同的 <strong>门（gates）</strong>, 用来决定传播到下一个时间的信息量：</p>
<ul>
<li><p><strong>遗忘门（Forget gate）</strong> (loss of memory rate):
<span class="math display">\[\boldsymbol{f}_{t}^{(1)}=\boldsymbol{f}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{f}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{f}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept) <span class="math inline">\(, U_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span> (excluding an intercept <span class="math inline">\(),\)</span> and where the activation function is evaluated element wise.</p></li>
<li><p><strong>输入门（Input gate）</strong> (memory update rate):
<span class="math display">\[\boldsymbol{i}_{t}^{(1)}=\boldsymbol{i}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{i}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{i}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span>.</p></li>
<li><p><strong>输出门（Output gate）</strong> (release of memory information rate):
<span class="math display">\[\boldsymbol{o}_{t}^{(1)}=\boldsymbol{o}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{o}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{o}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept) <span class="math inline">\(, U_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span>.</p></li>
</ul>
<p>注意：以上三种门的名字并不代表着它们在实际中的作用，它们的作用由网络参数决定，而网络参数是从数据中学到的。</p>
<p>令 <span class="math inline">\(\left(\boldsymbol{c}_{t}^{(1)}\right)_{t}\)</span> 表示 <strong>细胞状态（cell state）</strong> , 用以储存已获得的相关信息.</p>
<p>细胞状态的更新规则如下：<br />
<span class="math display">\[\begin{aligned}
\boldsymbol{c}_{t}^{(1)}&amp;=\boldsymbol{c}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)\\&amp;=\boldsymbol{f}_{t}^{(1)} \circ \boldsymbol{c}_{t-1}^{(1)}+\boldsymbol{i}_{t}^{(1)} \circ \phi_{\tanh }\left(\left\langle W_{c}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{c}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}
\end{aligned}\]</span>
for network parameters <span class="math inline">\(W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},\)</span> and <span class="math inline">\(\circ\)</span>
denotes the Hadamard product (element wise product).</p>
<p>最后，我们更新时刻 <span class="math inline">\(t\)</span> 时的活化状态 <span class="math inline">\(\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span>.<br />
<span class="math display">\[\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)=\boldsymbol{o}_{t}^{(1)} \circ \phi\left(\boldsymbol{c}_{t}^{(1)}\right) \in \mathbb{R}^{\tau_{1}}\]</span></p>
<p>至此，</p>
<ul>
<li><p>涉及的全部网络参数有: <span class="math display">\[W_{f}^{\top}, W_{i}^{\top}, W_{o}^{\top}, W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}，~~ U_{f}^{\top}, U_{i}^{\top}, U_{o}^{\top}, U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} .\]</span></p></li>
<li><p>一个LSTM层需要 <span class="math inline">\(4\left(\left(\tau_{0}+1\right) \tau_{1}+\tau_{1}^{2}\right)\)</span> 个网络参数。</p></li>
<li><p>以上定义的复杂映射在keras通过函数<code>layer_lstm()</code>即可实现。</p></li>
<li><p>这些参数均由梯度下降的变式算法（a variant of the gradient descent algorithm）学习得.</p></li>
</ul>
</div>
<div id="output-function" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Output Function</h3>
<p>基于 <span class="math inline">\(\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\)</span> , 我们来预测定义在 <span class="math inline">\(\mathcal{Y} \subset \mathbb{R}\)</span> 的随机变量 <span class="math inline">\(Y_{T}\)</span> .</p>
<p><span class="math display">\[\widehat{Y}_{T}=\widehat{Y}_{T}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{T}^{(1)}\right\rangle \in \mathcal{Y}\]</span>
其中，</p>
<ul>
<li><p><span class="math inline">\(z_{T}^{(1)}\)</span> 是最新的隐层神经元活化状态（hidden neuron activation）</p></li>
<li><p><span class="math inline">\(\boldsymbol{w} \in \mathbb{R}^{\tau_{1}+1}\)</span> 是输出权重(again including an intercept component)</p></li>
<li><p><span class="math inline">\(\varphi: \mathbb{R} \rightarrow \mathcal{Y}\)</span> 是一个恰当的输出激活函数，选择时需要考虑<span class="math inline">\(y\)</span>的取值范围。</p></li>
</ul>
</div>
<div id="time-distributed-layer" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Time-distributed Layer</h3>
<p>以上只考虑了根据最新的状态 <span class="math inline">\(\boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\)</span> 所确定的单一的输出 <span class="math inline">\(Y_{T}\)</span>.</p>
<p>但是我们可以考虑 <strong>所有</strong> 隐层神经元状态:<br />
<span class="math display">\[\boldsymbol{z}_{1}^{(1)}\left(\boldsymbol{x}_{1}\right), \boldsymbol{z}_{2}^{(1)}\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right), \boldsymbol{z}_{3}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{3}\right), \ldots, \boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\]</span>
每一个状态 <span class="math inline">\(\boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)\)</span> 都可以作为解释变量，用以估计 <span class="math inline">\(t\)</span> 时所对应的 <span class="math inline">\(Y_{t}\)</span> :</p>
<p><span class="math display">\[\widehat{Y}_{t}=\widehat{Y}_{t}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\right\rangle=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)\right\rangle\]</span>
其中过滤器（filter） <span class="math inline">\(\varphi\langle\boldsymbol{w}, \cdot\rangle\)</span> 对所有时间 <span class="math inline">\(t\)</span> 取相同函数.</p>
<p><strong>小结：LSTM的优势</strong></p>
<ul>
<li><p>时间序列结构和因果关系都可以得到正确的反应</p></li>
<li><p>由于参数不依赖时间，LSTM可以很容易地拓展到未来时间段</p></li>
</ul>
</div>
</div>
<div id="门控循环神经网络gated-recurrent-unit" class="section level2">
<h2><span class="header-section-number">6.4</span> 门控循环神经网络（Gated Recurrent Unit）</h2>
<p>另一个比较热门的RNN结构是：门控循环单元（gated recurrent unit, GRU), 由Cho et al. (2014) 提出，它比LSTM更加简洁，但同样可以缓解plain vanilla RNN中梯度消散的问题。</p>
<div id="gates" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Gates</h3>
<p>GRU只使用2个不同的<strong>门（gates）</strong>. 令 <span class="math inline">\(\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span> 表示 <span class="math inline">\((t-1)\)</span> 时神经元活化状态.</p>
<ul>
<li><p><strong>Reset gate</strong>: 类似于LSTM中的遗忘门
<span class="math display">\[\boldsymbol{r}_{t}^{(1)}=\boldsymbol{r}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{r}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{r}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span>.</p></li>
<li><p><strong>Update gate</strong>: 类似于LSTM中的输入门
<span class="math display">\[\boldsymbol{u}_{t}^{(1)}=\boldsymbol{u}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{u}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{u}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span></p></li>
</ul>
</div>
<div id="neuron-activations" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Neuron Activations</h3>
<p>以上门变量的作用是，已知 <span class="math inline">\(t-1\)</span> 时神经元活化状态 <span class="math inline">\(\boldsymbol{z}_{t-1}^{(1)}\)</span>, 计算 <span class="math inline">\(t\)</span> 时神经元活化状态 <span class="math inline">\(\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span> . 我们选用如下结构：
<span class="math display">\[\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\boldsymbol{r}_{t}^{(1)} \circ \boldsymbol{z}_{t-1}^{(1)}+\left(\mathbf{1}-\boldsymbol{r}_{t}^{(1)}\right) \circ \phi\left(\left\langle W, \boldsymbol{x}_{t}\right\rangle+\boldsymbol{u}_{t} \circ\left\langle U, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept) <span class="math inline">\(, U^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},\)</span> and where <span class="math inline">\(\circ\)</span> denotes the Hadamard product.</p>
<p>GRU网络比LSTM网络的结构更简洁，而且会产生相近的结果。
但是，GRU在稳健性上有较大缺陷，因此现阶段LSTM的使用更为广泛.</p>
</div>
</div>
<div id="案例分析case-study" class="section level2">
<h2><span class="header-section-number">6.5</span> 案例分析（Case study）</h2>
<p>本案例的数据来源于Human Mortality Database (HMD)中的数据，选择瑞士人口数据(HMD中标记为“CHE”)作为示例。</p>
<div id="数据描述-1" class="section level3">
<h3><span class="header-section-number">6.5.1</span> 数据描述</h3>
<p>数据包含7个变量，各变量说明如下：</p>
<table>
<thead>
<tr class="header">
<th align="center">变量</th>
<th align="center">类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Gender</td>
<td align="center">factor</td>
<td>两种性别——男性和女性</td>
</tr>
<tr class="even">
<td align="center">Year</td>
<td align="center">int</td>
<td>日历年，1950年到2016年</td>
</tr>
<tr class="odd">
<td align="center">Age</td>
<td align="center">int</td>
<td>年龄范围0-99岁</td>
</tr>
<tr class="even">
<td align="center">Country</td>
<td align="center">chr</td>
<td>“CHE”，代表瑞士</td>
</tr>
<tr class="odd">
<td align="center">imputed_flag</td>
<td align="center">logi</td>
<td>原始死亡率为0，用HMD中其余国家同日历年同年龄的平均死亡率代替，则该变量为TRUE</td>
</tr>
<tr class="even">
<td align="center">mx</td>
<td align="center">num</td>
<td>死亡率</td>
</tr>
<tr class="odd">
<td align="center">logmx</td>
<td align="center">num</td>
<td>对数死亡率</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb82-2"><a href="#cb82-2"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb82-3"><a href="#cb82-3"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb82-4"><a href="#cb82-4"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb82-5"><a href="#cb82-5"></a><span class="kw">length</span>(<span class="kw">unique</span>(all_mort<span class="op">$</span>Age))</span>
<span id="cb82-6"><a href="#cb82-6"></a><span class="kw">length</span>(<span class="kw">unique</span>(all_mort<span class="op">$</span>Year))</span>
<span id="cb82-7"><a href="#cb82-7"></a><span class="dv">67</span><span class="op">*</span><span class="dv">2</span><span class="op">*</span><span class="dv">100</span></span></code></pre></div>
</div>
<div id="死亡率热力图" class="section level3">
<h3><span class="header-section-number">6.5.2</span> 死亡率热力图</h3>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1"></a>gender &lt;-<span class="st"> &quot;Male&quot;</span></span>
<span id="cb83-2"><a href="#cb83-2"></a><span class="co">#gender &lt;- &quot;Female&quot;</span></span>
<span id="cb83-3"><a href="#cb83-3"></a>m0 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(all_mort<span class="op">$</span>logmx), <span class="kw">max</span>(all_mort<span class="op">$</span>logmx))</span>
<span id="cb83-4"><a href="#cb83-4"></a><span class="co"># rows are calendar year t, columns are ages x</span></span>
<span id="cb83-5"><a href="#cb83-5"></a>logmx &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">matrix</span>(<span class="kw">as.matrix</span>(all_mort[<span class="kw">which</span>(all_mort<span class="op">$</span>Gender<span class="op">==</span>gender),<span class="st">&quot;logmx&quot;</span>]), <span class="dt">nrow=</span><span class="dv">100</span>, <span class="dt">ncol=</span><span class="dv">67</span>))</span>
<span id="cb83-6"><a href="#cb83-6"></a><span class="co"># png(&quot;./plots/6/heat.png&quot;)</span></span>
<span id="cb83-7"><a href="#cb83-7"></a><span class="kw">image</span>(<span class="dt">z=</span>logmx, <span class="dt">useRaster=</span><span class="ot">TRUE</span>,  <span class="dt">zlim=</span>m0, <span class="dt">col=</span><span class="kw">rev</span>(<span class="kw">rainbow</span>(<span class="dt">n=</span><span class="dv">60</span>, <span class="dt">start=</span><span class="dv">0</span>, <span class="dt">end=</span>.<span class="dv">72</span>)), <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;Swiss &quot;</span>,gender, <span class="st">&quot; raw log-mortality rates&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">ylab=</span><span class="st">&quot;age x&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;calendar year t&quot;</span>)</span>
<span id="cb83-8"><a href="#cb83-8"></a><span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span>(<span class="dv">2016-1950</span>))<span class="op">/</span>(<span class="dv">2016-1950</span>), <span class="kw">c</span>(<span class="dv">1950</span><span class="op">:</span><span class="dv">2016</span>))                   </span>
<span id="cb83-9"><a href="#cb83-9"></a><span class="kw">axis</span>(<span class="dv">2</span>, <span class="dt">at=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">49</span>)<span class="op">/</span><span class="dv">50</span>, <span class="dt">labels=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">49</span>)<span class="op">*</span><span class="dv">2</span>)                   </span>
<span id="cb83-10"><a href="#cb83-10"></a><span class="kw">lines</span>(<span class="dt">x=</span><span class="kw">rep</span>((<span class="dv">1999-1950</span><span class="fl">+0.5</span>)<span class="op">/</span>(<span class="dv">2016-1950</span>), <span class="dv">2</span>), <span class="dt">y=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb83-11"><a href="#cb83-11"></a><span class="kw">dev.off</span>()</span></code></pre></div>
<p>图@ref(fig:heatplot)显示了男女性对数死亡率随时间的改善:</p>
<ul>
<li><p>左右两幅图的色标相同，蓝色表示死亡率小，红色表示死亡率大</p></li>
<li><p>该图显示过去几十年典型的死亡率改善——热图中颜色呈略微向上的对角线结构</p></li>
<li><p>平均而言，女性死亡率低于男性</p></li>
<li><p>图中位于2000年的垂直黑线表示对于训练数据<span class="math inline">\(\mathcal{T}\)</span>和验证数据<span class="math inline">\(\mathcal{V}\)</span>的划分:后续模型将使用日历年<span class="math inline">\(t=1950, \ldots, 1999\)</span>作为训练数据<span class="math inline">\(\mathcal{T}\)</span>进行学习，用<span class="math inline">\(2000, \ldots, 2016\)</span>作为验证数据<span class="math inline">\(\mathcal{V}\)</span>对死亡率做样本外验证。</p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="./plots/6/heat.png" alt="瑞士男女性死亡率热力图" width="60%" />
<p class="caption">
(#fig:heatplot)瑞士男女性死亡率热力图
</p>
</div>
</div>
<div id="lee-carter-模型" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Lee-Carter 模型</h3>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">1999</span></span>
<span id="cb84-2"><a href="#cb84-2"></a>gender &lt;-<span class="st"> &quot;Female&quot;</span></span>
<span id="cb84-3"><a href="#cb84-3"></a>train &lt;-<span class="st"> </span>all_mort[Year<span class="op">&lt;=</span>ObsYear][Gender <span class="op">==</span><span class="st"> </span>gender]</span>
<span id="cb84-4"><a href="#cb84-4"></a><span class="kw">min</span>(train<span class="op">$</span>Year)</span>
<span id="cb84-5"><a href="#cb84-5"></a>    </span>
<span id="cb84-6"><a href="#cb84-6"></a><span class="co">### fit via SVD</span></span>
<span id="cb84-7"><a href="#cb84-7"></a>train[,ax<span class="op">:</span><span class="er">=</span><span class="st"> </span><span class="kw">mean</span>(logmx), by =<span class="st"> </span>(Age)]</span>
<span id="cb84-8"><a href="#cb84-8"></a>train[,mx_adj<span class="op">:</span><span class="er">=</span><span class="st"> </span>logmx<span class="op">-</span>ax]  </span>
<span id="cb84-9"><a href="#cb84-9"></a>rates_mat &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">dcast.data.table</span>(Age<span class="op">~</span>Year, <span class="dt">value.var =</span> <span class="st">&quot;mx_adj&quot;</span>, sum))[,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb84-10"><a href="#cb84-10"></a><span class="kw">dim</span>(rates_mat)</span>
<span id="cb84-11"><a href="#cb84-11"></a>svd_fit &lt;-<span class="st"> </span><span class="kw">svd</span>(rates_mat)</span>
<span id="cb84-12"><a href="#cb84-12"></a>    </span>
<span id="cb84-13"><a href="#cb84-13"></a>ax &lt;-<span class="st"> </span>train[,<span class="kw">unique</span>(ax)]</span>
<span id="cb84-14"><a href="#cb84-14"></a>bx &lt;-<span class="st"> </span>svd_fit<span class="op">$</span>u[,<span class="dv">1</span>]<span class="op">*</span>svd_fit<span class="op">$</span>d[<span class="dv">1</span>]</span>
<span id="cb84-15"><a href="#cb84-15"></a>kt &lt;-<span class="st"> </span>svd_fit<span class="op">$</span>v[,<span class="dv">1</span>]</span>
<span id="cb84-16"><a href="#cb84-16"></a>      </span>
<span id="cb84-17"><a href="#cb84-17"></a>c1 &lt;-<span class="st"> </span><span class="kw">mean</span>(kt)</span>
<span id="cb84-18"><a href="#cb84-18"></a>c2 &lt;-<span class="st"> </span><span class="kw">sum</span>(bx)</span>
<span id="cb84-19"><a href="#cb84-19"></a>ax &lt;-<span class="st"> </span>ax<span class="op">+</span>c1<span class="op">*</span>bx</span>
<span id="cb84-20"><a href="#cb84-20"></a>bx &lt;-<span class="st"> </span>bx<span class="op">/</span>c2</span>
<span id="cb84-21"><a href="#cb84-21"></a>kt &lt;-<span class="st"> </span>(kt<span class="op">-</span>c1)<span class="op">*</span>c2</span>
<span id="cb84-22"><a href="#cb84-22"></a>    </span>
<span id="cb84-23"><a href="#cb84-23"></a><span class="co">### extrapolation and forecast</span></span>
<span id="cb84-24"><a href="#cb84-24"></a>vali  &lt;-<span class="st"> </span>all_mort[Year<span class="op">&gt;</span>ObsYear][Gender <span class="op">==</span><span class="st"> </span>gender]    </span>
<span id="cb84-25"><a href="#cb84-25"></a>t_forecast &lt;-<span class="st"> </span>vali[,<span class="kw">unique</span>(Year)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">length</span>()</span>
<span id="cb84-26"><a href="#cb84-26"></a>forecast_kt  =kt <span class="op">%&gt;%</span><span class="st"> </span>forecast<span class="op">::</span><span class="kw">rwf</span>(t_forecast, <span class="dt">drift =</span> T)</span>
<span id="cb84-27"><a href="#cb84-27"></a>kt_forecast =<span class="st"> </span>forecast_kt<span class="op">$</span>mean</span>
<span id="cb84-28"><a href="#cb84-28"></a> </span>
<span id="cb84-29"><a href="#cb84-29"></a><span class="co"># illustration selected drift</span></span>
<span id="cb84-30"><a href="#cb84-30"></a>plot_data &lt;-<span class="st"> </span><span class="kw">c</span>(kt, kt_forecast)</span>
<span id="cb84-31"><a href="#cb84-31"></a><span class="kw">plot</span>(plot_data, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">cex=</span><span class="dv">2</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">ylab=</span><span class="st">&quot;values k_t&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;calendar year t&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;estimated process k_t for &quot;</span>,gender, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), <span class="dt">cex=</span><span class="fl">1.5</span>)) </span>
<span id="cb84-32"><a href="#cb84-32"></a><span class="kw">points</span>(kt, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb84-33"><a href="#cb84-33"></a><span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(plot_data)), <span class="dt">labels=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(plot_data))<span class="op">+</span><span class="dv">1949</span>)                   </span>
<span id="cb84-34"><a href="#cb84-34"></a><span class="kw">abline</span>(<span class="dt">v=</span>(<span class="kw">length</span>(kt)<span class="op">+</span><span class="fl">0.5</span>), <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb84-35"><a href="#cb84-35"></a><span class="co"># in-sample and out-of-sample analysis    </span></span>
<span id="cb84-36"><a href="#cb84-36"></a>fitted =<span class="st"> </span>(ax<span class="op">+</span>(bx)<span class="op">%*%</span><span class="kw">t</span>(kt)) <span class="op">%&gt;%</span><span class="st"> </span>melt</span>
<span id="cb84-37"><a href="#cb84-37"></a>train<span class="op">$</span>pred_LC_svd =<span class="st"> </span>fitted<span class="op">$</span>value <span class="op">%&gt;%</span><span class="st"> </span>exp</span>
<span id="cb84-38"><a href="#cb84-38"></a>fitted_vali =<span class="st"> </span>(ax<span class="op">+</span>(bx)<span class="op">%*%</span><span class="kw">t</span>(kt_forecast)) <span class="op">%&gt;%</span><span class="st"> </span>melt</span>
<span id="cb84-39"><a href="#cb84-39"></a>vali<span class="op">$</span>pred_LC_svd =<span class="st">   </span>fitted_vali<span class="op">$</span>value <span class="op">%&gt;%</span><span class="st"> </span>exp</span>
<span id="cb84-40"><a href="#cb84-40"></a><span class="kw">round</span>(<span class="kw">c</span>((<span class="kw">mean</span>((train<span class="op">$</span>mx<span class="op">-</span>train<span class="op">$</span>pred_LC_svd)<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>) , (<span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali<span class="op">$</span>pred_LC_svd)<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>)),<span class="dv">4</span>)</span></code></pre></div>
<p>用带漂移项的随机游走预测<span class="math inline">\(t \in \mathcal{V}=\{2000, \ldots, 2016\}\)</span>的<span class="math inline">\(\hat{k}_{t}\)</span>，下图说明了结果。</p>
<div class="figure" style="text-align: center">
<img src="./plots/6/kt.png" alt="瑞士男女性kt的估计与预测值" width="60%"  />
<p class="caption">
(#fig:kt)瑞士男女性kt的估计与预测值
</p>
</div>
<p>图@ref(fig:kt)显示对于女性来说预测结果是相对合理的，但是对于男性而言，由此产生的漂移可能需要进一步的探索，下面<strong>男女性样本内外的MSE损失</strong>结果也表明了这一点：男性样本外损失较大</p>
<p><span class="math display">\[
\begin{array}{|c|cc|cc|}
\hline &amp; \ {\text { in-sample loss }} &amp; \ {\text { in-sample loss }}  &amp; \ {\text { out-of-sample loss }} &amp; \ {\text { out-of-sample loss }}\\
&amp; \text { female } &amp; \text { male } &amp; \text { female } &amp; \text { male } \\
\hline \text { LC model with SVD } &amp; 3.7573 &amp; 8.8110 &amp; 0.6045 &amp; 1.8152 \\
\hline
\end{array}
\]</span></p>
</div>
<div id="初试rnn" class="section level3">
<h3><span class="header-section-number">6.5.4</span> 初试RNN</h3>
<ol style="list-style-type: decimal">
<li>数据说明</li>
</ol>
<ul>
<li><p>选择性别为“女性”，提取<span class="math inline">\(1990, \ldots, 2001\)</span>年的对数死亡率，年龄为<span class="math inline">\(0 \leq x \leq 99\)</span></p></li>
<li><p>超参数设置：回顾周期<span class="math inline">\(T=10\)</span>；<span class="math inline">\(\tau_{0}=3\)</span></p></li>
<li><p>定义解释变量和响应变量：</p>
<p>对于<span class="math inline">\(1 \leq x \leq 98\)</span>，<span class="math inline">\(1 \leq t \leq T\)</span>，有</p>
<p><strong>解释变量</strong><span class="math inline">\(\boldsymbol{x}_{t, x}=\left(\log \left(M_{1999-(T-t), x-1}\right), \log \left(M_{1999-(T-t), x}\right), \log \left(M_{1999-(T-t), x+1}\right)\right)^{\top} \in \mathbb{R}^{\tau_{0}}\)</span></p>
<p><strong>响应变量</strong><span class="math inline">\(\boldsymbol{Y}_{T, x}=\log(M_{2000,x}) =\log \left(M_{1999-(T-T)+1, x}\right) \in \mathbb{R}_{-}\)</span></p>
<p>同时考虑<span class="math inline">\((x-1,x,x+1)\)</span>目的是用邻近的年龄来平滑输入。</p></li>
<li><p>选择训练数据和验证数据：</p>
<p><strong>训练数据</strong><span class="math inline">\(\mathcal{T}=\{(\boldsymbol{x}_{1,x}, \ldots,\boldsymbol{x}_{T,x};\boldsymbol{Y}_{T, x});1 \leq x \leq 98\}\)</span></p>
<p><strong>验证数据</strong><span class="math inline">\(\mathcal{V}=\{(\boldsymbol{x}_{2,x}, \ldots,\boldsymbol{x}_{T+1,x};\boldsymbol{Y}_{T+1, x});1 \leq x \leq 98\}\)</span>，在训练数据基础上时移了一个日历年</p></li>
<li><p>数据如下图所示：</p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="./plots/6/datatoy.png" alt="RNN初试中选择的数据" width="60%"  />
<p class="caption">
(#fig:unnamed-chunk-1)RNN初试中选择的数据
</p>
</div>
<p>黑线表示选定的解释变量<span class="math inline">\(\boldsymbol{x}_{t, x}\)</span>;蓝色的点是训练数据中的的响应变量<span class="math inline">\(\boldsymbol{Y}_{T, x}\)</span>；验证数据中响应变量<span class="math inline">\(\boldsymbol{Y}_{T+1, x}=\log(M_{2001,x})\)</span>用红色的点表示</p>
<ol start="2" style="list-style-type: decimal">
<li>数据预处理</li>
</ol>
<ul>
<li><p>对解释变量应用MinMaxScaler进行标准化处理</p></li>
<li><p>切换响应变量的符号</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>比较LSTMs和GRUs</li>
</ol>
<ul>
<li><p>在验证集<span class="math inline">\(\mathcal{V}\)</span>上跟踪过拟合</p></li>
<li><p>梯度下降优化算法选用的是<code>nadam</code></p></li>
<li><p>下图显示了5个模型的收敛行为</p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="./plots/6/loss1.png" alt="模型的样本内外损失" width="60%"  />
<p class="caption">
(#fig:loss1)模型的样本内外损失
</p>
</div>
<ul>
<li>根据过拟合确定的停止时间的模型校准结果如下表所示：</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|ccc|cc|}
\hline &amp; \# \text { param. } &amp; \text { epochs } &amp; \text { run time } &amp; \text { in-sample loss } &amp; \text { out-of-sample loss } \\
\hline \text { LSTM1 } &amp; 186 &amp; 150 &amp; 8 \mathrm{sec} &amp; 0.0655 &amp; 0.0936 \\
\text { LSTM2 } &amp; 345 &amp; 200 &amp; 15 \mathrm{sec} &amp; 0.0603 &amp; 0.0918 \\
\hline \text { GRU1 } &amp; 141 &amp; 100 &amp; 5 \mathrm{sec} &amp; 0.0671 &amp; 0.0860 \\
\text { GRU2 } &amp; 260 &amp; 200 &amp; 14 \mathrm{sec} &amp; 0.0651 &amp; 0.0958 \\
\hline \text { deep FNN } &amp; 184 &amp; 200 &amp; 5 \mathrm{sec} &amp; 0.0485 &amp; 0.1577 \\
\hline
\end{array}
\]</span></p>
<ul>
<li>表中所示模型的超参数设置</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>LSTM1和GRU1表示只有一个隐藏层的RNN，该隐藏层的神经元个数<span class="math inline">\(\tau_{1}=5\)</span></p></li>
<li><p>LSTM2和GRU2表示有两个隐藏层的RNN，第一个隐藏层神经元个数<span class="math inline">\(\tau_{1}=5\)</span>；第二个隐藏层神经元个数<span class="math inline">\(\tau_{2}=4\)</span></p></li>
<li><p>deep FNN表示有两个隐藏层的前馈神经网络结构，其中<span class="math inline">\((q1,q2)=(5,4)\)</span></p></li>
</ol>
<ul>
<li>结论</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>LSTM2与LSTM1模型预测质量相当，但LSTM2有更多参数及更长的运行时间</p></li>
<li><p>LTSM与GRU超参数选择相同时，普遍的观察结果是GRU比LSTM更快的过拟合，但GRU不稳定</p></li>
<li><p>前馈神经网络与RNN相比没有竞争力</p></li>
</ol>
<ul>
<li>在本文建模中<strong>未引入年龄变量的原因</strong>：</li>
</ul>
<p>协变量标准化到（-1,1）的过程是在所有年龄上同时进行的，因此协变量信息保留了死亡率水平，这和引入年龄变量具有相同的信息质量。</p>
<ol start="4" style="list-style-type: decimal">
<li>超参数选择</li>
</ol>
<ul>
<li>分别改变<span class="math inline">\(T、\tau_{0}、\tau_{1}\)</span>的值，得到结果如下表所示：</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|ccc|cc|}
\hline &amp; \text { # param. } &amp; \text { epochs } &amp; \text { run time } &amp; \text { in-sample } &amp; \text { out-of-sample } \\
\hline \text { base case: } &amp; &amp; &amp; &amp; &amp; \\
\text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=5\right) &amp; 186 &amp; 150 &amp; 8 \mathrm{sec} &amp; 0.0655 &amp; 0.0936 \\
\hline \text { LSTM1 }\left(T=10, \tau_{0}=1, \tau_{1}=5\right) &amp; 146 &amp; 100 &amp; 5 \mathrm{sec} &amp; 0.0647 &amp; 0.1195 \\
\text { LSTM1 }\left(T=10, \tau_{0}=5, \tau_{1}=5\right) &amp; 226 &amp; 150 &amp; 15 \mathrm{sec} &amp; 0.0583 &amp; 0.0798 \\
\hline \text { LSTM1 }\left(T=5, \tau_{0}=3, \tau_{1}=5\right) &amp; 186 &amp; 100 &amp; 4 \mathrm{sec} &amp; 0.0753 &amp; 0.1028 \\
\text { LSTM1 }\left(T=20, \tau_{0}=3, \tau_{1}=5\right) &amp; 186 &amp; 200 &amp; 16 \mathrm{sec} &amp; 0.0626 &amp; 0.0968 \\
\hline \text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=3\right) &amp; 88 &amp; 200 &amp; 10 \mathrm{sec} &amp; 0.0694 &amp; 0.0987 \\
\text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=10\right) &amp; 571 &amp; 100 &amp; 5 \mathrm{sec} &amp; 0.0626 &amp; 0.0883 \\
\hline
\end{array}
\]</span></p>
<ul>
<li>结论</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>分别令<span class="math inline">\(\tau_{0}=1,3,5\)</span>,需要更长的运行时间并提供更好的样本外结果；</p></li>
<li><p>分别令<span class="math inline">\(T=5,10,20\)</span>，结论同上；</p></li>
<li><p>分别令<span class="math inline">\(\tau_{1}=3,5,10\)</span>，导致更快的收敛，因为梯度下降算法有更多的自由度</p></li>
<li><p>最大的影响是通过设定一个更大的<span class="math inline">\(\tau_{0}\)</span>而产生的，因此后面RNN示例中设定<span class="math inline">\(\tau_{0}=5\)</span></p></li>
</ol>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1"></a><span class="co"># load corresponding data</span></span>
<span id="cb85-2"><a href="#cb85-2"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb85-3"><a href="#cb85-3"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb85-4"><a href="#cb85-4"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb85-5"><a href="#cb85-5"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb85-6"><a href="#cb85-6"></a><span class="co"># LSTMs and GRUs</span></span>
<span id="cb85-7"><a href="#cb85-7"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;</span>)</span>
<span id="cb85-8"><a href="#cb85-8"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb85-9"><a href="#cb85-9"></a>tau0 &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb85-10"><a href="#cb85-10"></a>tau1 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb85-11"><a href="#cb85-11"></a>tau2 &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb85-12"><a href="#cb85-12"></a><span class="kw">summary</span>(<span class="kw">LSTM1</span>(T0, tau0, tau1, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-13"><a href="#cb85-13"></a><span class="kw">summary</span>(<span class="kw">LSTM2</span>(T0, tau0, tau1, tau2, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-14"><a href="#cb85-14"></a><span class="kw">summary</span>(<span class="kw">LSTM_TD</span>(T0, tau0, tau1, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-15"><a href="#cb85-15"></a><span class="kw">summary</span>(<span class="kw">GRU1</span>(T0, tau0, tau1, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-16"><a href="#cb85-16"></a><span class="kw">summary</span>(<span class="kw">GRU2</span>(T0, tau0, tau1, tau2, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-17"><a href="#cb85-17"></a><span class="kw">summary</span>(<span class="kw">FNN</span>(T0, tau0, tau1, tau2, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-18"><a href="#cb85-18"></a><span class="co"># Bringing the data in the right structure for a toy example</span></span>
<span id="cb85-19"><a href="#cb85-19"></a>gender &lt;-<span class="st"> &quot;Female&quot;</span></span>
<span id="cb85-20"><a href="#cb85-20"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb85-21"><a href="#cb85-21"></a>mort_rates &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>(all_mort<span class="op">$</span>Gender<span class="op">==</span>gender), <span class="kw">c</span>(<span class="st">&quot;Year&quot;</span>, <span class="st">&quot;Age&quot;</span>, <span class="st">&quot;logmx&quot;</span>)] </span>
<span id="cb85-22"><a href="#cb85-22"></a>mort_rates &lt;-<span class="st"> </span><span class="kw">dcast</span>(mort_rates, Year <span class="op">~</span><span class="st"> </span>Age, <span class="dt">value.var=</span><span class="st">&quot;logmx&quot;</span>)</span>
<span id="cb85-23"><a href="#cb85-23"></a><span class="kw">dim</span>(mort_rates)</span>
<span id="cb85-24"><a href="#cb85-24"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span>     <span class="co"># lookback period</span></span>
<span id="cb85-25"><a href="#cb85-25"></a>tau0 &lt;-<span class="st"> </span><span class="dv">3</span>    <span class="co"># dimension of x_t (should be odd for our application)</span></span>
<span id="cb85-26"><a href="#cb85-26"></a>delta0 &lt;-<span class="st"> </span>(tau0<span class="dv">-1</span>)<span class="op">/</span><span class="dv">2</span></span>
<span id="cb85-27"><a href="#cb85-27"></a>toy_rates &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(mort_rates[<span class="kw">which</span>(mort_rates<span class="op">$</span>Year <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>((ObsYear<span class="op">-</span>T0)<span class="op">:</span>(ObsYear<span class="op">+</span><span class="dv">1</span>))),])</span>
<span id="cb85-28"><a href="#cb85-28"></a><span class="kw">dim</span>(toy_rates)</span>
<span id="cb85-29"><a href="#cb85-29"></a>xt &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(<span class="dv">2</span>,<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0, T0, tau0))</span>
<span id="cb85-30"><a href="#cb85-30"></a>YT &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(<span class="dv">2</span>,<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0))</span>
<span id="cb85-31"><a href="#cb85-31"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>){<span class="cf">for</span> (a0 <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0)){ </span>
<span id="cb85-32"><a href="#cb85-32"></a>    xt[i,a0,,] &lt;-<span class="st"> </span>toy_rates[<span class="kw">c</span>(i<span class="op">:</span>(T0<span class="op">+</span>i<span class="dv">-1</span>)),<span class="kw">c</span>((a0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(a0<span class="op">+</span>tau0))]</span>
<span id="cb85-33"><a href="#cb85-33"></a>    YT[i,a0] &lt;-<span class="st"> </span>toy_rates[T0<span class="op">+</span>i,a0<span class="op">+</span><span class="dv">1</span><span class="op">+</span>delta0]</span>
<span id="cb85-34"><a href="#cb85-34"></a>}}</span>
<span id="cb85-35"><a href="#cb85-35"></a><span class="kw">dim</span>(xt)</span>
<span id="cb85-36"><a href="#cb85-36"></a><span class="kw">dim</span>(YT)</span>
<span id="cb85-37"><a href="#cb85-37"></a><span class="kw">plot</span>(<span class="dt">x=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;white&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;calendar years&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;raw log-mortality rates&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;data toy example&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">xlim=</span><span class="kw">range</span>(toy_rates[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(toy_rates[,<span class="op">-</span><span class="dv">1</span>]), <span class="dt">type=</span><span class="st">&#39;l&#39;</span>)</span>
<span id="cb85-38"><a href="#cb85-38"></a><span class="cf">for</span> (a0 <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(toy_rates)){</span>
<span id="cb85-39"><a href="#cb85-39"></a>  <span class="cf">if</span> (a0 <span class="op">%in%</span><span class="st"> </span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>)<span class="op">*</span><span class="dv">3</span>)){</span>
<span id="cb85-40"><a href="#cb85-40"></a>    <span class="kw">lines</span>(<span class="dt">x=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,a0])    </span>
<span id="cb85-41"><a href="#cb85-41"></a>    <span class="kw">points</span>(<span class="dt">x=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),a0], <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb85-42"><a href="#cb85-42"></a>    <span class="kw">lines</span>(<span class="dt">x=</span>toy_rates[(T0)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">1</span>),<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[(T0)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">1</span>),a0], <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb85-43"><a href="#cb85-43"></a>    <span class="kw">lines</span>(<span class="dt">x=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),a0], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb85-44"><a href="#cb85-44"></a>    }}</span>
<span id="cb85-45"><a href="#cb85-45"></a><span class="co"># LSTMs and GRUs</span></span>
<span id="cb85-46"><a href="#cb85-46"></a>x.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(xt[<span class="dv">1</span>,,,]<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">/</span>(<span class="kw">max</span>(xt)<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">-</span><span class="dv">1</span>, <span class="kw">c</span>(<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0, T0, tau0))</span>
<span id="cb85-47"><a href="#cb85-47"></a>x.vali  &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(xt[<span class="dv">2</span>,,,]<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">/</span>(<span class="kw">max</span>(xt)<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">-</span><span class="dv">1</span>, <span class="kw">c</span>(<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0, T0, tau0))</span>
<span id="cb85-48"><a href="#cb85-48"></a>y.train &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>YT[<span class="dv">1</span>,]</span>
<span id="cb85-49"><a href="#cb85-49"></a>(y0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y.train))</span>
<span id="cb85-50"><a href="#cb85-50"></a>y.vali  &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>YT[<span class="dv">2</span>,]</span>
<span id="cb85-51"><a href="#cb85-51"></a><span class="kw">dim</span>(x.train)</span>
<span id="cb85-52"><a href="#cb85-52"></a><span class="kw">length</span>(y.train);<span class="kw">length</span>(y.vali)</span>
<span id="cb85-53"><a href="#cb85-53"></a><span class="co"># x.age.train&lt;-as.matrix(0:0)</span></span>
<span id="cb85-54"><a href="#cb85-54"></a><span class="co"># x.training&lt;-list(x.train,x.age.train)</span></span>
<span id="cb85-55"><a href="#cb85-55"></a><span class="co"># x.age.valid&lt;-as.matrix(0:0)</span></span>
<span id="cb85-56"><a href="#cb85-56"></a><span class="co"># x.validation&lt;-list(x.vali,x.age.valid)</span></span>
<span id="cb85-57"><a href="#cb85-57"></a><span class="co">### examples</span></span>
<span id="cb85-58"><a href="#cb85-58"></a>tau1 &lt;-<span class="st"> </span><span class="dv">5</span>    <span class="co"># dimension of the outputs z_t^(1) first RNN layer</span></span>
<span id="cb85-59"><a href="#cb85-59"></a>tau2 &lt;-<span class="st"> </span><span class="dv">4</span>    <span class="co"># dimension of the outputs z_t^(2) second RNN layer</span></span>
<span id="cb85-60"><a href="#cb85-60"></a>CBs &lt;-<span class="st"> </span><span class="kw">callback_model_checkpoint</span>(<span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model&quot;</span>, <span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">verbose =</span> <span class="dv">0</span>,  <span class="dt">save_best_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_weights_only =</span> <span class="ot">TRUE</span>,<span class="dt">save_freq =</span> <span class="ot">NULL</span>)</span>
<span id="cb85-61"><a href="#cb85-61"></a>model &lt;-<span class="st"> </span><span class="kw">LSTM2</span>(T0, tau0, tau1, tau2, y0, <span class="st">&quot;nadam&quot;</span>)     </span>
<span id="cb85-62"><a href="#cb85-62"></a><span class="kw">summary</span>(model)</span>
<span id="cb85-63"><a href="#cb85-63"></a><span class="co"># takes 40 seconds on my laptop</span></span>
<span id="cb85-64"><a href="#cb85-64"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb85-65"><a href="#cb85-65"></a>  fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="dt">x=</span>x.train, <span class="dt">y=</span>y.train, <span class="dt">validation_data=</span><span class="kw">list</span>(x.vali, y.vali), <span class="dt">batch_size=</span><span class="dv">10</span>, <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">verbose=</span><span class="dv">1</span>, <span class="dt">callbacks=</span>CBs)</span>
<span id="cb85-66"><a href="#cb85-66"></a> <span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb85-67"><a href="#cb85-67"></a><span class="kw">plot</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>val_loss,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>), <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;early stopping rule&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>),<span class="dt">xlab=</span><span class="st">&quot;epochs&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MSE loss&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb85-68"><a href="#cb85-68"></a><span class="kw">lines</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb85-69"><a href="#cb85-69"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.1</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</span>
<span id="cb85-70"><a href="#cb85-70"></a><span class="kw">legend</span>(<span class="dt">x=</span><span class="st">&quot;bottomleft&quot;</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;in-sample loss&quot;</span>, <span class="st">&quot;out-of-sample loss&quot;</span>))</span>
<span id="cb85-71"><a href="#cb85-71"></a><span class="kw">load_model_weights_hdf5</span>(model, <span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model&quot;</span>)</span>
<span id="cb85-72"><a href="#cb85-72"></a>Yhat.train1 &lt;-<span class="st"> </span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.train))</span>
<span id="cb85-73"><a href="#cb85-73"></a>Yhat.vali1 &lt;-<span class="st"> </span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.vali))</span>
<span id="cb85-74"><a href="#cb85-74"></a><span class="kw">c</span>(<span class="kw">round</span>(<span class="kw">mean</span>((Yhat.train1<span class="op">-</span>y.train)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>), <span class="kw">round</span>(<span class="kw">mean</span>((Yhat.vali1<span class="op">-</span>y.vali)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>))</span></code></pre></div>
</div>
<div id="rnn-1" class="section level3">
<h3><span class="header-section-number">6.5.5</span> RNN</h3>
<ol style="list-style-type: decimal">
<li>数据预处理</li>
</ol>
<ul>
<li><p>观察值：1950-1999年数据；预测值：2000-2016年对数死亡率</p></li>
<li><p>分别对男性和女性建立模型，先选定一个性别</p></li>
<li><p>参数设置：回顾周期<span class="math inline">\(T=10\)</span>；<span class="math inline">\(\tau_{0}=5\)</span></p></li>
<li><p>定义解释变量（需扩充年龄界限——复制边缘特征值）：</p>
<p>对于<span class="math inline">\(0 \leq x \leq 99\)</span>，<span class="math inline">\(1950 \leq t \leq 1999\)</span>，有</p>
<p><strong>解释变量</strong><span class="math inline">\(\boldsymbol{x}_{t, x}=\left(\log \left(M_{t,(x-2) \vee 0}\right), \log \left(M_{t,(x-1) \vee 0}\right), \log \left(M_{t, x}\right), \log \left(M_{t,(x+1) \wedge 99}\right), \log \left(M_{t,(x+2) \wedge 99}\right)\right)^{\top} \in \mathbb{R}^{5}\)</span></p>
<p>该式中：<span class="math inline">\(x_{0}\vee x_{1}=\text {max}\{x_{0},x_{1}\}\)</span>;<span class="math inline">\(x_{0}\wedge x_{1}=\text {min}\{x_{0},x_{1}\}\)</span></p></li>
<li><p>定义训练数据<span class="math inline">\(\mathcal{T}\)</span>和验证数据<span class="math inline">\(\mathcal{V}\)</span>：</p>
<p><strong>训练数据</strong><span class="math inline">\(\mathcal{T}=\{(\boldsymbol{x}_{t-T,x}, \ldots,\boldsymbol{x}_{t-1,x},\boldsymbol{Y}_{t, x});0 \leq x \leq 99\ , 1950+T \leq t \leq 1999\}\)</span></p>
<p>其中，<span class="math inline">\(\boldsymbol{Y}_{t, x}=\text {log}(M_{t,x})\)</span></p>
<p><strong>验证数据</strong>:</p>
<p><span class="math inline">\(s&gt;1999\)</span>的特征值要用相应的预测值替代</p></li>
</ul>
<p><span class="math display">\[
\widehat{\boldsymbol{x}}_{s, x}=\left(\log \left(\widehat{M}_{s,(x-2) \vee 0}\right), \log \left(\widehat{M}_{s,(x-1) \vee 0}\right), \log \left(\widehat{M}_{s, x}\right), \log \left(\widehat{M}_{s,(x+1) \wedge 99}\right), \log \left(\widehat{M}_{s,(x+2) \wedge 99}\right)\right)^{\top} \in \mathbb{R}^{5}
\]</span></p>
<p>因此验证数据<span class="math inline">\(\mathcal{V}=\{(\boldsymbol{x}_{t-T,x}, \ldots,\boldsymbol{x}_{1999,x},\widehat{\boldsymbol{x}}_{2000,x}, \ldots,\widehat{\boldsymbol{x}}_{t-1,x},\boldsymbol{Y}_{t, x});0 \leq x \leq 99\ , 2000 \leq t \leq 2016\}\)</span></p>
<ul>
<li><p>基于<strong>训练数据</strong>所有特征值的最大最小值对训练数据和验证数据应用<code>MinMaxScaler</code></p></li>
<li><p>切换响应变量符号</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>建立单个性别的RNN</li>
</ol>
<ul>
<li><p>将训练数据<span class="math inline">\(\mathcal T\)</span>随机划分学习集<span class="math inline">\(\mathcal T_{0}\)</span>(包含80%数据)以及测试集<span class="math inline">\(\mathcal T_{1}\)</span>(包含20%数据)<span class="math inline">\(\mathcal T_{0}\)</span>用于追踪样本内过拟合；</p></li>
<li><p>建立具有三个隐藏层的LSTM3和GRU3；</p></li>
<li><p>超参数设置：<span class="math inline">\(T=10\)</span>；<span class="math inline">\(\tau_{0}=5\)</span>；<span class="math inline">\(\tau_{1}=20\)</span>；<span class="math inline">\(\tau_{2}=15\)</span>；<span class="math inline">\(\tau_{3}=10\)</span>；</p></li>
<li><p>下图显示了分别对男性和女性建立两个模型的收敛行为</p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="./plots/6/loss2.png" alt="模型的样本内外损失" width="60%"  />
<p class="caption">
(#fig:loss2)模型的样本内外损失
</p>
</div>
<p>该图显示：GRU结构会导致更快的收敛，但后续会证实GRU结构不稳定，因此LSTM会更受欢迎</p>
<ul>
<li>下表给出三个模型(LSTM3/GRU3/LC)分性别的样本内外损失</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|cc|cc|cc|}
\hline &amp;  {\text { in-sample }} &amp; {\text { in-sample }}  &amp; {\text { out-of-sample }} &amp; {\text { out-of-sample }}&amp;  {\text { run times }}&amp;  {\text { run times }} \\
&amp; \text { female } &amp; \text { male } &amp; \text { female } &amp; \text { male } &amp; \text { female } &amp; \text { male } \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 2.5222 &amp; 6.9458 &amp; 0.3566 &amp; 1.3507 &amp; 225 \mathrm{s} &amp; 203 \mathrm{s} \\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 2.8370 &amp; 7.0907 &amp; 0.4788 &amp; 1.2435 &amp; 185 \mathrm{s} &amp; 198 \mathrm{s} \\
\hline \hline \text { LC model with SVD } &amp; 3.7573 &amp; 8.8110 &amp; 0.6045 &amp; 1.8152 &amp; - &amp; - \\
\hline
\end{array}
\]</span></p>
<p>能够看到，所有被选择RNN模型都优于LC模型的预测</p>
<ol start="3" style="list-style-type: decimal">
<li>探索RNN的预测中隐含的漂移项</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>中心化RNN预测的对数死亡率</li>
</ol>
<p><span class="math display">\[
\log \left(\widehat{M}_{t, x}^{\circ}\right)=\log(\widehat M_{t,x})-\widehat a_{x}
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>利用下式求得<span class="math inline">\(2000 \leq t \leq 2016\)</span>对应的<span class="math inline">\(k_{t}\)</span></li>
</ol>
<p><span class="math display">\[
\underset{k_{t}}{\arg \min } \sum_{x}\left(\log \left(\widehat{M}_{t, x}^{\circ}\right)-\widehat{b}_{x} k_{t}\right)^{2}
\]</span></p>
<p>式中，<span class="math inline">\((\widehat{b}_{x})_{x}\)</span>是从LC模型估计得到的。</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>估计结果如下图所示</li>
</ol>
<div class="figure" style="text-align: center">
<img src="./plots/6/kt2.png" alt="三种模型下kt的估计与预测值" width="60%"  />
<p class="caption">
(#fig:kt2)三种模型下kt的估计与预测值
</p>
</div>
<p>该图显示：对于女性，LSTM3的预测与LC的预测基本一致；而对于男性，LSTM3的预测的斜率略大于LC的预测并收敛与LC的预测；但是GRU3的结果并不令人信服，可能是因为从LC模型中得到的不随时间变化的参数bx的估计与GRU3模型产生的预测不符。</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1"></a><span class="co"># load corresponding data</span></span>
<span id="cb86-2"><a href="#cb86-2"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb86-3"><a href="#cb86-3"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb86-4"><a href="#cb86-4"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb86-5"><a href="#cb86-5"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb86-6"><a href="#cb86-6"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;</span>)</span>
<span id="cb86-7"><a href="#cb86-7"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R&quot;</span>)</span>
<span id="cb86-8"><a href="#cb86-8"></a><span class="co"># choice of parameters</span></span>
<span id="cb86-9"><a href="#cb86-9"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb86-10"><a href="#cb86-10"></a>tau0 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb86-11"><a href="#cb86-11"></a>gender &lt;-<span class="st"> &quot;Female&quot;</span></span>
<span id="cb86-12"><a href="#cb86-12"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">1999</span></span>
<span id="cb86-13"><a href="#cb86-13"></a><span class="co"># training data pre-processing </span></span>
<span id="cb86-14"><a href="#cb86-14"></a>data1 &lt;-<span class="st"> </span><span class="kw">data.preprocessing.RNNs</span>(all_mort, gender, T0, tau0, ObsYear)</span>
<span id="cb86-15"><a href="#cb86-15"></a><span class="kw">dim</span>(data1[[<span class="dv">1</span>]])</span>
<span id="cb86-16"><a href="#cb86-16"></a><span class="kw">dim</span>(data1[[<span class="dv">2</span>]])</span>
<span id="cb86-17"><a href="#cb86-17"></a><span class="co"># validation data pre-processing</span></span>
<span id="cb86-18"><a href="#cb86-18"></a>all_mort2 &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>((all_mort<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>(ObsYear<span class="dv">-10</span>))<span class="op">&amp;</span>(Gender<span class="op">==</span>gender)),]</span>
<span id="cb86-19"><a href="#cb86-19"></a>all_mortV &lt;-<span class="st"> </span>all_mort2</span>
<span id="cb86-20"><a href="#cb86-20"></a>vali.Y &lt;-<span class="st"> </span>all_mortV[<span class="kw">which</span>(all_mortV<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb86-21"><a href="#cb86-21"></a> </span>
<span id="cb86-22"><a href="#cb86-22"></a><span class="co"># MinMaxScaler data pre-processing</span></span>
<span id="cb86-23"><a href="#cb86-23"></a>x.min &lt;-<span class="st"> </span><span class="kw">min</span>(data1[[<span class="dv">1</span>]])</span>
<span id="cb86-24"><a href="#cb86-24"></a>x.max &lt;-<span class="st"> </span><span class="kw">max</span>(data1[[<span class="dv">1</span>]])</span>
<span id="cb86-25"><a href="#cb86-25"></a>x.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(data1[[<span class="dv">1</span>]]<span class="op">-</span>x.min)<span class="op">/</span>(x.min<span class="op">-</span>x.max)<span class="op">-</span><span class="dv">1</span>, <span class="kw">dim</span>(data1[[<span class="dv">1</span>]]))</span>
<span id="cb86-26"><a href="#cb86-26"></a>y.train &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>data1[[<span class="dv">2</span>]]</span>
<span id="cb86-27"><a href="#cb86-27"></a>y0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y.train)</span>
<span id="cb86-28"><a href="#cb86-28"></a><span class="co"># LSTM architectures</span></span>
<span id="cb86-29"><a href="#cb86-29"></a><span class="co"># network architecture deep 3 network</span></span>
<span id="cb86-30"><a href="#cb86-30"></a>tau1 &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb86-31"><a href="#cb86-31"></a>tau2 &lt;-<span class="st"> </span><span class="dv">15</span></span>
<span id="cb86-32"><a href="#cb86-32"></a>tau3 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb86-33"><a href="#cb86-33"></a>optimizer &lt;-<span class="st"> &#39;adam&#39;</span></span>
<span id="cb86-34"><a href="#cb86-34"></a><span class="co"># choose either LSTM or GRU network</span></span>
<span id="cb86-35"><a href="#cb86-35"></a>RNN.type &lt;-<span class="st"> &quot;LSTM&quot;</span></span>
<span id="cb86-36"><a href="#cb86-36"></a><span class="co">#RNN.type &lt;- &quot;GRU&quot;</span></span>
<span id="cb86-37"><a href="#cb86-37"></a>{<span class="cf">if</span> (RNN.type<span class="op">==</span><span class="st">&quot;LSTM&quot;</span>){model &lt;-<span class="st"> </span><span class="kw">LSTM3</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}<span class="cf">else</span>{model &lt;-<span class="st"> </span><span class="kw">GRU3</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}</span>
<span id="cb86-38"><a href="#cb86-38"></a> name.model &lt;-<span class="st"> </span><span class="kw">paste</span>(RNN.type,<span class="st">&quot;3_&quot;</span>, tau0, <span class="st">&quot;_&quot;</span>, tau1, <span class="st">&quot;_&quot;</span>, tau2, <span class="st">&quot;_&quot;</span>, tau3, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb86-39"><a href="#cb86-39"></a> file.name &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_&quot;</span>, name.model,<span class="st">&quot;_&quot;</span>, gender, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb86-40"><a href="#cb86-40"></a> <span class="kw">summary</span>(model)}</span>
<span id="cb86-41"><a href="#cb86-41"></a><span class="co"># define callback</span></span>
<span id="cb86-42"><a href="#cb86-42"></a>CBs &lt;-<span class="st"> </span><span class="kw">callback_model_checkpoint</span>(file.name, <span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">verbose =</span> <span class="dv">0</span>,  <span class="dt">save_best_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_weights_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_freq =</span> <span class="ot">NULL</span>)</span>
<span id="cb86-43"><a href="#cb86-43"></a><span class="co"># gradient descent fitting: takes roughly 200 seconds on my laptop</span></span>
<span id="cb86-44"><a href="#cb86-44"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb86-45"><a href="#cb86-45"></a>  fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="dt">x=</span>x.train, <span class="dt">y=</span>y.train, <span class="dt">validation_split=</span><span class="fl">0.2</span>,</span>
<span id="cb86-46"><a href="#cb86-46"></a>                                        <span class="dt">batch_size=</span><span class="dv">100</span>, <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">verbose=</span><span class="dv">1</span>, <span class="dt">callbacks=</span>CBs)                                        </span>
<span id="cb86-47"><a href="#cb86-47"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb86-48"><a href="#cb86-48"></a><span class="co"># plot loss figures</span></span>
<span id="cb86-49"><a href="#cb86-49"></a><span class="kw">plot.losses</span>(name.model, gender, fit[[<span class="dv">2</span>]]<span class="op">$</span>val_loss, fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)</span>
<span id="cb86-50"><a href="#cb86-50"></a><span class="co"># calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)</span></span>
<span id="cb86-51"><a href="#cb86-51"></a><span class="kw">load_model_weights_hdf5</span>(model, file.name)</span>
<span id="cb86-52"><a href="#cb86-52"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((<span class="kw">exp</span>(<span class="op">-</span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.train)))<span class="op">-</span><span class="kw">exp</span>(<span class="op">-</span>y.train))<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span>
<span id="cb86-53"><a href="#cb86-53"></a><span class="co"># calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)</span></span>
<span id="cb86-54"><a href="#cb86-54"></a>pred.result &lt;-<span class="st"> </span><span class="kw">recursive.prediction</span>(ObsYear, all_mort2, gender, T0, tau0, x.min, x.max, model)</span>
<span id="cb86-55"><a href="#cb86-55"></a>vali &lt;-<span class="st"> </span>pred.result[[<span class="dv">1</span>]][<span class="kw">which</span>(all_mort2<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb86-56"><a href="#cb86-56"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali.Y<span class="op">$</span>mx)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span></code></pre></div>
</div>
<div id="引入性别协变量" class="section level3">
<h3><span class="header-section-number">6.5.6</span> 引入性别协变量</h3>
<ol style="list-style-type: decimal">
<li>数据预处理</li>
</ol>
<ul>
<li><p>添加性别指示变量：0表示女性；1表示男性</p></li>
<li><p>在训练数据时交替使用性别</p></li>
<li><p>应用MinMaxScaler时的最大最小值是同时考虑两种性别所有训练数据的情况下得到的。</p></li>
<li><p>模型结构及超参数设置与单个性别RNN相同</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>建立模型</li>
</ol>
<ul>
<li>基于使得测试损失最小的LSTM3和GRU模型，预测1999年之后的死亡率，并在特征变量中加入性别指标，下表列出了相应的损失</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|c|cc|c|}
\hline &amp;  {\text { in-sample }} &amp; {\text { out-of-sample }} &amp; {\text { out-of-sample }}&amp;  {\text { run times }}\\
&amp; \text {both genders} &amp; \text { female } &amp; \text { male } &amp; \text {both genders} \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.7643 &amp; 0.3402 &amp; 1.1346 &amp; 404 \mathrm{s}\\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.6311 &amp; 0.4646 &amp; 1.2571 &amp;  379 \mathrm{s} \\
\hline \hline \text { LC model with SVD } &amp; 6.2841 &amp; 0.6045 &amp;1.8152 &amp;  - \\
\hline
\end{array}
\]</span></p>
<p>与LC模型相比，得到了一个有很大改进的模型，至少对未来16年的预测是这样的；此外，引入性别协变量的LSTM模型也优于单个性别的模型。</p>
<ol start="3" style="list-style-type: decimal">
<li>隐含的漂移项</li>
</ol>
<div class="figure" style="text-align: center">
<img src="./plots/6/kt3.png" alt="引入性别协变量建模的kt的估计与预测值" width="60%"  />
<p class="caption">
(#fig:kt3)引入性别协变量建模的kt的估计与预测值
</p>
</div>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1"></a><span class="co"># load corresponding data</span></span>
<span id="cb87-2"><a href="#cb87-2"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb87-3"><a href="#cb87-3"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb87-4"><a href="#cb87-4"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb87-5"><a href="#cb87-5"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb87-6"><a href="#cb87-6"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;</span>)</span>
<span id="cb87-7"><a href="#cb87-7"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R&quot;</span>)</span>
<span id="cb87-8"><a href="#cb87-8"></a><span class="co"># choice of parameters</span></span>
<span id="cb87-9"><a href="#cb87-9"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb87-10"><a href="#cb87-10"></a>tau0 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb87-11"><a href="#cb87-11"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">1999</span></span>
<span id="cb87-12"><a href="#cb87-12"></a><span class="co"># training data pre-processing </span></span>
<span id="cb87-13"><a href="#cb87-13"></a>data1 &lt;-<span class="st"> </span><span class="kw">data.preprocessing.RNNs</span>(all_mort, <span class="st">&quot;Female&quot;</span>, T0, tau0, ObsYear)</span>
<span id="cb87-14"><a href="#cb87-14"></a>data2 &lt;-<span class="st"> </span><span class="kw">data.preprocessing.RNNs</span>(all_mort, <span class="st">&quot;Male&quot;</span>, T0, tau0, ObsYear)</span>
<span id="cb87-15"><a href="#cb87-15"></a>xx &lt;-<span class="st"> </span><span class="kw">dim</span>(data1[[<span class="dv">1</span>]])[<span class="dv">1</span>]</span>
<span id="cb87-16"><a href="#cb87-16"></a>x.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">*</span>xx, <span class="kw">dim</span>(data1[[<span class="dv">1</span>]])[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]))</span>
<span id="cb87-17"><a href="#cb87-17"></a>y.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">*</span>xx))</span>
<span id="cb87-18"><a href="#cb87-18"></a>gender.indicator &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), xx)</span>
<span id="cb87-19"><a href="#cb87-19"></a><span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>xx){</span>
<span id="cb87-20"><a href="#cb87-20"></a>   x.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span>,,] &lt;-<span class="st"> </span>data1[[<span class="dv">1</span>]][l,,]</span>
<span id="cb87-21"><a href="#cb87-21"></a>   x.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">2</span>,,] &lt;-<span class="st"> </span>data2[[<span class="dv">1</span>]][l,,]</span>
<span id="cb87-22"><a href="#cb87-22"></a>   y.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="op">-</span>data1[[<span class="dv">2</span>]][l]</span>
<span id="cb87-23"><a href="#cb87-23"></a>   y.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">2</span>] &lt;-<span class="st"> </span><span class="op">-</span>data2[[<span class="dv">2</span>]][l]</span>
<span id="cb87-24"><a href="#cb87-24"></a>          }</span>
<span id="cb87-25"><a href="#cb87-25"></a><span class="co"># MinMaxScaler data pre-processing</span></span>
<span id="cb87-26"><a href="#cb87-26"></a>x.min &lt;-<span class="st"> </span><span class="kw">min</span>(x.train)</span>
<span id="cb87-27"><a href="#cb87-27"></a>x.max &lt;-<span class="st"> </span><span class="kw">max</span>(x.train)</span>
<span id="cb87-28"><a href="#cb87-28"></a>x.train &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(x.train<span class="op">-</span>x.min)<span class="op">/</span>(x.min<span class="op">-</span>x.max)<span class="op">-</span><span class="dv">1</span>, <span class="kw">dim</span>(x.train)), gender.indicator)</span>
<span id="cb87-29"><a href="#cb87-29"></a>y0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y.train)</span>
<span id="cb87-30"><a href="#cb87-30"></a><span class="co"># validation data pre-processing</span></span>
<span id="cb87-31"><a href="#cb87-31"></a>all_mort2.Female &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>((all_mort<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>(ObsYear<span class="dv">-10</span>))<span class="op">&amp;</span>(Gender<span class="op">==</span><span class="st">&quot;Female&quot;</span>)),]</span>
<span id="cb87-32"><a href="#cb87-32"></a>all_mortV.Female &lt;-<span class="st"> </span>all_mort2.Female</span>
<span id="cb87-33"><a href="#cb87-33"></a>vali.Y.Female &lt;-<span class="st"> </span>all_mortV.Female[<span class="kw">which</span>(all_mortV.Female<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-34"><a href="#cb87-34"></a>all_mort2.Male &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>((all_mort<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>(ObsYear<span class="dv">-10</span>))<span class="op">&amp;</span>(Gender<span class="op">==</span><span class="st">&quot;Male&quot;</span>)),]</span>
<span id="cb87-35"><a href="#cb87-35"></a>all_mortV.Male &lt;-<span class="st"> </span>all_mort2.Male</span>
<span id="cb87-36"><a href="#cb87-36"></a>vali.Y.Male &lt;-<span class="st"> </span>all_mortV.Male[<span class="kw">which</span>(all_mortV.Male<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-37"><a href="#cb87-37"></a><span class="co"># LSTM architectures</span></span>
<span id="cb87-38"><a href="#cb87-38"></a><span class="co"># network architecture deep 3 network</span></span>
<span id="cb87-39"><a href="#cb87-39"></a>tau1 &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb87-40"><a href="#cb87-40"></a>tau2 &lt;-<span class="st"> </span><span class="dv">15</span></span>
<span id="cb87-41"><a href="#cb87-41"></a>tau3 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb87-42"><a href="#cb87-42"></a>optimizer &lt;-<span class="st"> &#39;adam&#39;</span></span>
<span id="cb87-43"><a href="#cb87-43"></a><span class="co"># choose either LSTM or GRU network</span></span>
<span id="cb87-44"><a href="#cb87-44"></a>RNN.type &lt;-<span class="st"> &quot;LSTM&quot;</span></span>
<span id="cb87-45"><a href="#cb87-45"></a><span class="co">#RNN.type &lt;- &quot;GRU&quot;</span></span>
<span id="cb87-46"><a href="#cb87-46"></a>{<span class="cf">if</span> (RNN.type<span class="op">==</span><span class="st">&quot;LSTM&quot;</span>){model &lt;-<span class="st"> </span><span class="kw">LSTM3.Gender</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}<span class="cf">else</span>{model &lt;-<span class="st"> </span><span class="kw">GRU3.Gender</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}</span>
<span id="cb87-47"><a href="#cb87-47"></a> name.model &lt;-<span class="st"> </span><span class="kw">paste</span>(RNN.type,<span class="st">&quot;3_&quot;</span>, tau0, <span class="st">&quot;_&quot;</span>, tau1, <span class="st">&quot;_&quot;</span>, tau2, <span class="st">&quot;_&quot;</span>, tau3, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb87-48"><a href="#cb87-48"></a> <span class="co">#file.name &lt;- paste(&quot;./Model_Full_Param/best_model_&quot;, name.model, sep=&quot;&quot;)</span></span>
<span id="cb87-49"><a href="#cb87-49"></a> file.name &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_&quot;</span>, name.model, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb87-50"><a href="#cb87-50"></a> <span class="kw">summary</span>(model)}</span>
<span id="cb87-51"><a href="#cb87-51"></a><span class="co"># define callback</span></span>
<span id="cb87-52"><a href="#cb87-52"></a>CBs &lt;-<span class="st"> </span><span class="kw">callback_model_checkpoint</span>(file.name, <span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">verbose =</span> <span class="dv">0</span>,  <span class="dt">save_best_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_weights_only =</span> <span class="ot">TRUE</span>,<span class="dt">save_freq =</span> <span class="ot">NULL</span>)</span>
<span id="cb87-53"><a href="#cb87-53"></a><span class="co"># gradient descent fitting: takes roughly 400 seconds on my laptop</span></span>
<span id="cb87-54"><a href="#cb87-54"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb87-55"><a href="#cb87-55"></a>  fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="dt">x=</span>x.train, <span class="dt">y=</span>y.train, <span class="dt">validation_split=</span><span class="fl">0.2</span>,</span>
<span id="cb87-56"><a href="#cb87-56"></a>                                        <span class="dt">batch_size=</span><span class="dv">100</span>, <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">verbose=</span><span class="dv">1</span>, <span class="dt">callbacks=</span>CBs)                                        </span>
<span id="cb87-57"><a href="#cb87-57"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb87-58"><a href="#cb87-58"></a><span class="co"># plot loss figures</span></span>
<span id="cb87-59"><a href="#cb87-59"></a><span class="kw">plot.losses</span>(name.model, <span class="st">&quot;Both&quot;</span>, fit[[<span class="dv">2</span>]]<span class="op">$</span>val_loss, fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)</span>
<span id="cb87-60"><a href="#cb87-60"></a><span class="co"># calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)</span></span>
<span id="cb87-61"><a href="#cb87-61"></a><span class="kw">load_model_weights_hdf5</span>(model, file.name)</span>
<span id="cb87-62"><a href="#cb87-62"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((<span class="kw">exp</span>(<span class="op">-</span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.train)))<span class="op">-</span><span class="kw">exp</span>(<span class="op">-</span>y.train))<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span>
<span id="cb87-63"><a href="#cb87-63"></a><span class="co"># calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)</span></span>
<span id="cb87-64"><a href="#cb87-64"></a><span class="co"># Female</span></span>
<span id="cb87-65"><a href="#cb87-65"></a>pred.result &lt;-<span class="st"> </span><span class="kw">recursive.prediction.Gender</span>(ObsYear, all_mort2.Female, <span class="st">&quot;Female&quot;</span>, T0, tau0, x.min, x.max, model)</span>
<span id="cb87-66"><a href="#cb87-66"></a>vali &lt;-<span class="st"> </span>pred.result[[<span class="dv">1</span>]][<span class="kw">which</span>(all_mort2.Female<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-67"><a href="#cb87-67"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali.Y.Female<span class="op">$</span>mx)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span>
<span id="cb87-68"><a href="#cb87-68"></a><span class="co"># Male</span></span>
<span id="cb87-69"><a href="#cb87-69"></a>pred.result &lt;-<span class="st"> </span><span class="kw">recursive.prediction.Gender</span>(ObsYear, all_mort2.Male, <span class="st">&quot;Male&quot;</span>, T0, tau0, x.min, x.max, model)</span>
<span id="cb87-70"><a href="#cb87-70"></a>vali &lt;-<span class="st"> </span>pred.result[[<span class="dv">1</span>]][<span class="kw">which</span>(all_mort2.Male<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-71"><a href="#cb87-71"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali.Y.Male<span class="op">$</span>mx)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span></code></pre></div>
</div>
<div id="稳健性" class="section level3">
<h3><span class="header-section-number">6.5.7</span> 稳健性</h3>
<p>使用梯度下降法的早期停止解决方案的一个问题是由此产生的校准依赖于算法种子点（起始值）的选择</p>
<p>下图展示使用相同RNN结构、相同超参数和相同校准策略，针对100个不同种子点的选择所画的损失的箱线图</p>
<div class="figure" style="text-align: center">
<img src="./plots/6/Box.png" alt="100个不同种子下的损失箱线图" width="60%"  />
<p class="caption">
(#fig:Box)100个不同种子下的损失箱线图
</p>
</div>
<ul>
<li><p>红色表示联合性别的LSTM结构中的预测结果；蓝色表示联合性别的GRU结构中的预测；橙色水平线表示的是LC的预测</p></li>
<li><p>结论：</p></li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>左侧给出了样本内损失，与LC模型相比，两种RNN结构的样本内损失都有显著减少，平均而言，LSTM的损失比GRU的小，波动性也更小；</p></li>
<li><p>中间和右边分别表示女性和男性的样本外损失：不论男性还是女性，LSTM在几乎所有的100次迭代中都比LC模型好；GRU结构尽在大约一半次数的迭代中比LC模型表现好。</p></li>
</ol>
<ul>
<li><strong>改进方法</strong>：将不同种子点下得到的预测值进行平均，结果如下:</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|c|cc|c|}
\hline &amp;  {\text { in-sample }} &amp; {\text { out-of-sample }} &amp; {\text { out-of-sample }}&amp;  {\text { run times }}\\
&amp; \text {both genders} &amp; \text { female } &amp; \text { male } &amp; \text {both genders} \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.7643 &amp; 0.3402 &amp; 1.1346 &amp; 404 \mathrm{s}\\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.6311 &amp; 0.4646 &amp; 1.2571 &amp;  379 \mathrm{s} \\
\hline\hline \text { LSTM3 averaged over 100 different seeds} &amp; - &amp; 0.2451 &amp; 1.2093 &amp; 100 \cdot 404\mathrm{s}\\
\text { GRU3 averaged over 100 different seeds } &amp; - &amp; 0.2341&amp; 1.2746 &amp;  100 \cdot 379 \mathrm{s} \\
\hline \hline \text { LC model with SVD } &amp; 6.2841 &amp; 0.6045 &amp;1.8152 &amp;  - \\
\hline
\end{array}
\]</span></p>
<p>能够看到，平均之后得到了更稳健的解决方案，预测结果也很好，箱线图中绿色水平线表示的就是平均之后的预测损失，显示仅有极少数的在绿色水平线之下的种子点的选择在单独进行校准时效果会更好。</p>
</div>
<div id="预测结果图" class="section level3">
<h3><span class="header-section-number">6.5.8</span> 预测结果图</h3>
<div class="figure" style="text-align: center">
<img src="./plots/6/mortality.png" alt="对数死亡率的观察与预测值" width="60%"  />
<p class="caption">
(#fig:unnamed-chunk-2)对数死亡率的观察与预测值
</p>
</div>
<p>结论：</p>
<ol style="list-style-type: lower-alpha">
<li><p>20-40岁之间LSTM方法能够更好的捕捉到死亡率的改善，左边观察值清楚的表明LSTM这样的改善是合理的；</p></li>
<li><p>年龄较小人群的死亡率实际改善情况比按照本文方法预测的大，这可能是因为训练数据的青年死亡率改善情况无法代表2000年之后的青年死亡率改善情况。</p></li>
</ol>
<!--chapter:end:06-rnn.Rmd-->
</div>
</div>
</div>
<div id="nlp" class="section level1">
<h1><span class="header-section-number">7</span> 自然语言处理</h1>
<p><em>陈美昆、蒋慧华、高光远</em></p>
<div class="figure" style="text-align: center">
<img src="./plots/7/history.png" alt="NLP历史（2000年以后）" width="70%" />
<p class="caption">
(#fig:history)NLP历史（2000年以后）
</p>
</div>
<p>在保险业，大量的书面证据，如保单合同或索赔通知，以及客户与企业对话助理互动的记录，为数据科学家和精算师提供了越来越多的可供分析的文本信息。</p>
<p>NLP在保险行业中的应用：</p>
<ul>
<li><p>根据文字描述的索赔类型和严重程度对索赔进行分类</p></li>
<li><p>对电子邮件、保单、合同的分类</p></li>
<li><p>从文本数据中识别欺诈案例等</p></li>
</ul>
<p><strong>传统方法的基本过程</strong></p>
<ol style="list-style-type: decimal">
<li><p>文本预处理</p></li>
<li><p><strong>映射到实数集</strong>: bag-of-words, bag-of-POS, pre-trained word embedding</p></li>
<li><p>有监督机器学习算法: AdaBoost, random forest, XGBoost</p></li>
</ol>
<p><strong>循环神经网络的基本过程</strong></p>
<ol style="list-style-type: decimal">
<li><p>文本预处理</p></li>
<li><p>循环神经网络: RNN, GRU, LSTM</p></li>
</ol>
<p><strong>区别</strong></p>
<ol style="list-style-type: decimal">
<li><p>传统方法非常依赖第二步特征工程的效果,神经网络不需要特征工程,它通过监督学习.进行自动特征工程.</p></li>
<li><p>传统方法没有考虑文本的时间序列特征, 循环神经网络考虑了文本的时间序列特征.</p></li>
</ol>
<div id="预处理" class="section level2">
<h2><span class="header-section-number">7.1</span> 预处理</h2>
<ol style="list-style-type: decimal">
<li><p>输入原始文本和格式</p></li>
<li><p>将文本转化为小写</p></li>
<li><p>分词（ tokenization ）</p></li>
<li><p>删除停用词（ stopwords ）</p></li>
<li><p>词性标注（POS tagging, 这步在 bag-of-POS需要, 如果只是 bag-of-words，此步不需要)</p></li>
<li><p>词干提取或词形还原（Stemming or Lemmatization）</p></li>
</ol>
</div>
<div id="bag-of-words" class="section level2">
<h2><span class="header-section-number">7.2</span> Bag of words</h2>
<p>Bag-of-words模型是信息检索领域常用的文档表示方法。在信息检索中，BOW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。也就是说，文档中任意一个位置出现的任何单词，都不受该文档语意影响而独立选择的。例如有如下两个文档：</p>
<ol style="list-style-type: decimal">
<li><p>Bob likes to play basketball, Jim likes too.</p></li>
<li><p>Bob also likes to play football games.</p></li>
</ol>
<p>基于这两个文本文档，构造一个词典：</p>
<p>Dictionary = {1:“Bob”, 2. “like”, 3. “to”, 4. “play”, 5. “basketball”, 6. “also”, 7. “football”, 8. “games”, 9. “Jim”, 10. “too”}</p>
<p>这个词典一共包含<span class="math inline">\(10\)</span>个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个<span class="math inline">\(10\)</span>维向量表示（用整数数字<span class="math inline">\(0:n\)</span>（<span class="math inline">\(n\)</span>为正整数）表示某个单词在文档中出现的次数）：</p>
<p>1：<span class="math inline">\([1, 2, 1, 1, 1, 0, 0, 0, 1, 1]\)</span></p>
<p>2：<span class="math inline">\([1, 1, 1, 1 ,0, 1, 1, 1, 0, 0]\)</span></p>
<p>向量中每个元素表示词典中相关元素在本文本样本中出现的次数。不过，在构造文档向量的过程中可以看到，我们并没有表达单词在原来句子中出现的次序（这是本Bag-of-words模型的缺点之一，不过瑕不掩瑜甚至在此处无关紧要）。</p>
<p>我们使用<code>TfidfVectorizer</code>进行BOW,它不使用词出现的次数,而计算term frequency - inverse document frequency (tf-idf)。研究表明tf-idf更能反映文本的特征。</p>
</div>
<div id="bag-of-part-of-speech" class="section level2">
<h2><span class="header-section-number">7.3</span> Bag of part-of-speech</h2>
<p>相较于BOW，bag of POS仅仅多了一步，即对每个词语进行词性标注，然后使用<code>TfidfVectorizer</code>。可知bag of POS可以达到降维的目的，但它散失了原文本的词语的信息，只考虑了词性。</p>
</div>
<div id="word-embeddings" class="section level2">
<h2><span class="header-section-number">7.4</span> Word embeddings</h2>
<p>词嵌入考虑把每个词语映射到用多维实数空间中，有很多预训练的映射可供选择，这些映射通常考虑了词语的先后顺序和同义词反义词等。常用的词嵌入模型包括：</p>
<ul>
<li><p>Neural probabilistic language model</p></li>
<li><p>word2vec</p></li>
<li><p>Global vectors for word representation</p></li>
</ul>
<div id="pre-trained-word-embeddings" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Pre-trained word embeddings</h3>
<p>我们使用<code>en_core_web_sm</code>进行词嵌入，它可以把任意的词映射到<span class="math inline">\(\mathbb{R}^{96}\)</span>。另外，<code>en_core_web_mb</code>为更复杂的映射，可以把任意的词映射到<span class="math inline">\(\mathbb{R}^{300}\)</span>。对于一个包含多个词语的文本，我们用所有词语的词嵌入平均值来表示这个文本</p>
</div>
</div>
<div id="机器学习算法" class="section level2">
<h2><span class="header-section-number">7.5</span> 机器学习算法</h2>
<p>通过bag-of-words, bag-of-POS，word embedding 我们把每个文本转变为一个向量。这样可以利用如下几种机器学习算法进行文本分类等监督学习。</p>
<ul>
<li><p>AdaBoost</p></li>
<li><p>Random Forest</p></li>
<li><p>XGBoost</p></li>
</ul>
</div>
<div id="神经网络" class="section level2">
<h2><span class="header-section-number">7.6</span> 神经网络</h2>
<div id="数据预处理-4" class="section level3">
<h3><span class="header-section-number">7.6.1</span> 数据预处理</h3>
<p>在神经网络中，我们不需要进行以上bag-of-words, bag-of-POS, word embedding等人工特征工程，我们只需要用实数把文本中<strong>词语的顺序</strong>表征出来即可，神经网络可以同步进行特征工程和有监督训练。</p>
<p>在python中可以使用Keras 中的<code>Tokenizer</code>模块把词语映射到非负整数上，此方法保持了保持了词语的顺序，是前面几种方法没有达到的。</p>
<p>可以看到，使用神经网络时，我们仅仅需要进行很少的特征工程，词语的意义将由神经网络在监督学习中学到。文本是时间序列数据，常用于时间序列分析的模型包括</p>
<ul>
<li><p>LSTM</p></li>
<li><p>GRU</p></li>
</ul>
</div>
</div>
<div id="case-study-1" class="section level2">
<h2><span class="header-section-number">7.7</span> Case study</h2>
<div class="figure" style="text-align: center">
<img src="./plots/7/files.png" alt="文档结构" width="70%"  />
<p class="caption">
(#fig:files)文档结构
</p>
</div>
<div class="figure" style="text-align: center">
<img src="./plots/7/procedure.png" alt="nlp4class_exercise步骤" width="70%"  />
<p class="caption">
(#fig:procedure)nlp4class_exercise步骤
</p>
</div>
<ul>
<li><p>任务描述：根据电影评论文本判断该评论是“好评”还是“差评”</p></li>
<li><p>数据来源： Internet Movie Database (IMDb)</p></li>
<li><p>数据量：案例中共有5000条含分类信息（pos/neg）的原始数据，从计算资源是运行时间考虑，我们从原始数据中随机抽取了1%，即500条数据进行测试，测试数据在toymdb文件夹下，文件结构和原始数据一致（需要注意的是， toymdb文件夹包含train和test两个文件，但并不是实际处理时的“训练集”和“测试集”，实际训练时读取所有数据并重新划分“训练集”和“测试集” ，所以实际上train和test中的文件没有区别，只是沿袭了原始数据的存储结构）。</p></li>
<li><p>数据结构：评论位置代表类别，一个txt存储一个评论数据。</p></li>
</ul>
<div id="函数说明" class="section level3">
<h3><span class="header-section-number">7.7.1</span> 函数说明</h3>
</div>
<div id="可能遇到的问题" class="section level3">
<h3><span class="header-section-number">7.7.2</span> 可能遇到的问题</h3>
<ul>
<li>词性标注</li>
</ul>
<pre><code>  import nltk
  from nltk import pos_tag, word_tokenize
  出现LookupError</code></pre>
<pre><code>  解决方法：把&#39;nltk_data.zip&#39;里的文件全部拷贝至&#39;/Users/huihuajiang/nltk_data/&#39;         
  用以下命令可以查看你的 nltk_data 文件夹路径：             import nltk             
  print(nltk.data.path)</code></pre>
<ul>
<li>词嵌入</li>
</ul>
<pre><code>  import spacy
nlp = spacy.load(‘en_core_web_sm’) 
错误1：OSError: [E050] Can&#39;t find model &#39;en_core_web_sm&#39;.
错误2：numpy.core.multiarray failed to import</code></pre>
<pre><code>  错误1解决方法1（可能不行）：
  命令行运行命令”python -m spacy download en_core_web_sm”
  错误1解决方法2：把模型下载到本地进行安装
  具体操作请参考 https://www.freesion.com/article/73801416523/
  错误2解决方法：重启一下终端</code></pre>
</div>
<div id="结果比较" class="section level3">
<h3><span class="header-section-number">7.7.3</span> 结果比较</h3>
<p><em>以下仅为表例</em></p>
<p><span class="math display">\[
\begin{array}{|l|c|cc|c|}
\hline &amp;  {\text { in-sample }} &amp; {\text { out-of-sample }} &amp; {\text { out-of-sample }}&amp;  {\text { run times }}\\
&amp; \text {both genders} &amp; \text { female } &amp; \text { male } &amp; \text {both genders} \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.7643 &amp; 0.3402 &amp; 1.1346 &amp; 404 \mathrm{s}\\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.6311 &amp; 0.4646 &amp; 1.2571 &amp;  379 \mathrm{s} \\
\hline\hline \text { LSTM3 averaged over 100 different seeds} &amp; - &amp; 0.2451 &amp; 1.2093 &amp; 100 \cdot 404\mathrm{s}\\
\text { GRU3 averaged over 100 different seeds } &amp; - &amp; 0.2341&amp; 1.2746 &amp;  100 \cdot 379 \mathrm{s} \\
\hline \hline \text { LC model with SVD } &amp; 6.2841 &amp; 0.6045 &amp;1.8152 &amp;  - \\
\hline
\end{array}
\]</span></p>
</div>
</div>
<div id="结论-1" class="section level2">
<h2><span class="header-section-number">7.8</span> 结论</h2>
<p>我们用bag of words, bag of POS, word embeddings三种NLP模型对评论文本进行了向量化，并用ADA, RF, XGB三种机器学习方法对文档进行了分类，为此，我们引入了NLP管道来预处理文本数据 。最后，还采用RNN模型对文档进行了分类。</p>
<p>实验结果表明，</p>
<ul>
<li><p>bag of words总体上变现更好， bag of POS的表现不佳。</p></li>
<li><p>与bag of words相比， word embedding模型的效率更高。</p></li>
<li><p>Deep LSTM模型表现要比Single LSTM和更好。</p></li>
<li><p>与NLP模型相比，RNN模型性能更好，达到相同标准的时间更短。</p></li>
<li><p>如果用RNN的输入数据来拟合Ml模型，精度远不及RNN模型，可见RNN模型在该任务上能利用更少的信息实现更准确的分类。</p></li>
</ul>
<!--chapter:end:07-nlp.Rmd-->
</div>
</div>
<div id="flashlight" class="section level1">
<h1><span class="header-section-number">8</span> 通用模型解释方法</h1>
<p><em>龚齐翔、张谦、段诗悦、高光远</em></p>
<blockquote>
<p>the blackness of a model box seems no longer to be caused by the obscurity of the model itself but rather depends on the modeler switching on the light of the box. （手电筒的世界没有黑盒子）</p>
</blockquote>
<p><strong>目的</strong>：从精算的角度基于有监督的机器学习讲述了解释性机器学习和解释性人工智能一些方法的应用。</p>
<ol style="list-style-type: decimal">
<li><p>使我们更相信所使用的模型</p></li>
<li><p>发现模型的局限性</p></li>
<li><p>改进现有的模型</p></li>
<li><p>使模型不仅可以用来预测，还可以提供信息和验证模型假设</p></li>
</ol>
<p><strong>主要内容</strong></p>
<ol style="list-style-type: decimal">
<li><p>变量重要性</p></li>
<li><p>边缘效应（主效应）、交互效应</p></li>
<li><p>每个特征对单一预测值的贡献</p></li>
</ol>
<div id="数据" class="section level2">
<h2><span class="header-section-number">8.1</span> 数据</h2>
<p>数据集划分：80%训练集，20%测试集。有着相同<code>group_id</code>的数据会被同时划分到训练集或者测试集中以减少偏差和选择合适的参数。</p>
</div>
<div id="模型" class="section level2">
<h2><span class="header-section-number">8.2</span> 模型</h2>
<p>以下考虑三种模型：GLM，XGBoost，Neural Network。这三种模型最具有代表性，GLM是保险损失预测中最经典的模型，XGBoost是在数据分析竞赛中常用的机器学习方法，它不容易过拟合，神经网络包含了很多种layer，在图像识别、自然语言处理中发挥了巨大的作用。</p>
<div id="glm" class="section level3">
<h3><span class="header-section-number">8.2.1</span> GLM</h3>
<p>GLM的优点:</p>
<ul>
<li><p>解释性强，统计检验识别参数显著性。</p></li>
<li><p>比较灵活，可以加入交互项、变形后的解释变量、可以用非线性结构（样条曲线平滑）。</p></li>
</ul>
<p>考虑不带交叉项的泊松GLM模型：<code>VehAge</code>和<code>DrivAge</code>都用了自然三次样条进行平滑。</p>
<p><span class="math display">\[N_i \sim \text{Poi} (e_i\lambda(\boldsymbol{x_i}))\]</span>
有两种等价建模方式：</p>
<ol style="list-style-type: decimal">
<li><p>把<span class="math inline">\(\ln e_i\)</span>的系数固定为1。<code>glm (N ~ x + offset (log (e)), family = poisson (link=log) )</code></p></li>
<li><p>把<span class="math inline">\(N_i/e_i\)</span>当作因变量，每个样本的权重为<span class="math inline">\(e_i\)</span>。<code>glm ( I(N/e) ~ x, weights = e, family = poisson (link=log) )</code></p></li>
</ol>
</div>
<div id="xgboost-1" class="section level3">
<h3><span class="header-section-number">8.2.2</span> XGBoost</h3>
<p>用训练集上五折交叉验证来选择超参数。</p>
</div>
<div id="神经网络-1" class="section level3">
<h3><span class="header-section-number">8.2.3</span> 神经网络</h3>
<p>选取的神经网络有以下结构：</p>
<ul>
<li><p>前馈全连接神经网络，含有三个隐藏层，神经元数量<span class="math inline">\(20-15-10\)</span>，双曲正切激活函数。</p></li>
<li><p>输出神经元（索赔频率）的激活函数使用幂函数，即使用泊松模型中的规范连接函数（canonical link function）。</p></li>
<li><p>训练模型的参数使用Nesterov Adam optimizer，选择<code>batch sizes=10000, epochs=300，learning rate=0.002</code>。</p></li>
<li><p>为了消除在训练集上的总体索赔频率偏差（portfolio unbiased），在最后第三个隐藏层的10个神经元上建立Poisson-GLM模型。由于Poisson-GLM的参数估计为极大似然，其总体预测索赔频率等于样本的总经验索赔频率。</p></li>
</ul>
</div>
</div>
<div id="模型整体表现-model-performance" class="section level2">
<h2><span class="header-section-number">8.3</span> 模型整体表现 （model performance）</h2>
<p><strong>性能指标，损失函数</strong></p>
<p>泊松偏差损失
<span class="math display">\[D(\boldsymbol{y},\hat{\boldsymbol{y}})=\frac{\sum_{i=1}^n e_iL(y_i,\hat{y}_i)}{\sum_{i=1}^n e_i}\]</span></p>
<p>其中<span class="math inline">\(y_i=N_i/e_i\)</span>, <span class="math inline">\(\hat{y}_i\)</span> 为<span class="math inline">\(y_i\)</span>的预测值，单位泊松偏差<span class="math inline">\(L(y_i,\hat{y_i})\)</span>为：
<span class="math display">\[\begin{equation}
L(y_i,\hat{y}_i)=2(y_i\ln y_i-y_i\ln\hat{y}_i-y_i+\hat{y}_i). (\#eq:loss)
\end{equation}\]</span></p>
<p>Note that the Poisson deviance @ref(eq:loss) is a strictly consistent scoring function for the expectation <span class="math inline">\(\mathbb{E}(Y)\)</span>.</p>
<p><strong>Pseudo <span class="math inline">\(R^2\)</span></strong></p>
<p><span class="math display">\[\text{Pseudo}~~ R^2=1-\frac{D(\boldsymbol{y},\hat{\boldsymbol{y}})}{D(\boldsymbol{y},\bar{\boldsymbol{y}})}\]</span></p>
<p>Pseudo <span class="math inline">\(R^2\)</span>为偏差损失减少的相对值，衡量了可以被模型解释的偏差比例。</p>
</div>
<div id="变量重要性variable-importance" class="section level2">
<h2><span class="header-section-number">8.4</span> 变量重要性（variable importance）</h2>
<p>它提供了额外的信息，从而使我们能更深入的了解模型。可以通过删掉对模型不重要的变量从而简化模型。可以发现数据结构中的问题：如果一个协变量显示出非常相关而其他变量非常不相关，这可能是因为响应变量发生了信息泄露。</p>
<p>GLM模型可以用似然比检验统计量、<span class="math inline">\(t\)</span>-检验。基于树的模型可以用变量的拆分数量、split gains。</p>
<div id="permutation-importance" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Permutation importance</h3>
<p>对于一个协变量，它的值被随机置换，然后在新的协变量条件下预测<span class="math inline">\(\tilde{\boldsymbol{y}}\)</span>，进而计算D(,)。该变量的重要性可以通过<span class="math inline">\(D(\boldsymbol{y},\tilde{\boldsymbol{y}})-D(\boldsymbol{y},\hat{\boldsymbol{y}})\)</span>度量。</p>
<p>对于数据量大的样本，一次置换即可得到平稳的结果；对于数据量小的样本，需要置换多次并计算平均。</p>
</div>
</div>
<div id="边缘效应主效应" class="section level2">
<h2><span class="header-section-number">8.5</span> 边缘效应（主效应）</h2>
<p>如何衡量具有复杂非线性项和高阶交互作用的GLM模型，或黑箱模型中协变量对响应变量的效应？</p>
<p><strong>基本思路</strong>：当协变量在其取值范围内变动时，预测值是如何变化的。</p>
<p><strong>使用训练集还是测试集？</strong> 如果只使用模型的预测值，则训练集或测试集都可以。如果使用因变量的值，则需要使用测试集。</p>
<p>以下考虑三种model agnostic methods: individual conditional expectations (ICE), partial dependence profiles (PD), accumulated local effects profiles (ALE)。其中，第一种是后两种的基础。</p>
<div id="individual-conditional-expectationsice" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Individual conditional expectations（ICE）</h3>
<p>对于每个样本<span class="math inline">\((N_i,e_i,\boldsymbol{x}_i), i=1,\ldots,n\)</span>, 使其协变量<span class="math inline">\(x_{i,p}\)</span>在取值范围内变化，i.e., <span class="math inline">\(\{a_1,a_2,\ldots,a_m\}\)</span>，根据模型预测在不同协变量值下的索赔频率，<span class="math display">\[y_i^{(ice)}=\{\hat{y}_i(\boldsymbol{x}_{i,-p},x_{i,p}=a):a=a_1,\dots,a_m\}\]</span></p>
<p>以<span class="math inline">\(x_p\)</span>为横轴，<span class="math inline">\(y^{(ice)}\)</span>为纵轴，对于每个样本都可以画出一条ICE曲线。ICE profiles图像有以下特征：</p>
<ul>
<li><p>如果协变量是以相加的模式在模型中（线性模型），则不同样本ICE曲线应该是平行的。</p></li>
<li><p><span class="math inline">\(x_p\)</span>与其他变量的交互作用越强，不同样本的ICE曲线会相交。</p></li>
<li><p>为了更好解读ICE图像，可以垂直移动ICE曲线使得它们在一个点重合。</p></li>
</ul>
<p><strong>Remark</strong>：一些主要的boosting算法，如XGBoost、LightGBM或者CatBoost，在建模时可以约束协变量的单调性，这样可以增加模型的可解释性。在精算中，常常使用的约束如高免赔额的索赔频率低于低免赔额的索赔频率。XGBoost的相应代码为<code>monotone_constraints = c(0,-1,0,0,0,0,0)</code>。</p>
</div>
<div id="partial-dependence-profiles" class="section level3">
<h3><span class="header-section-number">8.5.2</span> Partial dependence profiles</h3>
<p>Partial dependence profile定义为ICE的平均值<span class="math display">\[\bar{f}_l(a)=\frac{1}{n}\sum_{i=1}^n\hat{y}_i(\boldsymbol{x}_{i,-p},x_{i,p}=a)\]</span>
它反映了协变量在这个样本上的平均效应，和样本中其他协变量的分布有关。</p>
<p>ICE和partial dependence profiles都可以推广到多个协变量。</p>
</div>
<div id="accumulated-local-effects-profiles-ale" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Accumulated local effects profiles (ALE)</h3>
<p>ICE和partial dependence profile 假定某个协变量变化而样本的其他变量保持不变（Ceteris Paribus assumption）。这个假设在实际中常常是不成立的。比如，我们不可能固定地区而让人口密度变化。</p>
<p>ALE一定程度上克服了以上缺点，它考虑了局部效应，在较强相关性的协变量的情况下能获得更自然、少偏的结果。ALE profile为</p>
<p><span class="math display">\[\tilde{f}_l(a)=\frac{1}{|\mathcal{I}_{x_p}(a)|}\sum_{i\in\mathcal{I}_{x_p}(a)}\hat{y}_i(\boldsymbol{x}_{i,-p},x_{i,p}=a)\]</span>
其中<span class="math inline">\(\mathcal{I}_{x_p}(a)=\{i:x_{i,p}\in[a-\epsilon,a+\epsilon]\}\)</span>。即ALE为局部ICE的平均（partial dependence profile为所有ICE的平均）。
如果协变量的交互作用不强或者与其他协变量相关性强，则ALE图会接近于partial dependence profile图。</p>
<p><strong>Further profile plots</strong></p>
<p>以下方法来自线性模型的传统诊断方法。</p>
<ol style="list-style-type: decimal">
<li><p>响应变量-协变量图：Averages (or boxplots) of responses are plotted against a covariable。该方法没有使用模型，只是数据的一种可视化工具。</p></li>
<li><p>预测值-协变量图（边际效应图）：Averages of predicted values are plotted against a covariable（marginal plot or M-plot）。</p></li>
<li><p>残差-协变量图： Averages (or boxplots) of residuals are plotted against a covariable。平均残差应该充分地接近零，如果平均残差系统的不为零，说明模型拟合的不好。如果训练集上平均残差接近零但测试集上平均残差不为零，则说明有过拟合问题。</p></li>
</ol>
</div>
</div>
<div id="交互效应" class="section level2">
<h2><span class="header-section-number">8.6</span> 交互效应</h2>
<p>协变量<span class="math inline">\(x_l,x_k\)</span>的交互作用可以用两个变量的Friedman’s H-statistic度量：
<span class="math display">\[H^2_{kl}=\frac{\sum_1^n\left[\bar{f}_{kl}(x_{i,k},x_{i,l})-\bar{f}_{k}(x_{i,k})-\bar{f}_{l}(x_{i,l})\right]^2}{\sum_i^n\left[\bar{f}_{kl}(x_{i,k},x_{i,l})\right]^2}\]</span></p>
<p>上式分子为交互效应，分母为联合效应，可以近似认为联合效应 = 边缘（主）效应 + 交互效应。
如果<span class="math inline">\(x_k,x_l\)</span>无交互作用，则Friedman’s H-statistic为<span class="math inline">\(0\)</span>；如果它们有强烈的交互作用，则Friedman’s H-statistic接近<span class="math inline">\(1\)</span>。</p>
<p>交互效应的绝对度量定义为<span class="math inline">\(H^2_{kl}\)</span>分子的正平方根：
<span class="math display">\[H_{kl}=\sqrt{\sum_1^n\left[\bar{f}_{kl}(x_{i,k},x_{i,l})-\bar{f}_{k}(x_{i,k})-\bar{f}_{l}(x_{i,l})\right]^2}\]</span>
它可以用来对交互效应排序。</p>
<p>交互效应的可视化有两种基本方法：</p>
<ol style="list-style-type: decimal">
<li><p>都为连续型协变量：<span class="math inline">\(\bar{f}_{kl}(x_k,x_l)\)</span>的等高线图。</p></li>
<li><p>分类变量<span class="math inline">\(x_k\)</span>和连续型变量<span class="math inline">\(x_l\)</span>：在<span class="math inline">\(x_k\)</span>的某一水平<span class="math inline">\(a\)</span>下，画<span class="math inline">\(\bar{f}_{kl}(x_k=a,x_l)\)</span>随<span class="math inline">\(x_l\)</span>的变化趋势图。</p></li>
</ol>
<p><strong>交互项约束</strong></p>
<p>有时基于技术或监管的现实情况，交互项在模型中会被禁用。在神经网络中，可以将不考虑交互效应的变量直接连接到输出神经元。在XGBoost中，可以通过改变<code>interaction_constraints</code>实现交互项约束。</p>
</div>
<div id="全局代理模型global-surrogate-models" class="section level2">
<h2><span class="header-section-number">8.7</span> 全局代理模型（Global surrogate models）</h2>
<p>对预测值重新使用易于解释的模型进行建模，如使用单棵决策树。然后再用这个global surrogate model (tree) 来进行解释。
为了评判替代模型的近似效果，有学者建议使用<span class="math inline">\(R^2\)</span>。</p>
</div>
<div id="局部解释样本解释" class="section level2">
<h2><span class="header-section-number">8.8</span> 局部解释（样本解释？）</h2>
<p>以下主要考虑四种model agnostic methods：LIME，LIVE，SHAP，Breakdown。</p>
<div id="lime和live" class="section level3">
<h3><span class="header-section-number">8.8.1</span> LIME和LIVE</h3>
<p>LIME全称Local Interpretable Model-agnostic Explanations，即模型的局部解释器。虽然无法使用线性模型完全“模仿”出SVM、神经网络等模型的“行为”，但可以在某个局部样本点上接近.</p>
<p>LIME和LIVE主要分为以下几步：</p>
<ol style="list-style-type: decimal">
<li><p>构造可解释的数据特征：??是原始用于模型预测的特征，而我们使用??来表示某个可解释特征是否存在的二元变量。</p></li>
<li><p>构造目标函数：<span class="math display">\[\xi(x)=\underset{g\in G}{\arg\min}~\mathcal{L(f,g,\pi_x)+\Omega(g)}\]</span></p></li>
<li><p>采样：</p></li>
</ol>
<p>LIME的优点是原理简单，适用范围广，可解释所有黑箱模型。但也存在一定的问题，例如局部范围大小不同，最终的解释也会不同甚至相悖。</p>
<div class="figure" style="text-align: center">
<img src="./plots/8/lime.png" alt="LIME" width="50%" />
<p class="caption">
(#fig:lime)LIME
</p>
</div>
</div>
<div id="shapshapley-additive-explanations" class="section level3">
<h3><span class="header-section-number">8.8.2</span> SHAP(Shapley Additive Explanations)</h3>
<p>SHAP的目的：与LIME相似，找到某个样本点上各个特征的重要性</p>
<p>Shapley值来自合作博弈论，从不同的合作组合中计算出单个个体的贡献，它度量了单个个体对总体的边际贡献。SHAP的主要思想来源于Shapley值</p>
<p>Shapley值的计算</p>
<p><strong>从Shapley值到SHAP</strong></p>
<ul>
<li><p>SHAP对预测的分解：</p></li>
<li><p>SHAP中特征贡献的计算</p></li>
</ul>
<p>SHAP把黑箱中复杂的映射关系表达成一个简单线性映射，便于把保费的构成解释给被保险人。但是，对于不同被保险人，同一个协变量值的效应（SHAP）可能不同，这是SHAP的缺点。（？我不太确定最后一句话）</p>
</div>
<div id="breakdown-and-approximate-shap" class="section level3">
<h3><span class="header-section-number">8.8.3</span> Breakdown and approximate SHAP</h3>
<p>对于一组给定进入顺序的变量<span class="math inline">\(x_{(1)},x_{(2)},\ldots,x_{(p)}\)</span>，解构方法如下：</p>
<ol style="list-style-type: decimal">
<li>初始预测<span class="math inline">\(\hat{y}^{(0)}_i\)</span>为训练集的索赔频率均值</li>
</ol>
<p>对于<span class="math inline">\(l=1,\ldots,p\)</span>，进行以下两步：</p>
<ol start="2" style="list-style-type: decimal">
<li><p>把所有样本的<span class="math inline">\(x_{(l)}\)</span>变为<span class="math inline">\(x_{i,(l)}\)</span>，计算所有样本上预测索赔频率均值<span class="math inline">\(\hat{y}^{(l)}_i\)</span>.</p></li>
<li><p>计算<span class="math inline">\(x_{(l)}\)</span>的效应：<span class="math inline">\(\hat{y}^{(l)}_i-\hat{y}^{(l-1)}_i\)</span></p></li>
</ol>
<p>可见Breakdown方法得出的结果是和顺序<span class="math inline">\(x_{(1)},x_{(2)},\ldots,x_{(p)}\)</span>相关的。对于<span class="math inline">\(p\)</span>个变量，有<span class="math inline">\(p!\)</span>种排列方式。一种处理方法是采取少量的排列方式，如20种，取平均。另一种是按照变量重要性降序进入，这里将前者称为<strong>approximate SHAP</strong>，将后者称为<strong>breakdown</strong>。</p>
</div>
<div id="from-local-to-global-properties" class="section level3">
<h3><span class="header-section-number">8.8.4</span> From local to global properties</h3>
<p>将多个样本使用SHAP或breakdown进行分解，然后也可以得到一些模型的全局性质：</p>
<ol style="list-style-type: decimal">
<li><p>变量重要性</p></li>
<li><p>主效应</p></li>
<li><p>交互效应</p></li>
</ol>
</div>
</div>
<div id="improving-the-glm-by-interpretable-machine-learning" class="section level2">
<h2><span class="header-section-number">8.9</span> Improving the GLM by interpretable machine learning</h2>
<p>通过之前的结果改进GLM模型：</p>
<p>1.建立简单的GLM模型和调优的ML模型</p>
<p>2.比较性能，如果GLM模型没有改进空间就停止</p>
<p>3.研究变量重要性决定保留哪些变量</p>
<p>4.通过对预测的影响效果，找出最强的预测因素，以调整变量（平方项、样条、删去部分类等）</p>
<p>5.根据相互作用强度改进交互项</p>
<p>6.利用以上因素改进GLM，然后回到第二步</p>
<ul>
<li><p>根据Variable importance，不删除变量</p></li>
<li><p>根据Partial dependence profiles，logDensity用三次样条函数表示</p></li>
<li><p>根据Interaction strength，增加VehAge:VehBrand，VehBrand:VehGas的交互作用</p></li>
</ul>
<p>基于以上改进建立新的glm</p>
</div>
<div id="案例分析" class="section level2">
<h2><span class="header-section-number">8.10</span> 案例分析</h2>
<div id="导入包" class="section level3">
<h3><span class="header-section-number">8.10.1</span> 导入包</h3>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1"></a><span class="kw">library</span>(CASdatasets)      <span class="co"># 1.0.6</span></span>
<span id="cb92-2"><a href="#cb92-2"></a><span class="kw">library</span>(dplyr)            <span class="co"># 0.8.5</span></span>
<span id="cb92-3"><a href="#cb92-3"></a><span class="kw">library</span>(forcats)          <span class="co"># 0.5.0</span></span>
<span id="cb92-4"><a href="#cb92-4"></a><span class="kw">library</span>(reshape2)         <span class="co"># 1.4.3</span></span>
<span id="cb92-5"><a href="#cb92-5"></a><span class="kw">library</span>(corrplot)         <span class="co"># 0.84</span></span>
<span id="cb92-6"><a href="#cb92-6"></a><span class="kw">library</span>(ggplot2)          <span class="co"># 3.3.0</span></span>
<span id="cb92-7"><a href="#cb92-7"></a><span class="kw">library</span>(splines)          <span class="co"># 3.6.3</span></span>
<span id="cb92-8"><a href="#cb92-8"></a><span class="kw">library</span>(splitTools)       <span class="co"># 0.2.0</span></span>
<span id="cb92-9"><a href="#cb92-9"></a><span class="kw">library</span>(xgboost)          <span class="co"># 1.0.0.2</span></span>
<span id="cb92-10"><a href="#cb92-10"></a><span class="kw">library</span>(keras)            <span class="co"># 2.2.5.0</span></span>
<span id="cb92-11"><a href="#cb92-11"></a><span class="kw">library</span>(MetricsWeighted)  <span class="co"># 0.5.0</span></span>
<span id="cb92-12"><a href="#cb92-12"></a><span class="kw">library</span>(flashlight)       <span class="co"># 0.7.2</span></span></code></pre></div>
</div>
<div id="预处理-1" class="section level3">
<h3><span class="header-section-number">8.10.2</span> 预处理</h3>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1"></a><span class="kw">data</span>(freMTPL2freq)</span>
<span id="cb93-2"><a href="#cb93-2"></a></span>
<span id="cb93-3"><a href="#cb93-3"></a>distinct &lt;-<span class="st"> </span>freMTPL2freq <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb93-4"><a href="#cb93-4"></a><span class="st">  </span><span class="kw">distinct_at</span>(<span class="kw">vars</span>(<span class="op">-</span><span class="kw">c</span>(IDpol, Exposure, ClaimNb))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb93-5"><a href="#cb93-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">group_id =</span> <span class="kw">row_number</span>())</span>
<span id="cb93-6"><a href="#cb93-6"></a></span>
<span id="cb93-7"><a href="#cb93-7"></a>dat &lt;-<span class="st"> </span>freMTPL2freq <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb93-8"><a href="#cb93-8"></a><span class="st">  </span><span class="kw">left_join</span>(distinct) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb93-9"><a href="#cb93-9"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Exposure =</span> <span class="kw">pmin</span>(<span class="dv">1</span>, Exposure),</span>
<span id="cb93-10"><a href="#cb93-10"></a>         <span class="dt">Freq =</span> <span class="kw">pmin</span>(<span class="dv">15</span>, ClaimNb <span class="op">/</span><span class="st"> </span>Exposure),</span>
<span id="cb93-11"><a href="#cb93-11"></a>         <span class="dt">VehPower =</span> <span class="kw">pmin</span>(<span class="dv">12</span>, VehPower),</span>
<span id="cb93-12"><a href="#cb93-12"></a>         <span class="dt">VehAge =</span> <span class="kw">pmin</span>(<span class="dv">20</span>, VehAge),</span>
<span id="cb93-13"><a href="#cb93-13"></a>         <span class="dt">VehGas =</span> <span class="kw">factor</span>(VehGas),</span>
<span id="cb93-14"><a href="#cb93-14"></a>         <span class="dt">DrivAge =</span> <span class="kw">pmin</span>(<span class="dv">85</span>, DrivAge),</span>
<span id="cb93-15"><a href="#cb93-15"></a>         <span class="dt">logDensity =</span> <span class="kw">log</span>(Density),</span>
<span id="cb93-16"><a href="#cb93-16"></a>         <span class="dt">VehBrand =</span> <span class="kw">factor</span>(VehBrand, <span class="dt">levels =</span> </span>
<span id="cb93-17"><a href="#cb93-17"></a>                             <span class="kw">paste0</span>(<span class="st">&quot;B&quot;</span>, <span class="kw">c</span>(<span class="dv">12</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">13</span>, <span class="dv">14</span>))),</span>
<span id="cb93-18"><a href="#cb93-18"></a>         <span class="dt">PolicyRegion =</span> <span class="kw">relevel</span>(Region, <span class="st">&quot;Ile-de-France&quot;</span>),</span>
<span id="cb93-19"><a href="#cb93-19"></a>         <span class="dt">AreaCode =</span> Area)</span>
<span id="cb93-20"><a href="#cb93-20"></a></span>
<span id="cb93-21"><a href="#cb93-21"></a><span class="co"># Covariables, Response, Weight</span></span>
<span id="cb93-22"><a href="#cb93-22"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;VehPower&quot;</span>, <span class="st">&quot;VehAge&quot;</span>,  <span class="st">&quot;VehBrand&quot;</span>, <span class="st">&quot;VehGas&quot;</span>, <span class="st">&quot;DrivAge&quot;</span>,</span>
<span id="cb93-23"><a href="#cb93-23"></a>       <span class="st">&quot;logDensity&quot;</span>, <span class="st">&quot;PolicyRegion&quot;</span>)</span>
<span id="cb93-24"><a href="#cb93-24"></a>y &lt;-<span class="st"> &quot;Freq&quot;</span></span>
<span id="cb93-25"><a href="#cb93-25"></a>w &lt;-<span class="st"> &quot;Exposure&quot;</span></span></code></pre></div>
</div>
<div id="描述性统计" class="section level3">
<h3><span class="header-section-number">8.10.3</span> 描述性统计</h3>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1"></a><span class="co"># Univariate description</span></span>
<span id="cb94-2"><a href="#cb94-2"></a></span>
<span id="cb94-3"><a href="#cb94-3"></a>melted &lt;-<span class="st"> </span>dat[<span class="kw">c</span>(<span class="st">&quot;Freq&quot;</span>, <span class="st">&quot;Exposure&quot;</span>, <span class="st">&quot;DrivAge&quot;</span>, <span class="st">&quot;VehAge&quot;</span>, <span class="st">&quot;VehPower&quot;</span>, <span class="st">&quot;logDensity&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-4"><a href="#cb94-4"></a><span class="st">  </span><span class="kw">stack</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-5"><a href="#cb94-5"></a><span class="st">  </span><span class="kw">filter</span>(ind <span class="op">!=</span><span class="st"> &quot;Freq&quot;</span> <span class="op">|</span><span class="st"> </span>values <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-6"><a href="#cb94-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ind =</span> <span class="kw">fct_recode</span>(ind, </span>
<span id="cb94-7"><a href="#cb94-7"></a>                          <span class="st">`</span><span class="dt">Driver&#39;s age</span><span class="st">`</span> =<span class="st"> &quot;DrivAge&quot;</span>, </span>
<span id="cb94-8"><a href="#cb94-8"></a>                          <span class="st">`</span><span class="dt">Vehicle&#39;s age</span><span class="st">`</span> =<span class="st"> &quot;VehAge&quot;</span>, </span>
<span id="cb94-9"><a href="#cb94-9"></a>                          <span class="st">`</span><span class="dt">Vehicle power</span><span class="st">`</span> =<span class="st"> &quot;VehPower&quot;</span>, </span>
<span id="cb94-10"><a href="#cb94-10"></a>                          <span class="st">`</span><span class="dt">Logarithmic density</span><span class="st">`</span> =<span class="st"> &quot;logDensity&quot;</span>))</span>
<span id="cb94-11"><a href="#cb94-11"></a></span>
<span id="cb94-12"><a href="#cb94-12"></a><span class="kw">ggplot</span>(melted, <span class="kw">aes</span>(<span class="dt">x=</span>values)) <span class="op">+</span></span>
<span id="cb94-13"><a href="#cb94-13"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">19</span>, <span class="dt">fill =</span> <span class="st">&quot;#E69F00&quot;</span>) <span class="op">+</span></span>
<span id="cb94-14"><a href="#cb94-14"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>ind, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span></span>
<span id="cb94-15"><a href="#cb94-15"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">element_blank</span>(), <span class="dt">y =</span> <span class="kw">element_blank</span>()) <span class="op">+</span></span>
<span id="cb94-16"><a href="#cb94-16"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title.y =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb94-17"><a href="#cb94-17"></a>        <span class="dt">axis.text.y =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb94-18"><a href="#cb94-18"></a>        <span class="dt">axis.ticks.y =</span> <span class="kw">element_blank</span>())</span>
<span id="cb94-19"><a href="#cb94-19"></a></span>
<span id="cb94-20"><a href="#cb94-20"></a><span class="co"># Bivariate description</span></span>
<span id="cb94-21"><a href="#cb94-21"></a></span>
<span id="cb94-22"><a href="#cb94-22"></a>cor_mat &lt;-<span class="st"> </span>dat <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-23"><a href="#cb94-23"></a><span class="st">  </span><span class="kw">select_at</span>(<span class="kw">c</span>(x, <span class="st">&quot;BonusMalus&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-24"><a href="#cb94-24"></a><span class="st">  </span><span class="kw">select_if</span>(is.numeric) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-25"><a href="#cb94-25"></a><span class="st">  </span><span class="kw">cor</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-26"><a href="#cb94-26"></a><span class="st">  </span><span class="kw">round</span>(<span class="dv">2</span>)</span>
<span id="cb94-27"><a href="#cb94-27"></a><span class="kw">corrplot</span>(cor_mat, <span class="dt">method =</span> <span class="st">&quot;square&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;lower&quot;</span>, <span class="dt">diag =</span> <span class="ot">FALSE</span>, <span class="dt">title =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb94-28"><a href="#cb94-28"></a>         <span class="dt">addCoef.col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">tl.col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb94-29"><a href="#cb94-29"></a></span>
<span id="cb94-30"><a href="#cb94-30"></a></span>
<span id="cb94-31"><a href="#cb94-31"></a><span class="co"># Boxplots</span></span>
<span id="cb94-32"><a href="#cb94-32"></a>th &lt;-<span class="st"> </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">45</span>, <span class="dt">hjust =</span> <span class="dv">1</span>, <span class="dt">vjust =</span> <span class="dv">1</span>))</span>
<span id="cb94-33"><a href="#cb94-33"></a></span>
<span id="cb94-34"><a href="#cb94-34"></a><span class="co"># BonusMalus nach DrivAge</span></span>
<span id="cb94-35"><a href="#cb94-35"></a>dat <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-36"><a href="#cb94-36"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">DrivAge =</span> <span class="kw">cut</span>(DrivAge, <span class="kw">c</span>(<span class="dv">17</span><span class="op">:</span><span class="dv">24</span>, <span class="kw">seq</span>(<span class="dv">25</span>, <span class="dv">85</span>, <span class="dv">10</span>)), </span>
<span id="cb94-37"><a href="#cb94-37"></a>                       <span class="dt">labels =</span> <span class="kw">c</span>(<span class="dv">18</span><span class="op">:</span><span class="dv">25</span>, <span class="st">&quot;26-35&quot;</span>, <span class="st">&quot;36-45&quot;</span>, <span class="st">&quot;46-55&quot;</span>, <span class="st">&quot;56-65&quot;</span>, <span class="st">&quot;66-75&quot;</span>, <span class="st">&quot;76+&quot;</span>),</span>
<span id="cb94-38"><a href="#cb94-38"></a>                       <span class="dt">include.lowest =</span> <span class="ot">TRUE</span>),</span>
<span id="cb94-39"><a href="#cb94-39"></a>         <span class="dt">DrivAge =</span> <span class="kw">fct_recode</span>(DrivAge)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-40"><a href="#cb94-40"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> DrivAge, <span class="dt">y =</span> BonusMalus)) <span class="op">+</span></span>
<span id="cb94-41"><a href="#cb94-41"></a><span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">outlier.shape =</span> <span class="ot">NA</span>, <span class="dt">fill =</span> <span class="st">&quot;#E69F00&quot;</span>) <span class="op">+</span></span>
<span id="cb94-42"><a href="#cb94-42"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">125</span>))</span>
<span id="cb94-43"><a href="#cb94-43"></a></span>
<span id="cb94-44"><a href="#cb94-44"></a><span class="co"># Brand/vehicle age</span></span>
<span id="cb94-45"><a href="#cb94-45"></a>dat <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-46"><a href="#cb94-46"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> VehBrand, <span class="dt">y =</span> VehAge)) <span class="op">+</span></span>
<span id="cb94-47"><a href="#cb94-47"></a><span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">outlier.shape =</span> <span class="ot">NA</span>, <span class="dt">fill =</span> <span class="st">&quot;#E69F00&quot;</span>) <span class="op">+</span></span>
<span id="cb94-48"><a href="#cb94-48"></a><span class="st">  </span>th</span>
<span id="cb94-49"><a href="#cb94-49"></a></span>
<span id="cb94-50"><a href="#cb94-50"></a><span class="co"># Density/Area</span></span>
<span id="cb94-51"><a href="#cb94-51"></a>dat <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-52"><a href="#cb94-52"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> AreaCode, <span class="dt">y =</span> logDensity)) <span class="op">+</span></span>
<span id="cb94-53"><a href="#cb94-53"></a><span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">fill =</span> <span class="st">&quot;#E69F00&quot;</span>) <span class="op">+</span></span>
<span id="cb94-54"><a href="#cb94-54"></a><span class="st">  </span>th</span>
<span id="cb94-55"><a href="#cb94-55"></a></span>
<span id="cb94-56"><a href="#cb94-56"></a><span class="co"># Density/Region</span></span>
<span id="cb94-57"><a href="#cb94-57"></a>dat <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb94-58"><a href="#cb94-58"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Region, <span class="dt">y =</span> logDensity)) <span class="op">+</span></span>
<span id="cb94-59"><a href="#cb94-59"></a><span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">outlier.shape =</span> <span class="ot">NA</span>, <span class="dt">fill =</span> <span class="st">&quot;#E69F00&quot;</span>) <span class="op">+</span></span>
<span id="cb94-60"><a href="#cb94-60"></a><span class="st">  </span>th</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/8/hist.png" alt="LIME" width="50%"  />
<p class="caption">
(#fig:hist)LIME
</p>
</div>
</div>
<div id="建模" class="section level3">
<h3><span class="header-section-number">8.10.4</span> 建模</h3>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1"></a>ind &lt;-<span class="st"> </span><span class="kw">partition</span>(dat[[<span class="st">&quot;group_id&quot;</span>]], <span class="dt">p =</span> <span class="kw">c</span>(<span class="dt">train =</span> <span class="fl">0.8</span>, <span class="dt">test =</span> <span class="fl">0.2</span>), </span>
<span id="cb95-2"><a href="#cb95-2"></a>                 <span class="dt">seed =</span> <span class="dv">22</span>, <span class="dt">type =</span> <span class="st">&quot;grouped&quot;</span>)</span>
<span id="cb95-3"><a href="#cb95-3"></a>train &lt;-<span class="st"> </span>dat[ind<span class="op">$</span>train, ]</span>
<span id="cb95-4"><a href="#cb95-4"></a>test &lt;-<span class="st"> </span>dat[ind<span class="op">$</span>test, ]</span></code></pre></div>
<div id="glm-1" class="section level4">
<h4><span class="header-section-number">8.10.4.1</span> glm</h4>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1"></a>fit_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(Freq <span class="op">~</span><span class="st"> </span>VehPower <span class="op">+</span><span class="st"> </span><span class="kw">ns</span>(VehAge, <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">+</span></span>
<span id="cb96-2"><a href="#cb96-2"></a><span class="st">                 </span>VehGas <span class="op">+</span><span class="st"> </span><span class="kw">ns</span>(DrivAge, <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span>logDensity <span class="op">+</span><span class="st"> </span>PolicyRegion,</span>
<span id="cb96-3"><a href="#cb96-3"></a>               <span class="dt">data =</span> train,</span>
<span id="cb96-4"><a href="#cb96-4"></a>               <span class="dt">family =</span> <span class="kw">quasipoisson</span>(),</span>
<span id="cb96-5"><a href="#cb96-5"></a>               <span class="dt">weights =</span> train[[w]])</span></code></pre></div>
</div>
<div id="xgboost-2" class="section level4">
<h4><span class="header-section-number">8.10.4.2</span> XGBoost</h4>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1"></a><span class="co"># Input maker</span></span>
<span id="cb97-2"><a href="#cb97-2"></a>prep_xgb &lt;-<span class="st"> </span><span class="cf">function</span>(dat, x) {</span>
<span id="cb97-3"><a href="#cb97-3"></a>  <span class="kw">data.matrix</span>(dat[, x, <span class="dt">drop =</span> <span class="ot">FALSE</span>])</span>
<span id="cb97-4"><a href="#cb97-4"></a>}</span>
<span id="cb97-5"><a href="#cb97-5"></a></span>
<span id="cb97-6"><a href="#cb97-6"></a><span class="co"># Data interface to XGBoost</span></span>
<span id="cb97-7"><a href="#cb97-7"></a>dtrain &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="kw">prep_xgb</span>(train, x), </span>
<span id="cb97-8"><a href="#cb97-8"></a>                      <span class="dt">label =</span> train[[y]], </span>
<span id="cb97-9"><a href="#cb97-9"></a>                      <span class="dt">weight =</span> train[[w]])</span>
<span id="cb97-10"><a href="#cb97-10"></a></span>
<span id="cb97-11"><a href="#cb97-11"></a><span class="co"># Parameters chosen by 5-fold grouped CV</span></span>
<span id="cb97-12"><a href="#cb97-12"></a>params_freq &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">learning_rate =</span> <span class="fl">0.2</span>,</span>
<span id="cb97-13"><a href="#cb97-13"></a>                    <span class="dt">max_depth =</span> <span class="dv">5</span>,</span>
<span id="cb97-14"><a href="#cb97-14"></a>                    <span class="dt">alpha =</span> <span class="dv">3</span>, <span class="co">#权重的l1正则项</span></span>
<span id="cb97-15"><a href="#cb97-15"></a>                    <span class="dt">lambda =</span> <span class="fl">0.5</span>, <span class="co">#权重的l2正则项</span></span>
<span id="cb97-16"><a href="#cb97-16"></a>                    <span class="dt">max_delta_step =</span> <span class="dv">2</span>, <span class="co">#权重改变最大步长，default为0</span></span>
<span id="cb97-17"><a href="#cb97-17"></a>                    <span class="dt">min_split_loss =</span> <span class="dv">0</span>, <span class="co">#节点分裂所需的最小损失函数下降值，default为0</span></span>
<span id="cb97-18"><a href="#cb97-18"></a>                    <span class="dt">colsample_bytree =</span> <span class="dv">1</span>, <span class="co">#控制每棵随机采样的列数的占比(每一列是一个特征)，default为1</span></span>
<span id="cb97-19"><a href="#cb97-19"></a>                    <span class="dt">subsample =</span> <span class="fl">0.9</span> <span class="co">#控制树的每级的每次分裂，对列数的采样的占比,default为1,和colsample_bytree功能重叠</span></span>
<span id="cb97-20"><a href="#cb97-20"></a>                    )</span>
<span id="cb97-21"><a href="#cb97-21"></a></span>
<span id="cb97-22"><a href="#cb97-22"></a><span class="co"># Fit</span></span>
<span id="cb97-23"><a href="#cb97-23"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb97-24"><a href="#cb97-24"></a>fit_xgb &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(params_freq, </span>
<span id="cb97-25"><a href="#cb97-25"></a>                     <span class="dt">data =</span> dtrain,</span>
<span id="cb97-26"><a href="#cb97-26"></a>                     <span class="dt">nrounds =</span> <span class="dv">580</span>,</span>
<span id="cb97-27"><a href="#cb97-27"></a>                     <span class="dt">objective =</span> <span class="st">&quot;count:poisson&quot;</span>,</span>
<span id="cb97-28"><a href="#cb97-28"></a>                     <span class="dt">watchlist =</span> <span class="kw">list</span>(<span class="dt">train =</span> dtrain),</span>
<span id="cb97-29"><a href="#cb97-29"></a>                     <span class="dt">print_every_n =</span> <span class="dv">10</span>)</span></code></pre></div>
</div>
<div id="为后文训练可变单调性约束和交互项约束的xgboost使用较小的nrounds" class="section level4">
<h4><span class="header-section-number">8.10.4.3</span> 为后文训练可变单调性约束和交互项约束的xgboost，使用较小的nrounds</h4>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1"></a>params_freq_constraints &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">learning_rate =</span> <span class="fl">0.2</span>,</span>
<span id="cb98-2"><a href="#cb98-2"></a>                    <span class="dt">max_depth =</span> <span class="dv">5</span>,</span>
<span id="cb98-3"><a href="#cb98-3"></a>                    <span class="dt">alpha =</span> <span class="dv">3</span>,</span>
<span id="cb98-4"><a href="#cb98-4"></a>                    <span class="dt">lambda =</span> <span class="fl">0.5</span>,</span>
<span id="cb98-5"><a href="#cb98-5"></a>                    <span class="dt">max_delta_step =</span> <span class="dv">2</span>,</span>
<span id="cb98-6"><a href="#cb98-6"></a>                    <span class="dt">min_split_loss =</span> <span class="dv">0</span>,</span>
<span id="cb98-7"><a href="#cb98-7"></a>                    <span class="dt">monotone_constraints =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="co">#可变单调性的约束</span></span>
<span id="cb98-8"><a href="#cb98-8"></a>                    <span class="dt">colsample_bytree =</span> <span class="dv">1</span>,</span>
<span id="cb98-9"><a href="#cb98-9"></a>                    <span class="dt">subsample =</span> <span class="fl">0.9</span>)</span>
<span id="cb98-10"><a href="#cb98-10"></a></span>
<span id="cb98-11"><a href="#cb98-11"></a>fit_xgb_constraints &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(params_freq_constraints, </span>
<span id="cb98-12"><a href="#cb98-12"></a>                     <span class="dt">data =</span> dtrain,</span>
<span id="cb98-13"><a href="#cb98-13"></a>                     <span class="dt">nrounds =</span> <span class="dv">80</span>,</span>
<span id="cb98-14"><a href="#cb98-14"></a>                     <span class="dt">objective =</span> <span class="st">&quot;count:poisson&quot;</span>,</span>
<span id="cb98-15"><a href="#cb98-15"></a>                     <span class="dt">watchlist =</span> <span class="kw">list</span>(<span class="dt">train =</span> dtrain),</span>
<span id="cb98-16"><a href="#cb98-16"></a>                     <span class="dt">print_every_n =</span> <span class="dv">10</span>)</span>
<span id="cb98-17"><a href="#cb98-17"></a></span>
<span id="cb98-18"><a href="#cb98-18"></a>params_freq_interaction_constraints &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">learning_rate =</span> <span class="fl">0.2</span>,</span>
<span id="cb98-19"><a href="#cb98-19"></a>                    <span class="dt">max_depth =</span> <span class="dv">5</span>,</span>
<span id="cb98-20"><a href="#cb98-20"></a>                    <span class="dt">alpha =</span> <span class="dv">3</span>,</span>
<span id="cb98-21"><a href="#cb98-21"></a>                    <span class="dt">lambda =</span> <span class="fl">0.5</span>,</span>
<span id="cb98-22"><a href="#cb98-22"></a>                    <span class="dt">max_delta_step =</span> <span class="dv">2</span>,</span>
<span id="cb98-23"><a href="#cb98-23"></a>                    <span class="dt">min_split_loss =</span> <span class="dv">0</span>,</span>
<span id="cb98-24"><a href="#cb98-24"></a>                    <span class="dt">interaction_constraints =</span> <span class="kw">list</span>(<span class="dv">4</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">6</span>)), <span class="co">#交互项约束，约束以嵌套列表的形式指定</span></span>
<span id="cb98-25"><a href="#cb98-25"></a>                    <span class="dt">colsample_bytree =</span> <span class="dv">1</span>,</span>
<span id="cb98-26"><a href="#cb98-26"></a>                    <span class="dt">subsample =</span> <span class="fl">0.9</span>)</span>
<span id="cb98-27"><a href="#cb98-27"></a></span>
<span id="cb98-28"><a href="#cb98-28"></a>fit_xgb_interaction_constraints &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(params_freq_interaction_constraints, </span>
<span id="cb98-29"><a href="#cb98-29"></a>                     <span class="dt">data =</span> dtrain,</span>
<span id="cb98-30"><a href="#cb98-30"></a>                     <span class="dt">nrounds =</span> <span class="dv">80</span>,</span>
<span id="cb98-31"><a href="#cb98-31"></a>                     <span class="dt">objective =</span> <span class="st">&quot;count:poisson&quot;</span>,</span>
<span id="cb98-32"><a href="#cb98-32"></a>                     <span class="dt">watchlist =</span> <span class="kw">list</span>(<span class="dt">train =</span> dtrain),</span>
<span id="cb98-33"><a href="#cb98-33"></a>                     <span class="dt">print_every_n =</span> <span class="dv">10</span>)</span></code></pre></div>
</div>
<div id="神经网络-2" class="section level4">
<h4><span class="header-section-number">8.10.4.4</span> 神经网络</h4>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1"></a>prep_nn &lt;-<span class="st"> </span><span class="cf">function</span>(dat, x, <span class="dt">cat_cols =</span> <span class="kw">c</span>(<span class="st">&quot;PolicyRegion&quot;</span>, <span class="st">&quot;VehBrand&quot;</span>)) {</span>
<span id="cb99-2"><a href="#cb99-2"></a>  dense_cols &lt;-<span class="st"> </span><span class="kw">setdiff</span>(x, cat_cols)</span>
<span id="cb99-3"><a href="#cb99-3"></a>  <span class="kw">c</span>(<span class="kw">list</span>(<span class="dt">dense1 =</span> <span class="kw">data.matrix</span>(dat[, dense_cols])), </span>
<span id="cb99-4"><a href="#cb99-4"></a>    <span class="kw">lapply</span>(dat[, cat_cols], <span class="cf">function</span>(z) <span class="kw">as.integer</span>(z) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb99-5"><a href="#cb99-5"></a>}</span>
<span id="cb99-6"><a href="#cb99-6"></a></span>
<span id="cb99-7"><a href="#cb99-7"></a><span class="co"># Initialize neural net</span></span>
<span id="cb99-8"><a href="#cb99-8"></a>new_neural_net &lt;-<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb99-9"><a href="#cb99-9"></a>  <span class="kw">k_clear_session</span>()</span>
<span id="cb99-10"><a href="#cb99-10"></a>  <span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb99-11"><a href="#cb99-11"></a>  <span class="cf">if</span> (<span class="st">&quot;set_seed&quot;</span> <span class="op">%in%</span><span class="st"> </span><span class="kw">names</span>(tensorflow<span class="op">::</span>tf<span class="op">$</span>random)) {</span>
<span id="cb99-12"><a href="#cb99-12"></a>    tensorflow<span class="op">::</span>tf<span class="op">$</span>random<span class="op">$</span><span class="kw">set_seed</span>(<span class="dv">0</span>)</span>
<span id="cb99-13"><a href="#cb99-13"></a>  } <span class="cf">else</span> <span class="cf">if</span> (<span class="st">&quot;set_random_seed&quot;</span> <span class="op">%in%</span><span class="st"> </span><span class="kw">names</span>(tensorflow<span class="op">::</span>tf<span class="op">$</span>random)) {</span>
<span id="cb99-14"><a href="#cb99-14"></a>    tensorflow<span class="op">::</span>tf<span class="op">$</span>random<span class="op">$</span><span class="kw">set_random_seed</span>(<span class="dv">0</span>)</span>
<span id="cb99-15"><a href="#cb99-15"></a>  } <span class="cf">else</span> {</span>
<span id="cb99-16"><a href="#cb99-16"></a>    <span class="kw">print</span>(<span class="st">&quot;Check tf version&quot;</span>)</span>
<span id="cb99-17"><a href="#cb99-17"></a>  }</span>
<span id="cb99-18"><a href="#cb99-18"></a>  </span>
<span id="cb99-19"><a href="#cb99-19"></a>  <span class="co"># Model architecture</span></span>
<span id="cb99-20"><a href="#cb99-20"></a>  dense_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dv">5</span>, <span class="dt">name =</span> <span class="st">&quot;dense1&quot;</span>, <span class="dt">dtype =</span> <span class="st">&quot;float32&quot;</span>)</span>
<span id="cb99-21"><a href="#cb99-21"></a>  PolicyRegion_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dv">1</span>, <span class="dt">name =</span> <span class="st">&quot;PolicyRegion&quot;</span>, <span class="dt">dtype =</span> <span class="st">&quot;int8&quot;</span>)</span>
<span id="cb99-22"><a href="#cb99-22"></a>  VehBrand_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dv">1</span>, <span class="dt">name =</span> <span class="st">&quot;VehBrand&quot;</span>, <span class="dt">dtype =</span> <span class="st">&quot;int8&quot;</span>)</span>
<span id="cb99-23"><a href="#cb99-23"></a></span>
<span id="cb99-24"><a href="#cb99-24"></a>  PolicyRegion_emb &lt;-<span class="st"> </span>PolicyRegion_input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-25"><a href="#cb99-25"></a><span class="st">    </span><span class="kw">layer_embedding</span>(<span class="dv">22</span>, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-26"><a href="#cb99-26"></a><span class="st">    </span><span class="kw">layer_flatten</span>()</span>
<span id="cb99-27"><a href="#cb99-27"></a>  </span>
<span id="cb99-28"><a href="#cb99-28"></a>  VehBrand_emb &lt;-<span class="st"> </span>VehBrand_input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-29"><a href="#cb99-29"></a><span class="st">    </span><span class="kw">layer_embedding</span>(<span class="dv">11</span>, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-30"><a href="#cb99-30"></a><span class="st">    </span><span class="kw">layer_flatten</span>()</span>
<span id="cb99-31"><a href="#cb99-31"></a></span>
<span id="cb99-32"><a href="#cb99-32"></a>  outputs &lt;-<span class="st"> </span><span class="kw">list</span>(dense_input, PolicyRegion_emb, VehBrand_emb) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-33"><a href="#cb99-33"></a><span class="st">    </span><span class="kw">layer_concatenate</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-34"><a href="#cb99-34"></a><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dv">20</span>, <span class="dt">activation =</span> <span class="st">&quot;tanh&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb99-35"><a href="#cb99-35"></a><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dv">15</span>, <span class="dt">activation =</span> <span class="st">&quot;tanh&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb99-36"><a href="#cb99-36"></a><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;tanh&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-37"><a href="#cb99-37"></a><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&quot;exponential&quot;</span>)</span>
<span id="cb99-38"><a href="#cb99-38"></a>  </span>
<span id="cb99-39"><a href="#cb99-39"></a>  inputs &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">dense1 =</span> dense_input, </span>
<span id="cb99-40"><a href="#cb99-40"></a>                 <span class="dt">PolicyRegion =</span> PolicyRegion_input, </span>
<span id="cb99-41"><a href="#cb99-41"></a>                 <span class="dt">VehBrand =</span> VehBrand_input)</span>
<span id="cb99-42"><a href="#cb99-42"></a>  </span>
<span id="cb99-43"><a href="#cb99-43"></a>  model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(inputs, outputs)</span>
<span id="cb99-44"><a href="#cb99-44"></a>  </span>
<span id="cb99-45"><a href="#cb99-45"></a>  model <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-46"><a href="#cb99-46"></a><span class="st">    </span><span class="kw">compile</span>(<span class="dt">loss =</span> loss_poisson,</span>
<span id="cb99-47"><a href="#cb99-47"></a>            <span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(),</span>
<span id="cb99-48"><a href="#cb99-48"></a>            <span class="dt">weighted_metrics =</span> <span class="st">&quot;poisson&quot;</span>)</span>
<span id="cb99-49"><a href="#cb99-49"></a>  </span>
<span id="cb99-50"><a href="#cb99-50"></a>  <span class="kw">return</span>(model)</span>
<span id="cb99-51"><a href="#cb99-51"></a>}</span>
<span id="cb99-52"><a href="#cb99-52"></a></span>
<span id="cb99-53"><a href="#cb99-53"></a>neural_net &lt;-<span class="st"> </span><span class="kw">new_neural_net</span>()</span>
<span id="cb99-54"><a href="#cb99-54"></a></span>
<span id="cb99-55"><a href="#cb99-55"></a>neural_net <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-56"><a href="#cb99-56"></a><span class="st">  </span><span class="kw">summary</span>()</span>
<span id="cb99-57"><a href="#cb99-57"></a></span>
<span id="cb99-58"><a href="#cb99-58"></a>history &lt;-<span class="st"> </span>neural_net <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb99-59"><a href="#cb99-59"></a><span class="st">  </span><span class="kw">fit</span>(<span class="dt">x =</span> <span class="kw">prep_nn</span>(train, x), </span>
<span id="cb99-60"><a href="#cb99-60"></a>      <span class="dt">y =</span> train[, y], </span>
<span id="cb99-61"><a href="#cb99-61"></a>      <span class="dt">sample_weight =</span> train[, w],</span>
<span id="cb99-62"><a href="#cb99-62"></a>      <span class="dt">batch_size =</span> <span class="fl">1e4</span>, </span>
<span id="cb99-63"><a href="#cb99-63"></a>      <span class="dt">epochs =</span> <span class="dv">300</span>,</span>
<span id="cb99-64"><a href="#cb99-64"></a>      <span class="dt">verbose =</span> <span class="dv">2</span>)  </span>
<span id="cb99-65"><a href="#cb99-65"></a>    </span>
<span id="cb99-66"><a href="#cb99-66"></a><span class="kw">plot</span>(history)</span></code></pre></div>
<p>在最后一个隐藏层的十维输出上训练泊松GLM</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1"></a><span class="co"># Calibrate by using last hidden layer activations as GLM input encoder</span></span>
<span id="cb100-2"><a href="#cb100-2"></a>encoder &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> neural_net<span class="op">$</span>input, </span>
<span id="cb100-3"><a href="#cb100-3"></a>                       <span class="dt">outputs =</span> <span class="kw">get_layer</span>(neural_net, <span class="st">&quot;dense_2&quot;</span>)<span class="op">$</span>output)</span>
<span id="cb100-4"><a href="#cb100-4"></a></span>
<span id="cb100-5"><a href="#cb100-5"></a><span class="co"># Creates input for calibration GLM (extends prep_nn)</span></span>
<span id="cb100-6"><a href="#cb100-6"></a>prep_nn_calib &lt;-<span class="st"> </span><span class="cf">function</span>(dat, x, <span class="dt">cat_cols =</span> <span class="kw">c</span>(<span class="st">&quot;PolicyRegion&quot;</span>, <span class="st">&quot;VehBrand&quot;</span>), </span>
<span id="cb100-7"><a href="#cb100-7"></a>                          <span class="dt">enc =</span> encoder) {</span>
<span id="cb100-8"><a href="#cb100-8"></a>  <span class="kw">prep_nn</span>(dat, x, cat_cols) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb100-9"><a href="#cb100-9"></a><span class="st">    </span><span class="kw">predict</span>(enc, ., <span class="dt">batch_size =</span> <span class="fl">1e4</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb100-10"><a href="#cb100-10"></a><span class="st">    </span><span class="kw">data.frame</span>()</span>
<span id="cb100-11"><a href="#cb100-11"></a>}</span>
<span id="cb100-12"><a href="#cb100-12"></a></span>
<span id="cb100-13"><a href="#cb100-13"></a><span class="co"># Calibration GLM</span></span>
<span id="cb100-14"><a href="#cb100-14"></a>fit_nn &lt;-<span class="st"> </span><span class="kw">glm</span>(Freq <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb100-15"><a href="#cb100-15"></a>              <span class="dt">data =</span> <span class="kw">cbind</span>(train[<span class="st">&quot;Freq&quot;</span>], <span class="kw">prep_nn_calib</span>(train, x)), </span>
<span id="cb100-16"><a href="#cb100-16"></a>              <span class="dt">family =</span> <span class="kw">quasipoisson</span>(), </span>
<span id="cb100-17"><a href="#cb100-17"></a>              <span class="dt">weights =</span> train[[w]])</span></code></pre></div>
</div>
</div>
<div id="解释" class="section level3">
<h3><span class="header-section-number">8.10.5</span> 解释</h3>
<div id="为模型建立解释器flashlight并将它们组合为multiflashlight" class="section level4">
<h4><span class="header-section-number">8.10.5.1</span> 为模型建立解释器(flashlight)，并将它们组合为multiflashlight</h4>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb101-2"><a href="#cb101-2"></a></span>
<span id="cb101-3"><a href="#cb101-3"></a>fillc &lt;-<span class="st"> &quot;#E69F00&quot;</span></span>
<span id="cb101-4"><a href="#cb101-4"></a></span>
<span id="cb101-5"><a href="#cb101-5"></a>fl_glm &lt;-<span class="st"> </span><span class="kw">flashlight</span>(</span>
<span id="cb101-6"><a href="#cb101-6"></a>  <span class="dt">model =</span> fit_glm, <span class="dt">label =</span> <span class="st">&quot;GLM&quot;</span>, </span>
<span id="cb101-7"><a href="#cb101-7"></a>  <span class="dt">predict_function =</span> <span class="cf">function</span>(fit, X) <span class="kw">predict</span>(fit, X, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb101-8"><a href="#cb101-8"></a>)</span>
<span id="cb101-9"><a href="#cb101-9"></a></span>
<span id="cb101-10"><a href="#cb101-10"></a>fl_nn &lt;-<span class="st"> </span><span class="kw">flashlight</span>(</span>
<span id="cb101-11"><a href="#cb101-11"></a>  <span class="dt">model =</span> fit_nn, <span class="dt">label =</span> <span class="st">&quot;NNet&quot;</span>, </span>
<span id="cb101-12"><a href="#cb101-12"></a>  <span class="dt">predict_function =</span> <span class="cf">function</span>(fit, X) </span>
<span id="cb101-13"><a href="#cb101-13"></a>    <span class="kw">predict</span>(fit, <span class="kw">prep_nn_calib</span>(X, x), <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb101-14"><a href="#cb101-14"></a>)</span>
<span id="cb101-15"><a href="#cb101-15"></a></span>
<span id="cb101-16"><a href="#cb101-16"></a>fl_xgb &lt;-<span class="st"> </span><span class="kw">flashlight</span>(</span>
<span id="cb101-17"><a href="#cb101-17"></a>  <span class="dt">model =</span> fit_xgb, <span class="dt">label =</span> <span class="st">&quot;XGBoost&quot;</span>, </span>
<span id="cb101-18"><a href="#cb101-18"></a>  <span class="dt">predict_function =</span> <span class="cf">function</span>(fit, X) <span class="kw">predict</span>(fit, <span class="kw">prep_xgb</span>(X, x))</span>
<span id="cb101-19"><a href="#cb101-19"></a>)</span>
<span id="cb101-20"><a href="#cb101-20"></a></span>
<span id="cb101-21"><a href="#cb101-21"></a>fl_xgb_constraints &lt;-<span class="st"> </span><span class="kw">flashlight</span>(</span>
<span id="cb101-22"><a href="#cb101-22"></a>  <span class="dt">model =</span> fit_xgb_constraints, <span class="dt">label =</span> <span class="st">&quot;XGBoost_constraints&quot;</span>, </span>
<span id="cb101-23"><a href="#cb101-23"></a>  <span class="dt">predict_function =</span> <span class="cf">function</span>(fit, X) <span class="kw">predict</span>(fit, <span class="kw">prep_xgb</span>(X, x))</span>
<span id="cb101-24"><a href="#cb101-24"></a>)</span>
<span id="cb101-25"><a href="#cb101-25"></a></span>
<span id="cb101-26"><a href="#cb101-26"></a>fl_xgb_interaction_constraints &lt;-<span class="st"> </span><span class="kw">flashlight</span>(</span>
<span id="cb101-27"><a href="#cb101-27"></a>  <span class="dt">model =</span> fit_xgb_interaction_constraints, <span class="dt">label =</span> <span class="st">&quot;XGBoost_interaction_constraints&quot;</span>, </span>
<span id="cb101-28"><a href="#cb101-28"></a>  <span class="dt">predict_function =</span> <span class="cf">function</span>(fit, X) <span class="kw">predict</span>(fit, <span class="kw">prep_xgb</span>(X, x))</span>
<span id="cb101-29"><a href="#cb101-29"></a>)</span>
<span id="cb101-30"><a href="#cb101-30"></a></span>
<span id="cb101-31"><a href="#cb101-31"></a><span class="co"># Combine them and add common elements like reference data</span></span>
<span id="cb101-32"><a href="#cb101-32"></a>metrics &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="st">`</span><span class="dt">Average deviance</span><span class="st">`</span> =<span class="st"> </span>deviance_poisson, </span>
<span id="cb101-33"><a href="#cb101-33"></a>                <span class="st">`</span><span class="dt">Relative deviance reduction</span><span class="st">`</span> =<span class="st"> </span>r_squared_poisson)</span>
<span id="cb101-34"><a href="#cb101-34"></a>fls &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(<span class="kw">list</span>(fl_glm, fl_nn, fl_xgb), <span class="dt">data =</span> test, </span>
<span id="cb101-35"><a href="#cb101-35"></a>                       <span class="dt">y =</span> y, <span class="dt">w =</span> w, <span class="dt">metrics =</span> metrics)</span>
<span id="cb101-36"><a href="#cb101-36"></a></span>
<span id="cb101-37"><a href="#cb101-37"></a>fls_xgb_constraints &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(<span class="kw">list</span>(fl_xgb,fl_xgb_constraints), <span class="dt">data =</span> test, </span>
<span id="cb101-38"><a href="#cb101-38"></a>                       <span class="dt">y =</span> y, <span class="dt">w =</span> w, <span class="dt">metrics =</span> metrics)</span>
<span id="cb101-39"><a href="#cb101-39"></a></span>
<span id="cb101-40"><a href="#cb101-40"></a>fls_xgb_interaction_constraints &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(<span class="kw">list</span>(fl_xgb_interaction_constraints), <span class="dt">data =</span> test, </span>
<span id="cb101-41"><a href="#cb101-41"></a>                       <span class="dt">y =</span> y, <span class="dt">w =</span> w, <span class="dt">metrics =</span> metrics)</span>
<span id="cb101-42"><a href="#cb101-42"></a></span>
<span id="cb101-43"><a href="#cb101-43"></a>fls_interaction_constraints &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(<span class="kw">list</span>(fl_glm, fl_nn, fl_xgb,fl_xgb_interaction_constraints), <span class="dt">data =</span> test, </span>
<span id="cb101-44"><a href="#cb101-44"></a>                       <span class="dt">y =</span> y, <span class="dt">w =</span> w, <span class="dt">metrics =</span> metrics)</span>
<span id="cb101-45"><a href="#cb101-45"></a></span>
<span id="cb101-46"><a href="#cb101-46"></a><span class="co"># Version on canonical scale</span></span>
<span id="cb101-47"><a href="#cb101-47"></a>fls_log &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(fls, <span class="dt">linkinv =</span> log)</span>
<span id="cb101-48"><a href="#cb101-48"></a>fls_xgb_interaction_constraints_log &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(fls_xgb_constraints, <span class="dt">linkinv =</span> log)</span>
<span id="cb101-49"><a href="#cb101-49"></a>fls_interaction_constraints_log &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(fls_interaction_constraints, <span class="dt">linkinv =</span> log)</span></code></pre></div>
</div>
<div id="对flashlight应用可解释性函数" class="section level4">
<h4><span class="header-section-number">8.10.5.2</span> 对flashlight应用可解释性函数</h4>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1"></a><span class="co">#模型表现</span></span>
<span id="cb102-2"><a href="#cb102-2"></a></span>
<span id="cb102-3"><a href="#cb102-3"></a>perf &lt;-<span class="st"> </span><span class="kw">light_performance</span>(fls)</span>
<span id="cb102-4"><a href="#cb102-4"></a>perf</span>
<span id="cb102-5"><a href="#cb102-5"></a><span class="kw">plot</span>(perf, <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>) <span class="op">+</span></span>
<span id="cb102-6"><a href="#cb102-6"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">element_blank</span>(), <span class="dt">y =</span> <span class="kw">element_blank</span>())</span>
<span id="cb102-7"><a href="#cb102-7"></a></span>
<span id="cb102-8"><a href="#cb102-8"></a><span class="co">#变量重要性</span></span>
<span id="cb102-9"><a href="#cb102-9"></a></span>
<span id="cb102-10"><a href="#cb102-10"></a>imp &lt;-<span class="st"> </span><span class="kw">light_importance</span>(fls, <span class="dt">v =</span> x)</span>
<span id="cb102-11"><a href="#cb102-11"></a><span class="kw">plot</span>(imp, <span class="dt">fill =</span> fillc, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
</div>
<div id="ice曲线" class="section level4">
<h4><span class="header-section-number">8.10.5.3</span> ICE曲线</h4>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1"></a><span class="co">#driveage基于中心化与非中心化和是否使用对数被解释变量作为预测结果</span></span>
<span id="cb103-2"><a href="#cb103-2"></a></span>
<span id="cb103-3"><a href="#cb103-3"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>)</span>
<span id="cb103-4"><a href="#cb103-4"></a>     , <span class="dt">alpha =</span> <span class="fl">0.1</span>)</span>
<span id="cb103-5"><a href="#cb103-5"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>, <span class="dt">center =</span> <span class="st">&quot;middle&quot;</span>)</span>
<span id="cb103-6"><a href="#cb103-6"></a>     , <span class="dt">alpha =</span> <span class="fl">0.03</span>)</span>
<span id="cb103-7"><a href="#cb103-7"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls_log, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>)</span>
<span id="cb103-8"><a href="#cb103-8"></a>     , <span class="dt">alpha =</span> <span class="fl">0.1</span>)</span>
<span id="cb103-9"><a href="#cb103-9"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls_log, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>, <span class="dt">center =</span> <span class="st">&quot;middle&quot;</span>)</span>
<span id="cb103-10"><a href="#cb103-10"></a>     , <span class="dt">alpha =</span> <span class="fl">0.03</span>)</span>
<span id="cb103-11"><a href="#cb103-11"></a></span>
<span id="cb103-12"><a href="#cb103-12"></a><span class="co">#考虑单调性约束的xgboost</span></span>
<span id="cb103-13"><a href="#cb103-13"></a></span>
<span id="cb103-14"><a href="#cb103-14"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls_xgb_constraints, <span class="dt">v =</span> <span class="st">&quot;VehAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>)</span>
<span id="cb103-15"><a href="#cb103-15"></a>     , <span class="dt">alpha =</span> <span class="fl">0.1</span>)</span>
<span id="cb103-16"><a href="#cb103-16"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls_xgb_constraints, <span class="dt">v =</span> <span class="st">&quot;VehAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>, <span class="dt">center =</span> <span class="st">&quot;middle&quot;</span>)</span>
<span id="cb103-17"><a href="#cb103-17"></a>     , <span class="dt">alpha =</span> <span class="fl">0.03</span>)</span>
<span id="cb103-18"><a href="#cb103-18"></a></span>
<span id="cb103-19"><a href="#cb103-19"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls_xgb_constraints, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>)</span>
<span id="cb103-20"><a href="#cb103-20"></a>     , <span class="dt">alpha =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1"></a><span class="co"># Partial dependence curves</span></span>
<span id="cb104-2"><a href="#cb104-2"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls, <span class="dt">v =</span> <span class="st">&quot;VehAge&quot;</span>, <span class="dt">pd_evaluate_at =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">20</span>))</span>
<span id="cb104-3"><a href="#cb104-3"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">n_bins =</span> <span class="dv">25</span>))</span>
<span id="cb104-4"><a href="#cb104-4"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls, <span class="dt">v =</span> <span class="st">&quot;logDensity&quot;</span>))</span>
<span id="cb104-5"><a href="#cb104-5"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls, <span class="dt">v =</span> <span class="st">&quot;VehGas&quot;</span>))</span>
<span id="cb104-6"><a href="#cb104-6"></a></span>
<span id="cb104-7"><a href="#cb104-7"></a><span class="co"># ALE versus partial dependence</span></span>
<span id="cb104-8"><a href="#cb104-8"></a>ale_DrivAge &lt;-<span class="st"> </span><span class="kw">light_effects</span>(fls, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">counts_weighted =</span> <span class="ot">TRUE</span>,</span>
<span id="cb104-9"><a href="#cb104-9"></a>                             <span class="dt">v_labels =</span> <span class="ot">FALSE</span>, <span class="dt">n_bins =</span> <span class="dv">20</span>, <span class="dt">cut_type =</span> <span class="st">&quot;quantile&quot;</span>)</span>
<span id="cb104-10"><a href="#cb104-10"></a><span class="kw">plot</span>(ale_DrivAge, <span class="dt">use =</span> <span class="kw">c</span>(<span class="st">&quot;pd&quot;</span>, <span class="st">&quot;ale&quot;</span>), <span class="dt">show_points =</span> <span class="ot">FALSE</span>)</span>
<span id="cb104-11"><a href="#cb104-11"></a></span>
<span id="cb104-12"><a href="#cb104-12"></a><span class="co"># Classic diagnostic plots</span></span>
<span id="cb104-13"><a href="#cb104-13"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls, <span class="dt">v =</span> <span class="st">&quot;VehAge&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;predicted&quot;</span>))</span>
<span id="cb104-14"><a href="#cb104-14"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls, <span class="dt">v =</span> <span class="st">&quot;VehAge&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;residual&quot;</span>)) <span class="op">+</span></span>
<span id="cb104-15"><a href="#cb104-15"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>)</span>
<span id="cb104-16"><a href="#cb104-16"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls, <span class="dt">v =</span> <span class="st">&quot;VehAge&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))</span>
<span id="cb104-17"><a href="#cb104-17"></a></span>
<span id="cb104-18"><a href="#cb104-18"></a></span>
<span id="cb104-19"><a href="#cb104-19"></a><span class="co"># Multiple aspects combined</span></span>
<span id="cb104-20"><a href="#cb104-20"></a>eff_DrivAge &lt;-<span class="st"> </span><span class="kw">light_effects</span>(fls, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">counts_weighted =</span> <span class="ot">TRUE</span>)</span>
<span id="cb104-21"><a href="#cb104-21"></a>p &lt;-<span class="st"> </span><span class="kw">plot</span>(eff_DrivAge, <span class="dt">show_points =</span> <span class="ot">FALSE</span>)</span>
<span id="cb104-22"><a href="#cb104-22"></a><span class="kw">plot_counts</span>(p, eff_DrivAge, <span class="dt">alpha =</span> <span class="fl">0.3</span>)</span></code></pre></div>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1"></a><span class="co"># Interaction (relative)</span></span>
<span id="cb105-2"><a href="#cb105-2"></a>interact_rel &lt;-<span class="st"> </span><span class="kw">light_interaction</span>(</span>
<span id="cb105-3"><a href="#cb105-3"></a>  fls_interaction_constraints_log, </span>
<span id="cb105-4"><a href="#cb105-4"></a>  <span class="dt">v =</span> <span class="kw">most_important</span>(imp, <span class="dv">4</span>), </span>
<span id="cb105-5"><a href="#cb105-5"></a>  <span class="dt">take_sqrt =</span> <span class="ot">FALSE</span>,</span>
<span id="cb105-6"><a href="#cb105-6"></a>  <span class="dt">pairwise =</span> <span class="ot">TRUE</span>, </span>
<span id="cb105-7"><a href="#cb105-7"></a>  <span class="dt">use_linkinv =</span> <span class="ot">TRUE</span>,</span>
<span id="cb105-8"><a href="#cb105-8"></a>  <span class="dt">seed =</span> <span class="dv">61</span></span>
<span id="cb105-9"><a href="#cb105-9"></a>)</span>
<span id="cb105-10"><a href="#cb105-10"></a><span class="kw">plot</span>(interact_rel, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">fill =</span> fillc, <span class="dt">rotate_x =</span> <span class="ot">TRUE</span>)</span>
<span id="cb105-11"><a href="#cb105-11"></a></span>
<span id="cb105-12"><a href="#cb105-12"></a><span class="co"># Interaction (absolute)</span></span>
<span id="cb105-13"><a href="#cb105-13"></a>interact_abs &lt;-<span class="st"> </span><span class="kw">light_interaction</span>(</span>
<span id="cb105-14"><a href="#cb105-14"></a>  fls_interaction_constraints_log, </span>
<span id="cb105-15"><a href="#cb105-15"></a>  <span class="dt">v =</span> <span class="kw">most_important</span>(imp, <span class="dv">4</span>), </span>
<span id="cb105-16"><a href="#cb105-16"></a>  <span class="dt">normalize =</span> <span class="ot">FALSE</span>,</span>
<span id="cb105-17"><a href="#cb105-17"></a>  <span class="dt">pairwise =</span> <span class="ot">TRUE</span>, </span>
<span id="cb105-18"><a href="#cb105-18"></a>  <span class="dt">use_linkinv =</span> <span class="ot">TRUE</span>,</span>
<span id="cb105-19"><a href="#cb105-19"></a>  <span class="dt">seed =</span> <span class="dv">61</span></span>
<span id="cb105-20"><a href="#cb105-20"></a>)</span>
<span id="cb105-21"><a href="#cb105-21"></a><span class="kw">plot</span>(interact_abs, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">fill =</span> fillc, <span class="dt">rotate_x =</span> <span class="ot">TRUE</span>)</span>
<span id="cb105-22"><a href="#cb105-22"></a></span>
<span id="cb105-23"><a href="#cb105-23"></a><span class="co"># Filter on largest three brands</span></span>
<span id="cb105-24"><a href="#cb105-24"></a>sub_data &lt;-<span class="st"> </span>test <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb105-25"><a href="#cb105-25"></a><span class="st">  </span><span class="kw">filter</span>(VehBrand <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;B1&quot;</span>, <span class="st">&quot;B2&quot;</span>, <span class="st">&quot;B12&quot;</span>))</span>
<span id="cb105-26"><a href="#cb105-26"></a></span>
<span id="cb105-27"><a href="#cb105-27"></a><span class="co"># Strong interaction</span></span>
<span id="cb105-28"><a href="#cb105-28"></a>pdp_vehAge_Brand &lt;-<span class="st"> </span><span class="kw">light_profile</span>(fls_interaction_constraints_log, <span class="dt">v =</span> <span class="st">&quot;VehAge&quot;</span>, <span class="dt">by =</span> <span class="st">&quot;VehBrand&quot;</span>, </span>
<span id="cb105-29"><a href="#cb105-29"></a>                                  <span class="dt">pd_seed =</span> <span class="dv">50</span>, <span class="dt">data =</span> sub_data)</span>
<span id="cb105-30"><a href="#cb105-30"></a><span class="kw">plot</span>(pdp_vehAge_Brand)</span>
<span id="cb105-31"><a href="#cb105-31"></a></span>
<span id="cb105-32"><a href="#cb105-32"></a><span class="co"># Weak interaction</span></span>
<span id="cb105-33"><a href="#cb105-33"></a>pdp_DrivAge_Gas &lt;-<span class="st"> </span><span class="kw">light_profile</span>(fls_interaction_constraints_log, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, </span>
<span id="cb105-34"><a href="#cb105-34"></a>                                 <span class="dt">by =</span> <span class="st">&quot;VehGas&quot;</span>, <span class="dt">pd_seed =</span> <span class="dv">50</span>)</span>
<span id="cb105-35"><a href="#cb105-35"></a><span class="kw">plot</span>(pdp_DrivAge_Gas)</span>
<span id="cb105-36"><a href="#cb105-36"></a></span>
<span id="cb105-37"><a href="#cb105-37"></a></span>
<span id="cb105-38"><a href="#cb105-38"></a><span class="kw">plot</span>(<span class="kw">light_ice</span>(fls_xgb_interaction_constraints, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">n_max =</span> <span class="dv">200</span>, <span class="dt">seed =</span> <span class="dv">3</span>)</span>
<span id="cb105-39"><a href="#cb105-39"></a>     , <span class="dt">alpha =</span> <span class="fl">0.1</span>)</span></code></pre></div>
</div>
<div id="全局代理模型" class="section level4">
<h4><span class="header-section-number">8.10.5.4</span> 全局代理模型</h4>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1"></a>surr_nn &lt;-<span class="st"> </span><span class="kw">light_global_surrogate</span>(fls_log<span class="op">$</span>NNet, <span class="dt">v =</span> x)</span>
<span id="cb106-2"><a href="#cb106-2"></a><span class="kw">plot</span>(surr_nn)</span>
<span id="cb106-3"><a href="#cb106-3"></a></span>
<span id="cb106-4"><a href="#cb106-4"></a>surr_xgb &lt;-<span class="st"> </span><span class="kw">light_global_surrogate</span>(fls_log<span class="op">$</span>XGBoost, <span class="dt">v =</span> x)</span>
<span id="cb106-5"><a href="#cb106-5"></a><span class="kw">plot</span>(surr_xgb)</span></code></pre></div>
</div>
</div>
<div id="局部性质" class="section level3">
<h3><span class="header-section-number">8.10.6</span> 局部性质</h3>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1"></a><span class="co">#局部性质</span></span>
<span id="cb107-2"><a href="#cb107-2"></a></span>
<span id="cb107-3"><a href="#cb107-3"></a>new_obs &lt;-<span class="st"> </span>test[<span class="dv">1</span>, ]</span>
<span id="cb107-4"><a href="#cb107-4"></a>new_obs[, x]</span>
<span id="cb107-5"><a href="#cb107-5"></a><span class="kw">unlist</span>(<span class="kw">predict</span>(fls, <span class="dt">data =</span> new_obs))</span>
<span id="cb107-6"><a href="#cb107-6"></a></span>
<span id="cb107-7"><a href="#cb107-7"></a><span class="co"># Breakdown</span></span>
<span id="cb107-8"><a href="#cb107-8"></a>bd &lt;-<span class="st"> </span><span class="kw">light_breakdown</span>(fls<span class="op">$</span>XGBoost, <span class="dt">new_obs =</span> new_obs, </span>
<span id="cb107-9"><a href="#cb107-9"></a>                      <span class="dt">v =</span> x, <span class="dt">n_max =</span> <span class="dv">1000</span>, <span class="dt">seed =</span> <span class="dv">20</span>)</span>
<span id="cb107-10"><a href="#cb107-10"></a><span class="kw">plot</span>(bd)</span>
<span id="cb107-11"><a href="#cb107-11"></a></span>
<span id="cb107-12"><a href="#cb107-12"></a><span class="co"># Extract same order of variables for visualization only</span></span>
<span id="cb107-13"><a href="#cb107-13"></a>v &lt;-<span class="st"> </span><span class="kw">setdiff</span>(bd<span class="op">$</span>data<span class="op">$</span>variable, <span class="kw">c</span>(<span class="st">&quot;baseline&quot;</span>, <span class="st">&quot;prediction&quot;</span>))</span>
<span id="cb107-14"><a href="#cb107-14"></a></span>
<span id="cb107-15"><a href="#cb107-15"></a><span class="co"># Approximate SHAP</span></span>
<span id="cb107-16"><a href="#cb107-16"></a>shap &lt;-<span class="st"> </span><span class="kw">light_breakdown</span>(fls<span class="op">$</span>XGBoost, new_obs, </span>
<span id="cb107-17"><a href="#cb107-17"></a>                        <span class="dt">visit_strategy =</span> <span class="st">&quot;permutation&quot;</span>,</span>
<span id="cb107-18"><a href="#cb107-18"></a>                        <span class="dt">v =</span> v, <span class="dt">n_max =</span> <span class="dv">1000</span>, <span class="dt">seed =</span> <span class="dv">20</span>)</span>
<span id="cb107-19"><a href="#cb107-19"></a><span class="kw">plot</span>(shap)</span>
<span id="cb107-20"><a href="#cb107-20"></a></span>
<span id="cb107-21"><a href="#cb107-21"></a>fl_with_shap &lt;-<span class="st"> </span><span class="kw">add_shap</span>(fls<span class="op">$</span>XGBoost, <span class="dt">v =</span> x, <span class="dt">n_shap =</span> <span class="dv">500</span>, </span>
<span id="cb107-22"><a href="#cb107-22"></a>                         <span class="dt">n_perm =</span> <span class="dv">12</span>, <span class="dt">n_max =</span> <span class="dv">1000</span>, <span class="dt">seed =</span> <span class="dv">100</span>)</span>
<span id="cb107-23"><a href="#cb107-23"></a></span>
<span id="cb107-24"><a href="#cb107-24"></a><span class="co">#以局部性质估计模型的全局性质</span></span>
<span id="cb107-25"><a href="#cb107-25"></a></span>
<span id="cb107-26"><a href="#cb107-26"></a><span class="kw">plot</span>(<span class="kw">light_importance</span>(fl_with_shap, <span class="dt">v =</span> x, <span class="dt">type =</span> <span class="st">&quot;shap&quot;</span>), </span>
<span id="cb107-27"><a href="#cb107-27"></a>     <span class="dt">fill =</span> fillc, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb107-28"><a href="#cb107-28"></a><span class="kw">plot</span>(<span class="kw">light_scatter</span>(fl_with_shap, <span class="dt">v =</span> <span class="st">&quot;DrivAge&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;shap&quot;</span>), <span class="dt">alpha =</span> <span class="fl">0.3</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="./plots/8/age.png" alt="LIME" width="50%"  />
<p class="caption">
(#fig:age)LIME
</p>
</div>
</div>
<div id="改进glm" class="section level3">
<h3><span class="header-section-number">8.10.7</span> 改进glm</h3>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1"></a>fit_glm2 &lt;-<span class="st"> </span><span class="kw">glm</span>(Freq <span class="op">~</span><span class="st"> </span>VehPower <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">*</span><span class="st"> </span>VehGas <span class="op">+</span><span class="st"> </span>PolicyRegion <span class="op">+</span><span class="st"> </span></span>
<span id="cb108-2"><a href="#cb108-2"></a><span class="st">                  </span><span class="kw">ns</span>(DrivAge, <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span>VehBrand <span class="op">*</span><span class="st"> </span><span class="kw">ns</span>(VehAge, <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb108-3"><a href="#cb108-3"></a><span class="st">                  </span><span class="kw">ns</span>(logDensity, <span class="dv">5</span>), </span>
<span id="cb108-4"><a href="#cb108-4"></a>                <span class="dt">data =</span> train, </span>
<span id="cb108-5"><a href="#cb108-5"></a>                <span class="dt">family =</span> <span class="kw">quasipoisson</span>(), </span>
<span id="cb108-6"><a href="#cb108-6"></a>                <span class="dt">weights =</span> train[[w]])</span>
<span id="cb108-7"><a href="#cb108-7"></a></span>
<span id="cb108-8"><a href="#cb108-8"></a><span class="co"># Setting up expainers</span></span>
<span id="cb108-9"><a href="#cb108-9"></a>fl_glm2 &lt;-<span class="st"> </span><span class="kw">flashlight</span>(</span>
<span id="cb108-10"><a href="#cb108-10"></a>  <span class="dt">model =</span> fit_glm2, <span class="dt">label =</span> <span class="st">&quot;Improved GLM&quot;</span>, </span>
<span id="cb108-11"><a href="#cb108-11"></a>  <span class="dt">predict_function =</span> <span class="cf">function</span>(fit, X) <span class="kw">predict</span>(fit, X, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb108-12"><a href="#cb108-12"></a>)</span>
<span id="cb108-13"><a href="#cb108-13"></a></span>
<span id="cb108-14"><a href="#cb108-14"></a><span class="co"># Combine them and add common elements like reference data</span></span>
<span id="cb108-15"><a href="#cb108-15"></a>fls2 &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(<span class="kw">list</span>(fl_glm, fl_glm2, fl_nn, fl_xgb), </span>
<span id="cb108-16"><a href="#cb108-16"></a>                        <span class="dt">metrics =</span> metrics, <span class="dt">data =</span> test, <span class="dt">y =</span> y, <span class="dt">w =</span> w)</span>
<span id="cb108-17"><a href="#cb108-17"></a>fls2_log &lt;-<span class="st"> </span><span class="kw">multiflashlight</span>(fls2, <span class="dt">linkinv =</span> log)</span>
<span id="cb108-18"><a href="#cb108-18"></a></span>
<span id="cb108-19"><a href="#cb108-19"></a><span class="co"># Some results</span></span>
<span id="cb108-20"><a href="#cb108-20"></a><span class="kw">plot</span>(<span class="kw">light_performance</span>(fls2), <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="dt">rotate_x =</span> <span class="ot">TRUE</span>)</span>
<span id="cb108-21"><a href="#cb108-21"></a><span class="kw">plot</span>(<span class="kw">light_importance</span>(fls2, <span class="dt">v =</span> x), <span class="dt">fill =</span> fillc, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">top_m =</span> <span class="dv">4</span>)</span>
<span id="cb108-22"><a href="#cb108-22"></a><span class="kw">plot</span>(<span class="kw">light_profile</span>(fls2, <span class="dt">v =</span> <span class="st">&quot;logDensity&quot;</span>))</span>
<span id="cb108-23"><a href="#cb108-23"></a>interact_rel_improved &lt;-<span class="st"> </span><span class="kw">light_interaction</span>(</span>
<span id="cb108-24"><a href="#cb108-24"></a>  fls2_log, <span class="dt">v =</span> <span class="kw">most_important</span>(imp, <span class="dv">4</span>), <span class="dt">take_sqrt =</span> <span class="ot">FALSE</span>,</span>
<span id="cb108-25"><a href="#cb108-25"></a>  <span class="dt">pairwise =</span> <span class="ot">TRUE</span>,  <span class="dt">use_linkinv =</span> <span class="ot">TRUE</span>, <span class="dt">seed =</span> <span class="dv">61</span>)</span>
<span id="cb108-26"><a href="#cb108-26"></a><span class="kw">plot</span>(interact_rel_improved, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">fill =</span> fillc, <span class="dt">top_m =</span> <span class="dv">4</span>)</span></code></pre></div>
<!--chapter:end:08-flashlight.Rmd-->
</div>
</div>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
