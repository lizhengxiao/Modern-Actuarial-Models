[["index.html", "ç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ ğŸ‘¨ğŸ« æ¬¢è¿ ğŸ¤” ç­”ç–‘", " ç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ Modern Actuarial Models 2021-06-02 15:04:56 ğŸ‘¨ğŸ« æ¬¢è¿ ã€Šç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ã€‹ä¸»è¦è®²è¿°å¦‚ä½•ä½¿ç”¨ç»Ÿè®¡å­¦ä¹ å’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæå‡ä¼ ç»Ÿçš„ç²¾ç®—ç»Ÿè®¡æ¨¡å‹æˆ–è€…è§£å†³æ–°çš„ç²¾ç®—é—®é¢˜ã€‚è¯¥æ•™ç¨‹çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºå¤§å®¶æä¾›ä¸€ä¸ªå¯¹æ•°æ®ç§‘å­¦å…¨é¢ä¸”æ˜“æ‡‚çš„ä»‹ç»ï¼Œè¯¥æ•™ç¨‹æä¾›äº†å¤šç¯‡æ–¹æ³•æ€§æ–‡ç« å¹¶å¼€æºä»£ç ï¼Œè¿™æ ·è¯»è€…å¯ä»¥ç›¸å¯¹å®¹æ˜“åœ°æŠŠè¿™äº›æ•°æ®ç§‘å­¦æ–¹æ³•ç”¨åœ¨è‡ªå·±çš„æ•°æ®ä¸Šã€‚ æˆ‘ä»¬å»ºè®®å¤§å®¶ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡çŒ®ï¼Œå°è¯•å¹¶ç†è§£æ‰€æœ‰ä»£ç ã€‚æ­¤ç½‘ç«™å°†ä½œä¸ºè¯¥è¯¾ç¨‹çš„è¾…åŠ©ï¼Œä¸ºå¤§å®¶ç­”ç–‘ï¼Œæ€»ç»“æ–‡çŒ®ï¼Œå¹¶å¯¹æ–‡çŒ®ä¸­çš„æ–¹æ³•åšæ‰©å±•ã€‚è¯¥ç½‘ç«™ç”±æˆè¯¾è€å¸ˆé«˜å…‰è¿œå’ŒåŠ©æ•™å¼ ç®é’°ç®¡ç†ï¼Œæ¬¢è¿å¤§å®¶åé¦ˆæ„è§åˆ°åŠ©æ•™ã€å¾®ä¿¡ç¾¤ã€æˆ–é‚®ç®± guangyuan.gao@ruc.edu.cnã€‚ ğŸ¤” ç­”ç–‘ æˆ‘å®šæœŸæŠŠåŒå­¦ä»¬çš„æ™®éç–‘é—®åœ¨è¿™é‡Œè§£ç­”ï¼Œæ¬¢è¿æé—®ï¼ ğŸ‘‰ Tensorflow for Apple M1 (2020/12/23) è´­ä¹°Apple M1çš„åŒå­¦éœ€è¦ç”¨è¿™ä¸ªpre-release tensorflowï¼Œä»pypiä¸‹è½½çš„tensorflowæš‚ä¸æ”¯æŒApple M1 ğŸ‘‰ NLP (2020/12/18) æ•°æ® è¿™ä¸ªæ•°æ®ç¬¬\\(i\\)è¡Œ\\(j\\)åˆ—è¡¨ç¤ºï¼Œåœ¨ç¬¬\\(i\\)ä¸ªè¯„è®ºä¸­ç¬¬\\(j\\)ä¸ªè¯çš„æ’å(ä¾ç…§æ€»å‡ºç°é¢‘ç‡)ï¼Œæ‰€ä»¥æ¯ä¸€è¡Œè¿˜ä¿æŒäº†å¥å­ä¸­è¯è¯­çš„å…ˆåé¡ºåºã€‚æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—æ•°æ®ï¼ˆæ ·æœ¬ï¼‰ã€‚ LSTM inputç»´åº¦æ˜¯batch size * length * 1ï¼Œå³ä»¥ä¸Šæ‰€ç¤ºçš„.csvçŸ©é˜µæ–‡æ¡£ã€‚ embedding_3 ä½œç”¨å°±æ˜¯æŠŠinputçš„æœ€åä¸€ä¸ªç»´åº¦çˆ†ç‚¸åˆ°256ï¼Œå‚æ•°ä¸ªæ•°ä¸ºvocab_size* embedding dimensionï¼Œå¯ä»¥çœ‹ä½œæŠŠ400ä¸ªé«˜é¢‘è¯æ˜ å°„åˆ°256ç»´ç©ºé—´ã€‚ embedding_3å’Œlstm_2è¾“å‡ºç»´åº¦ä¸­ï¼Œæœ‰ä¸¤ä¸ªnone,å…¶ä¸­ç¬¬ä¸€ä¸ªè¡¨ç¤ºbatch size, ç¬¬äºŒä¸ªè¡¨ç¤ºsequence lengthã€‚å› ä¸ºLSTMåœ¨æ—¶é—´ç»´åº¦ä¸Šå¾ªç¯ä½¿ç”¨å‚æ•°ï¼Œæ‰€ä»¥sequence lengthä¸å½±å“å‚æ•°çš„ä¸ªæ•°ã€‚ sequence lengthä¸å½±å“å‚æ•°ä¸ªæ•°ï¼Œå¯¹äºä¸åŒçš„å¥å­é•¿åº¦å¦‚100æˆ–è€…150ï¼Œè¯¥æ¨¡å‹éƒ½ä¸éœ€è¦è°ƒæ•´ï¼Œ(åº”è¯¥)å¯ä»¥ç›´æ¥è½½å…¥æ•°æ®è®­ç»ƒã€‚ lstm_3 åªæœ‰ä¸€ä¸ªnone, è¡¨ç¤ºbatch size, æˆ‘ä»¬è¦æ±‚lstm_3ä¸è¿”å›æ•´ä¸ªsequenceåªçœ‹æœ€è¿‘çš„çŠ¶æ€ã€‚ ğŸ‘‰ Reproducible results using Keras (2020/12/11) ä½¿ç”¨Keraså¤ç°ç»“æœçš„æ–¹æ³•ã€‚ https://cran.r-project.org/web/packages/keras/vignettes/faq.html ğŸ‘‰ ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨reluè§£å†³vanishing gradient è€Œè®¾è®¡å¤æ‚çš„lstm gru (2020/12/11) reluå€¼åŸŸåœ¨0åˆ°æ— ç©·ï¼Œä¸å¦‚tanhå’Œsigmoidç¨³å¥ï¼Œåä¸¤ç§å¯ä»¥è®¤ä¸ºæŠŠæå¤§æå°å€¼éƒ½æˆªæ–­äº†ã€‚ reluæ˜¯çº¿æ€§å˜æ¢ï¼Œå¯èƒ½æè¿°ä¸äº†éçº¿æ€§æ•ˆåº”ã€‚æˆ‘æœ€å¸¸ç”¨tanhã€‚ å½“ç„¶ï¼Œä½¿ç”¨reluä¼šæ˜æ˜¾æå‡è®¡ç®—é€Ÿåº¦ï¼Œå› ä¸ºreluçš„å¯¼æ•°å®¹æ˜“è®¡ç®—ã€‚ å¦å‚è§stackexchange ğŸ‘‰ xaringan (2020/12/06) htmlæ ¼å¼çš„slidesï¼š https://slides.yihui.org/xaringan/zh-CN.html#1 ğŸ‘‰ samme.r (2020/11/27) å…³äºsamme.rç®—æ³•, è¯·å‚è€ƒä¸‹é¢æ–‡ç« ä¸­çš„exponential loss function. https://web.stanford.edu/~hastie/Papers/samme.pdf ç®—æ³•samme.rä»…åœ¨ä»¥ä¸Šdraftä¸­å‡ºç°ï¼Œæ­£å¼å‘è¡¨æ—¶samme.rè¢«åˆ æ‰äº†ï¼Œæ¨æµ‹å®¡ç¨¿äººæœ‰å¼‚è®®ã€‚æ­£å¼æ–‡ç« å‚è€ƒ http://ww.web.stanford.edu/~hastie/Papers/SII-2-3-A8-Zhu.pdf ğŸ‘‰ éšæœºç§å­æ•° (2020/11/20) è¾“å…¥RNGversion(\"3.5.0\"); set.seed(100)ï¼Œä½¿å¾—ä½ çš„éšæœºç§å­æ•°å’Œpaperçš„ç›¸åŒï¼Œæ¨¡å‹ç»“æœç›¸è¿‘ã€‚ ğŸ‘‰ MAC OS, Linux, WIN (2020/11/16) æ®è§‚å¯Ÿï¼Œåœ¨MAC OSå’ŒLinuxç³»ç»Ÿä¸‹å®‰è£…kerasæˆåŠŸçš„æ¯”ä¾‹è¾ƒé«˜ã€‚WINç³»ç»Ÿä¸‹ï¼ŒPythonå„ä¸ªåŒ…çš„ä¾èµ–ä»¥åŠå’ŒRåŒ…çš„åŒ¹é…æœ‰ä¸€å®šçš„é—®é¢˜ï¼Œä»Šå¤©æ˜¯é€šè¿‡æ›´æ¢é•œåƒæºè§£å†³äº†Rä¸­æ— æ³•åŠ è½½tensorflow.kerasæ¨¡å—çš„é—®é¢˜ï¼Œæ¨æµ‹æ˜¯TUNAæºä¸­WINåŒ…ä¾èµ–å…³ç³»æ²¡æœ‰åŠæ—¶æ›´æ–°ã€‚ ä¸ºäº†è§£å†³é•œåƒæºæ›´æ–°å»¶è¿Ÿã€æˆ–è€…tensorflowç‰ˆæœ¬è¿‡ä½çš„é—®é¢˜ï¼Œè¿™é‡Œå…±äº«WINä¸‹ç»æµ‹è¯•çš„condaç¯å¢ƒé…ç½®ã€‚ä¸‹è½½è¯¥æ–‡æ¡£ï¼Œä»è¯¥æ–‡æ¡£æ‰€åœ¨æ–‡ä»¶å¤¹å¯åŠ¨å‘½ä»¤è¡Œï¼Œä½¿ç”¨å‘½ä»¤conda env create --name &lt;env&gt; --file filename.yamlï¼Œå®‰è£…è¯¥condaç¯å¢ƒã€‚åœ¨Rä¸­ä½¿ç”¨reticulate::use_condaenv(\"&lt;env&gt;\",required=T)å…³è”è¯¥ç¯å¢ƒã€‚ å¦å¤–ï¼Œå¯ä¸‹è½½MAC OSç³»ç»Ÿä¸‹ç»æµ‹è¯•çš„condaç¯å¢ƒé…ç½®ã€‚å¯é€šè¿‡conda env create --name &lt;env&gt; --file filename.yamlå®‰è£…ã€‚ ğŸ‘‰ CASdatasets (2020/11/13) æºæ–‡ä»¶åœ¨http://cas.uqam.ca/ï¼Œä½†ä¸‹è½½é€Ÿåº¦å¾ˆæ…¢ï¼Œæˆ‘æŠŠå®ƒæ”¾åœ¨åšæœäº‘å…±äº«ã€‚ä¸‹è½½åé€‰æ‹©install from local archive fileã€‚ "],["intro.html", "ç®€ä»‹", " ç®€ä»‹ ä¿é™©å…¬å¸ä¸ºç¤¾ä¼šä¸­æŸäº›ä¸å¯é¢„çŸ¥çš„ç»æµæŸå¤±å¸¦æ¥äº†ä¿éšœã€‚ä¿é™©å…¬å¸æ‰¿æ‹…äº†è¢«ä¿é™©äººçš„ä¸ç¡®å®šç»æµæŸå¤±çš„é£é™©ï¼Œè¢«ä¿é™©äººè·å¾—äº†ä¿éšœï¼Œä¿é™©å…¬å¸è·å¾—äº†ä¿è´¹ã€‚ é€šå¸¸ï¼Œä¿é™©å…¬å¸éœ€è¦åœ¨ä¿å•çš„ä¿é™©æœŸé™å¼€å§‹æ—¶ç¡®å®šä¿è´¹ï¼Œå³åœ¨ä¿é™©æŸå¤±è¿˜æœªå‘ç”Ÿæ—¶ç¡®å®šä¿è´¹ã€‚ç”±äºè¿™ç§å®šä»·æ–¹å¼ï¼Œä¿é™©äº§å“å’Œä¸€èˆ¬æ¶ˆè´¹äº§å“çš„ç”Ÿäº§å‘¨æœŸç›¸åï¼Œæœä»é€†ç”Ÿäº§å‘¨æœŸã€‚ å› æ­¤ï¼Œé¢„æµ‹æ¨¡å‹åœ¨ä¿é™©äº§å“çš„å®šä»·ä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ è®¡ç®—æœºæŠ€æœ¯çš„å‘å±•å¸¦æ¥äº†è®¡ç®—é€Ÿåº¦çš„æå¤§æå‡å’Œå­˜å‚¨èƒ½åŠ›çš„æå¤§æé«˜ï¼Œå¯ä»¥çœ‹åˆ°å½“å‰å¾ˆå¤šé¢†åŸŸçš„å‘å±•éƒ½å’Œè®¡ç®—æœºæŠ€æœ¯çš„é©æ–°å¯†åˆ‡ç›¸å…³ã€‚ æœºå™¨å­¦ä¹ ç®—æ³•ä½œä¸ºä¸€ç§é¢„æµ‹æ¨¡å‹ï¼Œç»™ä¼ ç»Ÿçš„ç²¾ç®—å›å½’æ¨¡å‹å¸¦æ¥äº†æŒ‘æˆ˜å’Œæœºé‡ã€‚ æœºå™¨å­¦ä¹ ç®—æ³•çš„é¢„æµ‹èƒ½åŠ›ç›¸è¾ƒäºä¼ ç»Ÿå›å½’æ¨¡å‹æ›´é«˜; æœºå™¨å­¦ä¹ ç®—æ³•çš„è§£é‡Šæ€§æ¯”è¾ƒå·®ã€‚ åŸºäºæœºå™¨å­¦ä¹ ç®—æ³•çš„å®šä»·æ¨¡å‹æœ‰åŠ©äºä¿é™©å…¬å¸ç»†åˆ†é£é™©ï¼Œç²¾ç¡®å®šä»·ï¼Œå‡å°‘é€†é€‰æ‹©é£é™©ã€‚ å‡è®¾ä¿é™©å…¬å¸Aåœ¨å®šä»·ä¸­æ²¡æœ‰è€ƒè™‘åˆ°æŸä¸ªé‡è¦çš„é£é™©å› å­ï¼Œå³å¯¹äºæ˜¯å¦æœ‰è¯¥ç±»é£é™©çš„è¢«ä¿é™©äººéƒ½æ”¶å–ç›¸åŒä¿è´¹ï¼›è€Œä¿é™©å…¬å¸Båœ¨å®šä»·ä¸­è€ƒè™‘åˆ°è¯¥é£é™©å› å­ã€‚ ä¿é™©å…¬å¸Bä¼šå‡­å€Ÿä½ä¿è´¹å¸å¼•ä½é£é™©å®¢æˆ·ï¼Œå‡­å€Ÿé«˜ä¿è´¹ä½¿å¾—é«˜é£é™©å®¢æˆ·ç•™åœ¨ä¿é™©å…¬å¸Aã€‚ ç”±äºè¢«ä¿é™©äººçš„é€‰æ‹©æ•ˆåº”ï¼Œæœ€ç»ˆä¿é™©å…¬å¸Aæ”¶å–çš„ä¿è´¹ä¸è¶³ä»¥æ”¯ä»˜è¢«ä¿é™©äººçš„æŸå¤±ï¼Œæˆ–è€…éš¾ä»¥è·å–é¢„æœŸçš„ä¿é™©æ”¶ç›Šã€‚ å¦ä¸€æ–¹é¢ï¼Œä¿é™©å…¬å¸æ‰¿æ‹…ç€é£é™©è½¬ç§»å’Œé£é™©å…±æ‹…çš„ç¤¾ä¼šè§’è‰²ï¼Œè¿‡åº¦çš„ç»†åˆ†é£é™©ä¼šé€ æˆé£é™©ä¸ªä½“åŒ–ï¼Œä½¿å¾—ä¿é™©å…¬å¸çš„é£é™©è½¬ç§»å’Œå…±æ‹…çš„ä½œç”¨æ¶ˆå¤±ã€‚ æ¯”å¦‚ï¼ŒåŸºäºè¢«ä¿é™©äººçš„é«˜é£é™©ç‰¹å¾ï¼Œä¿é™©å…¬å¸ä¼šæ”¶å–æé«˜çš„ä¿è´¹ï¼Œä½¿å¾—è¢«ä¿é™©äººçš„é£é™©è½¬ç§»ä»·å€¼è¡ç„¶æ— å­˜ï¼Œä¹Ÿæ²¡æœ‰è´­ä¹°ä¿é™©çš„åŠ¨åŠ›ã€‚ æ‰€ä»¥ï¼Œä¿é™©å…¬å¸éœ€è¦å¹³è¡¡é£é™©ç»†åˆ†å’Œé£é™©å…±æ‹…ï¼Œä½¿å¾—ä¿é™©å…¬å¸æ—¢å¯ä»¥é¿å…é€†é€‰æ‹©ï¼Œä¹Ÿèƒ½æä¾›æœ‰æ•ˆçš„é£é™©è½¬ç§»çš„ä¿é™©äº§å“ã€‚ ä¸€èˆ¬åœ°ï¼Œä¿é™©å®šä»·æ¨¡å‹å—åˆ°ä¿é™©ç›‘ç®¡æœºæ„çš„ä¸¥æ ¼çº¦æŸï¼Œè¿™äº›æ¨¡å‹åœ¨åº”ç”¨åˆ°å®é™…ä¸­æ—¶ï¼Œå¿…é¡»æ»¡è¶³ä¸€å®šçš„æ¡ä»¶ã€‚è¿™ç»™æœºå™¨å­¦ä¹ ç®—æ³•åœ¨ä¿é™©å®šä»·ä¸­çš„åº”ç”¨å¸¦æ¥äº†å¾ˆå¤šé˜»ç¢ã€‚ åœ¨ä¸­å›½ï¼Œä¿ç›‘ä¼šé™å®šäº†å•†ä¸šè½¦é™©ä¿è´¹çš„åŒºé—´ï¼Œéšç€å•†ä¸šè½¦é™©è´¹ç‡æ”¹é©ï¼Œè¿™ä¸ªåŒºé—´åœ¨ä¸æ–­æ‰©å¤§ï¼Œä¿é™©å…¬å¸çš„å®šä»·æ¨¡å‹å‘æŒ¥çš„ä½œç”¨è¶Šæ¥è¶Šå¤§ã€‚ European Unionâ€™s General Data Protection Regulation (2018) å»ºç«‹äº†algorithmic accountability of decision-making machine algorithmsåˆ¶åº¦ï¼Œè¯¥åˆ¶åº¦èµ‹äºˆäº†å‚ä¸è€…å¯¹äºæœºå™¨å­¦ä¹ ç®—æ³•èƒŒåé€»è¾‘çš„çŸ¥æƒ…æƒã€‚ æ€»ä¹‹ï¼Œå®šä»·æ¨¡å‹å¿…é¡»åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥è§£é‡Šç»™è¢«ä¿é™©äººã€ä¿é™©ç›‘ç®¡æœºæ„ç­‰ã€‚ä»è¢«ä¿é™©äººå’Œä¿é™©ç›‘ç®¡çš„è§’åº¦å‡ºå‘ï¼Œä»–ä»¬ä¹Ÿå¸Œæœ›äº§å“å®šä»·å’Œé£é™©ç®¡ç†æ˜¯å»ºç«‹åœ¨ä¸€ä¸ªè¾ƒé€æ˜çš„æ¨¡å‹è€Œä¸æ˜¯ä¸€ä¸ªé»‘ç›’å­ï¼Œè¿™æ ·æœ‰åˆ©äºç»´æŠ¤å¸‚åœºå…¬å¹³ã€ä¿éšœè¢«ä¿é™©äººçš„åˆ©ç›Šã€æ£€æµ‹é‡è¦é£é™©å› å­ã€å»ºç«‹é˜²èŒƒé£é™©æªæ–½ã€‚ "],["pre.html", "1 è®¡ç®—ç¯å¢ƒçš„æ„å»º 1.1 å¸¸ç”¨é“¾æ¥ 1.2 å…‹éš†ä»£ç  1.3 R interface to Keras 1.4 R interface to Python 1.5 Python", " 1 è®¡ç®—ç¯å¢ƒçš„æ„å»º â€œå·¥æ¬²å–„å…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ã€‚â€ åœ¨ä»¥ä¸‹æ­¥éª¤ä¸­ï¼Œå½“ä½ å‘ç°å®‰è£…éå¸¸æ…¢æ—¶ï¼Œå¯ä»¥å°è¯•4Gç½‘ç»œï¼Œå°è¯•VPNï¼Œå°è¯•æ”¹å˜CRANçš„é•œåƒæºï¼Œæˆ–å°è¯•æ”¹å˜condaçš„é•œåƒæºã€‚condaé•œåƒæºé€šè¿‡ä¿®æ”¹ç”¨æˆ·ç›®å½•ä¸‹çš„.condarcæ–‡ä»¶ä½¿ç”¨TUNAé•œåƒæºï¼Œä½†è¯¥é•œåƒæºå¯èƒ½æœ‰æ›´æ–°å»¶è¿Ÿã€‚ 1.1 å¸¸ç”¨é“¾æ¥ å‡†å¤‡å·¥ä½œä¸­å¸¸ç”¨çš„é“¾æ¥æœ‰ GitHub Git SSH key GitHub and RStudio Jupyter Notebook Anaconda Miniconda å¸¸ç”¨Condaå‘½ä»¤ TUNAé•œåƒæº R interface to Tensorflow and Keras reticulate Tensorflow Pytorch æ ¡çº§è®¡ç®—äº‘ CUDA cuDNN 1.2 å…‹éš†ä»£ç  GitHubæä¾›äº†å¤§é‡å¼€æºä»£ç ï¼Œè¿™é—¨è¯¾çš„ä»£ç ä¸»è¦æ¥è‡ªæ­¤é“¾æ¥ã€‚é€šå¸¸ï¼Œä½¿ç”¨GitHubå¼€æºä»£ç æœ€æ–¹ä¾¿çš„æ˜¯forkåˆ°è‡ªå·±GitHubè´¦æˆ·ä¸‹ï¼Œç„¶åcloneåˆ°æœ¬åœ°ã€‚å…·ä½“è€Œè¨€ï¼Œéœ€è¦è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š æ³¨å†ŒGitHubè´¦æˆ·ã€‚ Forkæ­¤é“¾æ¥åˆ°è‡ªå·±è´¦æˆ·ä¸‹çš„æ–°ä»“åº“,å¯é‡æ–°å‘½åä¸ºå¦‚Modern-Actuarial-Modelsæˆ–å…¶ä»–åç§°ã€‚ å®‰è£…gitã€‚åœ¨å‘½ä»¤çª—å£ä½¿ç”¨$ git config --global user.name \"Your Name\" å’Œ $ git config --global user.email \"youremail@yourdomain.com\" é…ç½®gitçš„ç”¨æˆ·åå’Œé‚®ç®±åˆ†åˆ«ä¸ºGitHubè´¦æˆ·çš„ç”¨æˆ·åå’Œé‚®ç®±ã€‚æœ€åå¯ä½¿ç”¨$ git config --listæŸ¥çœ‹é…ç½®ä¿¡æ¯ã€‚ (é€‰åš)åœ¨æœ¬åœ°ç”µè„‘åˆ›å»ºssh public keyï¼Œå¹¶æ‹·è´åˆ°GitHubä¸­Settingä¸‹SSH and GPG keysã€‚ssh public keyä¸€èˆ¬ä¿å­˜åœ¨æœ¬äººç›®å½•ä¸‹çš„éšè—æ–‡ä»¶å¤¹.sshä¸­ï¼Œæ‰©å±•åä¸º.pubã€‚è¯¦è§é“¾æ¥ã€‚è®¾ç«‹SSHå¯ä»¥é¿å…åç»­pushä»£ç åˆ°äº‘ç«¯æ—¶ï¼Œæ¯æ¬¡éƒ½éœ€è¦è¾“å…¥å¯†ç çš„éº»çƒ¦ ç”µè„‘è¿æ¥æ‰‹æœº4Gçƒ­ç‚¹ã€‚ä¸€èˆ¬åœ°ï¼Œåœ¨æ‰‹æœº4Gç½‘ç»œä¸‹å…‹éš†çš„é€Ÿåº¦æ¯”è¾ƒå¿«ã€‚ åœ¨RStudioä¸­åˆ›å»ºæ–°çš„é¡¹ç›®ï¼Œé€‰æ‹©Version Controlï¼Œç„¶åGitï¼Œåœ¨Repository URLä¸­è¾“å…¥ä½ çš„GitHubä¸­åˆšæ‰forkçš„æ–°ä»“åº“åœ°å€ï¼ˆåœ¨Codeä¸‹èƒ½æ‰¾åˆ°å…‹éš†åœ°å€ï¼Œå¦‚æœç¬¬4æ­¥å®Œæˆå¯ä»¥é€‰æ‹©SSHåœ°å€ï¼Œå¦‚æœç¬¬4æ­¥æ²¡å®Œæˆå¿…é¡»é€‰æ‹©HTTPSåœ°å€ï¼‰ï¼Œè¾“å…¥æ–‡ä»¶å¤¹åç§°ï¼Œé€‰æ‹©å­˜æ”¾ä½ç½®ï¼Œç‚¹å‡»create projectï¼ŒRStudioå¼€å§‹å…‹éš†GitHubä¸Šè¯¥ä»“åº“çš„æ‰€æœ‰å†…å®¹ã€‚ æ­¤æ—¶ï¼Œä½ åœ¨GitHubä¸Šä»“åº“çš„å†…å®¹å…¨éƒ¨å…‹éš†åˆ°äº†æœ¬åœ°ï¼Œä¸”æ”¾åœ¨äº†ä¸€ä¸ªR Projectä¸­ã€‚åœ¨è¯¥Projectä¸­ï¼Œä¼šå¤šä¸¤ä¸ªæ–‡ä»¶ï¼Œ.Rprojå’Œ.gitignoreï¼Œç¬¬ä¸€ä¸ªæ–‡ä»¶ä¿å­˜äº†Projectçš„è®¾ç½®ï¼Œç¬¬äºŒæ–‡ä»¶å‘Šè¯‰gitåœ¨pushæœ¬åœ°æ–‡ä»¶åˆ°GitHubæ—¶å“ªäº›æ–‡ä»¶è¢«å¿½ç•¥ã€‚ å¦‚æœä½ ä¿®æ”¹äº†æœ¬åœ°æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡Rä¸­å†…åµŒçš„Gitä¸Šä¼ åˆ°GitHubï¼ˆå…ˆcommitå†pushï¼‰ï¼Œè¿™æ ·æ–¹ä¾¿åœ¨ä¸åŒç”µè„‘ä¸ŠåŒæ­¥æ–‡ä»¶ã€‚gitæ˜¯ä»£ç ç‰ˆæœ¬æ§åˆ¶å·¥å…·ï¼Œåœ¨pushä¹‹å‰ï¼Œä½ å¯ä»¥æ¯”è¾ƒå’Œä¸Šä¸ªä»£ç ç‰ˆæœ¬çš„å·®å¼‚ã€‚GitHubè®°å½•äº†ä½ æ¯æ¬¡pushçš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸”å­˜æ”¾åœ¨æœ¬åœ°æ–‡ä»¶å¤¹.gitä¸­ã€‚åŒæ—¶ï¼Œå¦‚æœGitHubä¸Šä»£ç æœ‰å˜åŒ–ï¼Œä½ å¯ä»¥pullåˆ°æœ¬åœ°ã€‚å¦‚æœç»å¸¸åœ¨ä¸åŒç”µè„‘ä¸Šä½¿ç”¨æœ¬ä»“åº“ï¼Œä¸€èˆ¬éœ€è¦å…ˆpullæˆæœ€æ–°ç‰ˆæœ¬ï¼Œç„¶åå†ç¼–è¾‘ä¿®æ”¹ï¼Œæœ€åcommit-pushåˆ°GitHubã€‚ (é€‰åš) ä½ å¯ä»¥å»ºç«‹æ–°çš„branchï¼Œä½¿è‡ªå·±çš„ä¿®æ”¹å’Œæºä»£ç åˆ†å¼€ã€‚å…·ä½“æ“ä½œå¯å‚è€ƒé“¾æ¥ï¼Œæˆ–è€…å‚è€ƒè´¦æˆ·å»ºç«‹æ—¶è‡ªåŠ¨äº§ç”Ÿçš„getting-startedä»“åº“ã€‚ (é€‰åš) ä½ å¯ä»¥å°è¯•Github Desktopæˆ–è€…Jupyter Labï¼ˆåŠ è½½git extensionï¼‰ç®¡ç†ï¼Œä½†å¯¹äºè¿™é—¨è¯¾ï¼Œè¿™ä¸¤ç§æ–¹å¼ä¸æ˜¯æœ€ä¼˜ã€‚ ç†è®ºä¸Šï¼ŒGitHubä¸Šæ‰€æœ‰ä»“åº“éƒ½å¯ä»¥é‡‡ç”¨ä»¥ä¸Šæ–¹æ³•åœ¨RStudioä¸­ç®¡ç†ï¼Œå½“ç„¶ï¼ŒRStudioå¯¹äºRä»£ç ä»“åº“ç®¡ç†æœ€æœ‰æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨RStudioä¸­è¿è¡Œä»“åº“ä¸­çš„ä»£ç ã€‚ 1.3 R interface to Keras è¿™é‡Œä¸»è¦è¯´æ˜kerasåŒ…çš„å®‰è£…å’Œä½¿ç”¨ã€‚Kerasæ˜¯tensorflowçš„APIï¼Œåœ¨kerasä¸­å»ºç«‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹éƒ½ç”±tensorflowè®­ç»ƒã€‚å®‰è£…kerasåŒ…ä¸»è¦æ˜¯å®‰è£…Pythonåº“tensorflowï¼Œå¹¶è®©Rä¸ä¹‹ç›¸å…³è”ã€‚ 1.3.1 Rè‡ªåŠ¨å®‰è£… æœ€ç®€å•çš„å®‰è£…æ–¹å¼å¦‚ä¸‹ï¼š ä½¿ç”¨install.packages(\"tensorflow\")å®‰è£…æ‰€æœ‰ç›¸å…³çš„åŒ…ï¼Œç„¶ålibrary(\"tensorflow\")ã€‚ install_tensorflow() è¿™æ—¶å¤§æ¦‚ç‡ä¼šå‡ºç° No non-system installation of Python could be found. Would you like to download and install Miniconda? Miniconda is an open source environment management system for Python. See https://docs.conda.io/en/latest/miniconda.html for more details. Would you like to install Miniconda? [Y/n]: è™½ç„¶ä½ å¯èƒ½å·²ç»æœ‰Anacondaå’ŒPythonï¼Œä½†Ræ²¡æœ‰â€œæ™ºèƒ½â€åœ°è¯†åˆ«å‡ºæ¥ï¼Œè¿™æ—¶ä»å»ºè®®ä½ é€‰Yï¼Œè®©Rè‡ªå·±è£…ä¸€ä¸‹è‡ªå·±èƒ½æ›´å¥½è¯†åˆ«çš„Miniconda, è¿™ä¸ªå‘½ä»¤è¿˜ä¼šè‡ªåŠ¨å»ºç«‹ä¸€ä¸ªç‹¬ç«‹condaç¯å¢ƒr-reticulateï¼Œå¹¶åœ¨å…¶ä¸­è£…å¥½tensorflow, kerasç­‰ã€‚ ä¸Šæ­¥å¦‚æœæ­£å¸¸è¿è¡Œï¼Œç»“æŸåä¼šè‡ªåŠ¨é‡å¯Rã€‚è¿™æ—¶ä½ è¿è¡Œlibrary(tensorflow)ç„¶åtf$constant(\"Hellow Tensorflow\")ï¼Œå¦‚æœæ²¡æŠ¥é”™ï¼Œé‚£ç»§ç»­install_packages(\"keras\"),library(\"keras\")ã€‚ ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) å¦‚æœå‡ºç°ä»¥ä¸‹é”™è¯¯ é”™è¯¯: Installation of TensorFlow not found. Python environments searched for &#39;tensorflow&#39; package: C:\\Users\\...\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\python.exe You can install TensorFlow using the install_tensorflow() function. è¿™ä¸ªé”™è¯¯é€šå¸¸æ˜¯ç”±äºr-reticulateä¸­tensorflowå’Œå…¶ä»–åŒ…çš„ä¾èµ–å…³ç³»å‘ç”Ÿé”™è¯¯ï¼Œæˆ–è€…tensorflowç‰ˆæœ¬å¤ªä½ï¼Œä½ å¯ä»¥æ›´æ¢é•œåƒæºã€ä½¿ç”¨conda/pip installè°ƒæ•´è¯¥ç¯å¢ƒä¸­çš„tensorflowç‰ˆæœ¬å’Œä¾èµ–å…³ç³»ã€‚ æ›´å¥½çš„æ–¹å¼æ˜¯åœ¨condaä¸‹å®‰è£…å¥½æŒ‡å®šç‰ˆæœ¬çš„tensorflowç„¶åå…³è”åˆ°Rï¼Œæˆ–è€…ç”¨å…¶ä»–æ–¹å¼è®©Ræ‰¾åˆ°å…¶ä»–æ–¹å¼å®‰è£…çš„tensorflowã€‚è¿™æ—¶ï¼Œä½ å…ˆæŠŠä¹‹å‰å¤±è´¥çš„å®‰è£…C:\\Users\\...\\AppData\\Local\\r-minicondaï¼Œè¿™ä¸ªæ–‡ä»¶å¤¹å®Œå…¨åˆ æ‰ã€‚ç„¶åå‚è€ƒä»¥ä¸‹å®‰è£…æ­¥éª¤ã€‚ 1.3.2 ä½¿ç”¨reticulateå…³è”condaç¯å¢ƒ ä¸‹è½½å¹¶å®‰è£…Anacondaæˆ–è€…Minicondaã€‚ è¿è¡ŒAnaconda Promptæˆ–è€…Anaconda Powershell Promptï¼Œåœ¨å‘½ä»¤è¡Œè¾“å…¥conda create -n r-tensorflow tensorflow=2.1.0ï¼Œcondaä¼šåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„r-tensorflowç¯å¢ƒï¼Œå¹¶åœ¨å…¶ä¸­å®‰è£…tensorflowåŒ…ã€‚ ç»§ç»­åœ¨å‘½ä»¤è¡Œè¿è¡Œconda activate r-tensorflowåŠ è½½åˆšåˆšå®‰è£…çš„ç¯å¢ƒï¼Œå¹¶pip install h5py pyyaml requests Pillow scipyåœ¨è¯¥ç¯å¢ƒä¸‹å®‰è£…kerasä¾èµ–çš„åŒ…ã€‚è‡³æ­¤ï¼ŒRéœ€è¦çš„tensorflowç¯å¢ƒå·²ç»å‡†å¤‡å¥½ï¼Œæ¥ä¸‹æ¥è®©Rå…³è”æ­¤ç¯å¢ƒã€‚ é‡å¯Rï¼Œlibrary(\"reticulate\")ç„¶åuse_condaenv(\"r-tensorflow\",required=T),è¿™æ—¶Rå°±å’Œä¸Šé¢å»ºç«‹çš„ç¯å¢ƒå…³è”å¥½ã€‚ library(\"kerasâ€œ)ã€‚è¿™é‡Œå‡è®¾ä½ å·²ç»è£…å¥½tensorflowå’ŒkerasåŒ…ã€‚ ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 1.3.3 æŒ‡å®šcondaå®‰è£… ä¸‹è½½å¹¶å®‰è£…Anacondaæˆ–è€…Minicondaã€‚ å‘½ä»¤è¡Œè¾“å…¥which -a pythonï¼Œæ‰¾åˆ°Anacondaä¸­Pythonçš„è·¯å¾„è®°ä¸ºanapyã€‚ Rä¸­install_packages(\"tensorflow\")ï¼Œç„¶å install_tensorflow(method = &quot;conda&quot;, conda = &quot;anapy&quot;, envname = &quot;r-tensorflow&quot;, version = &quot;2.1.0&quot;) æ­¤å‘½ä»¤ä¼šåœ¨condaä¸‹åˆ›å»ºr-tensorflowçš„ç¯å¢ƒå¹¶è£…å¥½tensorflowåŒ…ã€‚ install_packages(\"keras\"); library(\"keras\") ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 1.3.4 ä½¿ç”¨reticulateå®‰è£… é‡å¯Rï¼Œlibrary(\"reticulate\")ã€‚ options(timeout=300)ï¼Œé˜²æ­¢ä¸‹è½½æ—¶é—´è¿‡é•¿ä¸­æ–­ã€‚ install_miniconda()ï¼Œå°†ä¼šå®‰è£…minicondaå¹¶åˆ›å»ºä¸€ä¸ªr-reticulatecondaç¯å¢ƒã€‚æ­¤ç¯å¢ƒä¸ºRé»˜è®¤è°ƒç”¨çš„Pythonç¯å¢ƒã€‚ ï¼ˆé‡å¯Rï¼‰library(\"tensorflow\"); install_tensorflow(version=\"2.1.0\")ï¼Œå°†ä¼šåœ¨r-reticulateå®‰è£…tensorflowã€‚ install_packages(\"keras\"); library(\"keras\") ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 1.4 R interface to Python RåŒ…reticulateä¸ºtensorflowçš„ä¾èµ–åŒ…ï¼Œå½“ä½ è£…tensorflowå®ƒä¹Ÿè¢«è‡ªåŠ¨å®‰è£…ã€‚å®ƒå¯ä»¥å»ºç«‹Rä¸Pythonçš„äº¤äº’ã€‚ 1.4.1 reticulate å¸¸è§å‘½ä»¤ conda_list()åˆ—å‡ºå·²å®‰è£…çš„condaç¯å¢ƒ virtualenv_list()åˆ—å‡ºå·²å­˜åœ¨çš„è™šæ‹Ÿç¯å¢ƒ use_python, use_condaenv, use_virtualenvå¯ä»¥æŒ‡å®šä¸Rå…³è”çš„pythonã€‚ py_config()å¯ä»¥æŸ¥çœ‹å½“å‰Pythonå…³è”ä¿¡æ¯ã€‚ å¾ˆå¤šæ—¶å€™ï¼ŒRä¼šåˆ›å»ºä¸€ä¸ªç‹¬ç«‹condaç¯å¢ƒr-miniconda/envs/r-reticulateã€‚ 1.4.2 åˆ‡æ¢Rå…³è”çš„condaç¯å¢ƒ æ ¹æ®éœ€è¦ï¼Œä½ å¯ä»¥åˆ‡æ¢Rå…³è”çš„condaç¯å¢ƒã€‚å…·ä½“æ­¥éª¤ä¸º é‡å¯R library(\"reticulate\") conda_list()åˆ—å‡ºå¯ä»¥å…³è”çš„ç¯å¢ƒå’Œè·¯å¾„ã€‚ use_condaenv(\"env-name\")ã€‚env-nameä¸ºå…³è”çš„condaç¯å¢ƒã€‚ py_configæŸ¥çœ‹æ˜¯å¦å…³è”æˆåŠŸã€‚ 1.5 Python ä¸€èˆ¬åœ¨æ¯ä¸ªPythonï¼ˆCondaï¼‰ç¯å¢ƒéƒ½éœ€è¦å®‰è£…ä¸€ä¸ªJupyter Notebook (conda install notebook)ã€‚ 1.5.1 Condaç¯å¢ƒ Pythonï¼ˆcondaï¼‰ç¯å¢ƒå»ºç«‹æ¯”è¾ƒç®€å•ï¼Œåœ¨ä½¿ç”¨reticulateå…³è”condaç¯å¢ƒæˆ‘ä»¬å·²ç»å»ºç«‹è¿‡ä¸€ä¸ªç¯å¢ƒr-tensorflowã€‚å…·ä½“æ“ä½œå¦‚ä¸‹: å»ºç«‹ç‹¬ç«‹ç¯å¢ƒconda create -n env-name python=3.8 tensorflow=2.1.0 notebookã€‚è¯¥å‘½ä»¤ä¼šå»ºç«‹env-nameçš„ç¯å¢ƒï¼Œå¹¶åœ¨å…¶ä¸­å®‰è£…python=3.8,tensorflowï¼ŒnotebookåŒ…åŠå…¶ä¾èµ–åŒ…ã€‚ æ¿€æ´»ç¯å¢ƒconda activate env-name. cd åˆ°ä½ çš„å·¥ä½œç›®å½•ã€‚ å¯åŠ¨jupyter notebook jupyter notebookã€‚ å¦‚é‡åˆ°ç¼ºå°‘çš„åŒ…ï¼Œåœ¨è¯¥ç¯å¢ƒenv-nameä¸‹ä½¿ç”¨conda install ***å®‰è£…ç¼ºå°‘çš„åŒ…ã€‚ 1.5.2 å¸¸ç”¨çš„Condaå‘½ä»¤ conda create -n env-name2 --clone env-name1:å¤åˆ¶ç¯å¢ƒ conda env listï¼šåˆ—å‡ºæ‰€æœ‰ç¯å¢ƒ conda deactivateï¼šé€€å‡ºå½“å‰ç¯å¢ƒ conda remove -n env-name --allï¼šåˆ é™¤ç¯å¢ƒenv-nameä¸­çš„æ‰€æœ‰åŒ… conda list -n env-name: åˆ—å‡ºç¯å¢ƒenv-nameæ‰€å®‰è£…çš„åŒ… conda clean -pï¼šåˆ é™¤ä¸ä½¿ç”¨çš„åŒ… conda clean -tï¼šåˆ é™¤ä¸‹è½½çš„åŒ… conda clean -aï¼šåˆ é™¤æ‰€æœ‰ä¸å¿…è¦çš„åŒ… pip freeze &gt; pip_pkg.txt, pip install -r pip_pkg.txt ä¿å­˜å½“å‰ç¯å¢ƒPyPIåŒ…ç‰ˆæœ¬ï¼Œä»æ–‡ä»¶å®‰è£…PyPIåŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda env export &gt; conda_pkg.yaml, conda env export --name env_name &gt; conda_pkg.yaml, conda env create --name env-name2 --file conda_pkg.yaml ä¿å­˜å½“å‰/env-nameç¯å¢ƒæ‰€æœ‰åŒ…ï¼Œä»æ–‡ä»¶å®‰è£…æ‰€æœ‰åŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda list --explicit &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt ä¿å­˜å½“å‰ç¯å¢ƒCondaåŒ…ä¸‹è½½åœ°å€ï¼Œä»æ–‡ä»¶å®‰è£…CondaåŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda list --export &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt ä¿å­˜å½“å‰ç¯å¢ƒæ‰€æœ‰åŒ…ï¼ˆç±»ä¼¼conda env exportï¼‰ï¼Œä»æ–‡ä»¶å®‰è£…æ‰€æœ‰åŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ 1.5.3 Tensorflow/Pytorch GPU version Tensorflowå¯ä»¥ç»¼åˆä½¿ç”¨CPUå’ŒGPUè¿›è¡Œè®¡ç®—ï¼ŒGPUçš„ç¡¬ä»¶ç»“æ„é€‚è¿›è¡Œå·ç§¯è¿ç®—ï¼Œæ‰€ä»¥é€‚äºCNNï¼ŒRNNç­‰æ¨¡å‹çš„æ±‚è§£ã€‚ ä½ å¯ä»¥ç”³è¯·ä½¿ç”¨æ ¡çº§è®¡ç®—äº‘æˆ–è€…ä½¿ç”¨å­¦é™¢è®¡ç®—äº‘ï¼Œå®ƒä»¬çš„æœåŠ¡å™¨éƒ½é…ç½®äº†GPUï¼Œå¹¶è£…å¥½äº†å¯ä»¥ä½¿ç”¨GPUçš„Tensorflowæˆ–è€…Pytorchã€‚ä½¿ç”¨æ ¡çº§è®¡ç®—äº‘æ—¶ï¼Œä½ é€šå¸¸åªéœ€è¦è¿è¡ŒJupyter Notebookå°±å¯ä»¥ä½¿ç”¨äº‘ç«¯GPUè¿›è¡Œè®¡ç®—ã€‚ä½¿ç”¨å­¦é™¢è®¡ç®—äº‘æ—¶ï¼Œä½ é€šå¸¸éœ€è¦çŸ¥é“ä¸€äº›å¸¸ç”¨çš„Linuxå‘½ä»¤ï¼Œä½ ä¹Ÿå¯ä»¥å®‰è£…Ubuntuæ¥ç†Ÿæ‚‰Linuxç³»ç»Ÿã€‚ æ ¡çº§è®¡ç®—äº‘å’Œå­¦é™¢è®¡ç®—äº‘æœ‰ä¸“é—¨çš„ITäººå‘˜å¸®ä½ è§£å†³å¦‚æœ¬é¡µæ‰€ç¤ºçš„å¤§éƒ¨åˆ†ITé—®é¢˜ã€‚ ä½ çš„æœºå™¨å¦‚æœæœ‰GPUï¼Œå¯ä»¥æŒ‰å¦‚ä¸‹æ­¥éª¤è®©GPUå‘æŒ¥å®ƒçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œå…³é”®ç‚¹æ˜¯è®©GPUå‹å·ã€GPUé©±åŠ¨ã€CUDAç‰ˆæœ¬ã€Tensorflowæˆ–Pytorchç‰ˆæœ¬å½¼æ­¤åŒ¹é…ï¼Œä¸”å½¼æ­¤â€œç›¸è¿â€ã€‚ç™¾åº¦æˆ–è€…å¿…åº”ä¸Šæœ‰å¾ˆå¤šç›¸å…³èµ„æ–™å¯ä»¥ä½œä¸ºå‚è€ƒã€‚ æŸ¥çœ‹ç”µè„‘GPUå’Œé©±åŠ¨ï¼Œä»¥åŠæ”¯æŒçš„CUDAç‰ˆæœ¬ã€‚ æˆ–è€…åœ¨ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼šnvidia-smiï¼ŒæŸ¥çœ‹ä½ çš„NVIDIAæ˜¾å¡é©±åŠ¨æ”¯æŒçš„CUDAç‰ˆæœ¬ã€‚ æŸ¥çœ‹å„ä¸ªTensorflowç‰ˆæœ¬ï¼ŒPytorchç‰ˆæœ¬å¯¹åº”çš„CUDAå’ŒcuDNN. ä¸‹è½½å¹¶å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„CUDAã€‚æ³¨å†Œã€ä¸‹è½½å¹¶å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„cuDNN é…ç½®CUDAå’ŒcuDNN. å®‰è£…Tensorflowæˆ–è€…Pytorch. "],["french.html", "2 ç»Ÿè®¡å­¦ä¹ ä¸è½¦é™©å®šä»· 2.1 èƒŒæ™¯ä»‹ç» 2.2 é¢„æµ‹æ¨¡å‹æ¦‚è¿° 2.3 ç‰¹å¾å·¥ç¨‹ 2.4 è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›† 2.5 æ³Šæ¾åå·®æŸå¤±å‡½æ•° 2.6 æ³Šæ¾å›å½’æ¨¡å‹ 2.7 æ³Šæ¾å¯åŠ æ¨¡å‹ 2.8 æ³Šæ¾å›å½’æ ‘ 2.9 éšæœºæ£®æ— 2.10 æ³Šæ¾æå‡æ ‘ 2.11 æ¨¡å‹æ¯”è¾ƒ", " 2 ç»Ÿè®¡å­¦ä¹ ä¸è½¦é™©å®šä»· â€œè§å¤šè¯†å¹¿ã€éšæœºåº”å˜â€ 2.1 èƒŒæ™¯ä»‹ç» è½¦é™©æ•°æ®é‡å¤§ï¼Œé£é™©ç‰¹å¾å¤šï¼Œå¯¹è½¦é™©æ•°æ®åˆ†ææ—¶å¯ä»¥ä½“ç°å‡ºæœºå™¨å­¦ä¹ ç®—æ³•çš„ä¼˜åŠ¿ï¼Œå³ä½¿ç”¨ç®—æ³•ä»å¤§æ•°æ®ä¸­æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€æå–ç‰¹å¾ã€‚ åœ¨ç²¾ç®—ä¸­ï¼Œå¸¸å¸¸ä½¿ç”¨è½¦é™©ä¿å•æ•°æ®å’Œå†å²ç´¢èµ”æ•°æ®è¿›è¡Œé£é™©åˆ†æã€è½¦é™©å®šä»·ç­‰ã€‚ä¿å•æ•°æ®åº“æ˜¯åœ¨æ‰¿ä¿çš„æ—¶å€™å»ºç«‹çš„ï¼Œç´¢èµ”æ•°æ®åº“æ˜¯åœ¨ç´¢èµ”å‘ç”Ÿæ—¶å»ºç«‹çš„ï¼Œå¤§éƒ¨åˆ†ä¿å•æ²¡æœ‰å‘ç”Ÿç´¢èµ”ï¼Œæ‰€ä»¥å®ƒä»¬ä¸ä¼šåœ¨ç´¢èµ”æ•°æ®åº“ä¸­ä½“ç°ã€‚ ä¿å•æ•°æ®åº“è®°å½•äº†è½¦é™©çš„é£é™©ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š é©¾é©¶å‘˜ç‰¹å¾ï¼šå¹´é¾„ã€æ€§åˆ«ã€å·¥ä½œã€å©šå§»ã€åœ°å€ç­‰ è½¦è¾†ç‰¹å¾ï¼šå“ç‰Œã€è½¦åº§æ•°ã€è½¦é¾„ã€ä»·æ ¼ã€é©¬åŠ›ç­‰ ä¿å•ä¿¡æ¯ï¼šä¿å•ç¼–å·ã€æ‰¿ä¿æ—¥æœŸã€åˆ°æœŸæ—¥æœŸ å¥–æƒ©ç³»æ•° ç´¢èµ”æ•°æ®åº“è®°å½•äº†ä¿å•çš„ç´¢èµ”ä¿¡æ¯ï¼Œå¯ä»¥å¾—åˆ°ç´¢èµ”æ¬¡æ•°\\(N\\)å’Œæ¯æ¬¡çš„ç´¢èµ”é‡‘é¢\\(Y_l,l=1,\\ldots,N\\)ã€‚ç†è®ºä¸Šï¼Œè½¦é™©çš„çº¯ä¿è´¹ä¸ºä»¥ä¸‹éšæœºå’Œçš„æœŸæœ› \\[S=\\sum_{l=1}^N Y_l\\] å‡è®¾ç´¢èµ”æ¬¡æ•°\\(N\\)å’Œç´¢èµ”é‡‘é¢\\(Y_l\\)ç‹¬ç«‹ä¸”\\(Y_l\\)æœä»ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œåˆ™ \\[\\mathbf{E}(S)=\\mathbf{E}(N)\\times\\mathbf{E}(Y)\\] æ‰€ä»¥ï¼Œè½¦é™©å®šä»·é—®é¢˜å¾ˆå¤šæ—¶å€™éƒ½è½¬åŒ–ä¸ºä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹ï¼šç´¢èµ”æ¬¡æ•°ï¼ˆé¢‘ç‡ï¼‰æ¨¡å‹å’Œç´¢èµ”é‡‘é¢ï¼ˆå¼ºåº¦ï¼‰æ¨¡å‹ã€‚å¯¹äºç´¢èµ”æ¬¡æ•°æ¨¡å‹ï¼Œé€šå¸¸å‡è®¾å› å˜é‡æœä»æ³Šæ¾åˆ†å¸ƒï¼Œå»ºç«‹æ³Šæ¾å›å½’æ¨¡å‹ï¼Œä½¿ç”¨çš„æ•°æ®é‡ç­‰äºä¿å•æ•°ï¼›å¯¹äºç´¢èµ”é‡‘é¢æ¨¡å‹ï¼Œé€šå¸¸å‡è®¾å› å˜é‡æœä»ä¼½é©¬åˆ†å¸ƒï¼Œå»ºç«‹ä¼½é©¬å›å½’æ¨¡å‹ï¼Œä½¿ç”¨çš„æ•°æ®é‡ç­‰äºå‘ç”Ÿç´¢èµ”çš„ä¿å•æ•°ã€‚é€šå¸¸ï¼Œåœ¨æ•°æ®é‡ä¸å¤§æ—¶ï¼Œç´¢èµ”é‡‘é¢æ¨¡å‹çš„å»ºç«‹éš¾äºç´¢èµ”æ¬¡æ•°æ¨¡å‹ï¼Œå› ä¸ºåªæœ‰å‘ç”Ÿç´¢èµ”çš„ä¿å•æ‰èƒ½ç”¨äºç´¢èµ”é‡‘é¢æ¨¡å‹çš„å»ºç«‹ã€‚ è®°ç¬¬\\(i\\)ä¸ªä¿å•çš„é£é™©ä¿¡æ¯ä¸º\\(x_i\\in\\mathcal{X}\\)ï¼Œä¿é™©å…¬å¸å®šä»·çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°ä¸¤ä¸ªï¼ˆæœ€ä¼˜ï¼‰å›å½’æ–¹ç¨‹ï¼ˆæ˜ å°„ï¼‰ï¼Œä½¿ä¹‹å°½å¯èƒ½å‡†ç¡®åœ°é¢„æµ‹ç´¢èµ”é¢‘ç‡å’Œç´¢èµ”å¼ºåº¦: \\[\\lambda: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\lambda(x_i)\\] \\[\\mu: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\mu(x_i)\\] è¿™é‡Œï¼Œ\\(\\lambda(x_i)\\)æ˜¯å¯¹\\(N\\)çš„æœŸæœ›çš„ä¼°è®¡ï¼Œ\\(\\mu(x_i)\\)æ˜¯å¯¹\\(Y\\)çš„æœŸæœ›çš„ä¼°è®¡ã€‚åŸºäºè¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œçº¯ä¿è´¹ä¼°è®¡ä¸º\\(\\lambda(x_i)\\mu(x_i)\\)ã€‚ 2.2 é¢„æµ‹æ¨¡å‹æ¦‚è¿° å¦‚ä½•å¾—åˆ°ä¸€ä¸ªå¥½çš„é¢„æµ‹æ¨¡å‹å‘¢ï¼Ÿå¯ä»¥ä»ä¸¤ä¸ªæ–¹é¢è€ƒè™‘ï¼š è®©é£é™©ä¿¡æ¯ç©ºé—´\\(\\mathcal{X}\\)ä¸°å¯Œï¼Œä¹Ÿç§°ä¸ºç‰¹å¾å·¥ç¨‹ï¼Œæ¯”å¦‚åŒ…å«\\(x,x^2,\\ln x\\)ã€æˆ–è€…åŠ å…¥è½¦è”ç½‘ä¿¡æ¯ã€‚ è®©æ˜ å°„ç©ºé—´\\(\\lambda\\in{\\Lambda},\\mu\\in M\\)ä¸°å¯Œï¼Œå¦‚GLMåªåŒ…å«çº¿æ€§æ•ˆåº”ã€ç›¸åŠ æ•ˆåº”ï¼Œæ˜ å°„ç©ºé—´è¾ƒå°ï¼Œç¥ç»ç½‘ç»œåŒ…å«éçº¿æ€§æ•ˆåº”ã€äº¤äº’ä½œç”¨ï¼Œæ˜ å°„ç©ºé—´è¾ƒå¤§ã€‚ å½“ä½ é€‰å–äº†æ˜ å°„ç©ºé—´è¾ƒå°çš„GLMï¼Œé€šå¸¸éœ€è¦è¿›è¡Œä»”ç»†çš„ç‰¹å¾å·¥ç¨‹ï¼Œä½¿å¾—é£é™©ä¿¡æ¯ç©ºé—´é€‚äºGLMï¼›å½“ä½ é€‰å–äº†æ˜ å°„ç©ºé—´è¾ƒå¤§çš„ç¥ç»ç½‘ç»œï¼Œé€šå¸¸ä¸éœ€è¦è¿›è¡Œç‰¹åˆ«ä»”ç»†çš„ç‰¹å¾å·¥ç¨‹ï¼Œç¥ç»ç½‘ç»œå¯ä»¥è‡ªåŠ¨è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œå‘æ˜é£é™©ä¿¡æ¯ä¸­çš„æœ‰ç”¨ç‰¹å¾ã€‚ å¯¹äºä¼ ç»Ÿçš„ç»Ÿè®¡å›å½’æ¨¡å‹ï¼ŒGLMï¼ŒGAMï¼ŒMARSï¼Œæˆ‘ä»¬ä½¿ç”¨æå¤§ä¼¼ç„¶æ–¹æ³•åœ¨æ˜ å°„ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä¼˜çš„å›å½’æ–¹ç¨‹ï¼Œåœ¨æå¤§ä¼¼ç„¶ä¸­ä½¿ç”¨çš„æ•°æ®é›†ç§°ä¸ºå­¦ä¹ é›†ï¼ˆlearning data setï¼‰ã€‚ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œåå˜é‡é€‰æ‹©ï¼Œå¯ä»¥åˆ æ‰ä¸æ˜¾è‘—çš„åå˜é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é€æ­¥å›å½’ã€æœ€ä¼˜å­é›†ã€LASSOç­‰ï¼Œåˆ¤æ–­æ ‡å‡†ä¸ºAICç­‰ã€‚ å¯¹äºæ ‘æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ recursive partitioning by binary splits ç®—æ³•å¯¹é£é™©ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—å„å­ç©ºé—´å†…çš„åº”å˜é‡å·®å¼‚æœ€å°ï¼Œå·®å¼‚é€šå¸¸ä½¿ç”¨åå·®æŸå¤±ï¼ˆdeviance lossï¼‰åº¦é‡ã€‚ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€šå¸¸ä½¿ç”¨äº¤å‰éªŒè¯å¯¹æ ‘çš„æ·±åº¦è¿›è¡Œæ§åˆ¶ã€‚æ ‘æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„æ•°æ®ä¸ºå­¦ä¹ é›†ã€‚ æ ‘æ¨¡å‹çš„æ‰©å±•ä¸ºbootstrap aggregationï¼ˆbaggingï¼‰å’Œrandom forestã€‚ç¬¬ä¸€ç§ç®—æ³•æ˜¯å¯¹æ¯ä¸ªbootstrapæ ·æœ¬å»ºç«‹æ ‘æ¨¡å‹ï¼Œç„¶åå¹³å‡æ¯ä¸ªæ ‘æ¨¡å‹çš„é¢„æµ‹ï¼›ç¬¬äºŒç§ç®—æ³•ç±»ä¼¼ç¬¬ä¸€ç§ï¼Œä½†åœ¨å»ºç«‹æ ‘æ¨¡å‹æ—¶ï¼Œè¦æ±‚åªåœ¨æŸäº›éšæœºé€‰å®šçš„åå˜é‡ä¸Šåˆ†æ”¯ã€‚è¿™ä¸¤ç§æ‰©å±•éƒ½å±äºé›†æˆå­¦ä¹ ï¼ˆensemble learningï¼‰ã€‚ æå‡ç®—æ³•æœ‰å¤šç§ä¸åŒå½¢å¼ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³ç±»ä¼¼é€æ­¥å›å½’ï¼ŒåŒºåˆ«æ˜¯æ¯æ­¥å›å½’ä¸­éœ€è¦ä¾æ®ä¸Šæ­¥çš„é¢„æµ‹ç»“æœè°ƒæ•´å„ä¸ªæ ·æœ¬çš„æƒé‡ï¼Œè®©ä¸Šæ­¥é¢„æµ‹ç»“æœå·®çš„æ ·æœ¬åœ¨ä¸‹æ­¥å›å½’ä¸­å çš„æƒé‡è¾ƒå¤§ã€‚é€šå¸¸ï¼Œæ¯æ­¥å›å½’ä½¿ç”¨çš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æ·±åº¦ä¸º3çš„æ ‘æ¨¡å‹ã€‚æå‡ç®—æ³•ä¹Ÿå±äºé›†æˆå­¦ä¹ ï¼Œå’Œå‰é¢ä¸åŒæ˜¯å®ƒçš„å¼±å­¦ä¹ å™¨ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œbaggingå’Œrandom forestçš„å¼±å­¦ä¹ å™¨æ˜¯å½¼æ­¤ç‹¬ç«‹çš„ã€‚ å¯¹äºé›†æˆç®—æ³•ï¼Œé€šå¸¸éœ€è¦è°ƒæ•´å¼±å­¦ä¹ å™¨çš„ç»“æ„å‚æ•°ï¼Œå¦‚æ ‘çš„æ·±åº¦ï¼Œä¹Ÿè¦åˆ¤æ–­å¼±å­¦ä¹ å™¨çš„ä¸ªæ•°ï¼Œè¿™äº›ç§°ä¸ºtuning parametersï¼Œé€šå¸¸é€šè¿‡æ¯”è¾ƒåœ¨éªŒè¯é›†ï¼ˆvalidationï¼‰çš„æŸå¤±è¿›è¡Œè°ƒå‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚å¼±å­¦ä¹ å™¨ä¸­çš„å‚æ•°é€šè¿‡åœ¨è®­ç»ƒé›†ï¼ˆtrainingï¼‰ä¸Šè®­ç»ƒæ¨¡å‹å¾—åˆ°ã€‚è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å¹¶é›†ä¸ºå­¦ä¹ é›†ã€‚ å‰é¦ˆç¥ç»ç½‘ç»œçš„è¾“å…¥ç¥ç»å…ƒä¸ºé£é™©ä¿¡æ¯ï¼Œä¸‹ä¸€å±‚ç¥ç»å…ƒä¸ºä¸Šä¸€å±‚ç¥ç»å…ƒçš„çº¿æ€§ç»„åˆå¹¶é€šè¿‡æ¿€æ´»å‡½æ•°çš„éçº¿æ€§å˜æ¢ï¼Œæœ€åè¾“å‡ºç¥ç»å…ƒä¸ºç¥ç»ç½‘ç»œå¯¹å› å˜é‡æœŸæœ›çš„é¢„æµ‹ï¼Œé€šè¿‡å‡å°è¾“å‡ºç¥ç»å…ƒä¸å› å˜é‡è§‚å¯Ÿå€¼çš„å·®å¼‚ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œä¸­çš„å‚æ•°ã€‚ç¥ç»ç½‘ç»œå«æœ‰éå¸¸å¤šçš„å‚æ•°ï¼Œå¾ˆéš¾æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ï¼Œè€Œä¸”æœ€ä¼˜è§£å¿…ç„¶é€ æˆè¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥ä¸€èˆ¬é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•å¯¹å‚æ•°è¿›è¡Œè¿­ä»£ï¼Œä½¿å¾—è®­ç»ƒé›†æŸå¤±åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½æœ‰ä¸‹é™è¶‹åŠ¿ã€‚é€šè¿‡æ¯”è¾ƒéªŒè¯é›†æŸå¤±ç¡®å®šè¿­ä»£æ¬¡æ•°å’Œç¥ç»ç½‘ç»œçš„ç»“æ„å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ å¦‚ä½•è¯„ä»·ä¸€ä¸ªé¢„æµ‹æ¨¡å‹çš„å¥½åå‘¢ï¼Ÿé€šå¸¸ç”¨æ ·æœ¬å¤–æŸå¤±ï¼ˆtest errorï¼‰è¯„ä»·ã€‚å¯¹äºç´¢èµ”é¢‘ç‡ï¼Œä½¿ç”¨æ³Šæ¾åå·®æŸå¤±ï¼Œå¯¹äºç´¢èµ”å¼ºåº¦ï¼Œä½¿ç”¨ä¼½é©¬åå·®æŸå¤±ï¼Œå¯ä»¥è¯æ˜è¿™ä¸¤ä¸ªæŸå¤±å‡½æ•°å’Œä¼¼ç„¶å‡½æ•°æˆè´Ÿç›¸å…³ã€‚å…¶ä¸­ï¼Œå¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸ºï¼š \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Kerasä¸­å®šä¹‰çš„æŸå¤±å‡½æ•°ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] 2.3 ç‰¹å¾å·¥ç¨‹ åŠ è½½åŒ…ã€‚ rm(list=ls()) library(CASdatasets) # data # library(keras) # neural network library(data.table) # fread,fwrite library(glmnet) # lasso library(plyr) # ddply library(mgcv) # gam library(rpart) # tree # library(rpart.plot) library(Hmisc) # error bar # devtools::install_github(&#39;henckr/distRforest&#39;) # library(distRforest) library(gbm) # boosting data(freMTPL2freq) #data(freMTPL2sev) # textwidth&lt;-7.3 #inch # fwrite(freMTPL2freq,&quot;data/freMTPL2freq.txt&quot;) # freMTPL2freq&lt;-fread(&quot;data/freMTPL2freq_mac.txt&quot;) &#39;data.frame&#39;: 678013 obs. of 12 variables: $ IDpol : num 1 3 5 10 11 13 15 17 18 21 ... $ ClaimNb : &#39;table&#39; num [1:678013(1d)] 1 1 1 1 1 1 1 1 1 1 ... $ Exposure : num 0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ... $ VehPower : int 5 5 6 7 7 6 6 7 7 7 ... $ VehAge : int 0 0 2 0 0 2 2 0 0 0 ... $ DrivAge : int 55 55 52 46 46 38 38 33 33 41 ... $ BonusMalus: int 50 50 50 50 50 50 50 68 68 50 ... $ VehBrand : Factor w/ 11 levels &quot;B1&quot;,&quot;B10&quot;,&quot;B11&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ VehGas : chr &quot;Regular&quot; &quot;Regular&quot; &quot;Diesel&quot; &quot;Diesel&quot; ... $ Area : Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 4 4 2 2 2 5 5 3 3 2 ... $ Density : int 1217 1217 54 76 76 3003 3003 137 137 60 ... $ Region : Factor w/ 21 levels &quot;Alsace&quot;,&quot;Aquitaine&quot;,..: 21 21 18 2 2 16 16 13 13 17 ... 2.3.1 æˆªæ–­ å‡å°‘outliers/influential points çš„å½±å“ éœ€æ ¹æ®æ¯ä¸ªå˜é‡çš„åˆ†å¸ƒç¡®å®šåœ¨å“ªé‡Œæˆªæ–­ ç´¢èµ”æ¬¡æ•°åœ¨4æˆªæ–­ é£é™©æš´éœ²åœ¨1æˆªæ–­ é©¬åŠ›åœ¨9æˆªæ–­ è½¦é¾„åœ¨20æˆªæ–­ å¹´é¾„åœ¨90æˆªæ–­ å¥–æƒ©ç³»æ•°åœ¨150æˆªæ–­ 2.3.2 ç¦»æ•£åŒ– ç›®çš„æ˜¯ä¸ºäº†åˆ»ç”»éçº¿æ€§æ•ˆåº” éœ€ç”»å‡ºåå˜é‡çš„è¾¹ç¼˜ç»éªŒç´¢èµ”é¢‘ç‡åˆ¤æ–­ ç¦»æ•£åŒ–é©¬åŠ›ã€è½¦é¾„ã€å¹´é¾„ VehPowerFac, VehAgeFacï¼ŒDrivAgeFac 2.3.3 è®¾å®šåŸºç¡€æ°´å¹³ æ–¹ä¾¿å‡è®¾æ£€éªŒ è®¾å®šå«æœ‰æœ€å¤šé£é™©æš´éœ²çš„æ°´å¹³ä¸ºåŸºå‡†æ°´å¹³ 2.3.4 åå˜é‡å˜å½¢ ç›®çš„æ˜¯ä¸ºäº†åˆ»ç”»éçº¿æ€§æ•ˆåº” è€ƒè™‘åå˜é‡åˆ†å¸ƒï¼Œä½¿ä¹‹å˜å½¢åè¿‘ä¼¼æœä»å¯¹ç§°åˆ†å¸ƒ DriveAgeLn/2/3/4, DensityLn dat1 &lt;- freMTPL2freq # claim number dat1$ClaimNb &lt;- pmin(dat1$ClaimNb, 4) # exposure dat1$Exposure &lt;- pmin(dat1$Exposure, 1) # vehicle power dat1$VehPowerFac &lt;- as.factor(pmin(dat1$VehPower,9)) aggregate(dat1$Exposure,by=list(dat1$VehPowerFac),sum) dat1[,&quot;VehPowerFac&quot;] &lt;-relevel(dat1[,&quot;VehPowerFac&quot;], ref=&quot;6&quot;) # vehicle age dat1$VehAge &lt;- pmin(dat1$VehAge,20) VehAgeFac &lt;- cbind(c(0:110), c(1, rep(2,5), rep(3,5),rep(4,5), rep(5,5), rep(6,111-21))) dat1$VehAgeFac &lt;- as.factor(VehAgeFac[dat1$VehAge+1,2]) aggregate(dat1$Exposure,by=list(dat1$VehAgeFac),sum) dat1[,&quot;VehAgeFac&quot;] &lt;-relevel(dat1[,&quot;VehAgeFac&quot;], ref=&quot;2&quot;) # driver age dat1$DrivAge &lt;- pmin(dat1$DrivAge,90) DrivAgeFac &lt;- cbind(c(18:100), c(rep(1,21-18), rep(2,26-21), rep(3,31-26), rep(4,41-31), rep(5,51-41), rep(6,71-51), rep(7,101-71))) dat1$DrivAgeFac &lt;- as.factor(DrivAgeFac[dat1$DrivAge-17,2]) aggregate(dat1$Exposure,by=list(dat1$DrivAgeFac),sum) dat1[,&quot;DrivAgeFac&quot;] &lt;-relevel(dat1[,&quot;DrivAgeFac&quot;], ref=&quot;6&quot;) dat1$DrivAgeLn&lt;-log(dat1$DrivAge) dat1$DrivAge2&lt;-dat1$DrivAge^2 dat1$DrivAge3&lt;-dat1$DrivAge^3 dat1$DrivAge4&lt;-dat1$DrivAge^4 # bm dat1$BonusMalus &lt;- as.integer(pmin(dat1$BonusMalus, 150)) # vehicle brand dat1$VehBrand &lt;- factor(dat1$VehBrand) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehBrand),sum) dat1[,&quot;VehBrand&quot;] &lt;-relevel(dat1[,&quot;VehBrand&quot;], ref=&quot;B1&quot;) # vehicle gas dat1$VehGas &lt;- factor(dat1$VehGas) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehGas),sum) dat1[,&quot;VehGas&quot;] &lt;-relevel(dat1[,&quot;VehGas&quot;], ref=&quot;Regular&quot;) # area (related to density) dat1$Area &lt;- as.integer(dat1$Area) # density dat1$DensityLn &lt;- as.numeric(log(dat1$Density)) # region aggregate(dat1$Exposure,by=list(dat1$Region),sum)[order( aggregate(dat1$Exposure,by=list(dat1$Region),sum)$x),] dat1[,&quot;Region&quot;] &lt;-relevel(dat1[,&quot;Region&quot;], ref=&quot;Centre&quot;) str(dat1) # model matrix for GLM design_matrix&lt;- model.matrix( ~ ClaimNb + Exposure + VehPowerFac + VehAgeFac + DrivAge + DrivAgeLn + DrivAge2 + DrivAge3 + DrivAge4 + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge as factor variables # design_matrix2&lt;- # model.matrix( ~ ClaimNb + Exposure + VehPower + VehAge + DrivAge + # BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge, and DrivAge as continuous variables # dim(design_matrix2) 2.4 è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›† æ¯”ä¾‹ä¸º\\(0.6:0.2:0.2\\) æ ¹æ®ç´¢èµ”æ¬¡æ•°åˆ†å±‚æŠ½æ · ç»éªŒç´¢èµ”é¢‘ç‡çº¦ä¸º\\(10\\%\\) seed_split&lt;-11 # claim 0/1 proportions index_zero&lt;-which(dat1$ClaimNb==0) index_one&lt;-which(dat1$ClaimNb&gt;0) prop_zero&lt;-round(length(index_zero)/(length(index_one)+length(index_zero)),2) prop_zero prop_one&lt;-round(length(index_one)/(length(index_one)+length(index_zero)),2) prop_one # 0.6:0.2:0.2 size_valid&lt;-round(nrow(dat1)*0.2,0) size_test&lt;-size_valid size_train&lt;-nrow(dat1)-2*size_valid # stratified sampling set.seed(seed_split) index_train_0&lt;-sample(index_zero,size_train*prop_zero) index_train_1&lt;-sample(index_one, size_train-length(index_train_0)) index_train&lt;-union(index_train_0,index_train_1) length(index_train);size_train index_valid&lt;-c(sample(setdiff(index_zero,index_train_0),round(size_valid*prop_zero,0)), sample(setdiff(index_one,index_train_1),size_valid-round(size_valid*prop_zero,0))) length(index_valid);size_valid index_test&lt;-setdiff(union(index_zero,index_one),union(index_train,index_valid)) index_learn&lt;-union(index_train,index_valid) length(index_train);length(index_valid);length(index_test) # train-validation-test; learn-test dat1_train&lt;-dat1[index_train,] dat1_valid&lt;-dat1[index_valid,] dat1_test&lt;-dat1[index_test,] dat1_learn&lt;-dat1[index_learn,] sum(dat1_train$ClaimNb)/sum(dat1_train$Exposure) sum(dat1_valid$ClaimNb)/sum(dat1_valid$Exposure) sum(dat1_test$ClaimNb)/sum(dat1_test$Exposure) sum(dat1_learn$ClaimNb)/sum(dat1_learn$Exposure) # glm matrix matrix_train&lt;-design_matrix[index_train,] matrix_valid&lt;-design_matrix[index_valid,] matrix_test&lt;-design_matrix[index_test,] matrix_learn&lt;-design_matrix[index_learn,] # gbm matrix (learn) dat1_learn_gbm&lt;-dat1_learn[,c(&quot;ClaimNb&quot;, &quot;Exposure&quot;, &quot;VehPower&quot;, &quot;VehAge&quot;, &quot;DrivAge&quot;, &quot;BonusMalus&quot;, &quot;VehBrand&quot;, &quot;VehGas&quot;, &quot;Area&quot;, &quot;DensityLn&quot;, &quot;Region&quot;)] class(dat1_learn_gbm) train_pro&lt;-size_train/(size_train+size_valid) 2.5 æ³Šæ¾åå·®æŸå¤±å‡½æ•° å¹³å‡æ³Šæ¾åå·®æŸå¤± \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Keraså®šä¹‰å¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] å› ä¸ºå¯¹äºå¤§éƒ¨åˆ†ä¿å•ï¼Œ\\(N_i-N_i\\ln N_i\\approx0\\)ï¼Œæ‰€ä»¥æ³Šæ¾åå·®æŸå¤±å‡½æ•°çº¦ä¸ºKeraså®šä¹‰çš„2å€ï¼ˆè‡³å°‘åœ¨ä¸€ä¸ªé‡çº§ï¼‰ã€‚ \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})\\approx2\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})\\] Poisson.Deviance &lt;- function(pred,obs) {200*(sum(pred)-sum(obs)+sum(log((obs/pred)^(obs))))/length(pred)} keras_poisson_dev&lt;-function(y_hat,y_true) {100*sum(y_hat-y_true*log(y_hat))/length(y_true)} f_keras&lt;-function(x) 100*(x-x*log(x)) f_keras(0.1);f_keras(0.2) # png(&quot;./plots/1/poi_dev.png&quot;) plot(seq(0.05,0.15,0.01),f_keras(seq(0.05,0.15,0.01)),type=&quot;l&quot;, xlab=&quot;frequency&quot;,ylab=&quot;approximated Poisson deviance&quot;,main=&quot;100(freq - freq * ln freq)&quot;) abline(v=0.1,lty=2);abline(h=f_keras((0.1)),lty=2) # dev.off() 2.6 æ³Šæ¾å›å½’æ¨¡å‹ ä½¿ç”¨æå¤§ä¼¼ç„¶æ–¹æ³•åœ¨æ˜ å°„ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä¼˜çš„å›å½’æ–¹ç¨‹ åœ¨æå¤§ä¼¼ç„¶ä¸­ä½¿ç”¨çš„æ•°æ®é›†ç§°ä¸ºå­¦ä¹ é›†ï¼ˆlearning data setï¼‰ã€‚ ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œåå˜é‡é€‰æ‹©ï¼Œå¯ä»¥åˆ æ‰ä¸æ˜¾è‘—çš„åå˜é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é€æ­¥å›å½’ã€æœ€ä¼˜å­é›†ã€LASSOç­‰ï¼Œåˆ¤æ–­æ ‡å‡†ä¸ºAICç­‰ã€‚ åŒè´¨æ¨¡å‹ \\[\\mathbf{E}(N)=\\beta_0\\] å…¨æ¨¡å‹ \\[\\ln \\mathbf{E}(N)=\\ln e + \\beta_0 + \\beta_{\\text{VehPowerFac}} + \\beta_{\\text{VehAgeFac}} \\\\ + \\beta_1\\text{DrivAge} + \\beta_2\\ln\\text{DrivAge} + \\beta_3\\text{DrivAge}^2 + \\beta_4\\text{DrivAge}^3 + \\beta_5\\text{DrivAge}^4 \\\\ \\beta_6\\text{BM} + \\beta_{\\text{VehBrand}} + \\beta_{\\text{VehGas}} + \\beta_7\\text{Area} + \\beta_8\\text{DensityLn} + \\beta_{\\text{Region}}\\] # homogeneous model d.glm0 &lt;- glm(ClaimNb ~ 1 + offset(log (Exposure)), data=data.frame(matrix_learn), family=poisson()) #summary(d.glm0) dat1_test$fitGLM0 &lt;- predict(d.glm0, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM0,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM0,matrix_test[,1]) # full GLM names(data.frame(matrix_learn)) {t1 &lt;- proc.time() d.glm1 &lt;- glm(ClaimNb ~ .-Exposure + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) (proc.time()-t1)} # summary(d.glm1) dat1_train$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_train), type=&quot;response&quot;) dat1_valid$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_valid), type=&quot;response&quot;) dat1_test$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_test), type=&quot;response&quot;) dat1_learn$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_learn), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM1,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM1,matrix_test[,1]) Step wiseã€LASSOåå˜é‡é€‰æ‹© é€æ­¥å›å½’éå¸¸æ…¢ï¼Œåœ¨Linux 8æ ¸i7 3.4GHz 16Gå†…å­˜éƒ½éœ€è¦50å¤šåˆ†é’Ÿã€‚ä¸”æ ·æœ¬å¤–æŸå¤±å’Œå…¨æ¨¡å‹æ²¡æœ‰æ˜æ˜¾å‡å°ã€‚ 5æŠ˜CV Lassoåœ¨Linux 8æ ¸i7 3.4GHz 16Gå†…å­˜éœ€è¦5åˆ†é’Ÿã€‚ æ ¹æ®5æŠ˜CV-erroré€‰å–Lassoæ­£åˆ™å‚æ•°beta=4*10^-5ã€‚ ä¸¤ç§æ–¹æ³•çš„æ ·æœ¬å¤–æŸå¤±å’Œå…¨æ¨¡å‹æ²¡æœ‰æ˜æ˜¾å‡å°ï¼Œè¯´æ˜æ²¡æœ‰å‘ç”Ÿæ˜æ˜¾è¿‡æ‹Ÿåˆã€‚ä¹Ÿè¯´æ˜éœ€è¦ä»éçº¿æ€§æ•ˆåº”å’Œäº¤äº’é¡¹å‡ºå‘æå‡æ¨¡å‹ã€‚ # step wise selectionï¼› this takes a long time (more than 50 minutes!) # d.glm00 &lt;- glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + # DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + # BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + # offset(log (Exposure)), # data=data.frame(matrix_learn), family=poisson()) # {t1 &lt;- proc.time() # d.glm2&lt;-step(d.glm00,direction=&quot;forward&quot;,trace = 1, # scope =list(lower=formula(d.glm00), upper=formula(d.glm1))) # (proc.time()-t1)} d.glm2&lt;-glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + VehPowerFac4 + VehPowerFac8 + RegionNord.Pas.de.Calais + VehPowerFac7 + RegionRhone.Alpes + RegionBretagne + RegionAuvergne + RegionLimousin + RegionLanguedoc.Roussillon + RegionIle.de.France + RegionAquitaine + RegionMidi.Pyrenees + RegionPays.de.la.Loire + RegionProvence.Alpes.Cotes.D.Azur + RegionPoitou.Charentes + RegionHaute.Normandie + VehBrandB5 + VehBrandB11 + RegionBasse.Normandie + VehBrandB14 + RegionCorse + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) summary(d.glm2) dat1_test$fitGLM2 &lt;- predict(d.glm2, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM2,data.frame(matrix_test)$ClaimNb) Poisson.Deviance(dat1_test$fitGLM2,matrix_test[,1]) # lasso regressionï¼› this takes a few minutes alpha0=1 # 1 for lasso, 0 for ridge. set.seed(7) # {t1 &lt;- proc.time() # cvfit = cv.glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], # family = &quot;poisson&quot;,offset=log(matrix_learn[,2]), # alpha = alpha0,nfolds = 5,trace.it = 1) # (proc.time()-t1)} # cvfit$lambda.min #4*10^-5 # cvfit$lambda.1se # 0.0016 # plot(cvfit) d.glm3 = glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], family = &quot;poisson&quot;, offset=log(matrix_learn[,2]), alpha=alpha0, lambda=4.024746e-05, trace.it = 1) dat1_test$fitLasso&lt;-predict(d.glm3, newx = matrix_test[,-c(1,2)], newoffset=log(matrix_test[,2]),type = &quot;response&quot;) keras_poisson_dev(dat1_test$fitLasso, matrix_test[,1]) Poisson.Deviance(dat1_test$fitLasso, matrix_test[,1]) 2.7 æ³Šæ¾å¯åŠ æ¨¡å‹ GAMè¾¹ç¼˜æå‡æ¨¡å‹ æ ·æœ¬å¤–æŸå¤±å‡å°‘ï¼Œè¯´æ˜éçº¿æ€§æ•ˆåº”å­˜åœ¨ã€‚ \\[\\ln \\mathbf{E}(N)=\\ln\\hat{\\lambda}^{\\text{GLM}}+s_1(\\text{VehAge})+s_2(\\text{BM})\\] \\(s_1,s_2\\)ä¸ºæ ·æ¡å¹³æ»‘å‡½æ•°ã€‚ ä½¿ç”¨ddplyèšåˆæ•°æ®ï¼Œæ‰¾åˆ°å……åˆ†ç»Ÿè®¡é‡ï¼ŒåŠ å¿«æ¨¡å‹æ‹Ÿåˆé€Ÿåº¦ã€‚ # GAM marginals improvement (VehAge and BonusMalus) {t1 &lt;- proc.time() dat.GAM &lt;- ddply(dat1_learn, .(VehAge, BonusMalus), summarise, fitGLM1=sum(fitGLM1), ClaimNb=sum(ClaimNb)) set.seed(1) d.gam &lt;- gam(ClaimNb ~ s(VehAge, bs=&quot;cr&quot;)+s(BonusMalus, bs=&quot;cr&quot;) + offset(log(fitGLM1)), data=dat.GAM, method=&quot;GCV.Cp&quot;, family=poisson) (proc.time()-t1)} summary(d.gam) dat1_train$fitGAM1 &lt;- predict(d.gam, newdata=dat1_train,type=&quot;response&quot;) dat1_valid$fitGAM1 &lt;- predict(d.gam, newdata=dat1_valid,type=&quot;response&quot;) dat1_test$fitGAM1 &lt;- predict(d.gam, newdata=dat1_test,type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGAM1, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGAM1,matrix_test[,1]) 2.8 æ³Šæ¾å›å½’æ ‘ ä½¿ç”¨ recursive partitioning by binary splits ç®—æ³•å¯¹é£é™©ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—å„å­ç©ºé—´å†…çš„åº”å˜é‡å·®å¼‚æœ€å°ã€‚ ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨äº¤å‰éªŒè¯ç¡®å®šcost-complexity parameterã€‚ cp=10^-3.421(1-SD rule)å‰ªææˆsplit=22çš„æ ‘ï¼Œæˆ–è€…cp=10^-3.949(min CV rule)å‰ªææˆsplit=55çš„æ ‘ã€‚ split=55(min CV rule)æ ‘çš„æ ·æœ¬å¤–æŸå¤±è¾ƒå°ã€‚ Variable importance (min CV rule) BonusMalus VehAge VehBrand DrivAge VehGas VehPower Region DensityLn 4675.0231 4396.8667 1389.2909 877.9473 795.6308 715.3584 480.3459 140.5463 # cross validation using xval in rpart.control names(dat1_learn) set.seed(1) {t1 &lt;- proc.time() tree0&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, control = rpart.control (xval=5, minbucket=1000, cp=10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} x0 &lt;- log10(tree0$cptable[,1]) err0&lt;-tree0$cptable[,4] std0&lt;-tree0$cptable[,5] xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;relative CV error&quot; (cp_min&lt;-x0[which.min(err0)]) (cp_1sd&lt;-x0[min(which(err0&lt;min(err0)+std0[which.min(err0)]))]) (nsplit_min&lt;-tree0$cptable[which.min(err0),2]) (nsplit_1sd&lt;-tree0$cptable[min(which(err0&lt;min(err0)+std0[which.min(err0)])),2]) # png(&quot;./plots/1/tree_cv.png&quot;) errbar(x=x0, y=err0*100, yplus=(err0+std0)*100, yminus=(err0-std0)*100, xlim=rev(range(x0)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x0, y=err0*100, col=&quot;blue&quot;) abline(h=c(min(err0+std0)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err0)*100), lty=1, col=&quot;magenta&quot;) abline(v=c(cp_1sd,cp_min),lty=2,col=c(&quot;orange&quot;,&quot;magenta&quot;)) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;orange&quot;,&quot;magenta&quot;), lty=c(1,1,1,2,2), lwd=c(1,1,1,1,1), pch=c(19,-1,-1,-1,-1), c(&quot;tree0&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;, paste(&quot;log cp = &quot;,round(cp_1sd,3)),paste(&quot;log cp = &quot;, round(cp_min,3)))) # dev.off() tree1 &lt;- prune(tree0, cp=10^mean(cp_min,min(x0[x0&gt;cp_min]))) tree11&lt;- prune(tree0, cp=10^mean(cp_1sd,min(x0[x0&gt;cp_1sd]))) tree1$cptable[nrow(tree1$cptable),2];nsplit_min tree11$cptable[nrow(tree11$cptable),2];nsplit_1sd dat1_test$fitRT_min &lt;- predict(tree1, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT_sd &lt;- predict(tree11, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT_min, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT_sd, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_min, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_sd, dat1_test$ClaimNb) tree1$variable.importance tree11$variable.importance äº¤å‰éªŒè¯å¯ä½¿ç”¨rpart(..., control=rpart.control(xval= ,...))æˆ–è€…xpred.rpart(tree, group)ã€‚ ä»¥ä¸Šä¸¤ç§æ–¹å¼å¾—åˆ°å¾ˆç›¸è¿‘çš„min CV ruleå‰ªææ ‘55 vs 51ï¼Œä½†1-SD ruleç›¸å·®è¾ƒå¤š22 vs 12ã€‚ # K-fold cross-validation using xpred.rpart set.seed(1) {t1 &lt;- proc.time() tree00&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, control = rpart.control (xval=1, minbucket=1000 ,cp=10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} (n_subtrees &lt;- dim(tree00$cptable)[1]) std1&lt;- numeric(n_subtrees) err1 &lt;- numeric(n_subtrees) K &lt;- 5 xgroup &lt;- rep(1:K, length = nrow(dat1_learn)) xfit &lt;- xpred.rpart(tree00, xgroup) dim(xfit);dim(dat1_learn) for (i in 1:n_subtrees){ err_group&lt;-rep(NA,K) for (k in 1:K){ ind_group &lt;- which(xgroup ==k) err_group[k] &lt;- keras_poisson_dev(dat1_learn[ind_group,&quot;Exposure&quot;]*xfit[ind_group,i], dat1_learn[ind_group,&quot;ClaimNb&quot;]) } err1[i] &lt;- mean(err_group) std1[i] &lt;- sd(err_group) } x1 &lt;- log10(tree00$cptable[,1]) (cp_min1&lt;-x1[which.min(err1)]) (cp_1sd1&lt;-x1[min(which(err1&lt;min(err1)+std1[which.min(err1)]))]) (nsplit_min1&lt;-tree00$cptable[which.min(err1),2]) (nsplit_1sd1&lt;-tree00$cptable[min(which(err1&lt;min(err1)+std1[which.min(err1)])),2]) xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;CV error (in 10^(-2))&quot; errbar(x=x1, y=err1*100, yplus=(err1+std1)*100, yminus=(err1-std1)*100, xlim=rev(range(x1)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x1, y=err1*100, col=&quot;blue&quot;) abline(h=c(min(err1+std1)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err1)*100), lty=1, col=&quot;magenta&quot;) abline(v=c(cp_1sd1,cp_min1),lty=2,col=c(&quot;orange&quot;,&quot;magenta&quot;)) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;orange&quot;,&quot;magenta&quot;), lty=c(1,1,1,2,2), lwd=c(1,1,1,1,1), pch=c(19,-1,-1,-1,-1), c(&quot;tree0&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;, paste(&quot;log cp = &quot;,round(cp_1sd1,3)),paste(&quot;log cp = &quot;, round(cp_min1,3)))) tree2 &lt;- prune(tree00, cp=10^mean(cp_min1,min(x1[x1&gt;cp_min1]))) tree22 &lt;- prune(tree00, cp=10^mean(cp_1sd1,min(x1[x1&gt;cp_1sd1]))) printcp(tree2) printcp(tree22) dat1_test$fitRT2 &lt;- predict(tree2, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT22 &lt;- predict(tree22, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT2, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT22, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT2, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT22, dat1_test$ClaimNb) sum((dat1_test$fitRT2-dat1_test$fitRT_min)^2) sum((dat1_test$fitRT22-dat1_test$fitRT_sd)^2) tree2$variable.importance tree1$variable.importance tree22$variable.importance tree11$variable.importance 2.9 éšæœºæ£®æ— ä½¿ç”¨https://github.com/henckr/distRforestå»ºç«‹æ³Šæ¾éšæœºæ£®æ—ã€‚ ncandæ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„åå˜é‡ä¸ªæ•°ï¼›subsampleè®­ç»ƒæ¯æ£µæ ‘çš„æ ·æœ¬ã€‚ ä½¿ç”¨éªŒè¯æŸå¤±ç¡®å®šæ ‘çš„æ•°é‡ã€‚ # fit the random forest library(distRforest) ntrees0&lt;-200 set.seed(1) {t1 &lt;- proc.time() forest1&lt;-rforest(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_train, method = &quot;poisson&quot;, control = rpart.control (xval=0, minbucket=1000 ,cp=10^-4, maxcompete = 0,maxsurrogate = 0, seed=1), parms=list(shrink=1), ncand=5,ntrees = ntrees0, subsample = 0.5, red_mem = T) (proc.time()-t1)} # determine number of trees using validation error fit_valid&lt;-rep(0,nrow(dat1_valid)) error_valid&lt;-rep(0,ntrees0) for (i in 1:ntrees0){ fit_valid&lt;-fit_valid + predict(forest1$trees[[i]], newdata=dat1_valid) * dat1_valid$Exposure fit_valid_norm &lt;- fit_valid/i error_valid[i]&lt;-Poisson.Deviance(fit_valid_norm, dat1_valid$ClaimNb) } # png(&quot;./plots/1/random_forest_error.png&quot;) plot(error_valid,type=&quot;l&quot;,xlab=&quot;number of trees&quot;,ylab=&quot;validation error in 10^-2&quot;) abline(v=which.min(error_valid),lty=2) # dev.off() (best.trees=which.min(error_valid)) # test error fitRF&lt;-rep(0,nrow(dat1_test)) for (i in 1:best.trees){ fitRF&lt;-fitRF+predict(forest1$trees[[i]], newdata=dat1_test)*dat1_test$Exposure } dat1_test$fitRF &lt;- fitRF/best.trees keras_poisson_dev(dat1_test$fitRF, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRF, dat1_test$ClaimNb) names(forest1$trees[[2]]$variable.importance) sum(forest1$trees[[3]]$variable.importance) 2.10 æ³Šæ¾æå‡æ ‘ n.trees æ ‘çš„æ•°é‡ï¼›shrinkage å­¦ä¹ æ­¥é•¿ï¼Œå’Œæ ‘çš„æ•°é‡æˆåæ¯”ï¼›interaction.depth äº¤äº’é¡¹æ·±åº¦ï¼›bag.fraction æ¯æ£µæ ‘ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹ï¼›train.fraction è®­ç»ƒé›†æ¯”ä¾‹ï¼›n.minobsinnodeå¶å­ä¸Šæœ€å°‘æ ·æœ¬é‡ã€‚ set.seed(1) {t1 &lt;- proc.time() gbm1 &lt;- gbm( ClaimNb ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region + offset(log(Exposure)), data = dat1_learn_gbm, distribution = &quot;poisson&quot;, n.trees = 500, shrinkage = 0.1, interaction.depth = 5, bag.fraction = 0.5, train.fraction = train_pro, cv.folds = 0, n.minobsinnode = 1000, verbose = T ) (proc.time()-t1)} # plot the performance # png(&quot;./plots/1/gbm_error.png&quot;) gbm.perf(gbm1,method=&quot;test&quot;) legend(&quot;topright&quot;,lty=c(1,1,2),col=c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;), c(&quot;training error&quot;, &quot;validation error&quot;, &quot;best iterations&quot;)) # dev.off() best.iter&lt;-gbm.perf(gbm1,method=&quot;test&quot;) dat1_test$fitGBM1&lt;- predict(gbm1, dat1_test,n.trees=best.iter,type=&quot;response&quot;)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitGBM1,dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGBM1,dat1_test$ClaimNb) æ ¹æ®éªŒè¯é›†æŸå¤±ç¡®å®šè¿­ä»£æ¬¡æ•°ã€‚ Variable importance rel.inf BonusMalus 27.687137 VehAge 19.976441 VehBrand 13.515198 Region 13.495375 DrivAge 9.284520 VehGas 7.082648 VehPower 4.583522 DensityLn 4.375159 Area 0.000000 é‡è¦å˜é‡çš„è¾¹ç¼˜æ•ˆåº” é‡è¦å˜é‡çš„äº¤äº’æ•ˆåº” 2.11 æ¨¡å‹æ¯”è¾ƒ ## model test_error test_error_keras ## 1 Intercept 33.5695 21.7647 ## 2 GLM 31.7731 20.8665 ## 3 GLM Lasso 31.8132 20.8866 ## 4 GAM 31.6651 20.8125 ## 5 Decision tree 30.9780 20.4690 ## 6 Random forest 30.9652 20.4626 ## 7 Generalized boosted model 30.8972 20.4286 ## 8 Neural network 31.0607 20.5080 Boosting &gt; RF &gt; Tree &gt; GAM &gt; GLM &gt; Homo "],["boosting.html", "3 æå‡æ–¹æ³•ä¸äº‹æ•…é¢„æµ‹ 3.1 AdaBoost 3.2 Logit Boost (real, discrete, gentle AdaBoost) 3.3 AdaBoost.M1 3.4 SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function) 3.5 SAMME.R (multi-class real AdaBoost) 3.6 Gradient Boosting 3.7 Newton Boosting 3.8 XGBoost 3.9 Case study 3.10 Appendix: Commonly used Python code (for py-beginners)", " 3 æå‡æ–¹æ³•ä¸äº‹æ•…é¢„æµ‹ Breiman called AdaBoost the â€˜best off-the-shelf classifier in the worldâ€™ (NIPS Workshop 1996). On the data science competition platform Kaggle, among 29 challenge winning solutions in 2015, 17 used XGBoost, a boosting algorithm introduced by Chen and Guestrin. AdaBooståŠç›¸è¿‘ç®—æ³• AdaBoostæ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯è®­ç»ƒä¸åŒçš„åˆ†ç±»å™¨(å¼±åˆ†ç±»å™¨\\(T\\))ï¼Œç„¶åæŠŠè¿™äº›å¼±åˆ†ç±»å™¨çº¿æ€§ç»„åˆèµ·æ¥ï¼Œæ„æˆä¸€ä¸ªæ›´å¼ºçš„æœ€ç»ˆåˆ†ç±»å™¨ï¼ˆå¼ºåˆ†ç±»å™¨\\(C\\)ï¼‰ã€‚ è¯¥ç®—æ³•æ˜¯ä¸€ä¸ªç®€å•çš„å¼±åˆ†ç±»ç®—æ³•æå‡è¿‡ç¨‹ï¼Œè¿™ä¸ªè¿‡ç¨‹é€šè¿‡ä¸æ–­çš„è®­ç»ƒï¼Œå¯ä»¥æé«˜å¯¹æ•°æ®çš„åˆ†ç±»èƒ½åŠ›ã€‚æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š é€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬\\((\\mathcal{D},\\mathbb{\\omega})\\)çš„å­¦ä¹ å¾—åˆ°ç¬¬\\(m-1\\)ä¸ªå¼±åˆ†ç±»å™¨WeakClassifier m-1, \\(T^{(m-1)}\\)ï¼› è®¡ç®—å¾—å‡ºå…¶åˆ†ç±»é”™è¯¯ç‡\\(\\epsilon^{(m-1)}\\)ï¼Œä»¥æ­¤è®¡ç®—å‡ºå…¶å¼±åˆ†ç±»å™¨æƒé‡\\(\\alpha^{(m-1)}\\)ä¸æ•°æ®æƒé‡\\(\\omega^{(m-1)}_i\\); ç”¨æƒé‡ä¸º\\(\\omega^{(m-1)}_i\\)çš„æ•°æ®é›†è®­ç»ƒå¾—åˆ°è®­ç»ƒå¼±åˆ†ç±»å™¨WeakClassifier m, \\(T^{(m)}\\); é‡å¤ä»¥ä¸Šä¸æ–­è¿­ä»£çš„è¿‡ç¨‹; æœ€ç»ˆç»“æœé€šè¿‡åŠ æƒæŠ•ç¥¨è¡¨å†³çš„æ–¹æ³•ï¼Œè®©æ‰€æœ‰å¼±åˆ†ç±»å™¨\\(T^{(m)}\\)è¿›è¡Œæƒé‡ä¸º\\(\\alpha^{(m)}\\)çš„æŠ•ç¥¨è¡¨å†³çš„æ–¹æ³•å¾—åˆ°æœ€ç»ˆé¢„æµ‹è¾“å‡ºã€‚ AdaBoost: Schapire and Freund (1997, 2012) LogitBoost: Friedman, Hastie, Tibshirani (1998) AdaBoost.M1: Schapire and Freund (1996, 1997) SAMME: Zhu, Zou, Rosset et al. (2006) SAMME.R: Zhu, Zou, Rosset et al. (2006) æ¢¯åº¦æå‡ç®—æ³• æ ¸å¿ƒæ€æƒ³: Gradient descent method and Newtonâ€™s method. Gradient descent method Minimize the following approximation over \\(y\\), \\[f(y)\\approx f(x)+\\nabla f(x)^T(y-x) +\\frac{1}{2t}||y-x||^2 \\] we have \\(y=x^+=x-t\\nabla f(x)\\). Newtonâ€™s method Minimize the following approximation over \\(y\\), \\[f(y)\\approx f(x)+\\nabla f(x)^T(y-x) +\\frac{1}{2}(y-x)^T\\nabla^2f(x)(y-x)\\] we have \\(y=x^+=x-\\frac{\\nabla f(x)}{\\nabla ^2f(x)}\\). Gradient Boost: Friedman (2001) Newton Boosting: Nielsen (2016) XGBoost: Chen and Guestrin (2016) 3.1 AdaBoost \\(Y\\in\\{0,1\\}\\) åˆå§‹æƒé‡ \\(\\omega^{(0)}_i=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},\\mathbf{\\omega}^{(m-1)})\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(T^{(m-1)}\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯ \\[\\epsilon^{(m-1)}=\\sum_{i=1}^n\\omega^{(m-1)}_i \\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i))\\] è®¡ç®—æ¨¡å‹æƒé‡ \\(\\alpha^{(m-1)}=\\ln\\beta^{(m-1)}\\), å…¶ä¸­\\[\\beta^{(m-1)}=\\frac{1-\\epsilon^{(m-1)}}{\\epsilon^{(m-1)}}\\] è®¡ç®—æ ·æœ¬æƒé‡\\[\\omega^{(m)}_i=\\omega^{(m-1)}_i\\exp\\left( \\alpha^{(m-1)}\\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i)) \\right)/w^{(m)}\\], å…¶ä¸­\\(w^{(m)}\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º \\[C(\\mathbf{x})= \\underset{k}{\\arg \\max} \\sum_{m=1}^M\\alpha^{(m)}\\mathbb{I}(T^{(m)}(\\mathbf{x})=k)\\] å¦å¤–ä¸€ç§ç­‰ä»·ç®—æ³• \\(Y\\in\\{-1,1\\}\\) åˆå§‹æƒé‡ \\(\\omega^{(0)}_i=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},\\mathbf{\\omega}^{(m-1)})\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(T^{(m-1)}\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯ \\[\\epsilon^{(m-1)}=\\sum_{i=1}^n\\omega^{(m-1)}_i \\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i))\\] è®¡ç®—æ¨¡å‹æƒé‡ \\(\\alpha^{(m-1)}=\\frac{1}{2}\\ln\\beta^{(m-1)}\\), å…¶ä¸­\\[\\beta^{(m-1)}=\\frac{1-\\epsilon^{(m-1)}}{\\epsilon^{(m-1)}}.\\] è®¡ç®—æ ·æœ¬æƒé‡\\[\\omega^{(m)}_i=\\omega^{(m-1)}_i\\exp\\left(-\\alpha^{(m-1)}y_i T^{(m-1)}(\\mathbf{x}_i) \\right)/w^{(m)},\\] å…¶ä¸­\\(w^{(m)}\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º\\[C(\\mathbf{x})= \\underset{k}{\\arg \\max} \\sum_{m=1}^M\\alpha^{(m)}\\mathbb{I}(T^{(m)}(\\mathbf{x})=k)\\] 3.2 Logit Boost (real, discrete, gentle AdaBoost) \\(Y\\in\\{-1,1\\}\\) åˆå§‹å¼±å­¦ä¹ æœº \\(T^{(0)}=0, C^{(0)}=0\\). è®¡ç®—é¢„æµ‹æ¦‚ç‡ \\[p^{(m-1)}(Y_i|\\mathbf{x_i})=\\frac{1}{1+\\exp(-Y_iT^{(m-1)}(\\mathbf{x_i}))}\\] æ³¨ï¼š\\[p^{(m-1)}(Y_i=1|\\mathbf{x_i})+p^{(m-1)}(Y_i=-1|\\mathbf{x_i})=1\\] è®¡ç®—æ ·æœ¬æƒé‡ \\[\\omega^{(m-1)}_i=p^{(m-1)}(Y_i=y_i|\\mathbf{x_i})(1-p^{(m-1)}(Y_i=y_i|\\mathbf{x_i}))\\] è®¡ç®—å·¥ä½œå› å˜é‡ \\[z^{(m)}_i = y_i(1+\\exp(-y_i C^{(m-1)}(\\mathbf{x_i})))\\] è®­ç»ƒå¼±å­¦ä¹ æœº\\(T^{(m)}\\)ï¼Œä½¿ä¹‹æœ€å°åŒ–å¦‚ä¸‹åŠ æƒæŸå¤±å‡½æ•° \\[\\sum_{i=1}^N \\omega_i^{(m-1)}(T^{(m)}(\\mathbf{x_i})-z^{(m-1)}_i)^2\\] ä»¤\\(C^{(m)}=C^{(m-1)}+T^{(m)}\\) æœ€ç»ˆé¢„æµ‹æ¦‚ç‡ä¸º\\[\\Pr(Y=y|\\mathbf{x})= \\frac{1}{1+\\exp(-yC^{(m)}(\\mathbf{x_i}))}\\] 3.3 AdaBoost.M1 \\(Y\\in\\{1,\\ldots,K\\}\\) åˆå§‹æƒé‡ \\(\\omega^{(0)}_i=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},\\mathbf{\\omega}^{(m-1)})\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(T^{(m-1)}\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯ \\[\\epsilon^{(m-1)}=\\sum_{i=1}^n\\omega^{(m-1)}_i \\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i))\\] è®¡ç®—æ¨¡å‹æƒé‡ \\(\\alpha^{(m-1)}=\\ln\\beta^{(m-1)}\\), å…¶ä¸­\\[\\beta^{(m-1)}=\\frac{1-\\epsilon^{(m-1)}}{\\epsilon^{(m-1)}}\\] è®¡ç®—æ ·æœ¬æƒé‡\\[\\omega^{(m)}_i=\\omega^{(m-1)}_i\\exp\\left( \\alpha^{(m-1)}\\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i)) \\right)/w^{(m)},\\] å…¶ä¸­\\(w^{(m)}\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º \\[C(\\mathbf{x})= \\underset{k}{\\arg \\max} \\sum_{m=1}^M\\alpha^{(m)}\\mathbb{I}(T^{(m)}(\\mathbf{x})=k)\\] 3.4 SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function) \\(Y\\in \\{1,\\ldots,K\\}\\) åˆå§‹æƒé‡ \\(\\omega^{(0)}_i=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},\\mathbf{\\omega}^{(m-1)})\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(T^{(m-1)}\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯ \\[\\epsilon^{(m-1)}=\\sum_{i=1}^n\\omega^{(m-1)}_i \\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i))\\] è®¡ç®—æ¨¡å‹æƒé‡ \\[\\alpha^{(m-1)}=\\eta\\left(\\ln\\beta^{(m-1)}+\\ln (k-1)\\right),\\] å…¶ä¸­\\[\\beta^{(m-1)}=\\frac{1-\\epsilon^{(m-1)}}{\\epsilon^{(m-1)}}\\] è®¡ç®—æ ·æœ¬æƒé‡\\[\\omega^{(m)}_i=\\omega^{(m-1)}_i\\exp\\left( \\alpha^{(m-1)}\\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i)) \\right)/w^{(m)},\\] å…¶ä¸­\\(w^{(m)}\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º \\[C(\\mathbf{x})= \\underset{k}{\\arg \\max} \\sum_{m=1}^M\\alpha^{(m)}\\mathbb{I}(T^{(m)}(\\mathbf{x})=k)\\] 3.5 SAMME.R (multi-class real AdaBoost) è§https://web.stanford.edu/~hastie/Papers/samme.pdf \\(Y\\in \\{1,\\ldots,K\\},\\mathbf{Z}=(Z_1,\\ldots,Z_k)&#39;\\in\\{1,-1/(K-1)\\}^K\\), å»ºç«‹æ˜ å°„\\(\\{1,\\ldots,K\\}\\rightarrow \\{1,-1/(K-1)\\}^K, Y\\mapsto \\mathbf{Z}(Y)\\), å…¶ä¸­\\(Z_k(k)=1\\), ä¸”\\(Z_{k&#39;}(k)=-1/(K-1), k&#39;\\neq k\\) åˆå§‹æƒé‡ \\(\\omega^{(0)}_i=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},\\mathbf{\\omega}^{(m-1)})\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(T^{(m-1)}\\). æ ¹æ®\\(T^{(m-1)}\\)è®¡ç®—(åŠ æƒ)é¢„æµ‹é¢‘ç‡ \\[p_k^{(m-1)}(\\mathbf{x})=\\Pr(y=k|\\mathbf{x}),\\] ä»¤\\(\\mathbf{p}^{(m-1)}(\\mathbf{x})=(p_1^{(m-1)}(\\mathbf{x}), \\ldots,p_K^{(m-1)}(\\mathbf{x}))&#39;\\) è®¡ç®—æ¨¡å‹(é¢„æµ‹ä¸º\\(k\\))æƒé‡ \\[h^{(m-1)}_k(\\mathbf{x})=(K-1)\\left(\\ln p^{(m-1)}_k(\\mathbf{x})-\\frac{1}{K}\\sum_{k&#39;\\neq k} \\ln p_{k&#39;}^{(m-1)}(\\mathbf{x})\\right)\\] è®¡ç®—æ ·æœ¬æƒé‡\\[\\omega^{(m)}_i=\\omega^{(m-1)}_i\\exp\\left( -\\frac{K-1}{K}\\mathbf{Z}(y_i)^{T}\\ln p^{(m-1)}(x_i) \\right)/w^{(m)},\\] å…¶ä¸­\\(w^{(m)}\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º \\[C(\\mathbf{x})= \\underset{k}{\\arg \\max} \\sum_{m=1}^M h_k^{(m)}(\\mathbf{x})\\] å‚è€ƒMulticlass exponential loss \\[L(Z,f)=\\exp\\left(-\\frac{1}{K}Z^Tf\\right)\\] 3.6 Gradient Boosting \\(Y\\in\\{-1,1\\}\\)ï¼Œè®¾å®šå­¦ä¹ ç‡\\(\\eta\\in(0,1]\\) åˆå§‹å¼±å­¦ä¹ å™¨ \\[f_0(\\mathbf{x})= \\underset{\\theta\\in\\mathbb{R}}{\\arg \\min} \\sum_{i=1}^n L(Y_i, \\theta)\\] è®¡ç®—ç¬¬\\(m\\)æ¬¡è¿­ä»£ä¸­,æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦\\[g_m(\\mathbf{x_i}) = - \\frac{\\partial L(Y_i, f_i)}{\\partial f_{m-1,i}}\\] æ±‚è§£å¼±å­¦ä¹ å™¨\\(T^m\\)å‚æ•° \\[h_m^{*} = \\underset{h\\in\\mathcal{F},\\beta}{\\arg \\min} \\sum_{i=1}^n(g_m(\\mathbf{x_i}) - \\beta h(\\mathbf{x_i}))^2 \\] é€šè¿‡çº¿æœç´¢å¾—åˆ°æ­¥é•¿ \\[\\rho_m = \\underset{\\rho&gt;0}{\\arg \\min} \\sum_{i=1}^n L(Y_i,f_{m-1}(\\mathbf{x_i}) + \\rho h_m^{*}(\\mathbf{x_i}))\\] shrinkageï¼Œä¹˜ä»¥æå‰ç»™å®šçš„å­¦ä¹ ç‡ \\[f_m^{*} = \\eta\\rho_m h_m^{*}\\] æ›´æ–°ï¼Œå‰\\(m\\)ä¸ªå¼±å­¦ä¹ å™¨çš„çº¿æ€§ç»„åˆ \\[f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + f_{m}^*(\\mathbf{x}) \\] æœ€ç»ˆé¢„æµ‹ç»“æœä¸º \\(f_M(\\mathbf{x})\\) 3.7 Newton Boosting \\(Y\\in\\{-1,1\\}\\)ï¼Œè®¾å®šå­¦ä¹ ç‡\\(\\eta\\in(0,1]\\) åˆå§‹å¼±å­¦ä¹ å™¨\\[f_0(\\mathbf{x})= \\underset{\\theta\\in\\mathbb{R}}{\\arg \\min} \\sum_{i=1}^n L(Y_i, \\theta)\\] è®¡ç®—ç¬¬\\(m\\)æ¬¡è¿­ä»£ä¸­çš„è´Ÿæ¢¯åº¦\\[g_m(\\mathbf{x_i}) = - \\frac{\\partial L(Y_i, \\theta)}{\\partial f_{m-1,i}}\\] è®¡ç®—Hessian Matrix \\[H_m(\\mathbf{x_i}) = (\\nabla^2\\mathcal{L}(\\mathbb{f_{m-1}}))_{ii}\\] æ±‚è§£å¼±å­¦ä¹ å™¨\\(T^m\\)å‚æ•°\\[h_m^{*} = \\underset{h\\in\\mathcal{F}}{\\arg \\min} \\sum_{i=1}^n(\\frac{g_m(\\mathbf{x_i})}{H_m(\\mathbf{x_i})} + \\frac{1}{2} H_m(\\mathbf{x_i})h(\\mathbf{x_i}))^2 \\\\ = \\underset{h\\in\\mathcal{F}}{\\arg \\min} \\sum_{i=1}^n\\frac{1}{2} H_m(\\mathbf{x_i})(-\\frac{g_m(\\mathbf{x_i})}{H_m(\\mathbf{x_i})} - h(\\mathbf{x_i}))^2 \\] shrinkageï¼Œä¹˜ä»¥æå‰ç»™å®šçš„å­¦ä¹ ç‡\\[f_m^{*} = \\eta h_m^{*}\\] æ›´æ–°ï¼Œå‰\\(m\\)ä¸ªå¼±å­¦ä¹ å™¨çš„çº¿æ€§ç»„åˆ \\[f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + f_{m}^*(\\mathbf{x}) \\] æœ€ç»ˆé¢„æµ‹ç»“æœä¸º \\(f_M(\\mathbf{x})\\) 3.8 XGBoost objective function: \\[\\mathcal{L} =\\sum_{i=1}^n L(\\hat{y_i},y_i) + \\sum_{m=1}^M\\Omega(f_m)\\] where \\(L(\\hat{y_i},y_i)\\) is a differential convex loss function and \\(\\Omega(f_m) = \\gamma T + \\frac{1}{2}\\lambda||\\omega||^2\\) is a regularization term to penalize model complexity (including number of leaves \\(T\\) and \\(L^2\\)-norm of leaf scores \\(\\omega\\)) Newton approximation of objective function: \\[\\tilde{\\mathcal{L}}^m = \\sum_{i=1}^n \\lbrack L(\\hat{y_i}^{m-1},y_i) + g_if_m(\\mathbf{x_i}) + \\frac{1}{2}h_if_m^2(\\mathbf{x_i}) \\rbrack+\\Omega(f_m) \\] where \\(g_i = \\frac{\\partial L}{\\partial\\hat{y_i}^{m-1}}|_{(\\hat{y_i}^{m-1}, y_i)}\\) and \\(h_i= \\frac{\\partial^2 L}{\\partial\\hat{y_i}^{m-1}\\partial\\hat{y_i}^{m-1}}|_{(\\hat{y_i}^{m-1}, y_i)}\\) æœ€å°åŒ–ä¸Šè¿°å¼å­ï¼Œå¾—åˆ°the optimal score (or weight) \\(\\omega_j^*\\) of leaf \\(j\\in\\{1,\\dots,T\\}\\)is:\\[ \\omega_j^*=-\\frac{\\sum_{i=1}^n g_i \\mathbb{I}[q(\\mathbf{x_i}) = j]}{\\lambda + \\sum_{i=1}^nh_i\\mathbb{I}[q(\\mathbf{x_i}) = j]} \\] 3.9 Case study æœ¬æ¡ˆä¾‹çš„æ•°æ®æ¥æºäºKaggleä¸Šçš„æ¯”èµ›â€œPorto Seguroâ€™s Safe Driver Prediction Competitionâ€ã€‚ æ¯”èµ›çš„ç›®æ ‡æ˜¯é¢„æµ‹æœªæ¥ä¸€å¹´å¸æœºæ˜¯å¦ä¼šå‘ç”Ÿäº¤é€šäº‹æ•…ï¼Œæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ã€‚æ­¤æ¡ˆä¾‹ä¸­ï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½ç»è¿‡äº†åŒ¿ååŒ–å¤„ç†ã€‚ 3.9.1 æ•°æ®æè¿° æ•°æ®åŒ…å«595212æ¡è®°å½•ï¼Œ59ä¸ªå˜é‡ã€‚ å˜é‡åŒ…å«ä¸‰ç±»ï¼š å”¯ä¸€ç¼–ç ï¼ˆ1ä¸ªï¼‰ï¼šid ç›®æ ‡å˜é‡ï¼ˆ1ä¸ªï¼‰ï¼štargetï¼Œå–å€¼ä¸º\\(\\{0, 1\\}\\) è§£é‡Šå˜é‡ï¼ˆ57ä¸ªï¼‰ï¼šps_*ï¼ŒåŒ…æ‹¬å››ç§ï¼Œåˆ†åˆ«æ˜¯äºŒåˆ†ç±»å˜é‡ï¼ˆbinaryï¼‰ï¼Œå¤šåˆ†ç±»å˜é‡ï¼ˆcategoricalï¼‰ï¼Œè¿ç»­å‹å˜é‡ï¼ˆcontinuousï¼‰ï¼Œå®šåºå˜é‡ï¼ˆordinalï¼‰ã€‚ æ­¤æ¡ˆä¾‹ä¸­ï¼Œç¼ºå¤±å€¼ç”¨-1è¡¨ç¤ºã€‚ 3.9.2 æ•°æ®é¢„å¤„ç† ç»Ÿè®¡ç¼ºå¤±å€¼ æ•°æ®ä¸­ï¼Œps_car_03_catå’Œps_car_05_catçš„ç¼ºå¤±å€¼è¾ƒå¤šï¼Œç¼ºå¤±å€¼åˆ†åˆ«å 69.09%å’Œ44.78%ï¼Œä¹‹åå°†è¿›è¡Œç¼ºå¤±å€¼å¤„ç†ã€‚ å•å˜é‡åˆ†æ å¯¹äºä¸åŒç±»å‹çš„è§£é‡Šå˜é‡ï¼Œæˆ‘ä»¬å°†ä¾ç…§ä¸åŒçš„æ–¹æ³•è¿›è¡Œå¤„ç†ã€‚ åˆ†ç±»å˜é‡ï¼Œç»Ÿè®¡å„ä¸ªç±»åˆ«çš„å æ¯” å®šè·å’Œå®šåºå˜é‡ï¼Œä½œæ¡å½¢å›¾ æ•°å€¼å‹å˜é‡ï¼Œä½œç›´æ–¹å›¾ å¯¹äºç›®æ ‡å˜é‡ï¼Œæˆ‘ä»¬å‘ç°å®ƒçš„å–å€¼éå¸¸ä¸å¹³è¡¡ã€‚ # levels for the target variable lev_target = ( pd.crosstab(index = data[&#39;target&#39;], columns = &#39;Frequency&#39;) / data.shape[0] ) * 100 lev_target.round(2) é€šå¸¸æ¥è¯´ï¼Œå¤„ç†ä¸å¹³è¡¡çš„åˆ†ç±»æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–å¦‚ä¸‹æ–¹æ³•ï¼š SMOTEï¼ˆSynthetic Minority Oversampling Techniqueï¼‰ ä¸Šé‡‡æ ·ï¼ˆOver-samplingï¼‰ ä¸‹é‡‡æ ·ï¼ˆUnder-samplingï¼‰ åŠ æƒæŠ½æ ·ï¼ˆSampling Weightingï¼‰ æˆæœ¬æ•æ„Ÿè®­ç»ƒï¼ˆCost-sensitive Trainingï¼‰ å¤šå˜é‡åˆ†æ åœ¨ä¸åŒçš„è§£é‡Šå˜é‡ä¹‹é—´ï¼Œæˆ‘ä»¬å¯ä»¥ä½œç›¸å…³ç³»æ•°çŸ©é˜µçƒ­å›¾å’Œæ•£ç‚¹å›¾çŸ©é˜µã€‚ ç›¸å…³ç³»æ•°çŸ©é˜µçƒ­å›¾ # Pearson correlation matrix: computation and visualization # use method=&#39;pearson&#39; resp. method=&#39;spearman&#39; to compute Pearson resp. Spearman correlations def corr_heatmap(v): correlations = data[v].corr(method=&#39;pearson&#39;) fig = plt.subplots(figsize=(10, 10)) sns.heatmap(correlations, center=0, fmt=&#39;.2f&#39;, cbar=False, square=True, linewidths=1, annot=True, cmap=&quot;YlGnBu&quot;) plt.xticks(rotation=90) plt.yticks(rotation=0) plt.show() # one applies the corr_heatmap function on the interval features v = meta[(meta.level == &#39;interval&#39;) &amp; (meta.keep)].index corr_heatmap(v) æ•£ç‚¹å›¾çŸ©é˜µ # scatterplot high correlation interval variables import seaborn high = pd.Index([&#39;ps_reg_01&#39;, &#39;ps_reg_02&#39;, &#39;ps_reg_03&#39;, &#39;ps_car_12&#39;, &#39;ps_car_13&#39;, &#39;ps_car_15&#39;]) pd.plotting.scatter_matrix(data[high], alpha = 0.2, figsize = (40, 40), diagonal = &#39;kde&#39;) åœ¨è§£é‡Šå˜é‡å’Œç›®æ ‡å˜é‡ä¹‹é—´ï¼Œæˆ‘ä»¬å¯ä»¥ä½œæ•£ç‚¹å›¾ã€ç®±çº¿å›¾ã€æ¡å½¢å›¾ç­‰ã€‚ 3.9.3 ç‰¹å¾å·¥ç¨‹ åˆ é™¤å˜é‡ ä¸ºäº†ç®€åŒ–åˆ†æã€æå‡è®¡ç®—é€Ÿåº¦ï¼Œåˆ é™¤14ä¸ªå®šè·å˜é‡ï¼ˆintervalï¼‰å’Œå®šåºå˜é‡ï¼ˆordinalï¼‰ ps_calc_01ï¼Œps_calc_02ï¼Œps_calc_03ï¼Œps_calc_04ï¼Œps_calc_05ï¼Œps_calc_06ï¼Œps_calc_07ï¼Œps_calc_08ï¼Œps_calc_09ï¼Œps_calc_10ï¼Œps_calc_11ï¼Œps_calc_12ï¼Œps_calc_13ï¼Œps_calc_14ã€‚ ç¼ºå¤±å€¼å¤„ç† åˆ é™¤ç¼ºå¤±å€¼è¾ƒå¤šçš„å˜é‡ps_car_03_catå’Œps_car_05_cat å‡å€¼æ’è¡¥è¿ç»­å‹å˜é‡ps_reg_03ï¼Œps_car_12å’Œps_car_14 ä¼—æ•°æ’è¡¥åˆ†ç±»å˜é‡ps_car_11 # dropping &#39;ps_car_03_cat&#39;, &#39;ps_car_05_cat&#39; and updating meta information vars_to_drop = [&#39;ps_car_03_cat&#39;, &#39;ps_car_05_cat&#39;] data.drop(vars_to_drop, inplace = True, axis = 1) meta.loc[(vars_to_drop), &#39;keep&#39;] = False # imputing with the mean or mode using Imputer from sklearn.preprocessing from sklearn.preprocessing import Imputer mean_imp = Imputer(missing_values = -1, strategy = &#39;mean&#39;, axis = 0) mode_imp = Imputer(missing_values = -1, strategy = &#39;most_frequent&#39;, axis = 0) data[&#39;ps_reg_03&#39;] = mean_imp.fit_transform(data[[&#39;ps_reg_03&#39;]]).ravel() data[&#39;ps_car_12&#39;] = mean_imp.fit_transform(data[[&#39;ps_car_12&#39;]]).ravel() data[&#39;ps_car_14&#39;] = mean_imp.fit_transform(data[[&#39;ps_car_14&#39;]]).ravel() data[&#39;ps_car_11&#39;] = mode_imp.fit_transform(data[[&#39;ps_car_11&#39;]]).ravel() ç”Ÿæˆå“‘å˜é‡ # creating dummy variables data = pd.get_dummies(data, columns = v, drop_first = True) print(&#39;After dummification we have {} variables in data&#39;.format(data.shape[1])) åˆ’åˆ†å­¦ä¹ é›†å’Œæµ‹è¯•é›† ä»¥80:20çš„æ¯”ä¾‹åˆ’åˆ†å­¦ä¹ é›†å’Œæµ‹è¯•é›†ï¼Œåˆ†åˆ«è®°ä½œ(X_train, y_train)å’Œ(X_test, y_test)ã€‚ ç»è¿‡åˆ’åˆ†ï¼Œå­¦ä¹ é›†æœ‰476169æ¡è®°å½•ï¼Œæµ‹è¯•é›†æœ‰119043æ¡è®°å½•ã€‚æ¨¡å‹å°†ä¼šåœ¨å­¦ä¹ é›†ä¸Šè®­ç»ƒï¼Œç„¶ååœ¨æµ‹è¯•é›†ä¸Šåˆ†æå…¶è¡¨ç°ã€‚ X_train, X_test, y_train, y_test = train_test_split(data.drop([&#39;id&#39;, &#39;target&#39;], axis=1), data[&#39;target&#39;], test_size=0.2, random_state=random_state ) 3.9.4 å»ºæ¨¡æµç¨‹ æ¨¡å‹ç¼–å· å»ºæ¨¡è¿‡ç¨‹ åŸºå‡†æ¨¡å‹ Baseline modeling ST0 ç”¨é»˜è®¤å‚æ•°è®­ç»ƒæ¨¡å‹ (no cv, no tunning) æ·±å…¥å»ºæ¨¡ In-depth modeling ST1, ST2 åŒ…æ‹¬å‚æ•°è°ƒæ•´(iteration, depth, learning rate)ã€äº¤å‰éªŒè¯ã€å¤–æ ·æœ¬æµ‹è¯•æ•´ä½“æ­¥éª¤ä¸ºï¼š1.é€‰æ‹©è‹¥å¹²å¥—å‚æ•° (hyper parameter tuning)2.å¯¹äºæ¯å¥—å‚æ•°ï¼Œåœ¨X_trainä¸Šè¿›è¡Œè®­ç»ƒå’Œäº¤å‰éªŒè¯ï¼Œè®¡ç®—æ¯ä¸€æŠ˜çš„è¡¨ç°(å…ˆç”¨1-4æŠ˜è®­ç»ƒï¼Œè®¡ç®—ç¬¬5æŠ˜çš„cv errorï¼›å†ç”¨ç¬¬1-3å’Œ5æŠ˜è®­ç»ƒï¼Œè®¡ç®—ç¬¬4æŠ˜çš„cv errorï¼›ä¾æ¬¡ç±»æ¨)3.è®¡ç®—æ¯å¥—å‚æ•°çš„å¹³å‡è¡¨ç°(GINIï¼ŒAUCï¼Œaccuracyï¼Œlogit loss function)4.é€‰æ‹©è¡¨ç°æœ€å¥½çš„ä¸€å¥—å‚æ•°åœ¨X_trainä¸Šè®­ç»ƒï¼Œä½œä¸ºæœ€ä¼˜æ¨¡å‹5.åœ¨X_testä¸Šå¯¹æœ€ä¼˜æ¨¡å‹è¿›è¡Œæµ‹è¯• 3.9.5 æ¨¡å‹åº¦é‡â€”â€”Giniç³»æ•° Giniç³»æ•° Giniç³»æ•°æ˜¯åº¦é‡æ¨¡å‹è¡¨ç°çš„ä¸€ä¸ªæŒ‡æ ‡ï¼Œå®ƒçš„è®¡ç®—å…¬å¼ä¸ºï¼š \\[Gini_{CAP} = \\frac {a_R} {a_P}\\] å…¶ä¸­ï¼Œ\\(a_R\\)æ˜¯æŸä¸€æ¨¡å‹CAPæ›²çº¿å’ŒéšæœºçŒœæ¨¡å‹CAPæ›²çº¿é—´çš„é¢ç§¯ï¼Œ\\(a_P\\)æ˜¯å®Œç¾æ¨¡å‹CAPæ›²çº¿å’ŒéšæœºçŒœæ¨¡å‹CAPæ›²çº¿é—´çš„é¢ç§¯ã€‚ Giniç³»æ•°æ¡ˆä¾‹ Gini Coefficient - An Intuitive Explanation \\[Gini = \\frac {0.189} {0.3} = 0.630\\] Giniç³»æ•°ä»£ç  åœ¨pythonä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç æ¥å®šä¹‰Giniç³»æ•°ã€‚ from sklearn.metrics import make_scorer # Gini coefficient def gini(actual, pred): # a structural check assert (len(actual) == len(pred)) # introducing an array called all all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float) #slicing along second axis # sorting the array along predicted probabilities (descending order) and along the index axis all[:, 2] in case of ties all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))] # # towards the Gini coefficient totalLosses = all[:, 0].sum() giniSum = all[:, 0].cumsum().sum() / totalLosses giniSum -= (len(actual) + 1) / 2. return giniSum / len(actual) # normalized Gini coefficient def gini_normalized_score(actual, pred): return gini(actual, pred) / gini(actual, actual) # score using the normalized Gini score_gini = make_scorer(gini_normalized_score, greater_is_better=True, needs_threshold = True) Giniç³»æ•°ä¸AUC Giniç³»æ•°ä¸AUCä¹‹é—´å­˜åœ¨å¦‚ä¸‹ç­‰å¼å…³ç³»ï¼š \\[Gini = 2 \\times AUC - 1\\] åœ¨æ­¤ä¾‹ä¸­ï¼ŒGiniç³»æ•°å°†ç”¨äºè®¡ç®—äº¤å‰éªŒè¯ä¸­æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ã€‚ 3.9.6 å»ºç«‹AdaBoostæ¨¡å‹ 3.9.6.1 ST0 é€‰å–treeä½œä¸ºåŸºæ¨¡å‹ï¼Œæ„å»ºSAMMEå’ŒSAMME.Ræ¨¡å‹ã€‚ é»˜è®¤å‚æ•°ï¼šn_estimators = 50ï¼Œlearning_rate = 1 SAMME.Ræ¨¡å‹çš„è¡¨ç°ä¼˜äºSAMMEæ¨¡å‹ã€‚ 3.9.6.2 ST1 æ„å»ºSAMME.Ræ¨¡å‹ï¼Œé€‰å–å‚æ•°n_estimators = 500ï¼Œlearning_rate = 1ï¼Œè§‚å¯Ÿä¸åŒmax_depthä¸‹å¤–æ ·æœ¬æµ‹è¯•çš„AUCã€‚ å·¦å›¾max_depth = 1ï¼Œå³å›¾max_depth = 3ã€‚ å½“max_depth = 1æ—¶ï¼Œæµ‹è¯•é›†ä¸Šï¼ŒSAMME.Rçš„æœ€å¤§AUCä¸º0.639ï¼Œåœ¨è¿­ä»£267æ¬¡æ—¶å–å¾—ï¼› å½“max_depth = 3æ—¶ï¼Œæµ‹è¯•é›†ä¸Šï¼ŒSAMME.Rçš„æœ€å¤§AUCä¸º0.624ï¼Œåœ¨è¿­ä»£8æ¬¡æ—¶å–å¾—ï¼Œå‡ºç°äº†è¿‡æ‹Ÿåˆï¼› å½“max_depth = 5ï¼Œè¿­ä»£æå°‘çš„æ¬¡æ•°å°±å‡ºç°äº†è¿‡æ‹Ÿåˆã€‚ å› æ­¤ï¼Œå½“å¢åŠ æ ‘çš„æœ€å¤§æ·±åº¦æ—¶ï¼Œå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆï¼Œè¿™æ—¶éœ€è¦é™ä½å­¦ä¹ ç‡ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚ 3.9.6.3 ST2 æ¥ä¸‹æ¥ï¼Œè®¾ç½®ä¸åŒçš„max_depthã€learning_rateã€n_estimatorsï¼Œæ„å»ºSAMME.Ræ¨¡å‹ï¼Œæœå¯»å¤–æ ·æœ¬æµ‹è¯•ä¸ŠAUCæœ€å¤§çš„æ¨¡å‹ã€‚ å¯ä»¥çœ‹å‡ºï¼Œæœ€ä½³æ¨¡å‹çš„å‚æ•°æ˜¯max_depth = 1ï¼Œlearning rate = 0.1ï¼Œn_estimators = 400ï¼Œæ­¤æ—¶AUCä¸º0.637ã€‚ 3.9.7 å»ºç«‹XGBoostæ¨¡å‹ 3.9.7.1 ST0 é‡‡ç”¨é»˜è®¤å‚æ•°ï¼šlearning rate = 0.1ï¼Œmax_depth = 3ï¼Œn_estimators = 100ï¼Œæ„å»ºXGBoostæ¨¡å‹ã€‚ å¾—å‡ºout-of-sample AUCä¸º0.638ï¼Œè¿™å·²ç»å¥½äºAdaBoostçš„ST0ï¼ˆ0.635ï¼‰å’ŒST2ï¼ˆ0.637ï¼‰ï¼Œç¨å·®äºST1ï¼ˆ0.639ï¼‰ã€‚ 3.9.7.2 ST1 è®¾ç½®ä¸åŒçš„max_depthã€learning_rateã€n_estimatorsï¼Œæ„å»ºXGBoostæ¨¡å‹ï¼Œæœå¯»å¤–æ ·æœ¬æµ‹è¯•ä¸ŠAUCæœ€å¤§çš„æ¨¡å‹ã€‚ å¯ä»¥çœ‹åˆ°ï¼Œåœ¨max_depth = 2ï¼Œlearning_rate = 0.1ï¼Œn_estimators = 500å’Œmax_depth = 3ï¼Œlearning_rate = 0.1ï¼Œn_estimators = 300ï¼ŒAUCéƒ½å–åˆ°äº†æœ€å¤§ä¸º0.643ã€‚ ç‰¹å¾é‡è¦æ€§æ’åºå¦‚ä¸‹å›¾ã€‚ 3.9.8 ç»“è®º æ€»çš„æ¥è¯´ï¼Œåœ¨æ­¤æ•°æ®é›†ä¸Š: SAMME.Rä¼˜äºSAMMEï¼ŒSAMME.Råœ¨å•å±‚æ ‘ã€é€‚åº¦çš„å­¦ä¹ ç‡å’Œè¾ƒå¤§çš„è¿­ä»£æ¬¡æ•°ä¸Šè¡¨ç°è¾ƒå¥½ï¼› XGBoostä¼˜äºAdaBoostï¼ŒXGBooståœ¨è¾ƒæµ…çš„æ ‘ã€é€‚åº¦çš„å­¦ä¹ ç‡å’Œè¾ƒå¤§çš„è¿­ä»£æ¬¡æ•°ä¸Šè¡¨ç°è¾ƒå¥½ã€‚ 3.10 Appendix: Commonly used Python code (for py-beginners) 3.10.1 Pythonæ ‡å‡†æ•°æ®ç±»å‹ Numbersï¼ˆæ•°å­—ï¼‰ï¼šç”¨äºå­˜å‚¨æ•°å€¼ï¼ŒåŒ…æ‹¬intï¼Œlongï¼Œfloatå’Œcomplexã€‚ Stringï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼šç”±æ•°å­—ã€å­—æ¯ã€ä¸‹åˆ’çº¿ç»„æˆçš„ä¸€ä¸²å­—ç¬¦ã€‚ Listï¼ˆåˆ—è¡¨ï¼‰ï¼šPythonä¸­ä½¿ç”¨æœ€é¢‘ç¹çš„æ•°æ®ç±»å‹ï¼Œå¯ä»¥å®Œæˆå¤§å¤šæ•°é›†åˆç±»çš„æ•°æ®ç»“æ„å®ç°ï¼Œå®ƒæ”¯æŒæ•°å­—ã€å­—ç¬¦ä¸²ç”šè‡³å¯ä»¥åŒ…å«åˆ—è¡¨ï¼ˆå³åµŒå¥—ï¼‰ã€‚ Tupleï¼ˆå…ƒç»„ï¼‰ï¼šå…ƒç»„ä¸èƒ½äºŒæ¬¡èµ‹å€¼ï¼Œç›¸å½“äºâ€œåªè¯»â€åˆ—è¡¨ã€‚ Dictionaryï¼ˆå­—å…¸ï¼‰ï¼šé™¤åˆ—è¡¨ä»¥å¤–pythonä¹‹ä¸­æœ€çµæ´»çš„å†…ç½®æ•°æ®ç»“æ„ç±»å‹ï¼Œä¸åˆ—è¡¨çš„åŒºåˆ«åœ¨äºâ€”â€”åˆ—è¡¨æ˜¯æœ‰åºçš„å¯¹è±¡é›†åˆï¼Œå­—å…¸æ˜¯æ— åºçš„å¯¹è±¡é›†åˆï¼Œå­—å…¸å½“ä¸­çš„å…ƒç´ æ˜¯é€šè¿‡é”®æ¥å­˜å–çš„ã€‚ n = 3.6 # æ•°å­— s = &#39;Hello, python!&#39; # å­—ç¬¦ä¸² L = [1, 2, &#39;a&#39;] # åˆ—è¡¨ t = (1, 2, &#39;a&#39;) # å…ƒç»„ d = {&#39;a&#39;:1, &#39;b&#39;:2} # å­—å…¸ print(n, s, L, t, d, sep = &#39;\\n\\n&#39;) 3.10.2 Pythonå†…ç½®å‡½æ•° è¾“å…¥è¾“å‡º print()å°†å¯¹è±¡è¾“å‡ºè‡³æ§åˆ¶å° open()æ‰“å¼€æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶å¯¹è±¡ input()è·å–æ§åˆ¶å°è¾“å…¥ è¿­ä»£ç›¸å…³ enumerate()è¿”å›å…ƒç´ çš„åºå·ä¸å¯¹åº”å€¼ zip()å°†å¤šä¸ªåºåˆ—ä¸­çš„å…ƒç´ é…å¯¹ï¼Œäº§ç”Ÿæ–°çš„å…ƒç»„åˆ—è¡¨ all()å¦‚æœç»™å®šçš„å¯è¿­ä»£å‚æ•°ä¸­çš„æ‰€æœ‰å…ƒç´ éƒ½ä¸ºTrueåˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False any()å¦‚æœç»™å®šçš„å¯è¿­ä»£å‚æ•°ä¸­çš„ä»»ä¸€å…ƒç´ ä¸ºTrueåˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False åºåˆ—å±æ€§ max()åºåˆ—æœ€å¤§å€¼ min()åºåˆ—æœ€å°å€¼ sum()åºåˆ—çš„å’Œ len()åºåˆ—é•¿åº¦ åºåˆ—æ“ä½œ range()ç”Ÿæˆåºåˆ— reversed()å°†åºåˆ—é€†ç½® sorted()å¯¹åºåˆ—è¿›è¡Œæ’åº å¯¹è±¡å±æ€§ dir()è¿”å›å±æ€§åˆ—è¡¨ id()è¿”å›å¯¹è±¡åœ°å€ isinstance()åˆ¤æ–­å¯¹è±¡çš„ç±»å‹ typeè¿”å›å¯¹è±¡çš„ç±»å‹ æ˜ å°„ç±»å‹ eval()å»é™¤å­—ç¬¦ä¸²çš„å•å¼•å·ï¼Œä»è€Œè·å–å¼•å·å†…éƒ¨å†…å®¹ map()å°†ä¼ è¿›æ¥çš„å‡½æ•°åº”ç”¨äºåºåˆ—ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå¹¶è¿”å›è¿­ä»£å™¨ slice()ç”Ÿæˆåˆ‡ç‰‡ 3.10.3 numpyåŒ… NumPy(Numerical Python)æ˜¯Pythonçš„ä¸€ä¸ªæ‰©å±•ç¨‹åºåº“ï¼Œæ”¯æŒå¤§é‡çš„ç»´åº¦æ•°ç»„ä¸çŸ©é˜µè¿ç®—ï¼Œå®ƒä¹Ÿé’ˆå¯¹æ•°ç»„è¿ç®—æä¾›å¤§é‡çš„æ•°å­¦å‡½æ•°åº“ã€‚ åˆ›å»ºndarrayæ•°ç»„ ndarrayæ˜¯ä¸€ç§å¤šç»´æ•°ç»„å¯¹è±¡ï¼Œå…¶ä¸­çš„æ‰€æœ‰å…ƒç´ å¿…é¡»æ˜¯ç›¸åŒç±»å‹çš„ã€‚ import numpy as np a1 = np.array([[1, 2, 3], [4, 5, 6]]) # åˆ›å»ºæ•°ç»„ print(a1) print(a1.ndim) # æ•°ç»„çš„ç»´åº¦ print(a1.shape) # æ•°ç»„çš„å½¢çŠ¶ print(a1.dtype) # æ•°ç»„çš„å…ƒç´ ç±»å‹ print(a1.itemsize) # æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚å•ä½é•¿åº¦ # å…¶ä»–åˆ›å»ºæ•°ç»„çš„æ–¹æ³• a2 = np.zeros(shape = (2,2), dtype = float) # åˆ›å»ºå…ƒç´ å…¨æ˜¯0çš„æ•°ç»„ a3 = np.ones(shape = (2,2), dtype = int) # åˆ›å»ºå…ƒç´ å…¨æ˜¯1çš„æ•°ç»„ a4 = np.arange(start = 10, stop = 20, step = 2) # åˆ›å»ºæŒ‡å®šæ•°æ®èŒƒå›´çš„æ•°ç»„ print(a1, a2, a3, a4, sep = &#39;\\n\\n&#39;) ndarrayå¯¹è±¡çš„åˆ‡ç‰‡å’Œç´¢å¼• import numpy as np a = np.arange(24).reshape((2, 3, 4)) # åˆ›å»º2ç»´ã€3è¡Œã€4åˆ—çš„æ•°ç»„ï¼Œå…ƒç´ ä»0-23å¡«å…… print(a[0, 0:2, 1:3]) # ç´¢å¼•ç¬¬1ä¸ªæ•°ç»„ç¬¬1-2è¡Œç¬¬2-3åˆ— import numpy as np arr = np.arange(10) # åˆ›å»ºå…ƒç´ ä¸º0-9çš„ä¸€ç»´æ•°ç»„ arr_s = arr[3:5] # åˆ‡ç‰‡ï¼Œæå‡ºæ•°ç»„çš„ç¬¬4ã€5ä¸ªå…ƒç´  arr_s[:] = 99 # å°†99èµ‹å€¼ç»™åˆ‡ç‰‡arr_sä¸­çš„æ‰€æœ‰å…ƒç´  print(arr_s, arr, sep = &#39;\\n\\n&#39;) # ä¿®æ”¹ä¼šç›´æ¥åæ˜ åˆ°æºæ•°ç»„ä¸Š æ•°å­¦è¿ç®— import numpy as np a = np.array([1.0, 5.55, 123, 0.567, 25.532]) b = np.arange(1, 6, 1) print(np.around(a, decimals = 1)) # å››èˆäº”å…¥è‡³1ä½å°æ•° print(np.floor(a)) # å‘ä¸‹å–æ•´ print(np.ceil(a)) # å‘ä¸Šå–æ•´ print(np.sqrt(a)) # å¼€æ ¹å· print(np.square(a)) # å¹³æ–¹ print(np.log(a)) # å–å¯¹æ•° print(np.exp(a)) # å–æŒ‡æ•° print(np.sign(a)) # å–ç¬¦å·å‡½æ•° print(np.add(a, b)) # ä¸¤ä¸ªæ•°ç»„ç›¸åŠ  print(np.subtract(a, b)) # ä¸¤ä¸ªæ•°ç»„ç›¸å‡ print(np.multiply(a, b)) # ä¸¤ä¸ªæ•°ç»„ç›¸ä¹˜ print(np.divide(a, b)) # ä¸¤ä¸ªæ•°ç»„ç›¸é™¤ ç»Ÿè®¡è¿ç®— import numpy as np a = np.array([[3, 7, 5], [8, 4, 3], [2, 4, 9]]) print(np.min(a, axis = 0)) # æ²¿çºµè½´çš„æœ€å°å€¼ print(np.max(a, axis = 1)) # æ²¿æ¨ªè½´çš„æœ€å¤§å€¼ # ä»¥ä¸‹å‡½æ•°å‡å¯ä»¥é€šè¿‡å‚æ•°axisé€‰æ‹©çºµè½´ï¼ˆaxis=0ï¼‰æˆ–æ¨ªè½´ï¼ˆaxis=1ï¼‰ print(np.ptp(a)) # æ•°ç»„ä¸­å…ƒç´ æœ€å¤§å€¼ä¸æœ€å°å€¼çš„å·® print(np.percentile(a, q = 70, axis = 0)) # ç™¾åˆ†ä½æ•° print(np.sum(a)) # æ±‚å’Œ print(np.median(a)) # ä¸­ä½æ•° print(np.mean(a)) # å‡å€¼ print(np.average(a, axis = 0, weights = [3, 2, 1], returned = True)) # åŠ æƒå¹³å‡æ•° print(np.std(a)) # æ ‡å‡†å·® print(np.var(a)) # æ–¹å·® æ’åº import numpy as np a = np.array([(&quot;raju&quot;,21), (&quot;anil&quot;,25), (&quot;ravi&quot;,17), (&quot;amar&quot;,27)], dtype = np.dtype([(&#39;name&#39;,&#39;S10&#39;), (&#39;age&#39;,int)])) print(a) print(np.sort(a, order = &#39;age&#39;)) # ä»å°åˆ°å¤§æ’åº print(np.argsort(a, order = &#39;age&#39;)) # ä»å°åˆ°å¤§æ’åºçš„ç´¢å¼• çº¿æ€§ä»£æ•° import numpy as np a = np.arange(6).reshape(2, 3) b = np.arange(6).reshape(3, 2) c = a.copy() # å¤åˆ¶ print(a) print(a.T) # è½¬ç½® print(np.dot(a,b)) # æ•°ç»„ç‚¹ç§¯ print(np.vdot(a,b)) # å‘é‡ç‚¹ç§¯, å¤šç»´æ•°ç»„ä¼šè¢«å±•å¼€ print(np.inner(a,c)) # å‘é‡å†…ç§¯ï¼Œå¯¹äºæ›´é«˜çš„ç»´åº¦ï¼Œå®ƒè¿”å›æœ€åä¸€ä¸ªè½´ä¸Šçš„å’Œçš„ä¹˜ç§¯ import numpy as np a = np.arange(4).reshape(2, 2) print(a) print(np.diag(a)) # å¯¹è§’é˜µ print(np.linalg.inv(a)) # é€† print(np.linalg.det(a)) # è¡Œåˆ—å¼ print(np.linalg.eig(a)) # ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡ print(np.linalg.svd(a)) # å¥‡å¼‚å€¼åˆ†è§£ éšæœºæ•° import numpy as np np.random.seed(123) # éšæœºæ•°ç§å­ print(np.random.rand(2, 2)) # å‡åŒ€åˆ†å¸ƒ print(np.random.randn(2, 3)) # æ ‡å‡†æ­£æ€åˆ†å¸ƒ print(np.random.randint(low = 0, high = 100, size = (2, 2))) # éšæœºæ•´æ•° # åˆ†å¸ƒ print(np.random.normal(loc = 3, scale = 9, size = 2)) # æ­£æ€ print(np.random.poisson(lam = 10, size = 6)) # æ³Šæ¾ print(np.random.binomial(n = 10, p = 0.1, size = (2, 2))) # äºŒé¡¹ print(np.random.negative_binomial(n = 10, p = 0.1, size = 1)) # è´ŸäºŒé¡¹ print(np.random.gamma(shape = 3, scale = 2, size = 10)) # ä¼½é©¬ 3.10.4 pandasåŒ… pandasæ˜¯åŸºäºNumPyçš„ä¸€ä¸ªä¸ºè§£å†³æ•°æ®åˆ†æä»»åŠ¡è€Œåˆ›å»ºçš„åŒ…ï¼Œæä¾›äº†å¤§é‡èƒ½ä½¿æˆ‘ä»¬å¿«é€Ÿä¾¿æ·åœ°å¤„ç†æ•°æ®çš„å‡½æ•°å’Œæ–¹æ³•ã€‚ åˆ›å»ºDataFrame import pandas as pd data1 = pd.read_csv(&#39;file.csv&#39;, encoding = &#39;gbk&#39;) # ä»å¤–éƒ¨è¯»å…¥csvæ–‡ä»¶ data2 = {&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], # å…ˆåˆ›å»ºå­—å…¸ &#39;year&#39;: [2000, 2001, 2002, 2001, 2002], &#39;pop&#39;: [1.5, 1.7, 3.6, 2.4, 2.9]} data2 = pd.DataFrame(data2, columns = [&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;]) # åŸºäºå­—å…¸åˆ›å»ºDataFrame data2[&#39;debt&#39;] = 16.5 # æ–°å¢ä¸€åˆ—debt print(data1, data2, sep = &#39;\\n\\n&#39;) print(data2.dtypes) # å…ƒç´ ç±»å‹ print(data2.columns) # åˆ—å print(data2.shape) # å½¢çŠ¶ print(data2.head(10)) # çœ‹å‰10æ¡è®°å½• print(data2.tail(5)) # çœ‹å5æ¡è®°å½• ç´¢å¼• import pandas as pd data = {&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], &#39;year&#39;: [2000, 2001, 2002, 2001, 2002], &#39;pop&#39;: [1.5, 1.7, 3.6, 2.4, 2.9]} data = pd.DataFrame(data, columns = [&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;]) data[&#39;debt&#39;] = 16.5 print(data) print(data[0:2]) # ç´¢å¼•ç¬¬1-2è¡Œ print(data.iloc[0:2]) # ç´¢å¼•ç¬¬1-2è¡Œ print(data.loc[0:2]) # ç´¢å¼•indexä¸º0-2çš„è¡Œ print(data[&#39;year&#39;]) # ç´¢å¼•åä¸ºyearçš„åˆ— print(data.loc[0,&#39;year&#39;]) print(data.iloc[0:2, 0:2]) # ç´¢å¼•ç¬¬1-2è¡Œã€ç¬¬1-2åˆ— print(data[data[&#39;pop&#39;]&gt;2]) # ç´¢å¼•pop&gt;2çš„è¡Œ print(data[(data[&#39;pop&#39;]&gt;2) &amp; (data[&#39;state&#39;] == &#39;Ohio&#39;)]) # ç´¢å¼•pop&gt;2ä¸”stateæ˜¯Ohioçš„è¡Œ print(data[(data[&#39;pop&#39;]&gt;2) &amp; (data[&#39;state&#39;] == &#39;Ohio&#39;)][[&#39;year&#39;, &#39;debt&#39;]]) # ç´¢å¼•pop&gt;2ä¸”stateæ˜¯Ohioçš„è¡Œã€åä¸ºyearå’Œdebtçš„åˆ— æ•°æ®é¢„å¤„ç† import pandas as pd data = {&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], &#39;year&#39;: [2000, 2001, 2001, 2001, 2002], &#39;pop&#39;: [1.5, 1.7, 1.7, 2.4, None]} data = pd.DataFrame(data, columns = [&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;]) data[&#39;debt&#39;] = 16.5 print(data) print(data.drop_duplicates()) # åˆ é™¤é‡å¤è¡Œ print(data.dropna(axis = 0, how = &quot;any&quot;)) # åˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ print(data.drop([&#39;debt&#39;], axis = 1)) # åˆ é™¤åˆ—debt print(pd.get_dummies(data, drop_first = True)) # ç”Ÿæˆå“‘å˜é‡ æ’åº import pandas as pd data = {&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], &#39;year&#39;: [2000, 2001, 2002, 2001, 2002], &#39;pop&#39;: [1.5, 1.7, 3.6, 2.4, 2.9]} data = pd.DataFrame(data, columns = [&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;]) data[&#39;debt&#39;] = 16.5 print(data.sort_values(by = &#39;year&#39;, ascending = True)) # æŒ‰ç…§yearçš„å€¼å‡åº print(data.sort_index(axis = 1)) # æŒ‰ç…§åˆ—ç´¢å¼•å‡åº print(data.rank()) # æ±‚ç§© ç»Ÿè®¡åˆ†æ import pandas as pd data = {&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], &#39;year&#39;: [2000, 2001, 2002, 2001, 2002], &#39;pop&#39;: [1.5, 1.7, 3.6, 2.4, 2.9]} data = pd.DataFrame(data, columns = [&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;]) data[&#39;debt&#39;] = 16.5 print(data.describe()) # å¯¹æ¯åˆ—è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡ print(data.count()) # è®¡æ•° print(data.max()) # æœ€å¤§å€¼ print(data.min()) # æœ€å°å€¼ print(data.sum()) # å’Œ print(data.mean()) # å‡å€¼ print(data.median()) # ä¸­ä½æ•° print(data.var()) # æ–¹å·® print(data.std()) # æ ‡å‡†å·® print(data.cov()) # åæ–¹å·® print(data.corr()) # ç›¸å…³ç³»æ•° 3.10.5 MatplotlibåŒ… Matplotlibæ˜¯ä¸€ä¸ªPython çš„2Dç»˜å›¾åº“ï¼Œå®ƒä»¥å„ç§ç¡¬æ‹·è´æ ¼å¼å’Œè·¨å¹³å°çš„äº¤äº’å¼ç¯å¢ƒç”Ÿæˆå‡ºç‰ˆè´¨é‡çº§åˆ«çš„å›¾å½¢ã€‚ æŠ˜çº¿å›¾ import numpy as np from matplotlib import pyplot as plt x = np.arange(1,11) y = 2*x + 5 plt.title(&#39;Matplotlib demo&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.plot(x, y, ls = &#39;--&#39;, marker = &#39;+&#39;, color = &#39;lightblue&#39;) # lsä¸ºçº¿å‹ï¼Œmarkerä¸ºæ ‡è®°ç±»å‹ plt.show() æ•£ç‚¹å›¾ import numpy as np from matplotlib import pyplot as plt x = np.random.random(100) y = np.random.random(100) plt.scatter(x, y, s=x*1000, color=&#39;pink&#39;, marker=(5,1), alpha=0.5, lw=2) # sä¸ºå›¾åƒå¤§å°ï¼Œlwä¸ºå›¾åƒè¾¹æ¡†å®½åº¦ plt.show() ç®±çº¿å›¾ import numpy as np from matplotlib import pyplot as plt x = np.random.gamma(shape = 3, scale = 2, size = 10) plt.boxplot(x, vert=True) # vertæ§åˆ¶æ–¹å‘ plt.show() æ¡å½¢å›¾ import numpy as np from matplotlib import pyplot as plt x_index = np.arange(5) #æŸ±çš„ç´¢å¼• x_data = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;] y1_data = [20, 35, 30, 35, 27] y2_data = [25, 32, 34, 20, 25] plt.bar(x_index, y1_data, width=0.35, alpha=0.8, color=&#39;lightblue&#39;, label=&#39;y1&#39;) # å‚æ•°ï¼šå·¦åç§»ã€é«˜åº¦ã€æŸ±å®½ã€é€æ˜åº¦ã€é¢œè‰²ã€å›¾ä¾‹ plt.bar(x_index + 0.35, y2_data, width=0.35, alpha=0.8, color=&#39;pink&#39;, label=&#39;y2&#39;) plt.xticks(x_index + bar_width/2, x_data) # xè½´åˆ»åº¦çº¿ plt.legend() # æ˜¾ç¤ºå›¾ä¾‹ plt.show() ç›´æ–¹å›¾ import numpy as np from matplotlib import pyplot as plt x = np.random.randn(10000) plt.hist(x, bins=40, density=True, histtype=&#39;bar&#39;, color=&#39;lightblue&#39;) plt.show() é¥¼å›¾ from matplotlib import pyplot as plt labels = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;] x = [15, 30, 45, 10] explode = (0, 0.1, 0, 0) colors = [&#39;pink&#39;, &#39;tomato&#39;, &#39;lightblue&#39;, &#39;lightyellow&#39;] plt.pie(x, labels=labels, autopct=&#39;%1.1f%%&#39;, shadow=False, explode=explode, startangle=90, colors=colors) plt.axis(&#39;equal&#39;) plt.legend(labels=labels, loc=&#39;right&#39;) plt.show() 3.10.6 å¸¸ç”¨æ•™ç¨‹ç½‘å€ PythonåŸºç¡€æ•™ç¨‹ Python3è¯´æ˜æ–‡æ¡£ matplotlibå®˜ç½‘ sklearnå­¦ä¹  "],["nn.html", "4 æ·±åº¦å­¦ä¹ ä¸ç´¢èµ”é¢‘ç‡é¢„æµ‹ 4.1 å»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ­¥éª¤ 4.2 æ•°æ®é¢„å¤„ç† 4.3 ç¥ç»ç½‘ç»œæå‡æ¨¡å‹ ï¼ˆcombined actuarial neural networkï¼‰ 4.4 ç¥ç»ç½‘ç»œç»“æ„ 4.5 è®­ç»ƒç¥ç»ç½‘ç»œ 4.6 æ€»ç»“ 4.7 å…¶å®ƒæ¨¡å‹", " 4 æ·±åº¦å­¦ä¹ ä¸ç´¢èµ”é¢‘ç‡é¢„æµ‹ æ­£å¦‚è®¡ç®—æœºé€Ÿåº¦çš„æå‡å’ŒMCMCæ–¹æ³•ç»™è´å¶æ–¯ç»Ÿè®¡å¸¦æ¥äº†ç”Ÿæœºï¼Œè®¡ç®—æœºè¿ç®—èƒ½åŠ›çš„æå‡å’Œåå‘ä¼ æ’­ç®—æ³•ï¼ˆback propagationï¼‰ä¹Ÿç»™ç¥ç»ç½‘ç»œå¸¦æ¥äº†é£é€Ÿå‘å±•ã€‚ Tensorflowå’ŒPytorchçš„æ›´æ–°é€Ÿåº¦åæ˜ äº†è¿™ä¸ªé¢†åŸŸçš„çƒ­åº¦ã€‚ 4.1 å»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ­¥éª¤ 4.1.1 æ˜ç¡®ç›®æ ‡å’Œæ•°æ®ç±»å‹ ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§éå‚éçº¿æ€§å›å½’æ¨¡å‹ï¼Œå®ƒå¯ä»¥åˆ»ç”»éçº¿æ€§æ•ˆåº”å’Œäº¤äº’æ•ˆåº”ã€‚ åœ¨ä½¿ç”¨ç¥ç»ç½‘ç»œå‰ï¼Œéœ€è¦ç†è§£ç ”ç©¶ç›®æ ‡å’Œæ•°æ®ç»“æ„ï¼Œæœ‰ä¸€äº›ç‰¹æ®Šçš„layerï¼Œå¦‚å·ç§¯å±‚ï¼Œä¸“é—¨ä¸ºæŸç§ä»»åŠ¡ã€æˆ–æŸç§æ•°æ®ç»“æ„è€Œè®¾ç«‹ï¼Œä¸æ˜¯å¯ä»¥ç”¨åœ¨ä»»ä½•çš„æ•°æ®ä¸Šã€‚ ç¥ç»ç½‘ç»œæœ‰å¤§é‡çš„å‚æ•°ï¼Œå…¨å±€æœ€ä¼˜è§£å¿…ç„¶ä¼šé€ æˆè¿‡æ‹Ÿåˆï¼Œé€šå¸¸åˆ©ç”¨éªŒè¯é›†æŸå¤±æ¥åˆ¤æ–­æ¢¯åº¦ä¸‹é™çš„æ¬¡æ•°ã€‚ 4.1.2 æ•°æ®é¢„å¤„ç† æè¿°æ€§ç»Ÿè®¡åˆ†æ ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å¤„ç† è¿ç»­å‹å˜é‡æ ‡å‡†åŒ–ï¼Œä¸»è¦æ˜¯ä¸ºäº†è®©æ¢¯åº¦ä¸‹é™æ³•æ›´æœ‰æ•ˆåœ°å·¥ä½œã€‚å¸¸ç”¨çš„æ ‡å‡†åŒ–æ–¹æ³•æœ‰MinMaxScaler \\[x^*=2\\frac{x-\\min x}{\\max x - \\min x}-1\\] åˆ†ç±»å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨dummy codingã€one-hot encodingï¼Œæˆ–è€…ä½¿ç”¨ç¥ç»ç½‘ç»œä¸­çš„embedding layerã€‚ç¬¬ä¸‰ç§åŠæ³•å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘å‚æ•°ä¸ªæ•°ã€‚ è®­ç»ƒ-éªŒè¯-æµ‹è¯•æ•°æ®åˆ†å‰²ï¼šè®­ç»ƒé›†ç”¨äºæ¢¯åº¦ä¸‹é™æ³•æ±‚è§£å‚æ•°ï¼ŒéªŒè¯é›†ç”¨äºåˆ¤æ–­epochæ¬¡æ•°ã€è°ƒæ•´æ¨¡å‹ç»“æ„çš„è¶…å‚æ•°ï¼Œæµ‹è¯•é›†ç”¨äºæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ ·æœ¬å¤–é¢„æµ‹èƒ½åŠ›ã€‚ 4.1.3 é€‰å–åˆé€‚çš„ç¥ç»ç½‘ç»œç±»å‹ å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼šé€‚ç”¨äºä¸€èˆ¬çš„å›å½’é—®é¢˜ï¼Œå¦‚ç´¢èµ”é¢‘ç‡é¢„æµ‹ã€‚ä¿¡æ¯ä¸€ç›´å‘å‰ä¼ é€’ã€‚å‚æ•°ä¸ªæ•°è¾ƒå¤šã€‚å¯è§£é‡Šæ€§å·®ã€‚ å·ç§¯ç¥ç»ç½‘ç»œï¼šé€‚ç”¨äºæ•°æ®æœ‰ç©ºé—´ç»“æ„ï¼Œä¸”ç›¸åŒçš„æ¨¡å¼å¯èƒ½å‡ºç°åœ¨ä¸åŒçš„ä½ç½®ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€‚ä¿¡æ¯ä¸€ç›´å‘å‰ä¼ é€’ã€‚å‚æ•°ä¸ªæ•°è¾ƒå°‘ã€‚æœ‰å¯è§£é‡Šæ€§ã€‚ é€’å½’ç¥ç»ç½‘ç»œï¼šé€‚ç”¨äºæ•°æ®æœ‰æ—¶é—´åºåˆ—ç‰¹å¾ï¼Œä¸”ç›¸åŒçš„æ¨¡å¼å¯èƒ½å‡ºç°åœ¨ä¸åŒæ—¶é—´ç‚¹ï¼Œå¦‚å¤©æ°”é¢„æŠ¥ã€è¯­éŸ³è¯†åˆ«ã€‚ä¿¡æ¯å¯ä»¥è¿”å›åˆ°å‰é¢çš„ç¥ç»å…ƒã€‚å‚æ•°ä¸ªæ•°è¾ƒå¤šã€‚å¯è§£é‡Šæ€§å·®ã€‚ 4.1.4 å»ºç«‹ç¥ç»ç½‘ç»œï¼ˆå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼‰ åœ¨å»ºç«‹ç¥ç»ç½‘ç»œæ—¶ï¼Œéœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ç‚¹ï¼š è¾“å…¥å±‚æ•°æ®ç±»å‹ éšè—å±‚å±‚æ•°ï¼Œéšè—å±‚æ€§è´¨ï¼Œç¥ç»å…ƒä¸ªæ•°ï¼Œæ¿€æ´»å‡½æ•°ï¼Œæ­£åˆ™åŒ–ï¼Œdropout è¾“å‡ºç¥ç»å…ƒæ•°æ®ç±»å‹ï¼Œè¾“å‡ºç¥ç»å…ƒæ¿€æ´»å‡½æ•° æŸå¤±å‡½æ•°é€‰æ‹© 4.1.5 è®­ç»ƒç¥ç»ç½‘ç»œ åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œéœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ç‚¹ æ¢¯åº¦ä¸‹é™æ³•ï¼ˆoptimizerï¼‰ è¿­ä»£æ¬¡æ•°ï¼ˆpatienceï¼‰ éå†æ¬¡æ•°ï¼ˆepochï¼‰ï¼Œæ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰ 4.1.6 è°ƒå‚ è¿”å›ç¬¬4æ­¥ï¼Œè°ƒæ•´æ¨¡å‹ç»“æ„çš„è¶…å‚æ•°ï¼ˆhyper-parameter tuningï¼‰ï¼Œè§‚å¯ŸéªŒè¯æŸå¤±çš„å˜åŒ–ï¼Œé€‰å–æœ€ç»ˆæ¨¡å‹ã€‚ 4.2 æ•°æ®é¢„å¤„ç† &#39;data.frame&#39;: 678013 obs. of 12 variables: $ IDpol : num 1 3 5 10 11 13 15 17 18 21 ... $ ClaimNb : &#39;table&#39; num [1:678013(1d)] 1 1 1 1 1 1 1 1 1 1 ... $ Exposure : num 0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ... $ VehPower : int 5 5 6 7 7 6 6 7 7 7 ... $ VehAge : int 0 0 2 0 0 2 2 0 0 0 ... $ DrivAge : int 55 55 52 46 46 38 38 33 33 41 ... $ BonusMalus: int 50 50 50 50 50 50 50 68 68 50 ... $ VehBrand : Factor w/ 11 levels &quot;B1&quot;,&quot;B10&quot;,&quot;B11&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ VehGas : chr &quot;Regular&quot; &quot;Regular&quot; &quot;Diesel&quot; &quot;Diesel&quot; ... $ Area : Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 4 4 2 2 2 5 5 3 3 2 ... $ Density : int 1217 1217 54 76 76 3003 3003 137 137 60 ... $ Region : Factor w/ 21 levels &quot;Alsace&quot;,&quot;Aquitaine&quot;,..: 21 21 18 2 2 16 16 13 13 17 ... åœ¨è¿›è¡Œä¸‹é¢codeä¹‹å‰ï¼Œéœ€è¦è¿è¡Œä¸Šä¸€ç« çš„ä»£ç ç›´åˆ°Treeä¹‹å‰ã€‚ è¿ç»­å‹å˜é‡ï¼šæ ‡å‡†åŒ–å¤„ç†ã€‚ PreProcess.Continuous &lt;- function(var1, dat1){ names(dat1)[names(dat1) == var1] &lt;- &quot;V1&quot; dat1$X &lt;- as.numeric(dat1$V1) dat1$X &lt;- 2*(dat1$X-min(dat1$X))/(max(dat1$X)-min(dat1$X))-1 names(dat1)[names(dat1) == &quot;V1&quot;] &lt;- var1 names(dat1)[names(dat1) == &quot;X&quot;] &lt;- paste(var1,&quot;X&quot;, sep=&quot;&quot;) dat1 } Features.PreProcess &lt;- function(dat1){ dat1$VehPower &lt;- pmin(dat1$VehPower,9) dat1 &lt;- PreProcess.Continuous(&quot;VehPower&quot;, dat1) dat1$VehAge &lt;- pmin(dat1$VehAge,20) dat1 &lt;- PreProcess.Continuous(&quot;VehAge&quot;, dat1) dat1$DrivAge &lt;- pmin(dat1$DrivAge,90) dat1 &lt;- PreProcess.Continuous(&quot;DrivAge&quot;, dat1) dat1$BonusMalus &lt;- pmin(dat1$BonusMalus,150) dat1 &lt;- PreProcess.Continuous(&quot;BonusMalus&quot;, dat1) dat1$VehBrandX &lt;- as.integer(dat1$VehBrand)-1 # categorical variable dat1$VehGas &lt;- as.factor(dat1$VehGas) dat1$VehGasX &lt;- as.integer(dat1$VehGas) - 1.5 # binary: continuous or categorical dat1 &lt;- PreProcess.Continuous(&quot;Area&quot;, dat1) dat1 &lt;- PreProcess.Continuous(&quot;Density&quot;, dat1) dat1$RegionX &lt;- as.integer(dat1$Region) - 1 # categorical dat1 } dat2 &lt;- Features.PreProcess(freMTPL2freq) names(dat2) dat2_train&lt;-dat2[index_train,] dat2_valid&lt;-dat2[index_valid,] dat2_test&lt;-dat2[index_test,] dat2_learn&lt;-dat2[index_learn,] åˆ†ç±»å˜é‡ï¼š é‡‡ç”¨embedding layerã€‚ è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›†ï¼šåˆ†å±‚æŠ½æ ·ã€‚ è°ƒæ•´æ•°æ®ç»“æ„ï¼Œä½¿å…¶åŒ¹é…ç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚åŠå…¶ç»´åº¦ã€‚è¾“å…¥æ•°æ®é›†åŒ…æ‹¬Xtrain, Brtain, Retrain, Vtrainï¼Œå› å˜é‡æ•°æ®é›†ä¸ºYtrainã€‚ lambda.hom &lt;- sum(dat2_train$ClaimNb)/sum(dat2_train$Exposure);lambda.hom names(dat2) # index of continous variables (non-categorical) features &lt;- c(13:16, 18:20) names(dat2_learn)[features] (q0 &lt;- length(features)) # training data Xtrain&lt;- as.matrix(dat2_train[, features]) # design matrix learning sample Brtrain &lt;- as.matrix(dat2_train$VehBrandX) Retrain &lt;- as.matrix(dat2_train$RegionX) Ytrain&lt;- as.matrix(dat2_train$ClaimNb) Vtrain&lt;-as.matrix(log(dat2_train$Exposure*lambda.hom)) # validation data Xvalid&lt;- as.matrix(dat2_valid[, features]) # design matrix learning sample Brvalid &lt;- as.matrix(dat2_valid$VehBrandX) Revalid &lt;- as.matrix(dat2_valid$RegionX) Yvalid&lt;- as.matrix(dat2_valid$ClaimNb) Vvalid&lt;-as.matrix(log(dat2_valid$Exposure*lambda.hom)) xxvalid&lt;-list(Xvalid,Brvalid,Revalid,Vvalid) # testing data Xtest &lt;- as.matrix(dat2_test[, features]) # design matrix test sample Brtest &lt;- as.matrix(dat2_test$VehBrandX) Retest &lt;- as.matrix(dat2_test$RegionX) Ytest &lt;- as.matrix(dat2_test$ClaimNb) Vtest &lt;- as.matrix(log(dat2_test$Exposure*lambda.hom)) 4.3 ç¥ç»ç½‘ç»œæå‡æ¨¡å‹ ï¼ˆcombined actuarial neural networkï¼‰ åŸºæœ¬ç»“æ„ \\[\\ln \\lambda(\\mathbf{x})= e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\] å…¶ä¸­ï¼Œ\\(\\hat{\\lambda}^{\\text{GAM}}\\)ä¸ºå¹¿ä¹‰å¯åŠ è¾¹ç¼˜æå‡æ¨¡å‹çš„ç´¢èµ”é¢‘ç‡ä¼°è®¡å€¼ï¼ˆå‚è§ä¸Šä¸€ç« ï¼‰ï¼Œ\\(\\hat{\\lambda}^{\\text{NN}}\\)ä¸ºç¥ç»ç½‘ç»œç´¢èµ”é¢‘ç‡çš„ä¼°è®¡å€¼ï¼Œç¬¬ä¸€é¡¹åœ¨æ¨¡å‹è®­ç»ƒä¸­ä¿æŒä¸å˜ã€‚ ä½¿ç”¨ä¸Šè¿°æ¨¡å‹çš„ä¼˜ç‚¹ï¼š \\(\\hat{\\lambda}^{\\text{GAM}}\\)çš„éƒ¨åˆ†å¯è§£é‡Šæ€§ã€‚ ç¥ç»ç½‘ç»œä»ä¸€ä¸ªç›¸å¯¹â€œè¾ƒå¥½â€çš„åˆå§‹çŠ¶æ€\\(e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\)å¼€å§‹è®­ç»ƒï¼Œå¾ˆå¿«æ”¶æ•›ã€‚ åœ¨ä»£ç å®ç°ä¸­ï¼Œå¯ä»¥æŠŠ\\(e\\hat{\\lambda}^{\\text{GAM}}\\)å½“ä½œä¼ªé£é™©æš´éœ²æ•°ã€‚ CANN &lt;- 1 # 0 = normal NN, 1=CANN if (CANN==1){ Vtrain &lt;- as.matrix(log(dat1_train$fitGAM1)) Vvalid&lt;- as.matrix(log(dat1_valid$fitGAM1)) Vtest &lt;- as.matrix(log(dat1_test$fitGAM1)) } 4.4 ç¥ç»ç½‘ç»œç»“æ„ åœ¨æ„å»ºç¥ç»ç½‘ç»œæ—¶ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š 4.4.1 ç»“æ„å‚æ•° ç¥ç»ç½‘ç»œç»“æ„çš„è¶…å‚æ•°é€‰æ‹©ï¼ŒBrLabelä¸ºè½¦å‹ä¸ªæ•°ï¼ŒReLabelä¸ºåœ°åŒºä¸ªæ•°ï¼Œq1-q4ä¸ºå››ä¸ªéšè—å±‚ä¸­ç¥ç»å…ƒä¸ªæ•°ï¼Œdä¸ºembedding layerä¸­ç¥ç»å…ƒä¸ªæ•°ã€‚ # hyperparameters of the neural network architecture (BrLabel &lt;- length(unique(dat2_train$VehBrandX))) (ReLabel &lt;- length(unique(dat2_train$RegionX))) q1 &lt;- 20 q2 &lt;- 15 q3 &lt;- 10 q4 &lt;- 5 d &lt;- 1 # dimensions embedding layers for categorical features 4.4.2 è¾“å…¥å±‚ è¾“å…¥å±‚åŒ…æ‹¬Design, VehBrand, Region, LogVolï¼Œå…¶ä¸­Designä¸ºè¿ç»­å‹åå˜é‡çš„è¾“å…¥å±‚ï¼ŒVehBrand, Regionä¸ºåˆ†ç±»å˜é‡çš„è¾“å…¥å±‚ï¼ŒLogVolç›´æ¥è¿æ¥åˆ°æ¨¡å‹è¾“å‡ºç¥ç»å…ƒã€‚ shapeè¡¨ç¤ºè¾“å…¥ï¼ˆè¾“å‡ºï¼‰ç»´åº¦ï¼ˆç¥ç»å…ƒä¸ªæ•°ï¼‰,dtypeè¡¨ç¤ºæ•°æ®ç±»å‹ï¼Œnameè¡¨ç¤ºå±‚åã€‚ shape=(None, 7) ä¸­Noneè¡¨ç¤ºæ ·æœ¬å¤§å°ï¼Œå› ä¸ºè¿˜æ²¡æœ‰æ•°æ®è¿›å…¥ç¥ç»ç½‘ç»œï¼Œæ•…æ­¤æ—¶ä¸ç¡®å®šã€‚ Tensor(â€œDesign_1:0â€, shape=(None, 7), dtype=float32) # input layer (Design &lt;- layer_input(shape = c(q0), dtype = &#39;float32&#39;, name = &#39;Design&#39;)) (VehBrand &lt;- layer_input(shape = c(1), dtype = &#39;int32&#39;, name = &#39;VehBrand&#39;)) (Region &lt;- layer_input(shape = c(1), dtype = &#39;int32&#39;, name = &#39;Region&#39;)) (LogVol &lt;- layer_input(shape = c(1), dtype = &#39;float32&#39;, name = &#39;LogVol&#39;)) 4.4.3 Embedding layer å»ºç«‹ä¸€ä¸ªlayerï¼Œéœ€è¦æ˜ç¡®è¾“å…¥ç¥ç»å…ƒä¸ªæ•°input_dimå’Œè¾“å‡ºç¥ç»å…ƒä¸ªæ•°output_dimï¼Œé€šå¸¸éœ€è¦æŒ‡å®šè¾“å‡ºç¥ç»å…ƒä¸ªæ•°ï¼Œè€Œè¾“å…¥ç¥ç»å…ƒä¸ªæ•°ç”±å®ƒçš„ä¸Šå±‚è¾“å‡ºç¥ç»å…ƒä¸ªæ•°å†³å®šã€‚ æŠŠåˆ†ç±»å˜é‡ç”¨layer_embeddingå¤„ç†ï¼Œè¿™ä¸¤ä¸ªlayer_embeddingçš„è¾“å‡ºç¥ç»å…ƒä¸ªæ•°ä¸º\\(d\\)ï¼Œå³æ¯ä¸ªæ°´å¹³é€šè¿‡layer_embeddingè¾“å‡º\\(d\\)ä¸ªè¿ç»­å‹å˜é‡ï¼Œå½“\\(d=1\\)ï¼Œlayer_embeddingç±»ä¼¼äºGLMå¯¹åˆ†ç±»å˜é‡çš„å¤„ç†ã€‚input_lengthä¸»è¦ç”¨äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¦‚æ¯ä¸ªæ ·æœ¬ä¸ºå¤šä¸ªè¯ç»„æˆçš„ä¸€å¥è¯ï¼Œè¿™é‡Œä¸€ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªæ°´å¹³ï¼Œæ•…input_length = 1ã€‚ layer_flattenç”¨äºè°ƒæ•´ç»´åº¦ï¼Œlayer_flattençš„è¾“å…¥ç»´åº¦æ˜¯\\(n\\times 1\\times d\\)ï¼Œè¾“å‡ºç»´åº¦æ˜¯\\(n\\times d\\)ï¼Œè¯¥å±‚æ²¡æœ‰å‚æ•°ã€‚è¯¥è¾“å‡ºç»´åº¦æ˜¯layer_denseè¦æ±‚çš„è¾“å…¥ç»´åº¦ã€‚å»ºç«‹ç¥ç»ç½‘ç»œéœ€è¦æ³¨æ„å±‚é—´ç»´åº¦åŒ¹é…ã€‚ BrandEmbå»ºç«‹çš„æ˜ å°„ä¸º\\(\\{1,\\ldots,11\\}\\rightarrow 1\\times\\mathbf{R}^d\\rightarrow\\mathbf{R}^d\\). RegionEmbå»ºç«‹çš„æ˜ å°„ä¸º\\(\\{1,\\ldots,21\\}\\rightarrow 1\\times\\mathbf{R}^d\\rightarrow\\mathbf{R}^d\\) # embedding layer (BrandEmb = VehBrand %&gt;% layer_embedding(input_dim = BrLabel, output_dim = d, input_length = 1, name = &#39;BrandEmb&#39;) %&gt;% layer_flatten(name=&#39;Brand_flat&#39;)) # input_dim is the size of vocabulary; input_length is the length of input sequences (RegionEmb = Region %&gt;% layer_embedding(input_dim = ReLabel, output_dim = d, input_length = 1, name = &#39;RegionEmb&#39;) %&gt;% layer_flatten(name=&#39;Region_flat&#39;)) 4.4.4 éšè—å±‚ 9 Networkå»ºç«‹çš„æ˜ å°„ä¸º\\[[-1,1]^{q0}\\times\\mathbf{R}^d\\times\\mathbf{R}^d\\rightarrow (-1,1)^{q1}\\rightarrow (-1,1)^{q2}\\\\ \\rightarrow (-1,1)^{q3}\\rightarrow (-1,1)^{q4}\\rightarrow\\mathbf{R}\\] layer_concatenateæŠŠä¸‰ä¸ªè¾“å…¥å±‚è¿èµ·æ¥ï¼Œlayer_dropoutä¸ºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œlayer_batch_normalizationä¸ºé˜²æ­¢vanishing gradient problemï¼Œè¿™ä¸‰ç§å±‚å†…æ— å‚æ•°ï¼Œä¸”ä¸ä¼šæ”¹å˜ä¸Šå±‚çš„ç»´åº¦ã€‚layer_dropoutä»¤ä¸€å®šæ¯”ä¾‹çš„ä¸Šå±‚ç¥ç»å…ƒä¸º0ï¼Œæ­£åˆ™åŒ–æ–¹æ³•è¿˜åŒ…æ‹¬åœ¨layer_denseä¸­ä½¿ç”¨\\(L^2\\)èŒƒæ•°æ­£åˆ™åŒ–kernel_regularizer = regularizer_l2ã€‚layer_batch_normalizationæŠŠè¾“å‡ºç¥ç»å…ƒæ˜ å°„åˆ°\\((-1,1)\\)ï¼Œé€šå¸¸åœ¨æ¿€æ´»å‡½æ•°ä¸ºreluæ›´æœ‰ç”¨ã€‚ å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ä¸ºtanh, relu, linear, exponential, softmax, sigmoidã€‚å…¶ä¸­ï¼Œsigmoid, softmaxé€‚ç”¨äºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»çš„è¾“å‡ºç¥ç»å…ƒï¼Œexponentialé€‚ç”¨äºå› å˜é‡ä¸ºæ­£ï¼Œå¦‚æ­¤æ—¶çš„ç´¢èµ”é¢‘ç‡é¢„æµ‹ã€‚æ­¤å¤–sigmoidå’Œtanhæœ‰çº¿æ€§å…³ç³»ï¼Œå¯ä»¥åªè€ƒè™‘å…¶ä¸­ä¸€ä¸ªã€‚ layer_denseçš„æ˜ å°„ä¸ºoutput = activation (dot (input, kernal) + bias)ï¼Œæ‰€ä»¥æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒéƒ½å«æœ‰è¾“å…¥ç¥ç»å…ƒçš„ä¿¡æ¯ã€‚å¦‚æœè€ƒè™‘å¤šä¸ªå…¨è¿æ¥å±‚ï¼Œå¯ä»¥åˆ»ç”»åå˜é‡çš„äº¤äº’æ•ˆåº”ç°ã€‚æ¿€æ´»å‡½æ•°å¦‚æœå–éçº¿æ€§å‡½æ•°ï¼Œåˆ™å¯ä»¥åˆ»ç”»åå˜é‡çš„éçº¿æ€§æ•ˆåº”ã€‚ Networkä¸­æœ€åä¸€å±‚çš„å‚æ•°è®¾å®šä¸º0ï¼Œä½¿å¾—Networkåˆå§‹å€¼ä¸º0ï¼Œè¿™æ ·ç¥ç»ç½‘ç»œåˆå§‹çŠ¶æ€ä¸ºGAMï¼Œæ¢¯åº¦ä¸‹é™å°†ä»GAMå¼€å§‹ã€‚ Network = list(Design, BrandEmb, RegionEmb) %&gt;% layer_concatenate(name=&#39;concate&#39;) %&gt;% layer_dense(units=q1, activation=&#39;tanh&#39;, name=&#39;hidden1&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=q2, activation=&#39;tanh&#39;, name=&#39;hidden2&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=q3, activation=&#39;tanh&#39;, name=&#39;hidden3&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=q4, activation=&#39;tanh&#39;, name=&#39;hidden4&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=1, activation=&#39;linear&#39;, name=&#39;Network&#39;, weights = list(array(0, dim=c(q4,1)), array(0, dim=c(1)))) 4.4.5 è¾“å‡ºå±‚ Responseå»ºç«‹çš„æ˜ å°„ä¸º\\(\\mathbf{R}\\times \\mathbf{R}\\rightarrow \\mathbf{R}^+\\)ï¼Œä¸”è¦æ±‚è¯¥æ˜ å°„ä¸­çš„å‚æ•°ä¸å‚åŠ æ¢¯åº¦ä¸‹é™æ³•ã€‚å¯ä»¥çœ‹åˆ°Networkçš„è¾“å‡ºç¥ç»å…ƒä¸º\\(\\ln \\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\)ï¼Œè¾“å…¥å±‚LogVolä¸º\\(\\ln e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\)ï¼ŒResponseçš„è¾“å‡ºç¥ç»å…ƒä¸º\\[\\exp\\left(\\ln \\hat{\\lambda}^{\\text{NN}}(\\mathbf{x}) + \\ln e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\right)=e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x}).\\] é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•ä½¿å¾—è¾“å‡ºç¥ç»å…ƒ\\(e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\)ä¸è§‚å¯Ÿå€¼\\(N\\)æœ€æ¥è¿‘ï¼ˆç”¨æ³Šæ¾åå·®æŸå¤±åº¦é‡ï¼‰ï¼Œè¿›è€Œè®­ç»ƒç¥ç»ç½‘ç»œ\\(\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\)ä¸­çš„å‚æ•°ã€‚ Keraså®šä¹‰å¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] Response = list(Network, LogVol) %&gt;% layer_add(name=&#39;Add&#39;) %&gt;% layer_dense(units=1, activation=k_exp, name = &#39;Response&#39;, trainable=FALSE, weights=list(array(1, dim=c(1,1)), array(0, dim=c(1)))) model &lt;- keras_model(inputs = c(Design, VehBrand, Region, LogVol), outputs = c(Response)) model %&gt;% compile(optimizer = optimizer_nadam(), loss = &#39;poisson&#39;) summary(model) ä¸‹è¡¨åˆ—å‡ºäº†ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼ŒåŒ…æ‹¬å±‚çš„åç§°ã€(å±‚çš„ç‰¹æ€§)ã€è¾“å‡ºç¥ç»å…ƒä¸ªæ•°ã€å‚æ•°ä¸ªæ•°ã€ä¸Šå±‚çš„åç§°ã€‚ Model: &quot;model&quot; ________________________________________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================================================ VehBrand (InputLayer) [(None, 1)] 0 ________________________________________________________________________________________________________________________________ Region (InputLayer) [(None, 1)] 0 ________________________________________________________________________________________________________________________________ BrandEmb (Embedding) (None, 1, 1) 11 VehBrand[0][0] ________________________________________________________________________________________________________________________________ RegionEmb (Embedding) (None, 1, 1) 21 Region[0][0] ________________________________________________________________________________________________________________________________ Design (InputLayer) [(None, 7)] 0 ________________________________________________________________________________________________________________________________ Brand_flat (Flatten) (None, 1) 0 BrandEmb[0][0] ________________________________________________________________________________________________________________________________ Region_flat (Flatten) (None, 1) 0 RegionEmb[0][0] ________________________________________________________________________________________________________________________________ concate (Concatenate) (None, 9) 0 Design[0][0] Brand_flat[0][0] Region_flat[0][0] ________________________________________________________________________________________________________________________________ hidden1 (Dense) (None, 20) 200 concate[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_1 (BatchNormalization (None, 20) 80 hidden1[0][0] ________________________________________________________________________________________________________________________________ dropout (Dropout) (None, 20) 0 batch_normalization_1[0][0] ________________________________________________________________________________________________________________________________ hidden2 (Dense) (None, 15) 315 dropout[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_2 (BatchNormalization (None, 15) 60 hidden2[0][0] ________________________________________________________________________________________________________________________________ dropout_1 (Dropout) (None, 15) 0 batch_normalization_2[0][0] ________________________________________________________________________________________________________________________________ hidden3 (Dense) (None, 10) 160 dropout_1[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_3 (BatchNormalization (None, 10) 40 hidden3[0][0] ________________________________________________________________________________________________________________________________ dropout_2 (Dropout) (None, 10) 0 batch_normalization_3[0][0] ________________________________________________________________________________________________________________________________ hidden4 (Dense) (None, 5) 55 dropout_2[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_4 (BatchNormalization (None, 5) 20 hidden4[0][0] ________________________________________________________________________________________________________________________________ dropout_3 (Dropout) (None, 5) 0 batch_normalization_4[0][0] ________________________________________________________________________________________________________________________________ Network (Dense) (None, 1) 6 dropout_3[0][0] ________________________________________________________________________________________________________________________________ LogVol (InputLayer) [(None, 1)] 0 ________________________________________________________________________________________________________________________________ Add (Add) (None, 1) 0 Network[0][0] LogVol[0][0] ________________________________________________________________________________________________________________________________ Response (Dense) (None, 1) 2 Add[0][0] ================================================================================================================================ Total params: 970 Trainable params: 868 Non-trainable params: 102 ________________________________________________________________________________________________________________________________ 4.5 è®­ç»ƒç¥ç»ç½‘ç»œ è®­ç»ƒç¥ç»ç½‘ç»œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š åˆå§‹åŒ–ç¥ç»ç½‘ç»œï¼Œå°†ä»GAMå¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä¸”GAMé¢„æµ‹éƒ¨åˆ†ä¿æŒä¸å˜ã€‚ å½“batch_sizeä¸ºå…¨ä½“è®­ç»ƒé›†æ—¶ï¼Œä¸ºsteepest gradient decent methodï¼Œå‚æ•°åœ¨ä¸€ä¸ªepochåªè¿­ä»£ä¸€æ¬¡ã€‚ å½“batch_sizeæ¯”å…¨ä½“è®­ç»ƒé›†å°æ—¶ï¼Œä¸ºstochastic gradient decent methodï¼Œå‚æ•°åœ¨ä¸€ä¸ªepochè¿­ä»£æ¬¡æ•°çº¦ä¸ºtraining size / batch sizeã€‚ æ¢¯åº¦ä¸‹é™æ³•å¸¸å¼•å…¥momentumï¼Œè¿›è€Œæå‡ä¼˜åŒ–æ•ˆç‡ï¼Œå¦‚adam, nadam,rmspropç­‰ï¼Œè¿™äº›ç®—æ³•è‡ªåŠ¨é€‰æ‹©learning rate, momentum parametersç­‰ã€‚ callback_early_stopping (monitor = \"val_loss\", patience =10)è¡¨ç¤ºå¦‚æœéªŒè¯é›†æŸå¤±åœ¨10æ¬¡å†…æ²¡æœ‰æå‡ï¼Œé‚£ä¹ˆåœæ­¢è®­ç»ƒï¼Œç”±æ­¤å¯ä»¥æ§åˆ¶è¿­ä»£æ¬¡æ•°ã€‚ ä½¿ç”¨predictåœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ã€‚ # fitting the neural network early_stop &lt;- callback_early_stopping(monitor = &quot;val_loss&quot;, patience =10) # print_dot_callback &lt;- callback_lambda( # on_epoch_end = function(epoch, logs) { # if (epoch %% 50 == 0) cat(&quot;\\n&quot;) # cat(&quot;.&quot;) # } # ) {t1 &lt;- proc.time(); fit &lt;- model %&gt;% fit(list(Xtrain, Brtrain, Retrain, Vtrain), Ytrain, epochs=500, batch_size=5000, verbose=1, validation_data=list(xxvalid,Yvalid), callbacks=list(early_stop)); (proc.time()-t1)} # png(&quot;./plots/1/nn.png&quot;) matplot(cbind(fit$metrics$loss,fit$metrics$val_loss), type=&quot;l&quot;,xlab=&quot;epoch&quot;,ylab=&quot;Keras Poisson Loss&quot;) legend(&quot;topright&quot;,c(&quot;training loss&quot;,&quot;validation loss&quot;),lty=c(1,2),col=1:2) # dev.off() # calculating the predictions dat2_test$fitNN &lt;- as.vector(model %&gt;% predict(list(Xtest, Brtest, Retest, Vtest))) keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb) Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb) 4.6 æ€»ç»“ dev_sum&lt;-fread(&quot;./plots/1/dev_sum.csv&quot;)[,-1] AD&lt;-data.frame(model=&quot;Neural network&quot;,test_error=0,test_error_keras=0) dev_sum&lt;-rbind(dev_sum,AD) dev_sum$test_error[8]&lt;-round(Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb),4) dev_sum$test_error_keras[8]&lt;-round(keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb),4) # write.csv(dev_sum,&quot;./plots/1/dev_sum.csv&quot;) Boosting &gt; RF &gt; Tree &gt; NN &gt; GAM &gt; GLM &gt; Homo Boosting, RF, Tree, NNç›¸è¾ƒäºGAMçš„æå‡ä¸»è¦åœ¨äºäº¤äº’ä½œç”¨ï¼›GAMç›¸è¾ƒäºGLMçš„æå‡ä¸å¤§ï¼ŒåŸå› æ˜¯åœ¨GLMä¸­è¿›è¡Œäº†åˆé€‚çš„ç‰¹å¾å·¥ç¨‹ï¼Œå¯ä»¥åˆ»ç”»éçº¿æ€§æ•ˆåº”ã€‚ 4.7 å…¶å®ƒæ¨¡å‹ é€šè¿‡å°è¯•å‘ç°ï¼Œä¸»è¦å­˜åœ¨ä¸¤ä¸ªäº¤äº’ä½œç”¨ï¼šVehPower, VehAge, VehGas, VehBrandå’ŒDriAge, BonusMalusï¼Œå¯è®¾ç«‹å¦‚ä¸‹ç®€åŒ–çš„ç¥ç»ç½‘ç»œæå‡æ¨¡å‹ã€‚ train.x &lt;- list(as.matrix(dat2_train[,c(&quot;VehPowerX&quot;, &quot;VehAgeX&quot;, &quot;VehGasX&quot;)]), as.matrix(dat2_train[,&quot;VehBrandX&quot;]), as.matrix(dat2_train[,c(&quot;DrivAgeX&quot;, &quot;BonusMalus&quot;)]), as.matrix(log(dat1_train$fitGAM1)) ) valid.x &lt;- list(as.matrix(dat2_valid[,c(&quot;VehPowerX&quot;, &quot;VehAgeX&quot;, &quot;VehGasX&quot;)]), as.matrix(dat2_valid[,&quot;VehBrandX&quot;]), as.matrix(dat2_valid[,c(&quot;DrivAgeX&quot;, &quot;BonusMalus&quot;)]), as.matrix(log(dat1_valid$fitGAM1)) ) test.x &lt;- list(as.matrix(dat2_test[,c(&quot;VehPowerX&quot;, &quot;VehAgeX&quot;, &quot;VehGasX&quot;)]), as.matrix(dat2_test[,&quot;VehBrandX&quot;]), as.matrix(dat2_test[,c(&quot;DrivAgeX&quot;, &quot;BonusMalus&quot;)]), as.matrix(log(dat1_test$fitGAM1)) ) neurons &lt;- c(15,10,5) model.2IA &lt;- function(Brlabel){ Cont1 &lt;- layer_input(shape = c(3), dtype = &#39;float32&#39;, name=&#39;Cont1&#39;) Cat1 &lt;- layer_input(shape = c(1), dtype = &#39;int32&#39;, name=&#39;Cat1&#39;) Cont2 &lt;- layer_input(shape = c(2), dtype = &#39;float32&#39;, name=&#39;Cont2&#39;) LogExposure &lt;- layer_input(shape = c(1), dtype = &#39;float32&#39;, name = &#39;LogExposure&#39;) x.input &lt;- c(Cont1, Cat1, Cont2, LogExposure) # Cat1_embed = Cat1 %&gt;% layer_embedding(input_dim = Brlabel, output_dim = 1, trainable=TRUE, input_length = 1, name = &#39;Cat1_embed&#39;) %&gt;% layer_flatten(name=&#39;Cat1_flat&#39;) # NNetwork1 = list(Cont1, Cat1_embed) %&gt;% layer_concatenate(name=&#39;cont&#39;) %&gt;% layer_dense(units=neurons[1], activation=&#39;relu&#39;, name=&#39;hidden1&#39;) %&gt;% layer_dense(units=neurons[2], activation=&#39;relu&#39;, name=&#39;hidden2&#39;) %&gt;% layer_dense(units=neurons[3], activation=&#39;relu&#39;, name=&#39;hidden3&#39;) %&gt;% layer_dense(units=1, activation=&#39;linear&#39;, name=&#39;NNetwork1&#39;, weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1)))) # NNetwork2 = Cont2 %&gt;% layer_dense(units=neurons[1], activation=&#39;relu&#39;, name=&#39;hidden4&#39;) %&gt;% layer_dense(units=neurons[2], activation=&#39;relu&#39;, name=&#39;hidden5&#39;) %&gt;% layer_dense(units=neurons[3], activation=&#39;relu&#39;, name=&#39;hidden6&#39;) %&gt;% layer_dense(units=1, activation=&#39;linear&#39;, name=&#39;NNetwork2&#39;, weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1)))) # NNoutput = list(NNetwork1, NNetwork2, LogExposure) %&gt;% layer_add(name=&#39;Add&#39;) %&gt;% layer_dense(units=1, activation=k_exp, name = &#39;NNoutput&#39;, trainable=FALSE, weights=list(array(c(1), dim=c(1,1)), array(0, dim=c(1)))) model &lt;- keras_model(inputs = x.input, outputs = c(NNoutput)) model %&gt;% compile(optimizer = optimizer_nadam(), loss = &#39;poisson&#39;) model } model &lt;- model.2IA(BrLabel) summary(model) early_stop &lt;- callback_early_stopping(monitor = &quot;val_loss&quot;, patience =10) # print_dot_callback &lt;- callback_lambda( # on_epoch_end = function(epoch, logs) { # if (epoch %% 50 == 0) cat(&quot;\\n&quot;) # cat(&quot;.&quot;) # } # ) # may take a couple of minutes if epochs is more than 100 {t1 &lt;- proc.time() fit &lt;- model %&gt;% fit(train.x, as.matrix(dat2_train$ClaimNb), epochs=500, batch_size=10000, verbose=1, validation_data=list(valid.x,dat2_valid$ClaimNb), callback=list(early_stop)) (proc.time()-t1)} matplot(cbind(fit$metrics$loss,fit$metrics$val_loss), type=&quot;l&quot;) dat2_test$fitGAMPlus &lt;- as.vector(model %&gt;% predict(test.x)) Poisson.Deviance(dat2_test$fitGAMPlus, dat2_test$ClaimNb) keras_poisson_dev(dat2_test$fitGAMPlus, dat2_test$ClaimNb) "],["cnn.html", "5 å·ç§¯ç¥ç»ç½‘ç»œä¸è½¦è”ç½‘æ•°æ®åˆ†æ 5.1 å·ç§¯å±‚ (Convolution) 5.2 æ± åŒ–å±‚ (Pooling) 5.3 æ‰¹æ ‡å‡†åŒ–å±‚ (Batch Normalization) 5.4 å…¶ä»–ç»„ä»¶ 5.5 ç‰¹æ€§ 5.6 éšè—å±‚å¯è§†åŒ– 5.7 é€†å·ç§¯ 5.8 Human Mortality Database (HMD) 5.9 MNIST dataset", " 5 å·ç§¯ç¥ç»ç½‘ç»œä¸è½¦è”ç½‘æ•°æ®åˆ†æ æ·±åº¦å­¦ä¹ ä¹‹æ‰€ä»¥è¿™ä¹ˆçƒ­ï¼Œå¤§éƒ¨åˆ†å½’åŠŸäºå·ç§¯ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰ä¸Šå–å¾—çš„å·¨å¤§æˆåŠŸã€‚å·ç§¯ç¥ç»ç½‘ç»œè¿˜å¯ä»¥ç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æ—¶é—´åºåˆ—åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€å¯ç©¿æˆ´è®¾å¤‡ä¸å¥åº·æ£€æµ‹ã€GOã€‚ æ ¡é—¨å£çš„äººè„¸è¯†åˆ«å¯ä»¥å¿«é€Ÿè¯†åˆ«å‡ºå­¦ç”Ÿè€å¸ˆæ ¡å¤–äººå‘˜ï¼Œé€šå¸¸æ˜¯å¦æˆ´å£ç½©ã€é å·¦é å³ã€ç¦»çš„è¿œè¿‘ç­‰ä¸ä¼šå½±å“ç»“æœï¼Œä½†æ­ªå¤´ç»å¸¸éš¾ä»¥è¢«è¯†åˆ«ï¼Œè¿™äº›å’Œä¸‹é¢å·ç§¯ç¥ç»ç½‘ç»œçš„ç‰¹æ€§å¯†åˆ‡ç›¸å…³ã€‚ Figure 5.1: äººå¤§å…¥å£ å¤§å‹é¢„å…ˆè®­ç»ƒçš„CNNsåº“å¯ç”¨äºå›¾åƒè¯†åˆ«ï¼šAlexNetï¼ŒGoogLeNetï¼ŒResNet, Inception, MobileNet,ï¼ŒVGGï¼Œ DenseNet,ï¼ŒNASNet ç­‰ã€‚å®ƒä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œå°†æŸä¸€å›¾åƒåˆ†ç±»è‡³å·²çŸ¥çš„ç±»åˆ«ä¹‹ä¸­ ä¹Ÿå¯ä»¥åº”ç”¨äºè¿ç§»å­¦ä¹ ã€‚ Figure 5.2: è¿ç§»å­¦ä¹  5.1 å·ç§¯å±‚ (Convolution) ä½œç”¨ï¼šç‰¹å¾æå–ï¼Œä¸€èˆ¬æƒ³è¦å¤šå°‘ç‰¹å¾ï¼Œå°±è®¾ç½®å¤šå°‘ä¸ªå·ç§¯æ ¸(filter)ã€‚ä¸åŒçš„å·ç§¯æ ¸ç›¸å½“äºä¸åŒçš„ç‰¹å¾æå–å™¨. è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š Figure 5.3: å·ç§¯è¿ç®— 5.1.1 è¶…å‚æ•° ä¸€ä¸ªå·ç§¯å±‚ä¸»è¦æœ‰ä»¥ä¸‹è¶…å‚æ•° Channels: é»‘ç™½å›¾åƒä¸€èˆ¬åªæœ‰ä¸€ä¸ªé€šé“ï¼Œå½©è‰²å›¾åƒä¸€èˆ¬æœ‰ä¸‰ä¸ªé€šé“ï¼Œå³RGB. Filters: ä¸€èˆ¬æƒ³è¦å¤šå°‘ç‰¹å¾ï¼Œå°±è®¾ç½®å¤šå°‘ä¸ªå·ç§¯æ ¸ã€‚ä¸åŒçš„å·ç§¯æ ¸ç›¸å½“äºä¸åŒçš„ç‰¹å¾æå–å™¨. Padding: è¡¥é›¶ã€‚ä½œç”¨ï¼šä¿æŒå›¾åƒå¤§å°ï¼Œä½¿ä¹‹å‡å°ä¸ä¼šå¤ªå¿«ï¼›è¿˜èƒ½ç…§é¡¾åˆ°è¾¹ç¼˜ç‰¹å¾ã€‚ Figure 5.4: Padding Dilation: è†¨èƒ€å·ç§¯ï¼ˆDilated Convolutionï¼‰ä¹Ÿç§°ä¸ºç©ºæ´å·ç§¯ï¼ˆAtrous Convolutionï¼‰æ˜¯ä¸€ç§ä¸å¢åŠ å‚æ•°æ•°é‡åŒæ—¶å¢åŠ è¾“å‡ºå•å…ƒæ„Ÿå—é‡çš„ä¸€ç§æ–¹æ³•ã€‚ç©ºæ´å·ç§¯é€šè¿‡ç»™å·ç§¯æ ¸æ’å…¥â€œç©ºæ´â€æ¥å˜ç›¸åœ°å¢åŠ å…¶å¤§å° Figure 5.5: Dilation kernal Figure 5.6: Dilation kernal Strides: æ­¥é•¿ã€‚å·ç§¯æ ¸æ¯æ¬¡æ»‘åŠ¨çš„æ­¥å¹…ã€‚ å‡è®¾ç¬¬\\(k-1\\)å±‚è¾“å‡ºå›¾åƒçš„ç»´åº¦ä¸º\\(n_1^{(k-1)}\\times n_2^{(k-1)}\\times m^{(k-1)}\\), ç»è¿‡ç¬¬\\(k\\)å±‚çš„å·ç§¯è¿ç®—ï¼Œå¾—åˆ°çš„å›¾ç‰‡ç»´åº¦ä¸º\\(n_1^{(k)}\\times n_2^{(k)}\\times m^{(k)}\\)ã€‚ç›¸å…³è¶…å‚æ•°æ€»ç»“å¦‚ä¸‹ï¼š ç‰¹å¾ è¶…å‚æ•° è¾“å…¥å¤§å° \\(n_1^{(k-1)}\\times n_2^{(k-1)}\\) è¾“å…¥ç‰¹å¾ \\(m^{(k-1)}\\) è¾“å‡ºç‰¹å¾(å·ç§¯æ ¸) \\(m^{(k)}\\) è¡¥é›¶ \\(p_1^{(k)},p_2^{(k)}\\) è†¨èƒ€ \\(d_1^{(k)},d_2^{(k)}\\) æ­¥é•¿ \\(s_1^{(k)},s_2^{(k)}\\) è¾“å‡ºå¤§å° \\(n_1^{(k)}\\times n_2^{(k)}\\) å…¶ä¸­ï¼Œè¾“å‡ºå¤§å°\\(n_1^{(k)}\\times n_2^{(k)}\\)ç”±è¾“å…¥å¤§å°\\(n_1^{(k-1)}\\times n_2^{(k-1)}\\)å’Œè¡¥é›¶ã€è†¨èƒ€ã€æ­¥é•¿å†³å®šã€‚ 5.1.2 å‚æ•°ä¸ªæ•°è®¡ç®— ä¸‹å›¾ä¸ºæ¡ˆä¾‹1ä¸­ç¥ç»ç½‘ç»œå‚æ•°ä¸ªæ•°çš„è®¡ç®—ã€‚ Figure 5.7: Number of Parameters å·ç§¯å±‚çš„è¾“å‡ºå¤§å°è®¡ç®—å…¬å¼ä¸ºï¼š\\(\\frac{{n - f + 2p}}{s} + 1\\)ï¼Œå…¶ä¸­nä¸ºè¾“å…¥çŸ©é˜µçš„å¤§å°ï¼Œ\\(f\\)ä¸ºå·ç§¯æ ¸çš„å¤§å°ï¼Œ\\(p\\)ä¸ºpaddingçš„å¤§å°ï¼Œ\\(s\\)ä¸ºæ­¥é•¿ æ± åŒ–å±‚çš„è¾“å‡ºå¤§å°è®¡ç®—å…¬å¼ä¸ºï¼š\\(\\frac{{n - f}}{s} + 1\\)ï¼Œè®¡ç®—å…¬å¼å’Œå·ç§¯å±‚å®é™…ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯æ± åŒ–å±‚ä¸€èˆ¬ä¸è¿›è¡Œå¡«å……ï¼Œ\\(p\\)ä¸€èˆ¬ä¸º\\(0\\)ã€‚ å‚æ•°ä¸ªæ•°çš„è®¡ç®—åŸºäºå¯¹å±‚ä½œç”¨çš„ç†è§£ï¼Œæ± åŒ–å±‚æ˜¯æ˜ å°„å…³ç³»ï¼Œæ•…è¯¥å±‚æ˜¯æ²¡æœ‰å¯å­¦ä¹ å‚æ•°çš„ï¼›æ‰¹æ ‡å‡†åŒ–å±‚æœ‰ä¸¤ä¸ªå¯å­¦ä¹ çš„å‚æ•°scaleå’Œshiftï¼›å…¨è¿æ¥å±‚éœ€è¦å­¦ä¹ çš„å‚æ•°ä¸€èˆ¬ä¸ºä¸‹æ¥ç¥ç»å…ƒçš„ä¸ªæ•°åŠ ä¸Šä¸€ä¸ªåç½®ï¼›å·ç§¯å±‚çš„å¯å­¦ä¹ å‚æ•°æ¯”è¾ƒå¤æ‚ï¼Œå›¾åƒæ•°æ®ä¸ºä¾‹ï¼Œä¸€èˆ¬ä¸ºé€šé“æ•°Ã—äºŒç»´å·ç§¯çš„sizeÃ—å·ç§¯æ ¸çš„ä¸ªæ•°+åç½®ä¸ªæ•° 5.2 æ± åŒ–å±‚ (Pooling) ä¹Ÿç§°ä¸‹é‡‡æ ·å±‚ï¼Œå…¶ä½œç”¨æ˜¯è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œé™ä½ç‰¹å¾æ•°é‡ï¼Œä»è€Œå‡å°‘å‚æ•°æ•°é‡ã€‚åœ¨å›¾åƒä¸­ï¼Œæœ€ä¸»è¦ä½œç”¨å°±æ˜¯å‹ç¼©å›¾åƒã€‚æ± åŒ–å±‚ä¸€èˆ¬åˆ†ä¸ºå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ã€‚ Figure 5.8: Pooling 5.3 æ‰¹æ ‡å‡†åŒ–å±‚ (Batch Normalization) åœ¨batchä¸Šè¿›è¡Œæ ‡å‡†åŒ–åå†é€å…¥ä¸‹ä¸€å±‚ï¼Œå®ƒå¯ä»¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚ä¸»è¦åˆ†ä¸ºä¸¤æ­¥ï¼š é€šè¿‡è®­ç»ƒæœŸé—´å„æ‰¹æ¬¡çš„å‚æ•°å¹³å‡å€¼å’Œæ–¹å·®å¯¹è¾“å…¥è¿›è¡Œç§»ä½å’Œç¼©æ”¾ã€‚ é€šè¿‡è®­ç»ƒæœŸé—´å­¦ä¹ çš„åä¸¤ä¸ªï¼ˆå¯å­¦ä¹ ï¼‰å‚æ•°è¿›è¡Œç§»ä½å’Œç¼©æ”¾ã€‚ Detailed Algorithmï¼š ç¬¬ä¸€æ­¥ä»…æ ¹æ®æ‰¹æ•°æ®è®¡ç®—å‡ºçš„å‡å€¼å’Œæ–¹å·®ï¼Œå°†æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå®Œå…¨åŸºäºæ‰¹æ•°æ®è®¡ç®—ï¼Œæ•…æ— éœ€è¦å­¦ä¹ çš„å‚æ•°ã€‚ ç¬¬ä¸€æ­¥çš„åŸºæœ¬æ€æƒ³å…¶å®ç›¸å½“ç›´è§‚ï¼šå› ä¸ºæ·±å±‚ç¥ç»ç½‘ç»œåœ¨åšéçº¿æ€§å˜æ¢å‰çš„æ¿€æ´»è¾“å…¥å€¼ä¼šéšç€ç½‘ç»œæ·±åº¦åŠ æ·±æˆ–è€…åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…¶åˆ†å¸ƒé€æ¸å‘ç”Ÿåç§»æˆ–è€…å˜åŠ¨ï¼Œä¹‹æ‰€ä»¥è®­ç»ƒæ”¶æ•›æ…¢ï¼Œä¸€èˆ¬æ˜¯æ•´ä½“åˆ†å¸ƒé€æ¸å¾€éçº¿æ€§å‡½æ•°çš„å–å€¼åŒºé—´çš„ä¸Šä¸‹é™ä¸¤ç«¯é è¿‘ï¼Œæ‰€ä»¥è¿™å¯¼è‡´åå‘ä¼ æ’­æ—¶ä½å±‚ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±ï¼Œè¿™æ˜¯è®­ç»ƒæ·±å±‚ç¥ç»ç½‘ç»œæ”¶æ•›è¶Šæ¥è¶Šæ…¢çš„æœ¬è´¨åŸå› ï¼Œè€ŒBNå°±æ˜¯é€šè¿‡ä¸€å®šçš„è§„èŒƒåŒ–æ‰‹æ®µï¼ŒæŠŠæ¯å±‚ç¥ç»ç½‘ç»œä»»æ„ç¥ç»å…ƒè¿™ä¸ªè¾“å…¥å€¼çš„åˆ†å¸ƒå¼ºè¡Œæ‹‰å›åˆ°å‡å€¼ä¸º0æ–¹å·®ä¸º1çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä½¿ä¹‹äº«å—è¾ƒå¤§çš„æ¢¯åº¦ã€‚ ç¬¬äºŒæ­¥æ˜¯å¯¹ç¬¬ä¸€æ­¥çš„æ”¹è¿›ï¼Œå¯¹ç¬¬ä¸€æ­¥æ ‡å‡†åŒ–åçš„xï¼Œå¢åŠ äº†ä¸¤ä¸ªé€šè¿‡ç½‘ç»œå­¦ä¹ çš„å‚æ•°scaleå’Œshiftï¼Œå¯¹æ•°æ®è¿›è¡Œå¹³ç§»å’Œç¼©æ”¾ï¼Œå³\\(y = scale*x + shift\\)ã€‚ è®¾æƒ³å¦‚æœæ¯å±‚éƒ½æ ‡å‡†åŒ–åˆ°åŒä¸€åˆ†å¸ƒï¼Œé‚£ä¹ˆæ•°æ®æ¯æ¬¡è¿›å…¥ç½‘ç»œéƒ½ä¼šæ˜¯åŒä¸€åˆ†å¸ƒï¼Œè¿™æ„å‘³ç€ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ä¸‹é™äº†ï¼Œå¤šå±‚çš„ç½‘ç»œçš„æ„ä¹‰å°±ä¸‹é™äº†ã€‚æ‰€ä»¥BNä¸ºäº†ä¿è¯éçº¿æ€§çš„ä¹ å¾—ï¼Œå¯¹å˜æ¢åçš„æ»¡è¶³å‡å€¼ä¸º\\(0\\)æ–¹å·®ä¸º\\(1\\)çš„\\(x\\)åˆè¿›è¡Œäº†scaleåŠ ä¸Šshiftæ“ä½œã€‚ 5.4 å…¶ä»–ç»„ä»¶ 5.4.1 å…¨è¿æ¥å±‚ (Dense) å…¨è¿æ¥å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒä¸å…¶å‰ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒè¿›è¡Œå…¨è¿æ¥ã€‚ CNNä¸­å› ä¸ºå›¾åƒæ˜¯äºŒç»´çš„ï¼Œæ‰€ä»¥åœ¨è¿›å…¥å…¨è¿æ¥å±‚çš„æ—¶å€™éœ€è¦ç»è¿‡ä¸€ä¸ªFlattenï¼ˆæ‰å¹³åŒ–ï¼‰çš„æ“ä½œã€‚ Flattenå±‚ä½œç”¨å°±æ˜¯é€šè¿‡é‡æ–°æ’åˆ—ç»´åº¦å¹¶ä¿ç•™æ‰€æœ‰å€¼çš„ç®€å•å˜æ¢. Figure 5.9: Dense layer 5.4.2 è¾“å‡ºç¥ç»å…ƒ å³æˆ‘ä»¬æœ€åè¾“å‡ºçš„ç»“æœï¼Œä¸€èˆ¬æ¥åœ¨å…¨è¿æ¥å±‚åã€‚ æ¡ˆä¾‹ä¸€æ˜¯ç”Ÿå­˜ç‡é—®é¢˜ï¼Œç»“æœå–å€¼åœ¨\\([0,1]\\)ä¸­ï¼Œæ‰€ä»¥ä½¿ç”¨sigmoidçš„å‡½æ•°å¯¹æœ€åçš„å€¼è¿›è¡Œç¼©æ”¾ã€‚æ¡ˆä¾‹äºŒæ˜¯å¤šåˆ†ç±»çš„è¾“å‡ºç»“æœï¼Œæ‰€ä»¥ä½¿ç”¨softmaxå‡½æ•°è¿›è¡Œè¾“å‡ºã€‚ 5.4.3 æ¿€æ´»å‡½æ•° (Activation) åœ¨ä¸Šé¢è®¨è®ºçš„ç½‘ç»œå±‚ï¼ˆå·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ï¼‰ä¸­ï¼Œæ‰€æœ‰çš„æ“ä½œå…¶å®éƒ½æ˜¯çº¿æ€§çš„ï¼Œä½†åªæœ‰ä½¿ç”¨éçº¿æ€§æ¿€æ´»ï¼Œç½‘ç»œå»ºæ¨¡çš„å…¨éƒ¨å¨åŠ›æ‰ä¼šå‘æŒ¥å‡ºæ¥ã€‚å¸¸ç”¨çš„éçº¿æ€§æ¿€æ´»å‡½æ•°æœ‰ï¼šReLUã€sigmoidã€tanhç­‰ Figure 5.10: Activation Functions 5.5 ç‰¹æ€§ 5.5.1 å¹³ç§»ä¸å˜æ€§ ç”±äºå·ç§¯æ ¸å¯¹äºç‰¹å®šçš„ç‰¹å¾æ‰ä¼šæœ‰è¾ƒå¤§æ¿€æ´»å€¼ï¼Œä¸”åº”ç”¨åˆ°ä¸åŒçš„ä½ç½®ï¼Œæ‰€ä»¥ä¸è®ºä¸Šä¸€å±‚ç‰¹å¾å›¾è°±ï¼ˆfeature mapï¼‰ä¸­çš„æŸä¸€ç‰¹å¾å¹³ç§»åˆ°ä½•å¤„ï¼Œå·ç§¯æ ¸éƒ½ä¼šæ‰¾åˆ°è¯¥ç‰¹å¾å¹¶åœ¨æ­¤å¤„å‘ˆç°è¾ƒå¤§çš„æ¿€æ´»å€¼ã€‚è¿™å°±æ˜¯â€œç­‰å˜æ€§â€ Figure 5.11: Shift invariance 5.5.2 æ—‹è½¬ä¸å˜æ€§ æ—‹è½¬ä¸å˜æ€§å¸¸è§äºå«æ˜Ÿå›¾åƒè¯†åˆ«ä¸­ï¼Œå¦‚æ¡¥æ¢å®šä½ã€æ´ªæ°´é¢ç§¯ä¼°è®¡ç­‰ã€‚ä½†åœ¨æ‰‹å†™è¯†åˆ«ä¸­ï¼Œæ—‹è½¬ä¸å˜æ€§ä¸æ»¡è¶³ã€‚ 5.5.3 å°ºåº¦ä¸å˜æ€§ åœ¨æ‰‹å†™è¯†åˆ«ä¸­ï¼Œå°ºåº¦ä¸å˜æ€§æ»¡è¶³ã€‚ 5.6 éšè—å±‚å¯è§†åŒ– ç¥ç»ç½‘ç»œæ¯å±‚æå–ä¸€ä¸ªç»†èŠ‚ç‰¹å¾ï¼Œä¸€èˆ¬å±‚æ•°è¶Šæ·±ï¼Œç»†èŠ‚ç‰¹å¾è¶Šä¸æ˜æ˜¾ï¼Œå¯è§†åŒ–åæ˜æ˜¾çœ‹å‡ºå‰å‡ å±‚ç‰¹å¾æ¯”è¾ƒæ˜æ˜¾ï¼Œåé¢çš„å±‚æ•°å°±å·²ç»çœ‹ä¸æ¸…æ•°å­—äº†ã€‚ 5.7 é€†å·ç§¯ é€†å·ç§¯ç›¸å¯¹äºå·ç§¯åœ¨ç¥ç»ç½‘ç»œç»“æ„çš„æ­£å‘å’Œåå‘ä¼ æ’­ä¸­åšç›¸åçš„è¿ç®—ã€‚å·ç§¯æ˜¯æå–ç‰¹å¾ï¼Œä½¿å›¾åƒå˜å°ã€‚åå·ç§¯å¯ä»¥ä½¿å¾—å›¾åƒå¯ä»¥å˜å¤§ã€‚åå·ç§¯çš„å¤§å°æ˜¯ç”±å·ç§¯æ ¸å¤§å°ä¸æ»‘åŠ¨æ­¥é•¿å†³å®šï¼Œ \\(in\\)æ˜¯è¾“å…¥å¤§å°ï¼Œ kæ˜¯å·ç§¯æ ¸å¤§å°ï¼Œ \\(s\\)æ˜¯æ»‘åŠ¨æ­¥é•¿ï¼Œ \\(out\\)æ˜¯è¾“å‡ºå¤§å°ã€‚è®¡ç®—å…¬å¼ä¸º \\(out = (in - 1) * s + k\\) 5.8 Human Mortality Database (HMD) ç›®æ ‡: æ ¹æ®æ­»äº¡ç‡è¡¨çš„å±€éƒ¨ç‰¹å¾(\\(10\\times10\\))ï¼Œæ£€æµ‹è¯¥å±€éƒ¨å¼‚å¸¸æ­»äº¡ç‡å¼ºåº¦ã€‚ 5.8.1 è¾“å…¥å’Œæ ‡ç­¾ æ­»äº¡ç‡\\(q_{x,t,c,g}\\)ã€äººå£æ•°é‡\\(E_{x,t,c,g}\\) å¹´é¾„\\(x\\), æ—¥å†å¹´\\(t\\), å›½å®¶\\(c\\), æ€§åˆ«\\(g\\). ç”±äºé¢†åœŸçš„å˜åŒ–ï¼ŒæŸäº›å¹´çš„æ•°æ®ä¼šå‡ºç°å˜æ›´å‰ä¸å˜æ›´åçš„çš„ä¸¤ä¸ªæ•°æ®ï¼Œå¤„ç†æ–¹å¼æ˜¯å–å¹³å‡å€¼ä½œä¸ºæœ€åçš„ç ”ç©¶æ•°æ®ã€‚ æŸäº›æ­»äº¡ç‡æ•°æ®å­˜åœ¨ç¼ºå¤±ï¼šå¦‚æœç›¸é‚»(ä»¥å¹´é¾„\\(x\\)å’Œæ—¥å†å¹´\\(t\\))å€¼å¯ç”¨ï¼Œæˆ‘ä»¬çº¿æ€§æ’è¡¥ï¼Œå¦åˆ™ä½¿ç”¨æœ€è¿‘é‚»çš„å€¼è¿›è¡Œæ’è¡¥ã€‚ å‡å®šæ²¡æœ‰äººå£çš„è¿ç§»ä»¥åŠå…¶ä»–çš„è¯¯å·®ï¼š\\[E_{x,t,c,g}=E_{x-1,t-1,c,g}(1-q_{{x-1},{t-1},c,g})\\] å®šä¹‰æ ‡å‡†åŒ–æ®‹å·®: \\[r_{x,t,c,g}=\\frac{E_{x,t,c,g}-E_{x-1,t-1,c,g}(1-q_{x-1,t-1,c,g})}{E_{x,t,c,g}}\\] \\(r_{x,t,c,g}&lt;0\\)è¡¨æ˜å¯èƒ½æœ‰äººå£è¿å‡ºæˆ–è€…æ•°æ®é”™è¯¯ï¼Œ \\(r_{x,t,c,g}&gt;0\\)è¡¨æ˜å¯èƒ½æœ‰äººå£è¿å…¥æˆ–è€…æ•°æ®é”™è¯¯ã€‚ ç»è¿‡é¢„å¤„ç†HMDï¼Œå¯¹æ¯ä¸ªå›½å®¶æ¯ä¸ªæ€§åˆ«æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå…³äºæ­»äº¡ç‡\\(q_{x,t,c,g}\\)çš„äºŒç»´æ•°ç»„ï¼Œå…¶ä¸­è¡Œä»£è¡¨ä¸åŒæ—¥å†å¹´ï¼Œåˆ—ä»£è¡¨ä¸åŒå¹´é¾„ã€‚ä¸ºäº†æ£€æµ‹æ­»äº¡ç‡çš„å¼‚å¸¸å€¼ï¼Œæˆ‘ä»¬è€ƒè™‘æ­»äº¡ç‡çš„å±€éƒ¨å˜åŒ–ç‰¹å¾ï¼Œä½¿ç”¨å¤§å°ä¸º\\(10\\times10\\)çš„çª—å£åœ¨æ­»äº¡ç‡äºŒç»´æ•°ç»„ä¸Šè¿›è¡Œç§»åŠ¨ï¼Œå¹¶è®¾ç½®æ­¥é•¿ä¸º\\(5\\)ã€‚å¯ä»¥å¾—åˆ°æ­»äº¡ç‡çš„å±€éƒ¨çŸ©é˜µ\\((q_{x,t,c,g})_{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}\\), å…¶ä¸­\\(x_i:=20+5i,t_i=1950+5i\\)ã€‚æˆ‘ä»¬å®šä¹‰å¦‚ä¸‹(åŸå§‹)è¾“å…¥ç‰¹å¾\\(W_{i,c}\\in\\mathbb{R}^{10\\times 10\\times 3}\\): \\[ \\begin{aligned} W_{i,c,\\cdot,\\cdot,1}:=&amp;(\\text{logit}(q_{x,t,c,males}))_{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}\\\\ W_{i,c,\\cdot,\\cdot,2}:=&amp;(\\text{logit}(q_{x,t,c,females}))_{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}\\\\ W_{i,c,\\cdot,\\cdot,3}:=&amp;W_{i,c,\\cdot,\\cdot,1}-W_{i,c,\\cdot,\\cdot,2} \\end{aligned} \\] å…¶ä¸­, \\(\\text{logit }q =\\log \\frac{q}{1-q}\\) Figure 5.12: Mortality Window ç„¶ååˆ†åˆ«å¯¹ä¸‰ä¸ªé€šé“è¿›è¡Œæ­£åˆ™åŒ–, å¾—åˆ°\\(\\boldsymbol{X}_{i,c}\\in[0,1]^{10\\times10\\times3}\\). é€šè¿‡å¯¹æ‰€æœ‰å›½å®¶è¿›è¡Œå¦‚ä¸Šå¤„ç†, å¯ä»¥å¾—åˆ°å¤§çº¦\\(4000\\)å¼ å›¾åƒ. æ¥ä¸‹æ¥, æˆ‘ä»¬å®šä¹‰æ¯å¼ å›¾çš„â€œæ ‡ç­¾â€. é¦–å…ˆ, å¯¹\\(r_{x,t,c,g}\\)è¿›è¡ŒMinMaxæ­£åˆ™åŒ–å¤„ç†, å¾—åˆ°\\(\\bar{r}_{x,t,c,g}\\in[0,1].\\) ç„¶å, å®šä¹‰æ ‡ç­¾ä¸ºå¼‚å¸¸å¼ºåº¦ \\[Y_{i,c}:=\\underset{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}{\\max} \\left|\\frac{\\bar{r}_{x,t,c,males}+\\bar{r}_{x,t,c,females}}{2} \\right|\\in[0,1].\\] æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŸºäºæ­»äº¡ç‡åœ¨å¤§å°ä¸º\\(10\\times10\\)ä¸Šçš„å±€éƒ¨ç‰¹å¾, é¢„æµ‹è¯¥èŒƒå›´å†…æ­»äº¡ç‡çš„å¼‚å¸¸å¼ºåº¦. åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶, é€‰å–å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°\\[\\mathcal{L}(Y,\\hat{\\mu}(\\boldsymbol{X});\\mathcal{I}):=\\frac{1}{|\\mathcal{I}|}\\sum_{(i,c)\\in\\mathcal{I}}(Y_{i,c}-\\hat{\\mu}(\\boldsymbol{X}_{i,c}))^2.\\] 5.8.2 è¯„ä¼°æŒ‡æ ‡ åœ¨è¯„ä¼°æ¨¡å‹æ—¶, æˆ‘ä»¬é€šè¿‡å¦‚ä¸‹æ­¥éª¤å®šä¹‰äºŒåˆ†ç±»AOCæŒ‡æ ‡: å®šä¹‰ä¼¯åŠªåˆ©éšæœºå˜é‡ \\[ b_{i,c}:= \\begin{cases} 1, Y_{i,c}\\geq q_{0.95}(Y), \\\\ 0, \\text{otherwise}, \\end{cases} \\] å…¶ä¸­, \\(q_{0.95}(Y)\\)ä¸ºæ‰€æœ‰å› å˜é‡\\(Y_{i,c}\\)çš„0.95åˆ†ä½æ•°, å³\\(b_{i,c}\\)ä¸ºâ€œéå¸¸å¼‚å¸¸â€æŒ‡ç¤ºæ ‡é‡. æŠŠç¥ç»ç½‘ç»œçš„è¾“å‡ºç»“æœ\\(\\hat{\\mu}(\\boldsymbol{X}_{i,c})\\)å½“ä½œæ¦‚ç‡\\(\\Pr (b_{i,c}=1)\\)çš„é¢„æµ‹. ç”»å‡ºè¯¥äºŒåˆ†ç±»é—®é¢˜çš„receiver operating characteristic curve (ROC), å¹¶è®¡ç®— area under the curve (AUC). åˆ©ç”¨ä»¥ä¸Šæ¨¡å‹è¯„ä¼°æ–¹æ³•, æˆ‘ä»¬å¯ä»¥å¯¹å›½å®¶æŒ‰ç…§â€œå¼‚å¸¸å¼ºåº¦â€çš„ç›¸ä¼¼æ€§è¿›è¡Œåˆ†ç±», å…·ä½“æ­¥éª¤å¦‚ä¸‹: å¯¹æ¯ä¸ªå›½å®¶\\(c\\)åˆ†åˆ«å»ºç«‹CNNæ¨¡å‹, å¹¶ä½¿ç”¨è¯¥æ¨¡å‹å¯¹å…¶ä»–å›½å®¶\\(c^*\\)çš„æ•°æ®è¿›è¡Œé¢„æµ‹, è®¡ç®—AUC \\(A_{c,c^*}\\). å¹¶å»ºç«‹çŸ©é˜µ\\(A=(A_{c,c^*})_{c,c*\\in\\mathcal{C}}\\), å…¶ä¸­\\(\\mathcal{C}\\)ä¸ºæ‰€æœ‰å›½å®¶çš„é›†åˆ. å¯¹\\(A\\)è¿›è¡Œåˆ—æ ‡å‡†åŒ–, å¹¶è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£, å¾—åˆ°å‰ä¸¤ä¸ªä¸»æˆåˆ†\\(P_{j,c}, j=1,2\\). å¯¹ä¸»æˆåˆ†\\(P_{j,c}, j=1,2\\)è¿›è¡Œèšç±», å¾—åˆ°4ä¸ªç°‡. 5.9 MNIST dataset ç›®æ ‡: å¯¹æ‰‹å†™\\(0-9\\)è¿›è¡Œåˆ†ç±»ã€‚ MNIST å…¨ç§°ä¸º Modified National Institute of Standards and Technology. ä¿®æ”¹è¿‡åçš„MNISTæ•°æ®é›†ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”±ä¸åŒçš„äººçš„æ‰‹å†™ä½“æ•°å­—ç»„æˆçš„å›¾ç‰‡æ•°æ®é›†ï¼ŒåŒ…å«äº†\\(7\\)ä¸‡å¼ å…³äºæ‰‹å†™æ•°å­—\\(0,1,\\ldots,9\\)çš„å›¾åƒï¼Œæ ¼å¼ä¸º\\(28Ã—28\\)çš„ç°åº¦åƒç´ ã€‚ ç¥ç»ç½‘ç»œçš„è¾“å…¥ä¸ºç”±ç°åº¦åƒç´ æ„æˆçš„\\(28\\times28\\)æ•°ç»„\\(\\boldsymbol{X}\\in[0,1]^{28\\times28}\\), è¾“å‡ºä¸ºåœ¨\\(\\{0,1,\\ldots,9\\}\\)ä¸Šçš„ç¦»æ•£åˆ†å¸ƒ\\((p_0,\\ldots,p_9)^T\\), å…¶ä¸­\\(\\sum_{j=0}^9p_j=1\\). å›¾åƒçš„æ ‡ç­¾ä¸ºå®é™…æ•°å­—çš„one-hotç¼–ç \\(Y\\in\\{0,1\\}^{10}\\). æŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µ(cross-entropy) \\[\\mathcal{L}(Y,\\hat{p}(\\boldsymbol{X});\\mathcal{I}):=-\\sum_{i\\in\\mathcal{I}}\\sum_{j=0}^9Y_{i,j}\\log\\hat{p}_j(\\boldsymbol{X}_i).\\] "],["rnn.html", "6 å¾ªç¯ç¥ç»ç½‘ç»œä¸æ­»äº¡ç‡é¢„æµ‹ 6.1 Lee-Carter Model 6.2 æ™®é€šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆrecurrent neural networkï¼‰ 6.3 é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œï¼ˆLong short-term memoryï¼‰ 6.4 é—¨æ§å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆGated Recurrent Unitï¼‰ 6.5 æ¡ˆä¾‹åˆ†æï¼ˆCase studyï¼‰", " 6 å¾ªç¯ç¥ç»ç½‘ç»œä¸æ­»äº¡ç‡é¢„æµ‹ ğŸ˜· æ–°å† è‚ºç‚æ­»äº¡ç‡æ•°æ®ï¼šhttps://mpidr.shinyapps.io/stmortality/ 6.1 Lee-Carter Model Lee Carteræ¨¡å‹ä¸­ï¼Œæ­»äº¡åŠ›ï¼ˆforce of mortalityï¼‰çš„å®šä¹‰å¦‚ä¸‹ï¼š \\[\\log \\left(m_{t, x}\\right)=a_{x}+b_{x} k_{t}\\] å…¶ä¸­ï¼Œ \\(m_{t, x}&gt;0\\) æ˜¯ \\(x\\) å²çš„äººåœ¨æ—¥å†å¹´ \\(t\\) çš„æ­»äº¡ç‡ï¼ˆmortality rateï¼‰, \\(a_{x}\\) æ˜¯ \\(x\\) å²çš„äººçš„å¹³å‡å¯¹æ•°æ­»äº¡ç‡, \\(b_{x}\\) æ˜¯æ­»äº¡ç‡å˜åŒ–çš„å¹´é¾„å› ç´ , \\(\\left(k_{t}\\right)_{t}\\) æ˜¯æ­»äº¡ç‡å˜åŒ–çš„æ—¥å†å¹´å› ç´ . ç”¨ \\(M_{t, x}\\) è¡¨ç¤ºæŸä¸€æ€§åˆ«æ­»äº¡ç‡çš„è§‚å¯Ÿå€¼ï¼ˆraw mortality ratesï¼‰. æˆ‘ä»¬å¯¹å¯¹æ•°æ­»äº¡ç‡ \\(\\log \\left(M_{t, x}\\right)\\) ä¸­å¿ƒåŒ–å¤„ç†ï¼š \\[\\log \\left(M_{t, x}^{\\circ}\\right)=\\log \\left(M_{t, x}\\right)-\\widehat{a}_{x}=\\log \\left(M_{t, x}\\right)-\\frac{1}{|\\mathcal{T}|} \\sum_{s \\in \\mathcal{T}} \\log \\left(M_{s, x}\\right)\\] å…¶ä¸­ï¼Œ \\(\\mathcal{T}\\) ä¸ºè®­ç»ƒé›†ä¸­æ—¥å†å¹´çš„é›†åˆ, \\(\\widehat{a}_{x}=\\frac{1}{|\\mathcal{T}|} \\sum_{s \\in \\mathcal{T}} \\log \\left(M_{s, x}\\right)\\) æ˜¯å¹³å‡å¯¹æ•°æ­»äº¡ç‡ \\(a_{x}\\) çš„ä¼°è®¡. å¯¹äº\\(b_x,k_t\\) æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ±‚å¦‚ä¸‹æœ€ä¼˜åŒ–é—®é¢˜ï¼š \\[\\underset{\\left(b_{x}\\right)_{x},\\left(k_{t}\\right)_{t}}{\\arg \\min } \\sum_{t, x}\\left(\\log \\left(M_{t, x}^{\\circ}\\right)-b_{x} k_{t}\\right)^{2}ã€‚\\] å®šä¹‰çŸ©é˜µ \\(A=\\left(\\log \\left(M_{t, x}^{\\circ}\\right)\\right)_{x, t}\\)ã€‚ä¸Šè¿°æœ€ä¼˜åŒ–é—®é¢˜å¯ä»¥é€šè¿‡å¯¹\\(A\\)è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è§£å†³\\[A=U\\Lambda V^\\intercal,\\] å…¶ä¸­\\(U\\)ç§°ä¸ºå·¦å¥‡å¼‚çŸ©é˜µï¼Œå¯¹è§’çŸ©é˜µ\\(\\Lambda=\\text{diag}(\\lambda_1,\\ldots,\\lambda_T)\\)ä¸­çš„å¯¹è§’å…ƒç´ \\(\\lambda_1\\geq\\lambda_2\\geq\\ldots\\geq\\lambda_T\\geq0\\)ç§°ä¸ºå¥‡å¼‚å€¼ï¼Œ\\(V\\)ç§°ä¸ºå³å¥‡å¼‚çŸ©é˜µã€‚ \\(A\\) çš„ç¬¬ä¸€ä¸ªå·¦å¥‡å¼‚å‘é‡\\(U_{\\cdot,1}\\)ä¸ç¬¬ä¸€ä¸ªå¥‡å¼‚å€¼\\(\\lambda_1\\)ç›¸ä¹˜ï¼Œå¯ä»¥å¾—åˆ° \\(\\left(b_{x}\\right)_{x}\\) çš„ä¸€ä¸ªä¼°è®¡ \\(\\left(\\widehat{b}_{x}\\right)_{x}\\)ã€‚ \\(A\\) çš„ç¬¬ä¸€ä¸ªå³å¥‡å¼‚å‘é‡\\(V_{\\cdot,1}\\)ç»™å‡ºäº† \\(\\left(k_{t}\\right)_{t}\\) çš„ä¸€ä¸ªä¼°è®¡ \\(\\left(\\widehat{k}_{t}\\right)_{t}\\)ã€‚ ä¸ºäº†æ±‚è§£ç»“æœçš„å”¯ä¸€æ€§ï¼Œå¢åŠ çº¦æŸï¼š \\[\\sum_{x} \\hat{b}_{x}=1 \\quad \\text { and } \\quad \\sum_{t \\in \\mathcal{T}} \\hat{k}_{t}=0\\] è‡³æ­¤å³å¯è§£å‡ºå”¯ä¸€çš„ \\(\\left(\\hat{a}_{x}, \\hat{b}_{x}\\right)_{x}, \\left(\\hat{k}_{t}\\right)_{t}\\) . è¿™å°±æ˜¯ç»å…¸çš„LCæ¨¡å‹æ„å»ºæ–¹æ³•. 6.2 æ™®é€šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆrecurrent neural networkï¼‰ è¾“å…¥å˜é‡ï¼ˆInputï¼‰ : \\(\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{T}\\right)\\) with components \\(\\boldsymbol{x}_{t} \\in \\mathbb{R}^{\\tau_{0}}\\) at times \\(t=1, \\ldots, T\\) (in time series structure). è¾“å‡ºå˜é‡ï¼ˆOutputï¼‰: \\(y \\in \\mathcal{Y} \\subset \\mathbb{R}\\) . é¦–å…ˆçœ‹ä¸€ä¸ªå…·æœ‰ \\(\\tau_{1} \\in \\mathbb{N}\\) ä¸ªéšå±‚ç¥ç»å…ƒï¼ˆhidden neuronsï¼‰å’Œå•ä¸€éšå±‚ï¼ˆhidden layerï¼‰çš„RNN. éšå±‚ç”±å¦‚ä¸‹æ˜ å°„ï¼ˆmappingï¼‰å®šä¹‰ï¼š \\[\\boldsymbol{z}^{(1)}: \\mathbb{R}^{\\tau_{0} \\times \\tau_{1}} \\rightarrow \\mathbb{R}^{\\tau_{1}}, \\quad\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}\\right) \\mapsto \\boldsymbol{z}_{t}^{(1)}=\\boldsymbol{z}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}\\right)\\] å…¶ä¸­ä¸‹æ ‡ \\(t\\) è¡¨ç¤ºæ—¶é—´,ä¸Šæ ‡ (1) è¡¨ç¤ºç¬¬ä¸€éšå±‚ï¼ˆæœ¬ä¾‹ä¸­ä¹Ÿæ˜¯å”¯ä¸€éšå±‚ï¼‰. éšå±‚ç»“æ„æ„é€ å¦‚ä¸‹ï¼š \\[ \\begin{aligned} \\boldsymbol{z}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}\\right) =&amp;\\left(\\phi\\left(\\left\\langle\\boldsymbol{w}_{1}^{(1)}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle\\boldsymbol{u}_{1}^{(1)}, \\boldsymbol{z}_{t-1}\\right\\rangle\\right), \\ldots, \\phi\\left(\\left\\langle\\boldsymbol{w}_{\\tau_{1}}^{(1)}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle\\boldsymbol{u}_{\\tau_{1}}^{(1)}, \\boldsymbol{z}_{t-1}\\right\\rangle\\right)\\right)^{\\top} \\\\ \\stackrel{\\text { def. }}{=} &amp;\\phi\\left(\\left\\langle W^{(1)}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle U^{(1)}, \\boldsymbol{z}_{t-1}\\right\\rangle\\right) \\end{aligned} \\] å…¶ä¸­ç¬¬ \\(1 \\leq j \\leq \\tau_{1}\\) ä¸ªç¥ç»å…ƒçš„ç»“æ„ä¸ºï¼š \\[\\phi\\left(\\left\\langle\\boldsymbol{w}_{j}^{(1)}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle\\boldsymbol{u}_{j}^{(1)}, \\boldsymbol{z}_{t-1}\\right\\rangle\\right)=\\phi\\left(w_{j, 0}^{(1)}+\\sum_{l=1}^{\\tau_{0}} w_{j, l}^{(1)} x_{t, l}+\\sum_{l=1}^{\\tau_{1}} u_{j, l}^{(1)} z_{t-1, l}\\right)\\] \\(\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}\\) æ˜¯éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰ ç½‘ç»œå‚æ•°ï¼ˆnetwork parametersï¼‰ä¸º \\[W^{(1)}=\\left(\\boldsymbol{w}_{j}^{(1)}\\right)_{1 \\leq j \\leq \\tau_{1}}^{\\top} \\in \\mathbb{R}^{\\tau \\times\\left(\\tau_{0}+1\\right)} \\text{(including an intercept)}\\] \\[U^{(1)}=\\left(\\boldsymbol{u}_{j}^{(1)}\\right)_{1 \\leq j \\leq \\tau_{1}}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}} \\text{(excluding an intercept)}\\] é™¤äº†ä¸Šè¿°å•éšå±‚çš„ç»“æ„ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è½»æ¾åœ°è®¾è®¡å¤šéšå±‚çš„RNN. ä¾‹å¦‚ï¼ŒåŒéšå±‚çš„RNNç»“æ„å¯ä»¥ä¸º: 1st variant : ä»…å…è®¸åŒçº§éšå±‚ä¹‹é—´çš„å¾ªç¯ \\[ \\begin{aligned} \\boldsymbol{z}_{t}^{(1)} &amp;=\\boldsymbol{z}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}\\right) \\\\ \\boldsymbol{z}_{t}^{(2)} &amp;=\\boldsymbol{z}^{(2)}\\left(\\boldsymbol{z}_{t}^{(1)}, \\boldsymbol{z}_{t-1}^{(2)}\\right) \\end{aligned} \\] 2nd variant : å…è®¸è·¨çº§éšå±‚å¾ªç¯ \\[ \\begin{aligned} \\boldsymbol{z}_{t}^{(1)} &amp;=\\boldsymbol{z}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}, \\boldsymbol{z}_{t-1}^{(2)}\\right) \\\\ \\boldsymbol{z}_{t}^{(2)} &amp;=\\boldsymbol{z}^{(2)}\\left(\\boldsymbol{z}_{t}^{(1)}, \\boldsymbol{z}_{t-1}^{(2)}\\right) \\end{aligned} \\] 3rd variant : å…è®¸äºŒçº§éšå±‚ä¸è¾“å…¥å±‚ \\(\\boldsymbol{x}_{t}\\) è¿›è¡Œå¾ªç¯ \\[ \\begin{aligned} \\boldsymbol{z}_{t}^{(1)} &amp;=\\boldsymbol{z}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}, \\boldsymbol{z}_{t-1}^{(2)}\\right) \\\\ \\boldsymbol{z}_{t}^{(2)} &amp;=\\boldsymbol{z}^{(2)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t}^{(1)}, \\boldsymbol{z}_{t-1}^{(2)}\\right) \\end{aligned} \\] 6.3 é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œï¼ˆLong short-term memoryï¼‰ ä»¥ä¸Šplain vanilla RNN æ— æ³•å¤„ç†é•¿è·ç¦»ä¾èµ–å’Œä¸”æœ‰æ¢¯åº¦æ¶ˆæ•£çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼ŒHochreiter-Schmidhuber (1997)æå‡ºäº†é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œ(Long Short Term Memory Network, LSTM)ã€‚ 6.3.1 æ¿€æ´»å‡½æ•°ï¼ˆActivation functionsï¼‰ LSTM ç”¨åˆ°3ç§ä¸åŒçš„ æ¿€æ´»å‡½æ•°ï¼ˆactivation functionsï¼‰: Sigmoidå‡½æ•°ï¼ˆSigmoid functionï¼‰ \\[\\phi_{\\sigma}(x)=\\frac{1}{1+e^{-x}} \\in(0,1)\\] åŒæ›²æ­£åˆ‡å‡½æ•°ï¼ˆHyberbolic tangent functionï¼‰ \\[\\phi_{\\tanh }(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=2 \\phi_{\\sigma}(2 x)-1 \\in(-1,1)\\] ä¸€èˆ¬çš„æ¿€æ´»å‡½æ•°ï¼ˆGeneral activation functionï¼‰ \\[\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}\\] 6.3.2 Gates and cell state ä»¤ \\(\\boldsymbol{z}_{t-1}^{(1)} \\in \\mathbb{R}^{\\tau_{1}}\\) è¡¨ç¤ºæ—¶é—´ \\((t-1)\\) æ—¶çš„æ´»åŒ–çŠ¶æ€ï¼ˆneuron activationsï¼‰. æˆ‘ä»¬å®šä¹‰3ä¸­ä¸åŒçš„ é—¨ï¼ˆgatesï¼‰, ç”¨æ¥å†³å®šä¼ æ’­åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çš„ä¿¡æ¯é‡ï¼š é—å¿˜é—¨ï¼ˆForget gateï¼‰ (loss of memory rate): \\[\\boldsymbol{f}_{t}^{(1)}=\\boldsymbol{f}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}\\right)=\\phi_{\\sigma}\\left(\\left\\langle W_{f}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle U_{f}, \\boldsymbol{z}_{t-1}^{(1)}\\right\\rangle\\right) \\in(0,1)^{\\tau_{1}}\\] for network parameters \\(W_{f}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}\\) (including an intercept) \\(, U_{f}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}}\\) (excluding an intercept \\(),\\) and where the activation function is evaluated element wise. è¾“å…¥é—¨ï¼ˆInput gateï¼‰ (memory update rate): \\[\\boldsymbol{i}_{t}^{(1)}=\\boldsymbol{i}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}\\right)=\\phi_{\\sigma}\\left(\\left\\langle W_{i}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle U_{i}, \\boldsymbol{z}_{t-1}^{(1)}\\right\\rangle\\right) \\in(0,1)^{\\tau_{1}}\\] for network parameters \\(W_{i}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}\\) (including an intercept), \\(U_{i}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}}\\). è¾“å‡ºé—¨ï¼ˆOutput gateï¼‰ (release of memory information rate): \\[\\boldsymbol{o}_{t}^{(1)}=\\boldsymbol{o}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}\\right)=\\phi_{\\sigma}\\left(\\left\\langle W_{o}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle U_{o}, \\boldsymbol{z}_{t-1}^{(1)}\\right\\rangle\\right) \\in(0,1)^{\\tau_{1}}\\] for network parameters \\(W_{o}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}\\) (including an intercept) \\(, U_{o}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}}\\). æ³¨æ„ï¼šä»¥ä¸Šä¸‰ç§é—¨çš„åå­—å¹¶ä¸ä»£è¡¨ç€å®ƒä»¬åœ¨å®é™…ä¸­çš„ä½œç”¨ï¼Œå®ƒä»¬çš„ä½œç”¨ç”±ç½‘ç»œå‚æ•°å†³å®šï¼Œè€Œç½‘ç»œå‚æ•°æ˜¯ä»æ•°æ®ä¸­å­¦åˆ°çš„ã€‚ ä»¤ \\(\\left(\\boldsymbol{c}_{t}^{(1)}\\right)_{t}\\) è¡¨ç¤º ç»†èƒçŠ¶æ€ï¼ˆcell stateï¼‰ , ç”¨ä»¥å‚¨å­˜å·²è·å¾—çš„ç›¸å…³ä¿¡æ¯. ç»†èƒçŠ¶æ€çš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š \\[\\begin{aligned} \\boldsymbol{c}_{t}^{(1)}&amp;=\\boldsymbol{c}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}, \\boldsymbol{c}_{t-1}^{(1)}\\right)\\\\&amp;=\\boldsymbol{f}_{t}^{(1)} \\circ \\boldsymbol{c}_{t-1}^{(1)}+\\boldsymbol{i}_{t}^{(1)} \\circ \\phi_{\\tanh }\\left(\\left\\langle W_{c}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle U_{c}, \\boldsymbol{z}_{t-1}^{(1)}\\right\\rangle\\right) \\in \\mathbb{R}^{\\tau_{1}} \\end{aligned}\\] for network parameters \\(W_{c}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}\\) (including an intercept), \\(U_{c}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}},\\) and \\(\\circ\\) denotes the Hadamard product (element wise product). æœ€åï¼Œæˆ‘ä»¬æ›´æ–°æ—¶åˆ» \\(t\\) æ—¶çš„æ´»åŒ–çŠ¶æ€ \\(\\boldsymbol{z}_{t}^{(1)} \\in \\mathbb{R}^{\\tau_{1}}\\). \\[\\boldsymbol{z}_{t}^{(1)}=\\boldsymbol{z}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}, \\boldsymbol{c}_{t-1}^{(1)}\\right)=\\boldsymbol{o}_{t}^{(1)} \\circ \\phi\\left(\\boldsymbol{c}_{t}^{(1)}\\right) \\in \\mathbb{R}^{\\tau_{1}}\\] è‡³æ­¤ï¼Œ æ¶‰åŠçš„å…¨éƒ¨ç½‘ç»œå‚æ•°æœ‰: \\[W_{f}^{\\top}, W_{i}^{\\top}, W_{o}^{\\top}, W_{c}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}ï¼Œ~~ U_{f}^{\\top}, U_{i}^{\\top}, U_{o}^{\\top}, U_{c}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}} .\\] ä¸€ä¸ªLSTMå±‚éœ€è¦ \\(4\\left(\\left(\\tau_{0}+1\\right) \\tau_{1}+\\tau_{1}^{2}\\right)\\) ä¸ªç½‘ç»œå‚æ•°ã€‚ ä»¥ä¸Šå®šä¹‰çš„å¤æ‚æ˜ å°„åœ¨kerasé€šè¿‡å‡½æ•°layer_lstm()å³å¯å®ç°ã€‚ è¿™äº›å‚æ•°å‡ç”±æ¢¯åº¦ä¸‹é™çš„å˜å¼ç®—æ³•ï¼ˆa variant of the gradient descent algorithmï¼‰å­¦ä¹ å¾—. 6.3.3 Output Function åŸºäº \\(\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{T}\\right)\\) , æˆ‘ä»¬æ¥é¢„æµ‹å®šä¹‰åœ¨ \\(\\mathcal{Y} \\subset \\mathbb{R}\\) çš„éšæœºå˜é‡ \\(Y_{T}\\) . \\[\\widehat{Y}_{T}=\\widehat{Y}_{T}\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{T}\\right)=\\varphi\\left\\langle\\boldsymbol{w}, \\boldsymbol{z}_{T}^{(1)}\\right\\rangle \\in \\mathcal{Y}\\] å…¶ä¸­ï¼Œ \\(z_{T}^{(1)}\\) æ˜¯æœ€æ–°çš„éšå±‚ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ï¼ˆhidden neuron activationï¼‰ \\(\\boldsymbol{w} \\in \\mathbb{R}^{\\tau_{1}+1}\\) æ˜¯è¾“å‡ºæƒé‡(again including an intercept component) \\(\\varphi: \\mathbb{R} \\rightarrow \\mathcal{Y}\\) æ˜¯ä¸€ä¸ªæ°å½“çš„è¾“å‡ºæ¿€æ´»å‡½æ•°ï¼Œé€‰æ‹©æ—¶éœ€è¦è€ƒè™‘\\(y\\)çš„å–å€¼èŒƒå›´ã€‚ 6.3.4 Time-distributed Layer ä»¥ä¸Šåªè€ƒè™‘äº†æ ¹æ®æœ€æ–°çš„çŠ¶æ€ \\(\\boldsymbol{z}_{T}^{(1)}\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{T}\\right)\\) æ‰€ç¡®å®šçš„å•ä¸€çš„è¾“å‡º \\(Y_{T}\\). ä½†æ˜¯æˆ‘ä»¬å¯ä»¥è€ƒè™‘ æ‰€æœ‰ éšå±‚ç¥ç»å…ƒçŠ¶æ€: \\[\\boldsymbol{z}_{1}^{(1)}\\left(\\boldsymbol{x}_{1}\\right), \\boldsymbol{z}_{2}^{(1)}\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}\\right), \\boldsymbol{z}_{3}^{(1)}\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{3}\\right), \\ldots, \\boldsymbol{z}_{T}^{(1)}\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{T}\\right)\\] æ¯ä¸€ä¸ªçŠ¶æ€ \\(\\boldsymbol{z}_{t}^{(1)}\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{t}\\right)\\) éƒ½å¯ä»¥ä½œä¸ºè§£é‡Šå˜é‡ï¼Œç”¨ä»¥ä¼°è®¡ \\(t\\) æ—¶æ‰€å¯¹åº”çš„ \\(Y_{t}\\) : \\[\\widehat{Y}_{t}=\\widehat{Y}_{t}\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{t}\\right)=\\varphi\\left\\langle\\boldsymbol{w}, \\boldsymbol{z}_{t}^{(1)}\\right\\rangle=\\varphi\\left\\langle\\boldsymbol{w}, \\boldsymbol{z}_{t}^{(1)}\\left(\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{t}\\right)\\right\\rangle\\] å…¶ä¸­è¿‡æ»¤å™¨ï¼ˆfilterï¼‰ \\(\\varphi\\langle\\boldsymbol{w}, \\cdot\\rangle\\) å¯¹æ‰€æœ‰æ—¶é—´ \\(t\\) å–ç›¸åŒå‡½æ•°. å°ç»“ï¼šLSTMçš„ä¼˜åŠ¿ æ—¶é—´åºåˆ—ç»“æ„å’Œå› æœå…³ç³»éƒ½å¯ä»¥å¾—åˆ°æ­£ç¡®çš„ååº” ç”±äºå‚æ•°ä¸ä¾èµ–æ—¶é—´ï¼ŒLSTMå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‹“å±•åˆ°æœªæ¥æ—¶é—´æ®µ 6.4 é—¨æ§å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆGated Recurrent Unitï¼‰ å¦ä¸€ä¸ªæ¯”è¾ƒçƒ­é—¨çš„RNNç»“æ„æ˜¯ï¼šé—¨æ§å¾ªç¯å•å…ƒï¼ˆgated recurrent unit, GRU), ç”±Cho et al. (2014) æå‡ºï¼Œå®ƒæ¯”LSTMæ›´åŠ ç®€æ´ï¼Œä½†åŒæ ·å¯ä»¥ç¼“è§£plain vanilla RNNä¸­æ¢¯åº¦æ¶ˆæ•£çš„é—®é¢˜ã€‚ 6.4.1 Gates GRUåªä½¿ç”¨2ä¸ªä¸åŒçš„é—¨ï¼ˆgatesï¼‰. ä»¤ \\(\\boldsymbol{z}_{t-1}^{(1)} \\in \\mathbb{R}^{\\tau_{1}}\\) è¡¨ç¤º \\((t-1)\\) æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€. Reset gate: ç±»ä¼¼äºLSTMä¸­çš„é—å¿˜é—¨ \\[\\boldsymbol{r}_{t}^{(1)}=\\boldsymbol{r}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}\\right)=\\phi_{\\sigma}\\left(\\left\\langle W_{r}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle U_{r}, \\boldsymbol{z}_{t-1}^{(1)}\\right\\rangle\\right) \\in(0,1)^{\\tau_{1}}\\] for network parameters \\(W_{r}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}\\) (including an intercept), \\(U_{r}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}}\\). Update gate: ç±»ä¼¼äºLSTMä¸­çš„è¾“å…¥é—¨ \\[\\boldsymbol{u}_{t}^{(1)}=\\boldsymbol{u}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}\\right)=\\phi_{\\sigma}\\left(\\left\\langle W_{u}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle U_{u}, \\boldsymbol{z}_{t-1}^{(1)}\\right\\rangle\\right) \\in(0,1)^{\\tau_{1}}\\] for network parameters \\(W_{u}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}\\) (including an intercept), \\(U_{u}^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}}\\) 6.4.2 Neuron Activations ä»¥ä¸Šé—¨å˜é‡çš„ä½œç”¨æ˜¯ï¼Œå·²çŸ¥ \\(t-1\\) æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ \\(\\boldsymbol{z}_{t-1}^{(1)}\\), è®¡ç®— \\(t\\) æ—¶ç¥ç»å…ƒæ´»åŒ–çŠ¶æ€ \\(\\boldsymbol{z}_{t}^{(1)} \\in \\mathbb{R}^{\\tau_{1}}\\) . æˆ‘ä»¬é€‰ç”¨å¦‚ä¸‹ç»“æ„ï¼š \\[\\boldsymbol{z}_{t}^{(1)}=\\boldsymbol{z}^{(1)}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{z}_{t-1}^{(1)}\\right)=\\boldsymbol{r}_{t}^{(1)} \\circ \\boldsymbol{z}_{t-1}^{(1)}+\\left(\\mathbf{1}-\\boldsymbol{r}_{t}^{(1)}\\right) \\circ \\phi\\left(\\left\\langle W, \\boldsymbol{x}_{t}\\right\\rangle+\\boldsymbol{u}_{t} \\circ\\left\\langle U, \\boldsymbol{z}_{t-1}^{(1)}\\right\\rangle\\right) \\in \\mathbb{R}^{\\tau_{1}}\\] for network parameters \\(W^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times\\left(\\tau_{0}+1\\right)}\\) (including an intercept) \\(, U^{\\top} \\in \\mathbb{R}^{\\tau_{1} \\times \\tau_{1}},\\) and where \\(\\circ\\) denotes the Hadamard product. GRUç½‘ç»œæ¯”LSTMç½‘ç»œçš„ç»“æ„æ›´ç®€æ´ï¼Œè€Œä¸”ä¼šäº§ç”Ÿç›¸è¿‘çš„ç»“æœã€‚ ä½†æ˜¯ï¼ŒGRUåœ¨ç¨³å¥æ€§ä¸Šæœ‰è¾ƒå¤§ç¼ºé™·ï¼Œå› æ­¤ç°é˜¶æ®µLSTMçš„ä½¿ç”¨æ›´ä¸ºå¹¿æ³›. 6.5 æ¡ˆä¾‹åˆ†æï¼ˆCase studyï¼‰ æœ¬æ¡ˆä¾‹çš„æ•°æ®æ¥æºäºHuman Mortality Database (HMD)ä¸­çš„æ•°æ®ï¼Œé€‰æ‹©ç‘å£«äººå£æ•°æ®(HMDä¸­æ ‡è®°ä¸ºâ€œCHEâ€)ä½œä¸ºç¤ºä¾‹ã€‚ 6.5.1 æ•°æ®æè¿° æ•°æ®åŒ…å«7ä¸ªå˜é‡ï¼Œå„å˜é‡è¯´æ˜å¦‚ä¸‹ï¼š å˜é‡ ç±»å‹ è¯´æ˜ Gender factor ä¸¤ç§æ€§åˆ«â€”â€”ç”·æ€§å’Œå¥³æ€§ Year int æ—¥å†å¹´ï¼Œ1950å¹´åˆ°2016å¹´ Age int å¹´é¾„èŒƒå›´0-99å² Country chr â€œCHEâ€ï¼Œä»£è¡¨ç‘å£« imputed_flag logi åŸå§‹æ­»äº¡ç‡ä¸º0ï¼Œç”¨HMDä¸­å…¶ä½™å›½å®¶åŒæ—¥å†å¹´åŒå¹´é¾„çš„å¹³å‡æ­»äº¡ç‡ä»£æ›¿ï¼Œåˆ™è¯¥å˜é‡ä¸ºTRUE mx num æ­»äº¡ç‡ logmx num å¯¹æ•°æ­»äº¡ç‡ path.data &lt;- &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot; # path and name of data file region &lt;- &quot;CHE&quot; # country to be loaded (code is for one selected country) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;) str(all_mort) length(unique(all_mort$Age)) length(unique(all_mort$Year)) 67*2*100 6.5.2 æ­»äº¡ç‡çƒ­åŠ›å›¾ gender &lt;- &quot;Male&quot; #gender &lt;- &quot;Female&quot; m0 &lt;- c(min(all_mort$logmx), max(all_mort$logmx)) # rows are calendar year t, columns are ages x logmx &lt;- t(matrix(as.matrix(all_mort[which(all_mort$Gender==gender),&quot;logmx&quot;]), nrow=100, ncol=67)) # png(&quot;./plots/6/heat.png&quot;) image(z=logmx, useRaster=TRUE, zlim=m0, col=rev(rainbow(n=60, start=0, end=.72)), xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=list(paste(&quot;Swiss &quot;,gender, &quot; raw log-mortality rates&quot;, sep=&quot;&quot;), cex=1.5), cex.lab=1.5, ylab=&quot;age x&quot;, xlab=&quot;calendar year t&quot;) axis(1, at=c(0:(2016-1950))/(2016-1950), c(1950:2016)) axis(2, at=c(0:49)/50, labels=c(0:49)*2) lines(x=rep((1999-1950+0.5)/(2016-1950), 2), y=c(0:1), lwd=2) dev.off() å›¾6.1æ˜¾ç¤ºäº†ç”·å¥³æ€§å¯¹æ•°æ­»äº¡ç‡éšæ—¶é—´çš„æ”¹å–„: å·¦å³ä¸¤å¹…å›¾çš„è‰²æ ‡ç›¸åŒï¼Œè“è‰²è¡¨ç¤ºæ­»äº¡ç‡å°ï¼Œçº¢è‰²è¡¨ç¤ºæ­»äº¡ç‡å¤§ è¯¥å›¾æ˜¾ç¤ºè¿‡å»å‡ åå¹´å…¸å‹çš„æ­»äº¡ç‡æ”¹å–„â€”â€”çƒ­å›¾ä¸­é¢œè‰²å‘ˆç•¥å¾®å‘ä¸Šçš„å¯¹è§’çº¿ç»“æ„ å¹³å‡è€Œè¨€ï¼Œå¥³æ€§æ­»äº¡ç‡ä½äºç”·æ€§ å›¾ä¸­ä½äº2000å¹´çš„å‚ç›´é»‘çº¿è¡¨ç¤ºå¯¹äºè®­ç»ƒæ•°æ®\\(\\mathcal{T}\\)å’ŒéªŒè¯æ•°æ®\\(\\mathcal{V}\\)çš„åˆ’åˆ†:åç»­æ¨¡å‹å°†ä½¿ç”¨æ—¥å†å¹´\\(t=1950, \\ldots, 1999\\)ä½œä¸ºè®­ç»ƒæ•°æ®\\(\\mathcal{T}\\)è¿›è¡Œå­¦ä¹ ï¼Œç”¨\\(2000, \\ldots, 2016\\)ä½œä¸ºéªŒè¯æ•°æ®\\(\\mathcal{V}\\)å¯¹æ­»äº¡ç‡åšæ ·æœ¬å¤–éªŒè¯ã€‚ Figure 6.1: ç‘å£«ç”·å¥³æ€§æ­»äº¡ç‡çƒ­åŠ›å›¾ 6.5.3 Lee-Carter æ¨¡å‹ ObsYear &lt;- 1999 gender &lt;- &quot;Female&quot; train &lt;- all_mort[Year&lt;=ObsYear][Gender == gender] min(train$Year) ### fit via SVD train[,ax:= mean(logmx), by = (Age)] train[,mx_adj:= logmx-ax] rates_mat &lt;- as.matrix(train %&gt;% dcast.data.table(Age~Year, value.var = &quot;mx_adj&quot;, sum))[,-1] dim(rates_mat) svd_fit &lt;- svd(rates_mat) ax &lt;- train[,unique(ax)] bx &lt;- svd_fit$u[,1]*svd_fit$d[1] kt &lt;- svd_fit$v[,1] c1 &lt;- mean(kt) c2 &lt;- sum(bx) ax &lt;- ax+c1*bx bx &lt;- bx/c2 kt &lt;- (kt-c1)*c2 ### extrapolation and forecast vali &lt;- all_mort[Year&gt;ObsYear][Gender == gender] t_forecast &lt;- vali[,unique(Year)] %&gt;% length() forecast_kt =kt %&gt;% forecast::rwf(t_forecast, drift = T) kt_forecast = forecast_kt$mean # illustration selected drift plot_data &lt;- c(kt, kt_forecast) plot(plot_data, pch=20, col=&quot;red&quot;, cex=2, cex.lab=1.5, xaxt=&#39;n&#39;, ylab=&quot;values k_t&quot;, xlab=&quot;calendar year t&quot;, main=list(paste(&quot;estimated process k_t for &quot;,gender, sep=&quot;&quot;), cex=1.5)) points(kt, col=&quot;blue&quot;, pch=20, cex=2) axis(1, at=c(1:length(plot_data)), labels=c(1:length(plot_data))+1949) abline(v=(length(kt)+0.5), lwd=2) # in-sample and out-of-sample analysis fitted = (ax+(bx)%*%t(kt)) %&gt;% melt train$pred_LC_svd = fitted$value %&gt;% exp fitted_vali = (ax+(bx)%*%t(kt_forecast)) %&gt;% melt vali$pred_LC_svd = fitted_vali$value %&gt;% exp round(c((mean((train$mx-train$pred_LC_svd)^2)*10^4) , (mean((vali$mx-vali$pred_LC_svd)^2)*10^4)),4) ç”¨å¸¦æ¼‚ç§»é¡¹çš„éšæœºæ¸¸èµ°é¢„æµ‹\\(t \\in \\mathcal{V}=\\{2000, \\ldots, 2016\\}\\)çš„\\(\\hat{k}_{t}\\)ï¼Œä¸‹å›¾è¯´æ˜äº†ç»“æœã€‚ Figure 6.2: ç‘å£«ç”·å¥³æ€§ktçš„ä¼°è®¡ä¸é¢„æµ‹å€¼ å›¾6.2æ˜¾ç¤ºå¯¹äºå¥³æ€§æ¥è¯´é¢„æµ‹ç»“æœæ˜¯ç›¸å¯¹åˆç†çš„ï¼Œä½†æ˜¯å¯¹äºç”·æ€§è€Œè¨€ï¼Œç”±æ­¤äº§ç”Ÿçš„æ¼‚ç§»å¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„æ¢ç´¢ï¼Œä¸‹é¢ç”·å¥³æ€§æ ·æœ¬å†…å¤–çš„MSEæŸå¤±ç»“æœä¹Ÿè¡¨æ˜äº†è¿™ä¸€ç‚¹ï¼šç”·æ€§æ ·æœ¬å¤–æŸå¤±è¾ƒå¤§ \\[ \\begin{array}{|c|cc|cc|} \\hline &amp; \\ {\\text { in-sample loss }} &amp; \\ {\\text { in-sample loss }} &amp; \\ {\\text { out-of-sample loss }} &amp; \\ {\\text { out-of-sample loss }}\\\\ &amp; \\text { female } &amp; \\text { male } &amp; \\text { female } &amp; \\text { male } \\\\ \\hline \\text { LC model with SVD } &amp; 3.7573 &amp; 8.8110 &amp; 0.6045 &amp; 1.8152 \\\\ \\hline \\end{array} \\] 6.5.4 åˆè¯•RNN æ•°æ®è¯´æ˜ é€‰æ‹©æ€§åˆ«ä¸ºâ€œå¥³æ€§â€ï¼Œæå–\\(1990, \\ldots, 2001\\)å¹´çš„å¯¹æ•°æ­»äº¡ç‡ï¼Œå¹´é¾„ä¸º\\(0 \\leq x \\leq 99\\) è¶…å‚æ•°è®¾ç½®ï¼šå›é¡¾å‘¨æœŸ\\(T=10\\)ï¼›\\(\\tau_{0}=3\\) å®šä¹‰è§£é‡Šå˜é‡å’Œå“åº”å˜é‡ï¼š å¯¹äº\\(1 \\leq x \\leq 98\\)ï¼Œ\\(1 \\leq t \\leq T\\)ï¼Œæœ‰ è§£é‡Šå˜é‡\\(\\boldsymbol{x}_{t, x}=\\left(\\log \\left(M_{1999-(T-t), x-1}\\right), \\log \\left(M_{1999-(T-t), x}\\right), \\log \\left(M_{1999-(T-t), x+1}\\right)\\right)^{\\top} \\in \\mathbb{R}^{\\tau_{0}}\\) å“åº”å˜é‡\\(\\boldsymbol{Y}_{T, x}=\\log(M_{2000,x}) =\\log \\left(M_{1999-(T-T)+1, x}\\right) \\in \\mathbb{R}_{-}\\) åŒæ—¶è€ƒè™‘\\((x-1,x,x+1)\\)ç›®çš„æ˜¯ç”¨é‚»è¿‘çš„å¹´é¾„æ¥å¹³æ»‘è¾“å…¥ã€‚ é€‰æ‹©è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®ï¼š è®­ç»ƒæ•°æ®\\(\\mathcal{T}=\\{(\\boldsymbol{x}_{1,x}, \\ldots,\\boldsymbol{x}_{T,x};\\boldsymbol{Y}_{T, x});1 \\leq x \\leq 98\\}\\) éªŒè¯æ•°æ®\\(\\mathcal{V}=\\{(\\boldsymbol{x}_{2,x}, \\ldots,\\boldsymbol{x}_{T+1,x};\\boldsymbol{Y}_{T+1, x});1 \\leq x \\leq 98\\}\\)ï¼Œåœ¨è®­ç»ƒæ•°æ®åŸºç¡€ä¸Šæ—¶ç§»äº†ä¸€ä¸ªæ—¥å†å¹´ æ•°æ®å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š Figure 5.1: RNNåˆè¯•ä¸­é€‰æ‹©çš„æ•°æ® é»‘çº¿è¡¨ç¤ºé€‰å®šçš„è§£é‡Šå˜é‡\\(\\boldsymbol{x}_{t, x}\\);è“è‰²çš„ç‚¹æ˜¯è®­ç»ƒæ•°æ®ä¸­çš„çš„å“åº”å˜é‡\\(\\boldsymbol{Y}_{T, x}\\)ï¼›éªŒè¯æ•°æ®ä¸­å“åº”å˜é‡\\(\\boldsymbol{Y}_{T+1, x}=\\log(M_{2001,x})\\)ç”¨çº¢è‰²çš„ç‚¹è¡¨ç¤º æ•°æ®é¢„å¤„ç† å¯¹è§£é‡Šå˜é‡åº”ç”¨MinMaxScalerè¿›è¡Œæ ‡å‡†åŒ–å¤„ç† åˆ‡æ¢å“åº”å˜é‡çš„ç¬¦å· æ¯”è¾ƒLSTMså’ŒGRUs åœ¨éªŒè¯é›†\\(\\mathcal{V}\\)ä¸Šè·Ÿè¸ªè¿‡æ‹Ÿåˆ æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•é€‰ç”¨çš„æ˜¯nadam ä¸‹å›¾æ˜¾ç¤ºäº†5ä¸ªæ¨¡å‹çš„æ”¶æ•›è¡Œä¸º Figure 6.3: æ¨¡å‹çš„æ ·æœ¬å†…å¤–æŸå¤± æ ¹æ®è¿‡æ‹Ÿåˆç¡®å®šçš„åœæ­¢æ—¶é—´çš„æ¨¡å‹æ ¡å‡†ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š \\[ \\begin{array}{|l|ccc|cc|} \\hline &amp; \\# \\text { param. } &amp; \\text { epochs } &amp; \\text { run time } &amp; \\text { in-sample loss } &amp; \\text { out-of-sample loss } \\\\ \\hline \\text { LSTM1 } &amp; 186 &amp; 150 &amp; 8 \\mathrm{sec} &amp; 0.0655 &amp; 0.0936 \\\\ \\text { LSTM2 } &amp; 345 &amp; 200 &amp; 15 \\mathrm{sec} &amp; 0.0603 &amp; 0.0918 \\\\ \\hline \\text { GRU1 } &amp; 141 &amp; 100 &amp; 5 \\mathrm{sec} &amp; 0.0671 &amp; 0.0860 \\\\ \\text { GRU2 } &amp; 260 &amp; 200 &amp; 14 \\mathrm{sec} &amp; 0.0651 &amp; 0.0958 \\\\ \\hline \\text { deep FNN } &amp; 184 &amp; 200 &amp; 5 \\mathrm{sec} &amp; 0.0485 &amp; 0.1577 \\\\ \\hline \\end{array} \\] è¡¨ä¸­æ‰€ç¤ºæ¨¡å‹çš„è¶…å‚æ•°è®¾ç½® LSTM1å’ŒGRU1è¡¨ç¤ºåªæœ‰ä¸€ä¸ªéšè—å±‚çš„RNNï¼Œè¯¥éšè—å±‚çš„ç¥ç»å…ƒä¸ªæ•°\\(\\tau_{1}=5\\) LSTM2å’ŒGRU2è¡¨ç¤ºæœ‰ä¸¤ä¸ªéšè—å±‚çš„RNNï¼Œç¬¬ä¸€ä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°\\(\\tau_{1}=5\\)ï¼›ç¬¬äºŒä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°\\(\\tau_{2}=4\\) deep FNNè¡¨ç¤ºæœ‰ä¸¤ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œç»“æ„ï¼Œå…¶ä¸­\\((q1,q2)=(5,4)\\) ç»“è®º LSTM2ä¸LSTM1æ¨¡å‹é¢„æµ‹è´¨é‡ç›¸å½“ï¼Œä½†LSTM2æœ‰æ›´å¤šå‚æ•°åŠæ›´é•¿çš„è¿è¡Œæ—¶é—´ LTSMä¸GRUè¶…å‚æ•°é€‰æ‹©ç›¸åŒæ—¶ï¼Œæ™®éçš„è§‚å¯Ÿç»“æœæ˜¯GRUæ¯”LSTMæ›´å¿«çš„è¿‡æ‹Ÿåˆï¼Œä½†GRUä¸ç¨³å®š å‰é¦ˆç¥ç»ç½‘ç»œä¸RNNç›¸æ¯”æ²¡æœ‰ç«äº‰åŠ› åœ¨æœ¬æ–‡å»ºæ¨¡ä¸­æœªå¼•å…¥å¹´é¾„å˜é‡çš„åŸå› ï¼š åå˜é‡æ ‡å‡†åŒ–åˆ°ï¼ˆ-1,1ï¼‰çš„è¿‡ç¨‹æ˜¯åœ¨æ‰€æœ‰å¹´é¾„ä¸ŠåŒæ—¶è¿›è¡Œçš„ï¼Œå› æ­¤åå˜é‡ä¿¡æ¯ä¿ç•™äº†æ­»äº¡ç‡æ°´å¹³ï¼Œè¿™å’Œå¼•å…¥å¹´é¾„å˜é‡å…·æœ‰ç›¸åŒçš„ä¿¡æ¯è´¨é‡ã€‚ è¶…å‚æ•°é€‰æ‹© åˆ†åˆ«æ”¹å˜\\(Tã€\\tau_{0}ã€\\tau_{1}\\)çš„å€¼ï¼Œå¾—åˆ°ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š \\[ \\begin{array}{|l|ccc|cc|} \\hline &amp; \\text { # param. } &amp; \\text { epochs } &amp; \\text { run time } &amp; \\text { in-sample } &amp; \\text { out-of-sample } \\\\ \\hline \\text { base case: } &amp; &amp; &amp; &amp; &amp; \\\\ \\text { LSTM1 }\\left(T=10, \\tau_{0}=3, \\tau_{1}=5\\right) &amp; 186 &amp; 150 &amp; 8 \\mathrm{sec} &amp; 0.0655 &amp; 0.0936 \\\\ \\hline \\text { LSTM1 }\\left(T=10, \\tau_{0}=1, \\tau_{1}=5\\right) &amp; 146 &amp; 100 &amp; 5 \\mathrm{sec} &amp; 0.0647 &amp; 0.1195 \\\\ \\text { LSTM1 }\\left(T=10, \\tau_{0}=5, \\tau_{1}=5\\right) &amp; 226 &amp; 150 &amp; 15 \\mathrm{sec} &amp; 0.0583 &amp; 0.0798 \\\\ \\hline \\text { LSTM1 }\\left(T=5, \\tau_{0}=3, \\tau_{1}=5\\right) &amp; 186 &amp; 100 &amp; 4 \\mathrm{sec} &amp; 0.0753 &amp; 0.1028 \\\\ \\text { LSTM1 }\\left(T=20, \\tau_{0}=3, \\tau_{1}=5\\right) &amp; 186 &amp; 200 &amp; 16 \\mathrm{sec} &amp; 0.0626 &amp; 0.0968 \\\\ \\hline \\text { LSTM1 }\\left(T=10, \\tau_{0}=3, \\tau_{1}=3\\right) &amp; 88 &amp; 200 &amp; 10 \\mathrm{sec} &amp; 0.0694 &amp; 0.0987 \\\\ \\text { LSTM1 }\\left(T=10, \\tau_{0}=3, \\tau_{1}=10\\right) &amp; 571 &amp; 100 &amp; 5 \\mathrm{sec} &amp; 0.0626 &amp; 0.0883 \\\\ \\hline \\end{array} \\] ç»“è®º åˆ†åˆ«ä»¤\\(\\tau_{0}=1,3,5\\),éœ€è¦æ›´é•¿çš„è¿è¡Œæ—¶é—´å¹¶æä¾›æ›´å¥½çš„æ ·æœ¬å¤–ç»“æœï¼› åˆ†åˆ«ä»¤\\(T=5,10,20\\)ï¼Œç»“è®ºåŒä¸Šï¼› åˆ†åˆ«ä»¤\\(\\tau_{1}=3,5,10\\)ï¼Œå¯¼è‡´æ›´å¿«çš„æ”¶æ•›ï¼Œå› ä¸ºæ¢¯åº¦ä¸‹é™ç®—æ³•æœ‰æ›´å¤šçš„è‡ªç”±åº¦ æœ€å¤§çš„å½±å“æ˜¯é€šè¿‡è®¾å®šä¸€ä¸ªæ›´å¤§çš„\\(\\tau_{0}\\)è€Œäº§ç”Ÿçš„ï¼Œå› æ­¤åé¢RNNç¤ºä¾‹ä¸­è®¾å®š\\(\\tau_{0}=5\\) # load corresponding data path.data &lt;- &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot; # path and name of data file region &lt;- &quot;CHE&quot; # country to be loaded (code is for one selected country) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;) str(all_mort) # LSTMs and GRUs source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;) T0 &lt;- 10 tau0 &lt;- 3 tau1 &lt;- 5 tau2 &lt;- 4 summary(LSTM1(T0, tau0, tau1, 0, &quot;nadam&quot;)) summary(LSTM2(T0, tau0, tau1, tau2, 0, &quot;nadam&quot;)) summary(LSTM_TD(T0, tau0, tau1, 0, &quot;nadam&quot;)) summary(GRU1(T0, tau0, tau1, 0, &quot;nadam&quot;)) summary(GRU2(T0, tau0, tau1, tau2, 0, &quot;nadam&quot;)) summary(FNN(T0, tau0, tau1, tau2, 0, &quot;nadam&quot;)) # Bringing the data in the right structure for a toy example gender &lt;- &quot;Female&quot; ObsYear &lt;- 2000 mort_rates &lt;- all_mort[which(all_mort$Gender==gender), c(&quot;Year&quot;, &quot;Age&quot;, &quot;logmx&quot;)] mort_rates &lt;- dcast(mort_rates, Year ~ Age, value.var=&quot;logmx&quot;) dim(mort_rates) T0 &lt;- 10 # lookback period tau0 &lt;- 3 # dimension of x_t (should be odd for our application) delta0 &lt;- (tau0-1)/2 toy_rates &lt;- as.matrix(mort_rates[which(mort_rates$Year %in% c((ObsYear-T0):(ObsYear+1))),]) dim(toy_rates) xt &lt;- array(NA, c(2,ncol(toy_rates)-tau0, T0, tau0)) YT &lt;- array(NA, c(2,ncol(toy_rates)-tau0)) for (i in 1:2){for (a0 in 1:(ncol(toy_rates)-tau0)){ xt[i,a0,,] &lt;- toy_rates[c(i:(T0+i-1)),c((a0+1):(a0+tau0))] YT[i,a0] &lt;- toy_rates[T0+i,a0+1+delta0] }} dim(xt) dim(YT) plot(x=toy_rates[1:T0,1], y=toy_rates[1:T0,2], col=&quot;white&quot;, xlab=&quot;calendar years&quot;, ylab=&quot;raw log-mortality rates&quot;, cex.lab=1.5, cex=1.5, main=list(&quot;data toy example&quot;, cex=1.5), xlim=range(toy_rates[,1]), ylim=range(toy_rates[,-1]), type=&#39;l&#39;) for (a0 in 2:ncol(toy_rates)){ if (a0 %in% (c(1:100)*3)){ lines(x=toy_rates[1:T0,1], y=toy_rates[1:T0,a0]) points(x=toy_rates[(T0+1):(T0+2),1], y=toy_rates[(T0+1):(T0+2),a0], col=c(&quot;blue&quot;, &quot;red&quot;), pch=20) lines(x=toy_rates[(T0):(T0+1),1], y=toy_rates[(T0):(T0+1),a0], col=&quot;blue&quot;, lty=2) lines(x=toy_rates[(T0+1):(T0+2),1], y=toy_rates[(T0+1):(T0+2),a0], col=&quot;red&quot;, lty=2) }} # LSTMs and GRUs x.train &lt;- array(2*(xt[1,,,]-min(xt))/(max(xt)-min(xt))-1, c(ncol(toy_rates)-tau0, T0, tau0)) x.vali &lt;- array(2*(xt[2,,,]-min(xt))/(max(xt)-min(xt))-1, c(ncol(toy_rates)-tau0, T0, tau0)) y.train &lt;- - YT[1,] (y0 &lt;- mean(y.train)) y.vali &lt;- - YT[2,] dim(x.train) length(y.train);length(y.vali) # x.age.train&lt;-as.matrix(0:0) # x.training&lt;-list(x.train,x.age.train) # x.age.valid&lt;-as.matrix(0:0) # x.validation&lt;-list(x.vali,x.age.valid) ### examples tau1 &lt;- 5 # dimension of the outputs z_t^(1) first RNN layer tau2 &lt;- 4 # dimension of the outputs z_t^(2) second RNN layer CBs &lt;- callback_model_checkpoint(&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model&quot;, monitor = &quot;val_loss&quot;, verbose = 0, save_best_only = TRUE, save_weights_only = TRUE,save_freq = NULL) model &lt;- LSTM2(T0, tau0, tau1, tau2, y0, &quot;nadam&quot;) summary(model) # takes 40 seconds on my laptop {t1 &lt;- proc.time() fit &lt;- model %&gt;% fit(x=x.train, y=y.train, validation_data=list(x.vali, y.vali), batch_size=10, epochs=500, verbose=1, callbacks=CBs) proc.time()-t1} plot(fit[[2]]$val_loss,col=&quot;red&quot;, ylim=c(0,0.5), main=list(&quot;early stopping rule&quot;, cex=1.5),xlab=&quot;epochs&quot;, ylab=&quot;MSE loss&quot;, cex=1.5, cex.lab=1.5) lines(fit[[2]]$loss,col=&quot;blue&quot;) abline(h=0.1, lty=1, col=&quot;black&quot;) legend(x=&quot;bottomleft&quot;, col=c(&quot;blue&quot;,&quot;red&quot;), lty=c(1,-1), lwd=c(1,-1), pch=c(-1,1), legend=c(&quot;in-sample loss&quot;, &quot;out-of-sample loss&quot;)) load_model_weights_hdf5(model, &quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model&quot;) Yhat.train1 &lt;- as.vector(model %&gt;% predict(x.train)) Yhat.vali1 &lt;- as.vector(model %&gt;% predict(x.vali)) c(round(mean((Yhat.train1-y.train)^2),4), round(mean((Yhat.vali1-y.vali)^2),4)) 6.5.5 RNN æ•°æ®é¢„å¤„ç† è§‚å¯Ÿå€¼ï¼š1950-1999å¹´æ•°æ®ï¼›é¢„æµ‹å€¼ï¼š2000-2016å¹´å¯¹æ•°æ­»äº¡ç‡ åˆ†åˆ«å¯¹ç”·æ€§å’Œå¥³æ€§å»ºç«‹æ¨¡å‹ï¼Œå…ˆé€‰å®šä¸€ä¸ªæ€§åˆ« å‚æ•°è®¾ç½®ï¼šå›é¡¾å‘¨æœŸ\\(T=10\\)ï¼›\\(\\tau_{0}=5\\) å®šä¹‰è§£é‡Šå˜é‡ï¼ˆéœ€æ‰©å……å¹´é¾„ç•Œé™â€”â€”å¤åˆ¶è¾¹ç¼˜ç‰¹å¾å€¼ï¼‰ï¼š å¯¹äº\\(0 \\leq x \\leq 99\\)ï¼Œ\\(1950 \\leq t \\leq 1999\\)ï¼Œæœ‰ è§£é‡Šå˜é‡\\(\\boldsymbol{x}_{t, x}=\\left(\\log \\left(M_{t,(x-2) \\vee 0}\\right), \\log \\left(M_{t,(x-1) \\vee 0}\\right), \\log \\left(M_{t, x}\\right), \\log \\left(M_{t,(x+1) \\wedge 99}\\right), \\log \\left(M_{t,(x+2) \\wedge 99}\\right)\\right)^{\\top} \\in \\mathbb{R}^{5}\\) è¯¥å¼ä¸­ï¼š\\(x_{0}\\vee x_{1}=\\text {max}\\{x_{0},x_{1}\\}\\);\\(x_{0}\\wedge x_{1}=\\text {min}\\{x_{0},x_{1}\\}\\) å®šä¹‰è®­ç»ƒæ•°æ®\\(\\mathcal{T}\\)å’ŒéªŒè¯æ•°æ®\\(\\mathcal{V}\\)ï¼š è®­ç»ƒæ•°æ®\\(\\mathcal{T}=\\{(\\boldsymbol{x}_{t-T,x}, \\ldots,\\boldsymbol{x}_{t-1,x},\\boldsymbol{Y}_{t, x});0 \\leq x \\leq 99\\ , 1950+T \\leq t \\leq 1999\\}\\) å…¶ä¸­ï¼Œ\\(\\boldsymbol{Y}_{t, x}=\\text {log}(M_{t,x})\\) éªŒè¯æ•°æ®: \\(s&gt;1999\\)çš„ç‰¹å¾å€¼è¦ç”¨ç›¸åº”çš„é¢„æµ‹å€¼æ›¿ä»£ \\[ \\widehat{\\boldsymbol{x}}_{s, x}=\\left(\\log \\left(\\widehat{M}_{s,(x-2) \\vee 0}\\right), \\log \\left(\\widehat{M}_{s,(x-1) \\vee 0}\\right), \\log \\left(\\widehat{M}_{s, x}\\right), \\log \\left(\\widehat{M}_{s,(x+1) \\wedge 99}\\right), \\log \\left(\\widehat{M}_{s,(x+2) \\wedge 99}\\right)\\right)^{\\top} \\in \\mathbb{R}^{5} \\] å› æ­¤éªŒè¯æ•°æ®\\(\\mathcal{V}=\\{(\\boldsymbol{x}_{t-T,x}, \\ldots,\\boldsymbol{x}_{1999,x},\\widehat{\\boldsymbol{x}}_{2000,x}, \\ldots,\\widehat{\\boldsymbol{x}}_{t-1,x},\\boldsymbol{Y}_{t, x});0 \\leq x \\leq 99\\ , 2000 \\leq t \\leq 2016\\}\\) åŸºäºè®­ç»ƒæ•°æ®æ‰€æœ‰ç‰¹å¾å€¼çš„æœ€å¤§æœ€å°å€¼å¯¹è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®åº”ç”¨MinMaxScaler åˆ‡æ¢å“åº”å˜é‡ç¬¦å· å»ºç«‹å•ä¸ªæ€§åˆ«çš„RNN å°†è®­ç»ƒæ•°æ®\\(\\mathcal T\\)éšæœºåˆ’åˆ†å­¦ä¹ é›†\\(\\mathcal T_{0}\\)(åŒ…å«80%æ•°æ®)ä»¥åŠæµ‹è¯•é›†\\(\\mathcal T_{1}\\)(åŒ…å«20%æ•°æ®)\\(\\mathcal T_{0}\\)ç”¨äºè¿½è¸ªæ ·æœ¬å†…è¿‡æ‹Ÿåˆï¼› å»ºç«‹å…·æœ‰ä¸‰ä¸ªéšè—å±‚çš„LSTM3å’ŒGRU3ï¼› è¶…å‚æ•°è®¾ç½®ï¼š\\(T=10\\)ï¼›\\(\\tau_{0}=5\\)ï¼›\\(\\tau_{1}=20\\)ï¼›\\(\\tau_{2}=15\\)ï¼›\\(\\tau_{3}=10\\)ï¼› ä¸‹å›¾æ˜¾ç¤ºäº†åˆ†åˆ«å¯¹ç”·æ€§å’Œå¥³æ€§å»ºç«‹ä¸¤ä¸ªæ¨¡å‹çš„æ”¶æ•›è¡Œä¸º Figure 6.4: æ¨¡å‹çš„æ ·æœ¬å†…å¤–æŸå¤± è¯¥å›¾æ˜¾ç¤ºï¼šGRUç»“æ„ä¼šå¯¼è‡´æ›´å¿«çš„æ”¶æ•›ï¼Œä½†åç»­ä¼šè¯å®GRUç»“æ„ä¸ç¨³å®šï¼Œå› æ­¤LSTMä¼šæ›´å—æ¬¢è¿ ä¸‹è¡¨ç»™å‡ºä¸‰ä¸ªæ¨¡å‹(LSTM3/GRU3/LC)åˆ†æ€§åˆ«çš„æ ·æœ¬å†…å¤–æŸå¤± \\[ \\begin{array}{|l|cc|cc|cc|} \\hline &amp; {\\text { in-sample }} &amp; {\\text { in-sample }} &amp; {\\text { out-of-sample }} &amp; {\\text { out-of-sample }}&amp; {\\text { run times }}&amp; {\\text { run times }} \\\\ &amp; \\text { female } &amp; \\text { male } &amp; \\text { female } &amp; \\text { male } &amp; \\text { female } &amp; \\text { male } \\\\ \\hline \\hline \\text { LSTM3 }\\left(T=10,\\left(\\tau_{0}, \\tau_{1}, \\tau_{2}, \\tau_{3}\\right)=(5,20,15,10)\\right) &amp; 2.5222 &amp; 6.9458 &amp; 0.3566 &amp; 1.3507 &amp; 225 \\mathrm{s} &amp; 203 \\mathrm{s} \\\\ \\text { GRU3 }\\left(T=10,\\left(\\tau_{0}, \\tau_{1}, \\tau_{2}, \\tau_{3}\\right)=(5,20,15,10)\\right) &amp; 2.8370 &amp; 7.0907 &amp; 0.4788 &amp; 1.2435 &amp; 185 \\mathrm{s} &amp; 198 \\mathrm{s} \\\\ \\hline \\hline \\text { LC model with SVD } &amp; 3.7573 &amp; 8.8110 &amp; 0.6045 &amp; 1.8152 &amp; - &amp; - \\\\ \\hline \\end{array} \\] èƒ½å¤Ÿçœ‹åˆ°ï¼Œæ‰€æœ‰è¢«é€‰æ‹©RNNæ¨¡å‹éƒ½ä¼˜äºLCæ¨¡å‹çš„é¢„æµ‹ æ¢ç´¢RNNçš„é¢„æµ‹ä¸­éšå«çš„æ¼‚ç§»é¡¹ ä¸­å¿ƒåŒ–RNNé¢„æµ‹çš„å¯¹æ•°æ­»äº¡ç‡ \\[ \\log \\left(\\widehat{M}_{t, x}^{\\circ}\\right)=\\log(\\widehat M_{t,x})-\\widehat a_{x} \\] åˆ©ç”¨ä¸‹å¼æ±‚å¾—\\(2000 \\leq t \\leq 2016\\)å¯¹åº”çš„\\(k_{t}\\) \\[ \\underset{k_{t}}{\\arg \\min } \\sum_{x}\\left(\\log \\left(\\widehat{M}_{t, x}^{\\circ}\\right)-\\widehat{b}_{x} k_{t}\\right)^{2} \\] å¼ä¸­ï¼Œ\\((\\widehat{b}_{x})_{x}\\)æ˜¯ä»LCæ¨¡å‹ä¼°è®¡å¾—åˆ°çš„ã€‚ ä¼°è®¡ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤º Figure 6.5: ä¸‰ç§æ¨¡å‹ä¸‹ktçš„ä¼°è®¡ä¸é¢„æµ‹å€¼ è¯¥å›¾æ˜¾ç¤ºï¼šå¯¹äºå¥³æ€§ï¼ŒLSTM3çš„é¢„æµ‹ä¸LCçš„é¢„æµ‹åŸºæœ¬ä¸€è‡´ï¼›è€Œå¯¹äºç”·æ€§ï¼ŒLSTM3çš„é¢„æµ‹çš„æ–œç‡ç•¥å¤§äºLCçš„é¢„æµ‹å¹¶æ”¶æ•›ä¸LCçš„é¢„æµ‹ï¼›ä½†æ˜¯GRU3çš„ç»“æœå¹¶ä¸ä»¤äººä¿¡æœï¼Œå¯èƒ½æ˜¯å› ä¸ºä»LCæ¨¡å‹ä¸­å¾—åˆ°çš„ä¸éšæ—¶é—´å˜åŒ–çš„å‚æ•°bxçš„ä¼°è®¡ä¸GRU3æ¨¡å‹äº§ç”Ÿçš„é¢„æµ‹ä¸ç¬¦ã€‚ # load corresponding data path.data &lt;- &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot; # path and name of data file region &lt;- &quot;CHE&quot; # country to be loaded (code is for one selected country) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;) str(all_mort) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R&quot;) # choice of parameters T0 &lt;- 10 tau0 &lt;- 5 gender &lt;- &quot;Female&quot; ObsYear &lt;- 1999 # training data pre-processing data1 &lt;- data.preprocessing.RNNs(all_mort, gender, T0, tau0, ObsYear) dim(data1[[1]]) dim(data1[[2]]) # validation data pre-processing all_mort2 &lt;- all_mort[which((all_mort$Year &gt; (ObsYear-10))&amp;(Gender==gender)),] all_mortV &lt;- all_mort2 vali.Y &lt;- all_mortV[which(all_mortV$Year &gt; ObsYear),] # MinMaxScaler data pre-processing x.min &lt;- min(data1[[1]]) x.max &lt;- max(data1[[1]]) x.train &lt;- array(2*(data1[[1]]-x.min)/(x.min-x.max)-1, dim(data1[[1]])) y.train &lt;- - data1[[2]] y0 &lt;- mean(y.train) # LSTM architectures # network architecture deep 3 network tau1 &lt;- 20 tau2 &lt;- 15 tau3 &lt;- 10 optimizer &lt;- &#39;adam&#39; # choose either LSTM or GRU network RNN.type &lt;- &quot;LSTM&quot; #RNN.type &lt;- &quot;GRU&quot; {if (RNN.type==&quot;LSTM&quot;){model &lt;- LSTM3(T0, tau0, tau1, tau2, tau3, y0, optimizer)}else{model &lt;- GRU3(T0, tau0, tau1, tau2, tau3, y0, optimizer)} name.model &lt;- paste(RNN.type,&quot;3_&quot;, tau0, &quot;_&quot;, tau1, &quot;_&quot;, tau2, &quot;_&quot;, tau3, sep=&quot;&quot;) file.name &lt;- paste(&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_&quot;, name.model,&quot;_&quot;, gender, sep=&quot;&quot;) summary(model)} # define callback CBs &lt;- callback_model_checkpoint(file.name, monitor = &quot;val_loss&quot;, verbose = 0, save_best_only = TRUE, save_weights_only = TRUE, save_freq = NULL) # gradient descent fitting: takes roughly 200 seconds on my laptop {t1 &lt;- proc.time() fit &lt;- model %&gt;% fit(x=x.train, y=y.train, validation_split=0.2, batch_size=100, epochs=500, verbose=1, callbacks=CBs) proc.time()-t1} # plot loss figures plot.losses(name.model, gender, fit[[2]]$val_loss, fit[[2]]$loss) # calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110) load_model_weights_hdf5(model, file.name) round(10^4*mean((exp(-as.vector(model %&gt;% predict(x.train)))-exp(-y.train))^2),4) # calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152) pred.result &lt;- recursive.prediction(ObsYear, all_mort2, gender, T0, tau0, x.min, x.max, model) vali &lt;- pred.result[[1]][which(all_mort2$Year &gt; ObsYear),] round(10^4*mean((vali$mx-vali.Y$mx)^2),4) 6.5.6 å¼•å…¥æ€§åˆ«åå˜é‡ æ•°æ®é¢„å¤„ç† æ·»åŠ æ€§åˆ«æŒ‡ç¤ºå˜é‡ï¼š0è¡¨ç¤ºå¥³æ€§ï¼›1è¡¨ç¤ºç”·æ€§ åœ¨è®­ç»ƒæ•°æ®æ—¶äº¤æ›¿ä½¿ç”¨æ€§åˆ« åº”ç”¨MinMaxScaleræ—¶çš„æœ€å¤§æœ€å°å€¼æ˜¯åŒæ—¶è€ƒè™‘ä¸¤ç§æ€§åˆ«æ‰€æœ‰è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å¾—åˆ°çš„ã€‚ æ¨¡å‹ç»“æ„åŠè¶…å‚æ•°è®¾ç½®ä¸å•ä¸ªæ€§åˆ«RNNç›¸åŒ å»ºç«‹æ¨¡å‹ åŸºäºä½¿å¾—æµ‹è¯•æŸå¤±æœ€å°çš„LSTM3å’ŒGRUæ¨¡å‹ï¼Œé¢„æµ‹1999å¹´ä¹‹åçš„æ­»äº¡ç‡ï¼Œå¹¶åœ¨ç‰¹å¾å˜é‡ä¸­åŠ å…¥æ€§åˆ«æŒ‡æ ‡ï¼Œä¸‹è¡¨åˆ—å‡ºäº†ç›¸åº”çš„æŸå¤± \\[ \\begin{array}{|l|c|cc|c|} \\hline &amp; {\\text { in-sample }} &amp; {\\text { out-of-sample }} &amp; {\\text { out-of-sample }}&amp; {\\text { run times }}\\\\ &amp; \\text {both genders} &amp; \\text { female } &amp; \\text { male } &amp; \\text {both genders} \\\\ \\hline \\hline \\text { LSTM3 }\\left(T=10,\\left(\\tau_{0}, \\tau_{1}, \\tau_{2}, \\tau_{3}\\right)=(5,20,15,10)\\right) &amp; 4.7643 &amp; 0.3402 &amp; 1.1346 &amp; 404 \\mathrm{s}\\\\ \\text { GRU3 }\\left(T=10,\\left(\\tau_{0}, \\tau_{1}, \\tau_{2}, \\tau_{3}\\right)=(5,20,15,10)\\right) &amp; 4.6311 &amp; 0.4646 &amp; 1.2571 &amp; 379 \\mathrm{s} \\\\ \\hline \\hline \\text { LC model with SVD } &amp; 6.2841 &amp; 0.6045 &amp;1.8152 &amp; - \\\\ \\hline \\end{array} \\] ä¸LCæ¨¡å‹ç›¸æ¯”ï¼Œå¾—åˆ°äº†ä¸€ä¸ªæœ‰å¾ˆå¤§æ”¹è¿›çš„æ¨¡å‹ï¼Œè‡³å°‘å¯¹æœªæ¥16å¹´çš„é¢„æµ‹æ˜¯è¿™æ ·çš„ï¼›æ­¤å¤–ï¼Œå¼•å…¥æ€§åˆ«åå˜é‡çš„LSTMæ¨¡å‹ä¹Ÿä¼˜äºå•ä¸ªæ€§åˆ«çš„æ¨¡å‹ã€‚ éšå«çš„æ¼‚ç§»é¡¹ Figure 6.6: å¼•å…¥æ€§åˆ«åå˜é‡å»ºæ¨¡çš„ktçš„ä¼°è®¡ä¸é¢„æµ‹å€¼ # load corresponding data path.data &lt;- &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot; # path and name of data file region &lt;- &quot;CHE&quot; # country to be loaded (code is for one selected country) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;) str(all_mort) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;) source(file=&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R&quot;) # choice of parameters T0 &lt;- 10 tau0 &lt;- 5 ObsYear &lt;- 1999 # training data pre-processing data1 &lt;- data.preprocessing.RNNs(all_mort, &quot;Female&quot;, T0, tau0, ObsYear) data2 &lt;- data.preprocessing.RNNs(all_mort, &quot;Male&quot;, T0, tau0, ObsYear) xx &lt;- dim(data1[[1]])[1] x.train &lt;- array(NA, dim=c(2*xx, dim(data1[[1]])[c(2,3)])) y.train &lt;- array(NA, dim=c(2*xx)) gender.indicator &lt;- rep(c(0,1), xx) for (l in 1:xx){ x.train[(l-1)*2+1,,] &lt;- data1[[1]][l,,] x.train[(l-1)*2+2,,] &lt;- data2[[1]][l,,] y.train[(l-1)*2+1] &lt;- -data1[[2]][l] y.train[(l-1)*2+2] &lt;- -data2[[2]][l] } # MinMaxScaler data pre-processing x.min &lt;- min(x.train) x.max &lt;- max(x.train) x.train &lt;- list(array(2*(x.train-x.min)/(x.min-x.max)-1, dim(x.train)), gender.indicator) y0 &lt;- mean(y.train) # validation data pre-processing all_mort2.Female &lt;- all_mort[which((all_mort$Year &gt; (ObsYear-10))&amp;(Gender==&quot;Female&quot;)),] all_mortV.Female &lt;- all_mort2.Female vali.Y.Female &lt;- all_mortV.Female[which(all_mortV.Female$Year &gt; ObsYear),] all_mort2.Male &lt;- all_mort[which((all_mort$Year &gt; (ObsYear-10))&amp;(Gender==&quot;Male&quot;)),] all_mortV.Male &lt;- all_mort2.Male vali.Y.Male &lt;- all_mortV.Male[which(all_mortV.Male$Year &gt; ObsYear),] # LSTM architectures # network architecture deep 3 network tau1 &lt;- 20 tau2 &lt;- 15 tau3 &lt;- 10 optimizer &lt;- &#39;adam&#39; # choose either LSTM or GRU network RNN.type &lt;- &quot;LSTM&quot; #RNN.type &lt;- &quot;GRU&quot; {if (RNN.type==&quot;LSTM&quot;){model &lt;- LSTM3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)}else{model &lt;- GRU3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)} name.model &lt;- paste(RNN.type,&quot;3_&quot;, tau0, &quot;_&quot;, tau1, &quot;_&quot;, tau2, &quot;_&quot;, tau3, sep=&quot;&quot;) #file.name &lt;- paste(&quot;./Model_Full_Param/best_model_&quot;, name.model, sep=&quot;&quot;) file.name &lt;- paste(&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_&quot;, name.model, sep=&quot;&quot;) summary(model)} # define callback CBs &lt;- callback_model_checkpoint(file.name, monitor = &quot;val_loss&quot;, verbose = 0, save_best_only = TRUE, save_weights_only = TRUE,save_freq = NULL) # gradient descent fitting: takes roughly 400 seconds on my laptop {t1 &lt;- proc.time() fit &lt;- model %&gt;% fit(x=x.train, y=y.train, validation_split=0.2, batch_size=100, epochs=500, verbose=1, callbacks=CBs) proc.time()-t1} # plot loss figures plot.losses(name.model, &quot;Both&quot;, fit[[2]]$val_loss, fit[[2]]$loss) # calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110) load_model_weights_hdf5(model, file.name) round(10^4*mean((exp(-as.vector(model %&gt;% predict(x.train)))-exp(-y.train))^2),4) # calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152) # Female pred.result &lt;- recursive.prediction.Gender(ObsYear, all_mort2.Female, &quot;Female&quot;, T0, tau0, x.min, x.max, model) vali &lt;- pred.result[[1]][which(all_mort2.Female$Year &gt; ObsYear),] round(10^4*mean((vali$mx-vali.Y.Female$mx)^2),4) # Male pred.result &lt;- recursive.prediction.Gender(ObsYear, all_mort2.Male, &quot;Male&quot;, T0, tau0, x.min, x.max, model) vali &lt;- pred.result[[1]][which(all_mort2.Male$Year &gt; ObsYear),] round(10^4*mean((vali$mx-vali.Y.Male$mx)^2),4) 6.5.7 ç¨³å¥æ€§ ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•çš„æ—©æœŸåœæ­¢è§£å†³æ–¹æ¡ˆçš„ä¸€ä¸ªé—®é¢˜æ˜¯ç”±æ­¤äº§ç”Ÿçš„æ ¡å‡†ä¾èµ–äºç®—æ³•ç§å­ç‚¹ï¼ˆèµ·å§‹å€¼ï¼‰çš„é€‰æ‹© ä¸‹å›¾å±•ç¤ºä½¿ç”¨ç›¸åŒRNNç»“æ„ã€ç›¸åŒè¶…å‚æ•°å’Œç›¸åŒæ ¡å‡†ç­–ç•¥ï¼Œé’ˆå¯¹100ä¸ªä¸åŒç§å­ç‚¹çš„é€‰æ‹©æ‰€ç”»çš„æŸå¤±çš„ç®±çº¿å›¾ Figure 6.7: 100ä¸ªä¸åŒç§å­ä¸‹çš„æŸå¤±ç®±çº¿å›¾ çº¢è‰²è¡¨ç¤ºè”åˆæ€§åˆ«çš„LSTMç»“æ„ä¸­çš„é¢„æµ‹ç»“æœï¼›è“è‰²è¡¨ç¤ºè”åˆæ€§åˆ«çš„GRUç»“æ„ä¸­çš„é¢„æµ‹ï¼›æ©™è‰²æ°´å¹³çº¿è¡¨ç¤ºçš„æ˜¯LCçš„é¢„æµ‹ ç»“è®ºï¼š å·¦ä¾§ç»™å‡ºäº†æ ·æœ¬å†…æŸå¤±ï¼Œä¸LCæ¨¡å‹ç›¸æ¯”ï¼Œä¸¤ç§RNNç»“æ„çš„æ ·æœ¬å†…æŸå¤±éƒ½æœ‰æ˜¾è‘—å‡å°‘ï¼Œå¹³å‡è€Œè¨€ï¼ŒLSTMçš„æŸå¤±æ¯”GRUçš„å°ï¼Œæ³¢åŠ¨æ€§ä¹Ÿæ›´å°ï¼› ä¸­é—´å’Œå³è¾¹åˆ†åˆ«è¡¨ç¤ºå¥³æ€§å’Œç”·æ€§çš„æ ·æœ¬å¤–æŸå¤±ï¼šä¸è®ºç”·æ€§è¿˜æ˜¯å¥³æ€§ï¼ŒLSTMåœ¨å‡ ä¹æ‰€æœ‰çš„100æ¬¡è¿­ä»£ä¸­éƒ½æ¯”LCæ¨¡å‹å¥½ï¼›GRUç»“æ„å°½åœ¨å¤§çº¦ä¸€åŠæ¬¡æ•°çš„è¿­ä»£ä¸­æ¯”LCæ¨¡å‹è¡¨ç°å¥½ã€‚ æ”¹è¿›æ–¹æ³•ï¼šå°†ä¸åŒç§å­ç‚¹ä¸‹å¾—åˆ°çš„é¢„æµ‹å€¼è¿›è¡Œå¹³å‡ï¼Œç»“æœå¦‚ä¸‹: \\[ \\begin{array}{|l|c|cc|c|} \\hline &amp; {\\text { in-sample }} &amp; {\\text { out-of-sample }} &amp; {\\text { out-of-sample }}&amp; {\\text { run times }}\\\\ &amp; \\text {both genders} &amp; \\text { female } &amp; \\text { male } &amp; \\text {both genders} \\\\ \\hline \\hline \\text { LSTM3 }\\left(T=10,\\left(\\tau_{0}, \\tau_{1}, \\tau_{2}, \\tau_{3}\\right)=(5,20,15,10)\\right) &amp; 4.7643 &amp; 0.3402 &amp; 1.1346 &amp; 404 \\mathrm{s}\\\\ \\text { GRU3 }\\left(T=10,\\left(\\tau_{0}, \\tau_{1}, \\tau_{2}, \\tau_{3}\\right)=(5,20,15,10)\\right) &amp; 4.6311 &amp; 0.4646 &amp; 1.2571 &amp; 379 \\mathrm{s} \\\\ \\hline\\hline \\text { LSTM3 averaged over 100 different seeds} &amp; - &amp; 0.2451 &amp; 1.2093 &amp; 100 \\cdot 404\\mathrm{s}\\\\ \\text { GRU3 averaged over 100 different seeds } &amp; - &amp; 0.2341&amp; 1.2746 &amp; 100 \\cdot 379 \\mathrm{s} \\\\ \\hline \\hline \\text { LC model with SVD } &amp; 6.2841 &amp; 0.6045 &amp;1.8152 &amp; - \\\\ \\hline \\end{array} \\] èƒ½å¤Ÿçœ‹åˆ°ï¼Œå¹³å‡ä¹‹åå¾—åˆ°äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œé¢„æµ‹ç»“æœä¹Ÿå¾ˆå¥½ï¼Œç®±çº¿å›¾ä¸­ç»¿è‰²æ°´å¹³çº¿è¡¨ç¤ºçš„å°±æ˜¯å¹³å‡ä¹‹åçš„é¢„æµ‹æŸå¤±ï¼Œæ˜¾ç¤ºä»…æœ‰æå°‘æ•°çš„åœ¨ç»¿è‰²æ°´å¹³çº¿ä¹‹ä¸‹çš„ç§å­ç‚¹çš„é€‰æ‹©åœ¨å•ç‹¬è¿›è¡Œæ ¡å‡†æ—¶æ•ˆæœä¼šæ›´å¥½ã€‚ 6.5.8 é¢„æµ‹ç»“æœå›¾ Figure 5.2: å¯¹æ•°æ­»äº¡ç‡çš„è§‚å¯Ÿä¸é¢„æµ‹å€¼ ç»“è®ºï¼š 20-40å²ä¹‹é—´LSTMæ–¹æ³•èƒ½å¤Ÿæ›´å¥½çš„æ•æ‰åˆ°æ­»äº¡ç‡çš„æ”¹å–„ï¼Œå·¦è¾¹è§‚å¯Ÿå€¼æ¸…æ¥šçš„è¡¨æ˜LSTMè¿™æ ·çš„æ”¹å–„æ˜¯åˆç†çš„ï¼› å¹´é¾„è¾ƒå°äººç¾¤çš„æ­»äº¡ç‡å®é™…æ”¹å–„æƒ…å†µæ¯”æŒ‰ç…§æœ¬æ–‡æ–¹æ³•é¢„æµ‹çš„å¤§ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®çš„é’å¹´æ­»äº¡ç‡æ”¹å–„æƒ…å†µæ— æ³•ä»£è¡¨2000å¹´ä¹‹åçš„é’å¹´æ­»äº¡ç‡æ”¹å–„æƒ…å†µã€‚ "],["nlp.html", "7 è‡ªç„¶è¯­è¨€å¤„ç†åˆæ­¥ 7.1 é¢„å¤„ç† 7.2 Bag of words 7.3 Bag of part-of-speech 7.4 Word embeddings 7.5 æœºå™¨å­¦ä¹ ç®—æ³• 7.6 ç¥ç»ç½‘ç»œ 7.7 Case study 7.8 ç»“è®º", " 7 è‡ªç„¶è¯­è¨€å¤„ç†åˆæ­¥ Figure 7.1: NLPå†å²ï¼ˆ2000å¹´ä»¥åï¼‰ åœ¨ä¿é™©ä¸šï¼Œå¤§é‡çš„ä¹¦é¢è¯æ®ï¼Œå¦‚ä¿å•åˆåŒæˆ–ç´¢èµ”é€šçŸ¥ï¼Œä»¥åŠå®¢æˆ·ä¸ä¼ä¸šå¯¹è¯åŠ©ç†äº’åŠ¨çš„è®°å½•ï¼Œä¸ºæ•°æ®ç§‘å­¦å®¶å’Œç²¾ç®—å¸ˆæä¾›äº†è¶Šæ¥è¶Šå¤šçš„å¯ä¾›åˆ†æçš„æ–‡æœ¬ä¿¡æ¯ã€‚ NLPåœ¨ä¿é™©è¡Œä¸šä¸­çš„åº”ç”¨ï¼š æ ¹æ®æ–‡å­—æè¿°çš„ç´¢èµ”ç±»å‹å’Œä¸¥é‡ç¨‹åº¦å¯¹ç´¢èµ”è¿›è¡Œåˆ†ç±» å¯¹ç”µå­é‚®ä»¶ã€ä¿å•ã€åˆåŒçš„åˆ†ç±» ä»æ–‡æœ¬æ•°æ®ä¸­è¯†åˆ«æ¬ºè¯ˆæ¡ˆä¾‹ç­‰ ä¼ ç»Ÿæ–¹æ³•çš„åŸºæœ¬è¿‡ç¨‹ æ–‡æœ¬é¢„å¤„ç† æ˜ å°„åˆ°å®æ•°é›†: bag-of-words, bag-of-POS, pre-trained word embedding æœ‰ç›‘ç£æœºå™¨å­¦ä¹ ç®—æ³•: AdaBoost, random forest, XGBoost å¾ªç¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬è¿‡ç¨‹ æ–‡æœ¬é¢„å¤„ç† å¾ªç¯ç¥ç»ç½‘ç»œ: RNN, GRU, LSTM åŒºåˆ« ä¼ ç»Ÿæ–¹æ³•éå¸¸ä¾èµ–ç¬¬äºŒæ­¥ç‰¹å¾å·¥ç¨‹çš„æ•ˆæœ,ç¥ç»ç½‘ç»œä¸éœ€è¦ç‰¹å¾å·¥ç¨‹,å®ƒé€šè¿‡ç›‘ç£å­¦ä¹ .è¿›è¡Œè‡ªåŠ¨ç‰¹å¾å·¥ç¨‹. ä¼ ç»Ÿæ–¹æ³•æ²¡æœ‰è€ƒè™‘æ–‡æœ¬çš„æ—¶é—´åºåˆ—ç‰¹å¾, å¾ªç¯ç¥ç»ç½‘ç»œè€ƒè™‘äº†æ–‡æœ¬çš„æ—¶é—´åºåˆ—ç‰¹å¾. 7.1 é¢„å¤„ç† è¾“å…¥åŸå§‹æ–‡æœ¬å’Œæ ¼å¼ å°†æ–‡æœ¬è½¬åŒ–ä¸ºå°å†™ åˆ†è¯ï¼ˆ tokenization ï¼‰ åˆ é™¤åœç”¨è¯ï¼ˆ stopwords ï¼‰ è¯æ€§æ ‡æ³¨ï¼ˆPOS tagging, è¿™æ­¥åœ¨ bag-of-POSéœ€è¦, å¦‚æœåªæ˜¯ bag-of-wordsï¼Œæ­¤æ­¥ä¸éœ€è¦) è¯å¹²æå–æˆ–è¯å½¢è¿˜åŸï¼ˆStemming or Lemmatizationï¼‰ 7.2 Bag of words Bag-of-wordsæ¨¡å‹æ˜¯ä¿¡æ¯æ£€ç´¢é¢†åŸŸå¸¸ç”¨çš„æ–‡æ¡£è¡¨ç¤ºæ–¹æ³•ã€‚åœ¨ä¿¡æ¯æ£€ç´¢ä¸­ï¼ŒBOWæ¨¡å‹å‡å®šå¯¹äºä¸€ä¸ªæ–‡æ¡£ï¼Œå¿½ç•¥å®ƒçš„å•è¯é¡ºåºå’Œè¯­æ³•ã€å¥æ³•ç­‰è¦ç´ ï¼Œå°†å…¶ä»…ä»…çœ‹ä½œæ˜¯è‹¥å¹²ä¸ªè¯æ±‡çš„é›†åˆï¼Œæ–‡æ¡£ä¸­æ¯ä¸ªå•è¯çš„å‡ºç°éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¾èµ–äºå…¶å®ƒå•è¯æ˜¯å¦å‡ºç°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ–‡æ¡£ä¸­ä»»æ„ä¸€ä¸ªä½ç½®å‡ºç°çš„ä»»ä½•å•è¯ï¼Œéƒ½ä¸å—è¯¥æ–‡æ¡£è¯­æ„å½±å“è€Œç‹¬ç«‹é€‰æ‹©çš„ã€‚ä¾‹å¦‚æœ‰å¦‚ä¸‹ä¸¤ä¸ªæ–‡æ¡£ï¼š Bob likes to play basketball, Jim likes too. Bob also likes to play football games. åŸºäºè¿™ä¸¤ä¸ªæ–‡æœ¬æ–‡æ¡£ï¼Œæ„é€ ä¸€ä¸ªè¯å…¸ï¼š Dictionary = {1:â€œBobâ€, 2. â€œlikeâ€, 3. â€œtoâ€, 4. â€œplayâ€, 5. â€œbasketballâ€, 6. â€œalsoâ€, 7. â€œfootballâ€, 8. â€œgamesâ€, 9. â€œJimâ€, 10. â€œtooâ€} è¿™ä¸ªè¯å…¸ä¸€å…±åŒ…å«\\(10\\)ä¸ªä¸åŒçš„å•è¯ï¼Œåˆ©ç”¨è¯å…¸çš„ç´¢å¼•å·ï¼Œä¸Šé¢ä¸¤ä¸ªæ–‡æ¡£æ¯ä¸€ä¸ªéƒ½å¯ä»¥ç”¨ä¸€ä¸ª\\(10\\)ç»´å‘é‡è¡¨ç¤ºï¼ˆç”¨æ•´æ•°æ•°å­—\\(0:n\\)ï¼ˆ\\(n\\)ä¸ºæ­£æ•´æ•°ï¼‰è¡¨ç¤ºæŸä¸ªå•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼‰ï¼š 1ï¼š\\([1, 2, 1, 1, 1, 0, 0, 0, 1, 1]\\) 2ï¼š\\([1, 1, 1, 1 ,0, 1, 1, 1, 0, 0]\\) å‘é‡ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºè¯å…¸ä¸­ç›¸å…³å…ƒç´ åœ¨æœ¬æ–‡æœ¬æ ·æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ä¸è¿‡ï¼Œåœ¨æ„é€ æ–‡æ¡£å‘é‡çš„è¿‡ç¨‹ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¡¨è¾¾å•è¯åœ¨åŸæ¥å¥å­ä¸­å‡ºç°çš„æ¬¡åºï¼ˆè¿™æ˜¯æœ¬Bag-of-wordsæ¨¡å‹çš„ç¼ºç‚¹ä¹‹ä¸€ï¼Œä¸è¿‡ç‘•ä¸æ©ç‘œç”šè‡³åœ¨æ­¤å¤„æ— å…³ç´§è¦ï¼‰ã€‚ æˆ‘ä»¬ä½¿ç”¨TfidfVectorizerè¿›è¡ŒBOW,å®ƒä¸ä½¿ç”¨è¯å‡ºç°çš„æ¬¡æ•°,è€Œè®¡ç®—term frequency - inverse document frequency (tf-idf)ã€‚ç ”ç©¶è¡¨æ˜tf-idfæ›´èƒ½åæ˜ æ–‡æœ¬çš„ç‰¹å¾ã€‚ 7.3 Bag of part-of-speech ç›¸è¾ƒäºBOWï¼Œbag of POSä»…ä»…å¤šäº†ä¸€æ­¥ï¼Œå³å¯¹æ¯ä¸ªè¯è¯­è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼Œç„¶åä½¿ç”¨TfidfVectorizerã€‚å¯çŸ¥bag of POSå¯ä»¥è¾¾åˆ°é™ç»´çš„ç›®çš„ï¼Œä½†å®ƒæ•£å¤±äº†åŸæ–‡æœ¬çš„è¯è¯­çš„ä¿¡æ¯ï¼Œåªè€ƒè™‘äº†è¯æ€§ã€‚ 7.4 Word embeddings è¯åµŒå…¥è€ƒè™‘æŠŠæ¯ä¸ªè¯è¯­æ˜ å°„åˆ°ç”¨å¤šç»´å®æ•°ç©ºé—´ä¸­ï¼Œæœ‰å¾ˆå¤šé¢„è®­ç»ƒçš„æ˜ å°„å¯ä¾›é€‰æ‹©ï¼Œè¿™äº›æ˜ å°„é€šå¸¸è€ƒè™‘äº†è¯è¯­çš„å…ˆåé¡ºåºå’ŒåŒä¹‰è¯åä¹‰è¯ç­‰ã€‚å¸¸ç”¨çš„è¯åµŒå…¥æ¨¡å‹åŒ…æ‹¬ï¼š Neural probabilistic language model word2vec Global vectors for word representation 7.4.1 Neural probabilistic language model ä¼ ç»Ÿçš„N-grams æ¨¡å‹è®¤ä¸ºï¼Œåœ¨ç¬¬1ä¸ªè¯åˆ°ç¬¬t-1ä¸ªè¯å‡ºç°çš„æƒ…å†µä¸‹ç¬¬tä¸ªè¯å‡ºç°çš„æ¡ä»¶æ¦‚ç‡ï¼Œçº¦ç­‰äºåœ¨ç¬¬t-n+1ä¸ªè¯åˆ°ç¬¬t-1ä¸ªè¯å‡ºç°çš„æƒ…å†µä¸‹ç¬¬tä¸ªè¯å‡ºç°çš„æ¡ä»¶æ¦‚ç‡ï¼ˆN=n-1ï¼‰ã€‚ä½†ä¼ ç»Ÿæ¨¡å‹æ²¡æœ‰å¯¹â€œè¯æ±‡ç›¸ä¼¼æ€§â€çš„æ¦‚å¿µè¿›è¡Œç¼–ç ï¼Œä¹Ÿå­˜åœ¨æµ‹è¯•æ—¶è¾“å…¥çš„è¯åºåˆ—å¾ˆå¯èƒ½ä¸åŒäºæ‰€æœ‰ç”¨æ¥è®­ç»ƒæ¨¡å‹çš„å•è¯åºåˆ—çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥å‰é¦ˆå¼ç¥ç»ç½‘ç»œã€‚ è¯¥å‰é¦ˆå¼ç¥ç»ç½‘ç»œç”¨è¾“å…¥çš„ç¬¬tä¸ªè¯å‰çš„n-1ä¸ªè¯é¢„æµ‹ç¬¬tä¸ªè¯å‡ºç°çš„æ¦‚ç‡ï¼Œçš„ç»“æ„å¦‚ä¸‹ï¼š Figure 7.2: Neural probalilistic language model ä½¿ç”¨ç¥ç»ç½‘ç»œçš„ç›®çš„æ˜¯å¯»æ‰¾è¯è¡¨Våˆ°dç»´å‘é‡ç©ºé—´Rçš„æ˜ å°„ï¼Œå³å›¾ä¸­çš„çŸ©é˜µCã€‚ 7.4.2 word2vec word2vecä½¿ç”¨äº†ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰æ¦‚å¿µã€‚ è¯ w çš„ä¸Šä¸‹æ–‡ C(w) æŒ‡ wå‰é¢å’Œåé¢çš„æœ‰é™ä¸ªè¯ã€‚C(w)çš„é•¿åº¦è¢«å®šä¹‰ä¸ºéè´Ÿæ•´æ•°cï¼Œ2c=|C(w)|ã€‚ä¾‹å¦‚ï¼š â€˜Koalas and platypuses are mammals living in Australiaâ€™,c=2,w=â€˜areâ€™ C(w)={â€˜andâ€™,â€˜platypusesâ€™,â€˜mammalsâ€™,â€˜livingâ€™} Word2vecæœ‰ä¸¤ä¸ªå˜ä½“ï¼Œskip-gram modelå’Œthe continuous bag of words (CBOW) modelã€‚ å‰è€…çš„æ€è·¯æ˜¯ç»™å®šä¸­å¿ƒå•è¯wï¼Œé¢„æµ‹å…¶ä¸Šä¸‹æ–‡ C(w)ã€‚åè€…çš„æ€è·¯æ˜¯ç»™å®šä¸Šä¸‹æ–‡C(w)ï¼Œé¢„æµ‹ä¸­å¿ƒè¯wã€‚ ä¸¤è€…çš„ç»“æ„å›¾å¦‚ä¸‹ï¼š Figure 7.3: Skip-gram model Figure 7.4: Skip-gram model Figure 7.5: Continuous bag of words ä½¿ç”¨word2vecå¯»æ‰¾è¯è¡¨Våˆ°dç»´å‘é‡ç©ºé—´Rçš„æ˜ å°„ï¼Œå³å¯»æ‰¾éšè—å±‚çš„æƒé‡çŸ©é˜µï¼ˆskip-gram modelç¬¬äºŒå¼ ç»“æ„å›¾ä¸­çš„æ©™è‰²çŸ©é˜µï¼‰ã€‚ä½¿ç”¨one-hotç¼–ç çš„è¯å‘é‡ï¼Œä¹˜ä»¥éšè—å±‚çš„æƒé‡çŸ©é˜µåï¼Œå³å¾—ç›®æ ‡word vectorã€‚ 7.4.3 Global vectors for word representation(Glove) word2vecä»…åœ¨ä¸Šä¸‹æ–‡è€ƒè™‘è¯ä¸è¯å…±ç°çš„æ¦‚ç‡ï¼ŒGloveé€šè¿‡æ•´ä¸ªè¯­æ–™åº“è®¡ç®—è¯-è¯å…±ç°æ¬¡æ•°çŸ©é˜µã€‚ ä¾‹å¦‚ï¼ŒC={â€˜Cats and koalas are mammals.â€™,â€˜Tortoises are not mammals.â€™,â€˜I like cats,koalas and tortoisesâ€™.} å…¶è¯-è¯å…±ç°æ¬¡æ•°çŸ©é˜µä¸ºï¼š Figure 7.6: è¯-è¯å…±ç°æ¬¡æ•°çŸ©é˜µ ä¸è¯-è¯å…±ç°æ¬¡æ•°æˆ–æ¦‚ç‡ç›¸æ¯”ï¼Œè¯-è¯å…±ç°æ¦‚ç‡æ¯”æ›´èƒ½åŒºåˆ†ç›¸å…³è¯å’Œä¸ç›¸å…³è¯ã€‚ è®°Pijä¸ºè¯jå‡ºç°åœ¨è¯iä¸Šä¸‹æ–‡çš„æ¦‚ç‡ã€‚Gloveçš„æ€è·¯æ˜¯æ‰¾åˆ°ä¸€ä¸ªå®šä¹‰åœ¨ä¸­å¿ƒè¯içš„åµŒå…¥å‘é‡ã€ä¸­å¿ƒè¯jçš„åµŒå…¥å‘é‡ï¼Œä»¥åŠä¸Šä¸‹æ–‡è¯kçš„åµŒå…¥å‘é‡ä¸Šçš„ä¸€ä¸ªå‡½æ•°Fï¼Œä½¿ä¹‹å°½å¯èƒ½ç­‰äºPik/Pjkã€‚ è¿™è½¬åŒ–ä¸ºä¸€ä¸ªåŠ æƒæœ€å°äºŒä¹˜é—®é¢˜ï¼š å…¶ä¸­ï¼Œg()æ˜¯æƒé‡å‡½æ•°ï¼Œbæ˜¯æ ‡é‡åå·®ï¼ŒXikæ˜¯è¯iä¸è¯kçš„å…¨å±€å…±ç°æ•°ã€‚ Gloveçš„ç›®æ ‡æ˜¯æœ€å°åŒ–Jï¼Œä»¥æœŸæ±‚å¾—è¯çš„åµŒå…¥å‘é‡ã€‚ 7.4.4 Pre-trained word embeddings æˆ‘ä»¬ä½¿ç”¨en_core_web_smè¿›è¡Œè¯åµŒå…¥ï¼Œå®ƒå¯ä»¥æŠŠä»»æ„çš„è¯æ˜ å°„åˆ°\\(\\mathbb{R}^{96}\\)ã€‚å¦å¤–ï¼Œen_core_web_mbä¸ºæ›´å¤æ‚çš„æ˜ å°„ï¼Œå¯ä»¥æŠŠä»»æ„çš„è¯æ˜ å°„åˆ°\\(\\mathbb{R}^{300}\\)ã€‚å¯¹äºä¸€ä¸ªåŒ…å«å¤šä¸ªè¯è¯­çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬ç”¨æ‰€æœ‰è¯è¯­çš„è¯åµŒå…¥å¹³å‡å€¼æ¥è¡¨ç¤ºè¿™ä¸ªæ–‡æœ¬ 7.5 æœºå™¨å­¦ä¹ ç®—æ³• é€šè¿‡bag-of-words, bag-of-POSï¼Œword embedding æˆ‘ä»¬æŠŠæ¯ä¸ªæ–‡æœ¬è½¬å˜ä¸ºä¸€ä¸ªå‘é‡ã€‚è¿™æ ·å¯ä»¥åˆ©ç”¨å¦‚ä¸‹å‡ ç§æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œæ–‡æœ¬åˆ†ç±»ç­‰ç›‘ç£å­¦ä¹ ã€‚ï¼ˆè¯¦è§ç¬¬å››ç« æå‡æ–¹æ³•ï¼ˆBoostingï¼‰ï¼‰ AdaBoost Random Forest XGBoost 7.6 ç¥ç»ç½‘ç»œ RNN Figure 7.7: RNN LSTM Figure 7.8: LSTM GRU Figure 7.9: GRU 7.6.1 æ•°æ®é¢„å¤„ç† åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œä»¥ä¸Šbag-of-words, bag-of-POS, word embeddingç­‰äººå·¥ç‰¹å¾å·¥ç¨‹ï¼Œæˆ‘ä»¬åªéœ€è¦ç”¨å®æ•°æŠŠæ–‡æœ¬ä¸­è¯è¯­çš„é¡ºåºè¡¨å¾å‡ºæ¥å³å¯ï¼Œç¥ç»ç½‘ç»œå¯ä»¥åŒæ­¥è¿›è¡Œç‰¹å¾å·¥ç¨‹å’Œæœ‰ç›‘ç£è®­ç»ƒã€‚ åœ¨pythonä¸­å¯ä»¥ä½¿ç”¨Keras ä¸­çš„Tokenizeræ¨¡å—æŠŠè¯è¯­æ˜ å°„åˆ°éè´Ÿæ•´æ•°ä¸Šï¼Œæ­¤æ–¹æ³•ä¿æŒäº†ä¿æŒäº†è¯è¯­çš„é¡ºåºï¼Œæ˜¯å‰é¢å‡ ç§æ–¹æ³•æ²¡æœ‰è¾¾åˆ°çš„ã€‚ å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬ä»…ä»…éœ€è¦è¿›è¡Œå¾ˆå°‘çš„ç‰¹å¾å·¥ç¨‹ï¼Œè¯è¯­çš„æ„ä¹‰å°†ç”±ç¥ç»ç½‘ç»œåœ¨ç›‘ç£å­¦ä¹ ä¸­å­¦åˆ°ã€‚æ–‡æœ¬æ˜¯æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¸¸ç”¨äºæ—¶é—´åºåˆ—åˆ†æçš„æ¨¡å‹åŒ…æ‹¬ LSTM Shallow LSTM architecture Figure 7.10: Shallow LSTM Deep LSTM architecture Figure 7.11: Deep LSTM GRU Shallow GRU architecture Figure 7.12: Shallow GRU 7.7 Case study Figure 7.13: æ–‡æ¡£ç»“æ„ Figure 7.14: nlp4class_exerciseæ­¥éª¤ ä»»åŠ¡æè¿°ï¼šæ ¹æ®ç”µå½±è¯„è®ºæ–‡æœ¬åˆ¤æ–­è¯¥è¯„è®ºæ˜¯â€œå¥½è¯„â€è¿˜æ˜¯â€œå·®è¯„â€ æ•°æ®æ¥æºï¼š Internet Movie Database (IMDb) æ•°æ®é‡ï¼šæ¡ˆä¾‹ä¸­å…±æœ‰5000æ¡å«åˆ†ç±»ä¿¡æ¯ï¼ˆpos/negï¼‰çš„åŸå§‹æ•°æ®ï¼Œä»è®¡ç®—èµ„æºæ˜¯è¿è¡Œæ—¶é—´è€ƒè™‘ï¼Œæˆ‘ä»¬ä»åŸå§‹æ•°æ®ä¸­éšæœºæŠ½å–äº†1%ï¼Œå³500æ¡æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œæµ‹è¯•æ•°æ®åœ¨toymdbæ–‡ä»¶å¤¹ä¸‹ï¼Œæ–‡ä»¶ç»“æ„å’ŒåŸå§‹æ•°æ®ä¸€è‡´ï¼ˆéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ toymdbæ–‡ä»¶å¤¹åŒ…å«trainå’Œtestä¸¤ä¸ªæ–‡ä»¶ï¼Œä½†å¹¶ä¸æ˜¯å®é™…å¤„ç†æ—¶çš„â€œè®­ç»ƒé›†â€å’Œâ€œæµ‹è¯•é›†â€ï¼Œå®é™…è®­ç»ƒæ—¶è¯»å–æ‰€æœ‰æ•°æ®å¹¶é‡æ–°åˆ’åˆ†â€œè®­ç»ƒé›†â€å’Œâ€œæµ‹è¯•é›†â€ ï¼Œæ‰€ä»¥å®é™…ä¸Štrainå’Œtestä¸­çš„æ–‡ä»¶æ²¡æœ‰åŒºåˆ«ï¼Œåªæ˜¯æ²¿è¢­äº†åŸå§‹æ•°æ®çš„å­˜å‚¨ç»“æ„ï¼‰ã€‚ æ•°æ®ç»“æ„ï¼šè¯„è®ºä½ç½®ä»£è¡¨ç±»åˆ«ï¼Œä¸€ä¸ªtxtå­˜å‚¨ä¸€ä¸ªè¯„è®ºæ•°æ®ã€‚ 7.7.1 å‡½æ•°è¯´æ˜ åˆ†è¯ import re def preprocessor(text): text = re.sub(&#39;&lt;[^&gt;]*&gt;&#39;, &#39;&#39;, text) emoticons = re.findall(&#39;(?::|;|=)(?:-)?(?:\\)|\\(|D|P)&#39;, text) text = (re.sub(&#39;[\\W]+&#39;, &#39; &#39;, text.lower()) + &#39; &#39;.join(emoticons).replace(&#39;-&#39;, &#39;&#39;)) return text å¼•å…¥Pythonä¸­çš„reæ¨¡å—ï¼Œå®šä¹‰preprocessorå‡½æ•°å¯¹æ–‡æœ¬è¿›è¡Œç¬¬ä¸€æ­¥çš„æ¸…ç†ï¼Œä½œç”¨å»æ‰æ–‡æœ¬ä¸­çš„éå­—æ¯æ•°å­—ç¬¦å·ï¼Œå¹¶å°†æ‰€æœ‰å­—æ¯éƒ½è½¬åŒ–æˆå°å†™ã€‚ from nltk.tokenize import word_tokenize tokens = word_tokenize(text) ä»nltk.tokenizeå¯¼å…¥word_tokenizeå‡½æ•°å¯¹æ¸…ç†åçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè¿”å›å€¼ç”±å•è¯ç»„æˆçš„åˆ—è¡¨ã€‚ åˆ é™¤åœæ­¢è¯ import nltk from nltk.corpus import stopwords stopwords = list(set(stopwords.words(&#39;english&#39;))) filtered_tokens = [word for word in tokens if word not in stopwords] å¯¼å…¥nltk.corpusä¸­çš„åœç”¨è¯è¡¨ï¼Œåˆ é™¤åˆ—è¡¨ä¸­å‡ºç°åœ¨åœç”¨è¯è¡¨ä¸­çš„æ‰€æœ‰å•è¯ï¼Œä¸æ”¹å˜è¯åºã€‚ è¯æ€§æ ‡æ³¨ from nltk import pos_tag nltk.download(&#39;averaged_perceptron_tagger&#39;) def pos_tags(text_processed): return &quot;-&quot;.join( tag for (word, tag) in nltk.pos_tag(text_processed)) pos_tag = pos_tags(filtered_tokens) å¯¼å…¥nltkä¸­çš„pos_tagå‡½æ•°ï¼Œè¯¥å‡½æ•°çš„ä½œç”¨æ˜¯æ ¹æ®è®­ç»ƒå¥½çš„æ¨¡å‹ç»™å‡ºå•è¯å¯¹åº”çš„è¯æ€§ï¼Œè¿”å›å€¼æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œå…ƒç»„ä¸­çš„ä¸¤ä¸ªå…ƒç´ åˆ†åˆ«æ˜¯åŸå•è¯å’Œå¯¹åº”çš„è¯æ€§æ ‡æ³¨ã€‚ é‡æ–°å®šä¹‰è¯æ€§æ ‡æ³¨å‡½æ•°pos_tagsï¼Œæœ€ç»ˆè¿”å›ä¸€ä¸ªç”±â€œ-â€è¿æ¥çš„å­—ç¬¦ä¸²ï¼Œå®ƒæŒ‰é¡ºåºè¿æ¥äº†ç»è¿‡é¢„å¤„ç†çš„æ–‡æœ¬ä¸­æ¯ä¸ªå•è¯çš„è¯æ€§æ•°æ®ã€‚ TfidfVectorizer() from sklearn.feature_extraction.text import TfidfVectorizer import pandas as pd tfidf = c(strip_accents=None, lowercase=False, preprocessor=None) tfidf_wm = tfidf.fit_transform([pos_tag]) TfidfVectorizerå°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡ï¼› å‘é‡çš„å±æ€§å¯¹åº”è¯å…¸ä¸­çš„å•è¯(nä¸ª)ï¼Œå±æ€§å€¼æ˜¯æ¯ä¸ªå•è¯çš„tfidfå€¼ï¼› æ¯ä¸ªå‘é‡ä»£è¡¨ä¸€ä¸ªæ–‡æœ¬ï¼› nä¸ªæ–‡æœ¬æ„æˆn*pçŸ©é˜µï¼› TfidfVectorizer.fit_transformçš„è¾“å…¥å€¼æ˜¯ç»è¿‡é¢„å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨æ—¶ï¼Œå¾—åˆ°bag of wordsæ¨¡å‹çš„è¾“å…¥çŸ©é˜µï¼›å½“è¾“å…¥å€¼æ˜¯ç›¸åº”çš„è¯æ€§åºåˆ—åˆ—è¡¨æ—¶ï¼Œå¾—åˆ°bag of POSæ¨¡å‹çš„è¾“å…¥çŸ©é˜µã€‚ word embeddings import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) import numpy as np emb = nlp(text_filtered).vector æ ¹æ®â€™en_core_web_smâ€™æ¨¡å‹å°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡ï¼Œå‘é‡çš„ç»´æ•°æ˜¯ç”±æ¨¡å‹æœ¬èº«å†³å®šçš„ã€‚å¯ä»¥æ ¹æ®å®é™…ç ”ç©¶éœ€è¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚ç”±è¿™æ ·çš„æ–‡æœ¬å‘é‡æ„æˆçš„çŸ©é˜µå°±æ˜¯word embeddingsæ¨¡å‹çš„æœ€ç»ˆè¾“å…¥ã€‚ RNNçš„é¢„å¤„ç† from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences vocab_size = 400 #å–å‰400ä¸ªæœ€é«˜é¢‘è¯æ±‡ä½œä¸ºæ–‡æœ¬çš„è¯åºæ ‡è®°ï¼Œå³åªå¯¹æ–‡æœ¬ä¸­å‡ºç°åœ¨å‰400ä¸ªæœ€é«˜é¢‘è¯æ±‡ä¸­çš„å•è¯è¿›è¡Œè¯åºæ˜ å°„ #å¦‚æŸè¯„è®ºä¸º&#39;w1 w2 w3&#39;ï¼Œå…¶ä¸­å•è¯w1ã€w2å’Œw3å¯¹åº”çš„é¢‘æ•°æ’ååˆ†åˆ«ä¸º30ï¼Œ405ï¼Œ108ï¼Œ #åˆ™ä¸è¯¥è¯„è®ºå¯¹åº”çš„è¯åºå‘é‡ä¸º[30, 108] tokenizer = Tokenizer(num_words=vocab_size, filters=&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n&#39;, # filters out all punctuation other than &#39; lower=True, # convert to lowercase split=&#39; &#39;) # split on whitespaces tokenizer.fit_on_texts(df[&#39;review&#39;]) list_tokenized = tokenizer.texts_to_sequences(df[&#39;review&#39;]) sequence_length = 200 #ç»Ÿä¸€æ–‡æœ¬çš„è¯åºå‘é‡é•¿åº¦ä¸º200 #è‹¥å‰é¢ç”Ÿæˆçš„è¯åºå‘é‡é•¿åº¦å¤§äº200ï¼ˆå³æ–‡æœ¬ä¸­æœ‰è¶…è¿‡200ä¸ªå•è¯å‡ºç°åœ¨æˆ‘ä»¬ç”¨äºæ˜ å°„çš„é«˜é¢‘è¯æ±‡è¡¨ä¸­ï¼Œæ­¤ä¾‹ä¸­ä¸ºå‰400ï¼‰ï¼Œåˆ™å–å200ä¸ªå…ƒç´  #è‹¥å‰é¢ç”Ÿæˆçš„è¯åºå‘é‡é•¿åº¦å°äº200ï¼Œåˆ™å‰é¢ç”¨0è¡¥è¶³ sequences = pad_sequences(list_tokenized, maxlen=sequence_length) åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œæ–‡æœ¬å‘é‡åŒ–ä¸éœ€è¦ç”¨åˆ°nlpä¸­äº‹å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ–‡æœ¬ä¸­çš„è¯è¯­æ›¿æ¢ä¸ºè¯¥è¯åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ï¼ˆå³é¢‘æ•°æ’åæ¬¡åºï¼‰ã€‚ 7.7.2 å¯èƒ½é‡åˆ°çš„é—®é¢˜ è¯æ€§æ ‡æ³¨ import nltk from nltk import pos_tag, word_tokenize å‡ºç°LookupError è§£å†³æ–¹æ³•ï¼šæŠŠ&#39;nltk_data.zip&#39;é‡Œçš„æ–‡ä»¶å…¨éƒ¨æ‹·è´è‡³&#39;/Users/huihuajiang/nltk_data/&#39; ç”¨ä»¥ä¸‹å‘½ä»¤å¯ä»¥æŸ¥çœ‹ä½ çš„ nltk_data æ–‡ä»¶å¤¹è·¯å¾„ï¼š import nltk print(nltk.data.path) è¯åµŒå…¥ import spacy nlp = spacy.load(â€˜en_core_web_smâ€™) é”™è¯¯1ï¼šOSError: [E050] Can&#39;t find model &#39;en_core_web_sm&#39;. é”™è¯¯2ï¼šnumpy.core.multiarray failed to import é”™è¯¯1è§£å†³æ–¹æ³•1ï¼ˆå¯èƒ½ä¸è¡Œï¼‰ï¼š å‘½ä»¤è¡Œè¿è¡Œå‘½ä»¤â€python -m spacy download en_core_web_smâ€ é”™è¯¯1è§£å†³æ–¹æ³•2ï¼šæŠŠæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°è¿›è¡Œå®‰è£… å…·ä½“æ“ä½œè¯·å‚è€ƒ https://www.freesion.com/article/73801416523/ é”™è¯¯2è§£å†³æ–¹æ³•ï¼šé‡å¯ä¸€ä¸‹ç»ˆç«¯ 7.7.3 ç»“æœæ¯”è¾ƒ 7.8 ç»“è®º æˆ‘ä»¬ç”¨bag of words, bag of POS, word embeddingsä¸‰ç§NLPæ¨¡å‹å¯¹è¯„è®ºæ–‡æœ¬è¿›è¡Œäº†å‘é‡åŒ–ï¼Œå¹¶ç”¨ADA, RF, XGBä¸‰ç§æœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ–‡æ¡£è¿›è¡Œäº†åˆ†ç±»ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†NLPç®¡é“æ¥é¢„å¤„ç†æ–‡æœ¬æ•°æ® ã€‚æœ€åï¼Œè¿˜é‡‡ç”¨RNNæ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œäº†åˆ†ç±»ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œ bag of wordsæ€»ä½“ä¸Šå˜ç°æ›´å¥½ï¼Œ bag of POSçš„è¡¨ç°ä¸ä½³ã€‚ ä¸bag of wordsç›¸æ¯”ï¼Œ word embeddingæ¨¡å‹çš„æ•ˆç‡æ›´é«˜ã€‚ Deep LSTMæ¨¡å‹è¡¨ç°è¦æ¯”Single LSTMå’Œæ›´å¥½ã€‚ ä¸NLPæ¨¡å‹ç›¸æ¯”ï¼ŒRNNæ¨¡å‹æ€§èƒ½æ›´å¥½ï¼Œè¾¾åˆ°ç›¸åŒæ ‡å‡†çš„æ—¶é—´æ›´çŸ­ã€‚ å¦‚æœç”¨RNNçš„è¾“å…¥æ•°æ®æ¥æ‹ŸåˆMlæ¨¡å‹ï¼Œç²¾åº¦è¿œä¸åŠRNNæ¨¡å‹ï¼Œå¯è§RNNæ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šèƒ½åˆ©ç”¨æ›´å°‘çš„ä¿¡æ¯å®ç°æ›´å‡†ç¡®çš„åˆ†ç±»ã€‚ "],["flashlight.html", "8 é€šç”¨æ¨¡å‹è§£é‡Šæ–¹æ³• 8.1 æ•°æ® 8.2 æ¨¡å‹ 8.3 æ¨¡å‹æ•´ä½“è¡¨ç° ï¼ˆmodel performanceï¼‰ 8.4 å˜é‡é‡è¦æ€§ï¼ˆvariable importanceï¼‰ 8.5 è¾¹ç¼˜æ•ˆåº”ï¼ˆä¸»æ•ˆåº”ï¼‰ 8.6 äº¤äº’æ•ˆåº” 8.7 å…¨å±€ä»£ç†æ¨¡å‹ï¼ˆGlobal surrogate modelsï¼‰ 8.8 å±€éƒ¨è§£é‡Š 8.9 Improving the GLM by interpretable machine learning 8.10 æ¡ˆä¾‹åˆ†æ", " 8 é€šç”¨æ¨¡å‹è§£é‡Šæ–¹æ³• the blackness of a model box seems no longer to be caused by the obscurity of the model itself but rather depends on the modeler switching on the light of the box. ï¼ˆæ‰‹ç”µç­’çš„ä¸–ç•Œæ²¡æœ‰é»‘ç›’å­ï¼‰ ç›®çš„ï¼šä»ç²¾ç®—çš„è§’åº¦åŸºäºæœ‰ç›‘ç£çš„æœºå™¨å­¦ä¹ è®²è¿°äº†è§£é‡Šæ€§æœºå™¨å­¦ä¹ å’Œè§£é‡Šæ€§äººå·¥æ™ºèƒ½ä¸€äº›æ–¹æ³•çš„åº”ç”¨ã€‚ ä½¿æˆ‘ä»¬æ›´ç›¸ä¿¡æ‰€ä½¿ç”¨çš„æ¨¡å‹ å‘ç°æ¨¡å‹çš„å±€é™æ€§ æ”¹è¿›ç°æœ‰çš„æ¨¡å‹ ä½¿æ¨¡å‹ä¸ä»…å¯ä»¥ç”¨æ¥é¢„æµ‹ï¼Œè¿˜å¯ä»¥æä¾›ä¿¡æ¯å’ŒéªŒè¯æ¨¡å‹å‡è®¾ ä¸»è¦å†…å®¹ å˜é‡é‡è¦æ€§ è¾¹ç¼˜æ•ˆåº”ï¼ˆä¸»æ•ˆåº”ï¼‰ã€äº¤äº’æ•ˆåº” æ¯ä¸ªç‰¹å¾å¯¹å•ä¸€é¢„æµ‹å€¼çš„è´¡çŒ® 8.1 æ•°æ® æ•°æ®é›†åˆ’åˆ†ï¼š80%è®­ç»ƒé›†ï¼Œ20%æµ‹è¯•é›†ã€‚æœ‰ç€ç›¸åŒgroup_idçš„æ•°æ®ä¼šè¢«åŒæ—¶åˆ’åˆ†åˆ°è®­ç»ƒé›†æˆ–è€…æµ‹è¯•é›†ä¸­ä»¥å‡å°‘åå·®å’Œé€‰æ‹©åˆé€‚çš„å‚æ•°ã€‚ 8.2 æ¨¡å‹ ä»¥ä¸‹è€ƒè™‘ä¸‰ç§æ¨¡å‹ï¼šGLMï¼ŒXGBoostï¼ŒNeural Networkã€‚è¿™ä¸‰ç§æ¨¡å‹æœ€å…·æœ‰ä»£è¡¨æ€§ï¼ŒGLMæ˜¯ä¿é™©æŸå¤±é¢„æµ‹ä¸­æœ€ç»å…¸çš„æ¨¡å‹ï¼ŒXGBoostæ˜¯åœ¨æ•°æ®åˆ†æç«èµ›ä¸­å¸¸ç”¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå®ƒä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç¥ç»ç½‘ç»œåŒ…å«äº†å¾ˆå¤šç§layerï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å‘æŒ¥äº†å·¨å¤§çš„ä½œç”¨ã€‚ 8.2.1 GLM GLMçš„ä¼˜ç‚¹: è§£é‡Šæ€§å¼ºï¼Œç»Ÿè®¡æ£€éªŒè¯†åˆ«å‚æ•°æ˜¾è‘—æ€§ã€‚ æ¯”è¾ƒçµæ´»ï¼Œå¯ä»¥åŠ å…¥äº¤äº’é¡¹ã€å˜å½¢åçš„è§£é‡Šå˜é‡ã€å¯ä»¥ç”¨éçº¿æ€§ç»“æ„ï¼ˆæ ·æ¡æ›²çº¿å¹³æ»‘ï¼‰ã€‚ è€ƒè™‘ä¸å¸¦äº¤å‰é¡¹çš„æ³Šæ¾GLMæ¨¡å‹ï¼šVehAgeå’ŒDrivAgeéƒ½ç”¨äº†è‡ªç„¶ä¸‰æ¬¡æ ·æ¡è¿›è¡Œå¹³æ»‘ã€‚ \\[N_i \\sim \\text{Poi} (e_i\\lambda(\\boldsymbol{x_i}))\\] æœ‰ä¸¤ç§ç­‰ä»·å»ºæ¨¡æ–¹å¼ï¼š æŠŠ\\(\\ln e_i\\)çš„ç³»æ•°å›ºå®šä¸º1ã€‚glm (N ~ x + offset (log (e)), family = poisson (link=log) ) æŠŠ\\(N_i/e_i\\)å½“ä½œå› å˜é‡ï¼Œæ¯ä¸ªæ ·æœ¬çš„æƒé‡ä¸º\\(e_i\\)ã€‚glm ( I(N/e) ~ x, weights = e, family = poisson (link=log) ) 8.2.2 XGBoost ç”¨è®­ç»ƒé›†ä¸Šäº”æŠ˜äº¤å‰éªŒè¯æ¥é€‰æ‹©è¶…å‚æ•°ã€‚ 8.2.3 ç¥ç»ç½‘ç»œ é€‰å–çš„ç¥ç»ç½‘ç»œæœ‰ä»¥ä¸‹ç»“æ„ï¼š å‰é¦ˆå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œå«æœ‰ä¸‰ä¸ªéšè—å±‚ï¼Œç¥ç»å…ƒæ•°é‡\\(20-15-10\\)ï¼ŒåŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°ã€‚ è¾“å‡ºç¥ç»å…ƒï¼ˆç´¢èµ”é¢‘ç‡ï¼‰çš„æ¿€æ´»å‡½æ•°ä½¿ç”¨å¹‚å‡½æ•°ï¼Œå³ä½¿ç”¨æ³Šæ¾æ¨¡å‹ä¸­çš„è§„èŒƒè¿æ¥å‡½æ•°ï¼ˆcanonical link functionï¼‰ã€‚ è®­ç»ƒæ¨¡å‹çš„å‚æ•°ä½¿ç”¨Nesterov Adam optimizerï¼Œé€‰æ‹©batch sizes=10000, epochs=300ï¼Œlearning rate=0.002ã€‚ ä¸ºäº†æ¶ˆé™¤åœ¨è®­ç»ƒé›†ä¸Šçš„æ€»ä½“ç´¢èµ”é¢‘ç‡åå·®ï¼ˆportfolio unbiasedï¼‰ï¼Œåœ¨æœ€åç¬¬ä¸‰ä¸ªéšè—å±‚çš„10ä¸ªç¥ç»å…ƒä¸Šå»ºç«‹Poisson-GLMæ¨¡å‹ã€‚ç”±äºPoisson-GLMçš„å‚æ•°ä¼°è®¡ä¸ºæå¤§ä¼¼ç„¶ï¼Œå…¶æ€»ä½“é¢„æµ‹ç´¢èµ”é¢‘ç‡ç­‰äºæ ·æœ¬çš„æ€»ç»éªŒç´¢èµ”é¢‘ç‡ã€‚ 8.3 æ¨¡å‹æ•´ä½“è¡¨ç° ï¼ˆmodel performanceï¼‰ æ€§èƒ½æŒ‡æ ‡ï¼ŒæŸå¤±å‡½æ•° æ³Šæ¾åå·®æŸå¤± \\[D(\\boldsymbol{y},\\hat{\\boldsymbol{y}})=\\frac{\\sum_{i=1}^n e_iL(y_i,\\hat{y}_i)}{\\sum_{i=1}^n e_i}\\] å…¶ä¸­\\(y_i=N_i/e_i\\), \\(\\hat{y}_i\\) ä¸º\\(y_i\\)çš„é¢„æµ‹å€¼ï¼Œå•ä½æ³Šæ¾åå·®\\(L(y_i,\\hat{y_i})\\)ä¸ºï¼š \\[\\begin{equation} L(y_i,\\hat{y}_i)=2(y_i\\ln y_i-y_i\\ln\\hat{y}_i-y_i+\\hat{y}_i). \\tag{8.1} \\end{equation}\\] Note that the Poisson deviance (8.1) is a strictly consistent scoring function for the expectation \\(\\mathbb{E}(Y)\\). Pseudo \\(R^2\\) \\[\\text{Pseudo}~~ R^2=1-\\frac{D(\\boldsymbol{y},\\hat{\\boldsymbol{y}})}{D(\\boldsymbol{y},\\bar{\\boldsymbol{y}})}\\] Pseudo \\(R^2\\)ä¸ºåå·®æŸå¤±å‡å°‘çš„ç›¸å¯¹å€¼ï¼Œè¡¡é‡äº†å¯ä»¥è¢«æ¨¡å‹è§£é‡Šçš„åå·®æ¯”ä¾‹ã€‚ 8.4 å˜é‡é‡è¦æ€§ï¼ˆvariable importanceï¼‰ å®ƒæä¾›äº†é¢å¤–çš„ä¿¡æ¯ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½æ›´æ·±å…¥çš„äº†è§£æ¨¡å‹ã€‚å¯ä»¥é€šè¿‡åˆ æ‰å¯¹æ¨¡å‹ä¸é‡è¦çš„å˜é‡ä»è€Œç®€åŒ–æ¨¡å‹ã€‚å¯ä»¥å‘ç°æ•°æ®ç»“æ„ä¸­çš„é—®é¢˜ï¼šå¦‚æœä¸€ä¸ªåå˜é‡æ˜¾ç¤ºå‡ºéå¸¸ç›¸å…³è€Œå…¶ä»–å˜é‡éå¸¸ä¸ç›¸å…³ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºå“åº”å˜é‡å‘ç”Ÿäº†ä¿¡æ¯æ³„éœ²ã€‚ GLMæ¨¡å‹å¯ä»¥ç”¨ä¼¼ç„¶æ¯”æ£€éªŒç»Ÿè®¡é‡ã€\\(t\\)-æ£€éªŒã€‚åŸºäºæ ‘çš„æ¨¡å‹å¯ä»¥ç”¨å˜é‡çš„æ‹†åˆ†æ•°é‡ã€split gainsã€‚ 8.4.1 Permutation importance å¯¹äºä¸€ä¸ªåå˜é‡ï¼Œå®ƒçš„å€¼è¢«éšæœºç½®æ¢ï¼Œç„¶ååœ¨æ–°çš„åå˜é‡æ¡ä»¶ä¸‹é¢„æµ‹\\(\\tilde{\\boldsymbol{y}}\\)ï¼Œè¿›è€Œè®¡ç®—D(,)ã€‚è¯¥å˜é‡çš„é‡è¦æ€§å¯ä»¥é€šè¿‡\\(D(\\boldsymbol{y},\\tilde{\\boldsymbol{y}})-D(\\boldsymbol{y},\\hat{\\boldsymbol{y}})\\)åº¦é‡ã€‚ å¯¹äºæ•°æ®é‡å¤§çš„æ ·æœ¬ï¼Œä¸€æ¬¡ç½®æ¢å³å¯å¾—åˆ°å¹³ç¨³çš„ç»“æœï¼›å¯¹äºæ•°æ®é‡å°çš„æ ·æœ¬ï¼Œéœ€è¦ç½®æ¢å¤šæ¬¡å¹¶è®¡ç®—å¹³å‡ã€‚ 8.5 è¾¹ç¼˜æ•ˆåº”ï¼ˆä¸»æ•ˆåº”ï¼‰ å¦‚ä½•è¡¡é‡å…·æœ‰å¤æ‚éçº¿æ€§é¡¹å’Œé«˜é˜¶äº¤äº’ä½œç”¨çš„GLMæ¨¡å‹ï¼Œæˆ–é»‘ç®±æ¨¡å‹ä¸­åå˜é‡å¯¹å“åº”å˜é‡çš„æ•ˆåº”ï¼Ÿ åŸºæœ¬æ€è·¯ï¼šå½“åå˜é‡åœ¨å…¶å–å€¼èŒƒå›´å†…å˜åŠ¨æ—¶ï¼Œé¢„æµ‹å€¼æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚ ä½¿ç”¨è®­ç»ƒé›†è¿˜æ˜¯æµ‹è¯•é›†ï¼Ÿ å¦‚æœåªä½¿ç”¨æ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œåˆ™è®­ç»ƒé›†æˆ–æµ‹è¯•é›†éƒ½å¯ä»¥ã€‚å¦‚æœä½¿ç”¨å› å˜é‡çš„å€¼ï¼Œåˆ™éœ€è¦ä½¿ç”¨æµ‹è¯•é›†ã€‚ ä»¥ä¸‹è€ƒè™‘ä¸‰ç§model agnostic methods: individual conditional expectations (ICE), partial dependence profiles (PD), accumulated local effects profiles (ALE)ã€‚å…¶ä¸­ï¼Œç¬¬ä¸€ç§æ˜¯åä¸¤ç§çš„åŸºç¡€ã€‚ 8.5.1 Individual conditional expectationsï¼ˆICEï¼‰ å¯¹äºæ¯ä¸ªæ ·æœ¬\\((N_i,e_i,\\boldsymbol{x}_i), i=1,\\ldots,n\\), ä½¿å…¶åå˜é‡\\(x_{i,p}\\)åœ¨å–å€¼èŒƒå›´å†…å˜åŒ–ï¼Œi.e., \\(\\{a_1,a_2,\\ldots,a_m\\}\\)ï¼Œæ ¹æ®æ¨¡å‹é¢„æµ‹åœ¨ä¸åŒåå˜é‡å€¼ä¸‹çš„ç´¢èµ”é¢‘ç‡ï¼Œ\\[y_i^{(ice)}=\\{\\hat{y}_i(\\boldsymbol{x}_{i,-p},x_{i,p}=a):a=a_1,\\dots,a_m\\}\\] ä»¥\\(x_p\\)ä¸ºæ¨ªè½´ï¼Œ\\(y^{(ice)}\\)ä¸ºçºµè½´ï¼Œå¯¹äºæ¯ä¸ªæ ·æœ¬éƒ½å¯ä»¥ç”»å‡ºä¸€æ¡ICEæ›²çº¿ã€‚ICE profileså›¾åƒæœ‰ä»¥ä¸‹ç‰¹å¾ï¼š å¦‚æœåå˜é‡æ˜¯ä»¥ç›¸åŠ çš„æ¨¡å¼åœ¨æ¨¡å‹ä¸­ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰ï¼Œåˆ™ä¸åŒæ ·æœ¬ICEæ›²çº¿åº”è¯¥æ˜¯å¹³è¡Œçš„ã€‚ \\(x_p\\)ä¸å…¶ä»–å˜é‡çš„äº¤äº’ä½œç”¨è¶Šå¼ºï¼Œä¸åŒæ ·æœ¬çš„ICEæ›²çº¿ä¼šç›¸äº¤ã€‚ ä¸ºäº†æ›´å¥½è§£è¯»ICEå›¾åƒï¼Œå¯ä»¥å‚ç›´ç§»åŠ¨ICEæ›²çº¿ä½¿å¾—å®ƒä»¬åœ¨ä¸€ä¸ªç‚¹é‡åˆã€‚ Remarkï¼šä¸€äº›ä¸»è¦çš„boostingç®—æ³•ï¼Œå¦‚XGBoostã€LightGBMæˆ–è€…CatBoostï¼Œåœ¨å»ºæ¨¡æ—¶å¯ä»¥çº¦æŸåå˜é‡çš„å•è°ƒæ€§ï¼Œè¿™æ ·å¯ä»¥å¢åŠ æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚åœ¨ç²¾ç®—ä¸­ï¼Œå¸¸å¸¸ä½¿ç”¨çš„çº¦æŸå¦‚é«˜å…èµ”é¢çš„ç´¢èµ”é¢‘ç‡ä½äºä½å…èµ”é¢çš„ç´¢èµ”é¢‘ç‡ã€‚XGBoostçš„ç›¸åº”ä»£ç ä¸ºmonotone_constraints = c(0,-1,0,0,0,0,0)ã€‚ 8.5.2 Partial dependence profiles Partial dependence profileå®šä¹‰ä¸ºICEçš„å¹³å‡å€¼\\[\\bar{f}_l(a)=\\frac{1}{n}\\sum_{i=1}^n\\hat{y}_i(\\boldsymbol{x}_{i,-p},x_{i,p}=a)\\] å®ƒåæ˜ äº†åå˜é‡åœ¨è¿™ä¸ªæ ·æœ¬ä¸Šçš„å¹³å‡æ•ˆåº”ï¼Œå’Œæ ·æœ¬ä¸­å…¶ä»–åå˜é‡çš„åˆ†å¸ƒæœ‰å…³ã€‚ ICEå’Œpartial dependence profileséƒ½å¯ä»¥æ¨å¹¿åˆ°å¤šä¸ªåå˜é‡ã€‚ 8.5.3 Accumulated local effects profiles (ALE) ICEå’Œpartial dependence profile å‡å®šæŸä¸ªåå˜é‡å˜åŒ–è€Œæ ·æœ¬çš„å…¶ä»–å˜é‡ä¿æŒä¸å˜ï¼ˆCeteris Paribus assumptionï¼‰ã€‚è¿™ä¸ªå‡è®¾åœ¨å®é™…ä¸­å¸¸å¸¸æ˜¯ä¸æˆç«‹çš„ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬ä¸å¯èƒ½å›ºå®šåœ°åŒºè€Œè®©äººå£å¯†åº¦å˜åŒ–ã€‚ ALEä¸€å®šç¨‹åº¦ä¸Šå…‹æœäº†ä»¥ä¸Šç¼ºç‚¹ï¼Œå®ƒè€ƒè™‘äº†å±€éƒ¨æ•ˆåº”ï¼Œåœ¨è¾ƒå¼ºç›¸å…³æ€§çš„åå˜é‡çš„æƒ…å†µä¸‹èƒ½è·å¾—æ›´è‡ªç„¶ã€å°‘åçš„ç»“æœã€‚ALE profileä¸º \\[\\tilde{f}_l(a)=\\frac{1}{|\\mathcal{I}_{x_p}(a)|}\\sum_{i\\in\\mathcal{I}_{x_p}(a)}\\hat{y}_i(\\boldsymbol{x}_{i,-p},x_{i,p}=a)\\] å…¶ä¸­\\(\\mathcal{I}_{x_p}(a)=\\{i:x_{i,p}\\in[a-\\epsilon,a+\\epsilon]\\}\\)ã€‚å³ALEä¸ºå±€éƒ¨ICEçš„å¹³å‡ï¼ˆpartial dependence profileä¸ºæ‰€æœ‰ICEçš„å¹³å‡ï¼‰ã€‚ å¦‚æœåå˜é‡çš„äº¤äº’ä½œç”¨ä¸å¼ºæˆ–è€…ä¸å…¶ä»–åå˜é‡ç›¸å…³æ€§å¼ºï¼Œåˆ™ALEå›¾ä¼šæ¥è¿‘äºpartial dependence profileå›¾ã€‚ Further profile plots ä»¥ä¸‹æ–¹æ³•æ¥è‡ªçº¿æ€§æ¨¡å‹çš„ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•ã€‚ å“åº”å˜é‡-åå˜é‡å›¾ï¼šAverages (or boxplots) of responses are plotted against a covariableã€‚è¯¥æ–¹æ³•æ²¡æœ‰ä½¿ç”¨æ¨¡å‹ï¼Œåªæ˜¯æ•°æ®çš„ä¸€ç§å¯è§†åŒ–å·¥å…·ã€‚ é¢„æµ‹å€¼-åå˜é‡å›¾ï¼ˆè¾¹é™…æ•ˆåº”å›¾ï¼‰ï¼šAverages of predicted values are plotted against a covariableï¼ˆmarginal plot or M-plotï¼‰ã€‚ æ®‹å·®-åå˜é‡å›¾ï¼š Averages (or boxplots) of residuals are plotted against a covariableã€‚å¹³å‡æ®‹å·®åº”è¯¥å……åˆ†åœ°æ¥è¿‘é›¶ï¼Œå¦‚æœå¹³å‡æ®‹å·®ç³»ç»Ÿçš„ä¸ä¸ºé›¶ï¼Œè¯´æ˜æ¨¡å‹æ‹Ÿåˆçš„ä¸å¥½ã€‚å¦‚æœè®­ç»ƒé›†ä¸Šå¹³å‡æ®‹å·®æ¥è¿‘é›¶ä½†æµ‹è¯•é›†ä¸Šå¹³å‡æ®‹å·®ä¸ä¸ºé›¶ï¼Œåˆ™è¯´æ˜æœ‰è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ 8.6 äº¤äº’æ•ˆåº” åå˜é‡\\(x_l,x_k\\)çš„äº¤äº’ä½œç”¨å¯ä»¥ç”¨ä¸¤ä¸ªå˜é‡çš„Friedmanâ€™s H-statisticåº¦é‡ï¼š \\[H^2_{kl}=\\frac{\\sum_1^n\\left[\\bar{f}_{kl}(x_{i,k},x_{i,l})-\\bar{f}_{k}(x_{i,k})-\\bar{f}_{l}(x_{i,l})\\right]^2}{\\sum_i^n\\left[\\bar{f}_{kl}(x_{i,k},x_{i,l})\\right]^2}\\] ä¸Šå¼åˆ†å­ä¸ºäº¤äº’æ•ˆåº”ï¼Œåˆ†æ¯ä¸ºè”åˆæ•ˆåº”ï¼Œå¯ä»¥è¿‘ä¼¼è®¤ä¸ºè”åˆæ•ˆåº” = è¾¹ç¼˜ï¼ˆä¸»ï¼‰æ•ˆåº” + äº¤äº’æ•ˆåº”ã€‚ å¦‚æœ\\(x_k,x_l\\)æ— äº¤äº’ä½œç”¨ï¼Œåˆ™Friedmanâ€™s H-statisticä¸º\\(0\\)ï¼›å¦‚æœå®ƒä»¬æœ‰å¼ºçƒˆçš„äº¤äº’ä½œç”¨ï¼Œåˆ™Friedmanâ€™s H-statisticæ¥è¿‘\\(1\\)ã€‚ äº¤äº’æ•ˆåº”çš„ç»å¯¹åº¦é‡å®šä¹‰ä¸º\\(H^2_{kl}\\)åˆ†å­çš„æ­£å¹³æ–¹æ ¹ï¼š \\[H_{kl}=\\sqrt{\\sum_1^n\\left[\\bar{f}_{kl}(x_{i,k},x_{i,l})-\\bar{f}_{k}(x_{i,k})-\\bar{f}_{l}(x_{i,l})\\right]^2}\\] å®ƒå¯ä»¥ç”¨æ¥å¯¹äº¤äº’æ•ˆåº”æ’åºã€‚ äº¤äº’æ•ˆåº”çš„å¯è§†åŒ–æœ‰ä¸¤ç§åŸºæœ¬æ–¹æ³•ï¼š éƒ½ä¸ºè¿ç»­å‹åå˜é‡ï¼š\\(\\bar{f}_{kl}(x_k,x_l)\\)çš„ç­‰é«˜çº¿å›¾ã€‚ åˆ†ç±»å˜é‡\\(x_k\\)å’Œè¿ç»­å‹å˜é‡\\(x_l\\)ï¼šåœ¨\\(x_k\\)çš„æŸä¸€æ°´å¹³\\(a\\)ä¸‹ï¼Œç”»\\(\\bar{f}_{kl}(x_k=a,x_l)\\)éš\\(x_l\\)çš„å˜åŒ–è¶‹åŠ¿å›¾ã€‚ äº¤äº’é¡¹çº¦æŸ æœ‰æ—¶åŸºäºæŠ€æœ¯æˆ–ç›‘ç®¡çš„ç°å®æƒ…å†µï¼Œäº¤äº’é¡¹åœ¨æ¨¡å‹ä¸­ä¼šè¢«ç¦ç”¨ã€‚åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¯ä»¥å°†ä¸è€ƒè™‘äº¤äº’æ•ˆåº”çš„å˜é‡ç›´æ¥è¿æ¥åˆ°è¾“å‡ºç¥ç»å…ƒã€‚åœ¨XGBoostä¸­ï¼Œå¯ä»¥é€šè¿‡æ”¹å˜interaction_constraintså®ç°äº¤äº’é¡¹çº¦æŸã€‚ 8.7 å…¨å±€ä»£ç†æ¨¡å‹ï¼ˆGlobal surrogate modelsï¼‰ å¯¹é¢„æµ‹å€¼é‡æ–°ä½¿ç”¨æ˜“äºè§£é‡Šçš„æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œå¦‚ä½¿ç”¨å•æ£µå†³ç­–æ ‘ã€‚é¦–å…ˆç”¨é»‘ç®±æ¨¡å‹å¯¹ç›®æ ‡æ•°æ®é›†è¿›è¡Œå»ºæ¨¡ä¸é¢„æµ‹ï¼Œå†ç”¨global surrogate model (tree) æ¥å¯¹é»‘ç®±æ¨¡å‹çš„é¢„æµ‹å€¼è¿›è¡Œå»ºæ¨¡å’Œè§£é‡Šã€‚ ä¸ºäº†è¯„åˆ¤æ›¿ä»£æ¨¡å‹çš„è¿‘ä¼¼æ•ˆæœï¼Œæœ‰å­¦è€…å»ºè®®ä½¿ç”¨\\(R^2\\)ã€‚ 8.8 å±€éƒ¨è§£é‡Š ä¸Šè¿°æ¨¡å‹è§£é‡Šæ–¹æ³•éƒ½æ˜¯ä»æ•´ä½“æ¥è§£é‡Šæ¨¡å‹ï¼Œä¸‹é¢ä»‹ç»é»‘ç®±æ¨¡å‹å¦‚ä½•å¾—åˆ°ä¸€ä¸ªå…·ä½“çš„æ ·æœ¬é¢„æµ‹å€¼ï¼Œå³æ¨¡å‹çš„å±€éƒ¨è§£é‡Šã€‚ ä»¥ä¸‹ä¸»è¦è€ƒè™‘å››ç§model agnostic methodsï¼šLIMEï¼ŒLIVEï¼ŒSHAPï¼ŒBreakdownã€‚ 8.8.1 LIMEå’ŒLIVE LIMEå…¨ç§°Local Interpretable Model-agnostic Explanationsï¼Œå³æ¨¡å‹çš„å±€éƒ¨è§£é‡Šå™¨ã€‚è™½ç„¶æ— æ³•ä½¿ç”¨çº¿æ€§æ¨¡å‹å®Œå…¨â€œæ¨¡ä»¿â€å‡ºSVMã€ç¥ç»ç½‘ç»œç­‰æ¨¡å‹çš„â€œè¡Œä¸ºâ€ï¼Œä½†å¯ä»¥åœ¨æŸä¸ªå±€éƒ¨æ ·æœ¬ç‚¹ä¸Šæ¥è¿‘. Figure 8.1: LIME ä»ä¸Šå›¾å¯ä»¥å¯¹LIMEè¿›è¡Œç›´è§‚çš„ç†è§£ï¼šæµ…è“è‰²å’Œæµ…çº¢è‰²åŒºåŸŸä¸ºé»‘ç®±æ¨¡å‹å¯¹äºŒåˆ†ç±»é—®é¢˜çš„é¢„æµ‹ç»“æœï¼Œæ˜¾ç„¶æˆ‘ä»¬éš¾ä»¥ç”¨ä¸€ä¸ªçº¿æ€§æ¨¡å‹æ¥è§£é‡Šå®ƒã€‚ä½†å¯¹äºä¸€ä¸ªæŒ‡å®šçš„æ ·æœ¬ç‚¹xï¼ˆåŠ ç²—çº¢è‰²åå­—ï¼‰ï¼Œå¯¹å…¶è¿›è¡Œå±€éƒ¨æ‰°åŠ¨å¾—åˆ°è‹¥å¹²ä¸ªæ ·æœ¬ç‚¹{z}ï¼Œå°†å…¶æ”¾å…¥é»‘ç®±æ¨¡å‹ä¸­å¾—åˆ°åˆ†ç±»ç»“æœï¼ˆçº¢è‰²åå­—å’Œè“è‰²åœ†åœˆï¼‰ï¼Œå¹¶æ ¹æ®å…¶ä¸xä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦èµ‹äºˆæƒé‡ï¼Œå¾—åˆ°å›¾ä¸­æ‰€ç¤ºè™šçº¿ä½œä¸ºæ ·æœ¬ç‚¹xçš„å±€éƒ¨è§£é‡Šå™¨ã€‚ LIMEå’ŒLIVEä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥ï¼š æ„é€ å¯è§£é‡Šçš„æ•°æ®ç‰¹å¾ï¼š\\(x\\in R^d\\)æ˜¯åŸå§‹ç”¨äºæ¨¡å‹é¢„æµ‹çš„ç‰¹å¾ï¼Œå¦‚NLPä¸­çš„è¯å‘é‡ï¼Œè€Œæˆ‘ä»¬ä½¿ç”¨\\(x&#39;\\in \\{0,1\\}^{d&#39;}\\)æ¥è¡¨ç¤ºæŸä¸ªå¯è§£é‡Šç‰¹å¾æ˜¯å¦å­˜åœ¨çš„äºŒå…ƒå˜é‡ï¼Œå¦‚NLPä¸­æŸä¸ªè¯çš„å­˜åœ¨ä¸å¦ã€‚ æ„é€ ç›®æ ‡å‡½æ•°ï¼š\\[\\xi(x)=\\underset{g\\in G}{\\arg\\min}~\\mathcal{L(f,g,\\pi_x)+\\Omega(g)}\\] å…¶ä¸­ï¼Œfä¸ºè¢«è§£é‡Šæ¨¡å‹ï¼Œgä¸ºè§£é‡Šå™¨ï¼Œå¦‚çº¿æ€§æ¨¡å‹ï¼Œå†³ç­–æ ‘ç­‰ã€‚\\(\\pi_x(z)\\)æ˜¯zåˆ°xçš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œå³å®šä¹‰xçš„å±€éƒ¨ã€‚\\(\\mathcal{L(Â·)}\\)ä¸ºæŸå¤±å‡½æ•°ï¼Œ\\(\\Omega(g)\\)ä¸ºè§£é‡Šå™¨gçš„å¤æ‚åº¦å‡½æ•°ï¼Œå¦‚çº¿æ€§æ¨¡å‹ä¸­éé›¶ç³»æ•°ä¸ªæ•°ï¼Œå†³ç­–æ ‘çš„æ·±åº¦ç­‰ã€‚ç›®æ ‡å‡½æ•°æ˜¯æŸå¤±å‡½æ•°å’Œå¤æ‚åº¦å‡½æ•°çš„å’Œï¼Œæˆ‘ä»¬å¸Œæœ›å…¶æœ€å°åŒ–ã€‚ é‡‡æ ·ï¼šå¯¹ç”¨å¯è§£é‡Šç‰¹å¾æè¿°çš„æ ·æœ¬ç‚¹xâ€™è¿›è¡Œå±€éƒ¨æ‰°åŠ¨ï¼Œå¾—åˆ°zâ€™ï¼Œè§£é‡Šå™¨gä»¥å¯è§£é‡Šç‰¹å¾zâ€™ä¸ºè¾“å…¥å˜é‡ï¼Œå¾—åˆ°g(zâ€™)ï¼Œè¢«è§£é‡Šæ¨¡å‹fä»¥åŸå§‹ç‰¹å¾ä¸ºè¾“å…¥å˜é‡ï¼Œå› æ­¤å°†zâ€™è¿˜åŸåˆ°zï¼Œå¾—åˆ°f(z)ï¼Œå¹¶é€šè¿‡\\(\\pi_x\\)èµ‹äºˆå…¶æƒé‡ã€‚ æ±‚è§£ã€‚ LIMEçš„ä¼˜ç‚¹æ˜¯åŸç†ç®€å•ï¼Œé€‚ç”¨èŒƒå›´å¹¿ï¼Œå¯è§£é‡Šæ‰€æœ‰é»‘ç®±æ¨¡å‹ã€‚ä½†ä¹Ÿå­˜åœ¨ä¸€å®šçš„é—®é¢˜ï¼Œä¾‹å¦‚å±€éƒ¨èŒƒå›´å¤§å°ä¸åŒï¼Œæœ€ç»ˆçš„è§£é‡Šä¹Ÿä¼šä¸åŒç”šè‡³ç›¸æ‚–ã€‚ 8.8.2 SHAP(Shapley Additive Explanations) SHAPçš„ç›®çš„ï¼šä¸LIMEç›¸ä¼¼ï¼Œæ‰¾åˆ°æŸä¸ªæ ·æœ¬ç‚¹ä¸Šå„ä¸ªç‰¹å¾çš„é‡è¦æ€§ Shapleyå€¼æ¥è‡ªåˆä½œåšå¼ˆè®ºï¼Œä»ä¸åŒçš„åˆä½œç»„åˆä¸­è®¡ç®—å‡ºå•ä¸ªä¸ªä½“çš„è´¡çŒ®ï¼Œå®ƒåº¦é‡äº†å•ä¸ªä¸ªä½“å¯¹æ€»ä½“çš„è¾¹é™…è´¡çŒ®ã€‚SHAPçš„ä¸»è¦æ€æƒ³æ¥æºäºShapleyå€¼ Shapleyå€¼çš„è®¡ç®—:\\[\\phi_j(v)=\\sum_{S\\in D\\backslash\\{j\\}}\\frac{|S|!(|D|-|S|-1)!}{|D|!}(v(S\\cup\\{j\\}-v(S)))\\] å…¶ä¸­ï¼ŒDä¸ºæ‰€æœ‰ç‰¹å¾çš„é›†åˆï¼ŒSä¸ºæŸä¸ªä¸åŒ…å«å…ƒç´ jçš„ç‰¹å¾å­é›†ï¼Œv(S)ä¸ºSé›†åˆå¯¹åº”çš„ä»·å€¼ï¼Œ\\(\\phi_j(v)\\)ä¸ºå…ƒç´ jçš„Shapleyå€¼ã€‚ è¿™ä¸ªå¼å­çš„å«ä¹‰ä¸ºï¼š ç‰¹å¾jçš„è´¡çŒ®å€¼ä¸å…¶è¿›å…¥çš„é¡ºåºæœ‰å…³ï¼Œä¸åŒçš„Sä»£è¡¨äº†ç‰¹å¾jçš„ä¸åŒè¿›å…¥é¡ºåºï¼Œå¦‚\\(S={x_1,x_2}\\)ï¼Œä»£è¡¨ç‰¹å¾jåœ¨\\(x_1,x_2\\)ä¹‹åè¿›å…¥ï¼Œåœ¨å‰©ä½™ç‰¹å¾ä¹‹å‰è¿›å…¥ï¼Œæ­¤å¤–ï¼Œç‰¹å¾jåœ¨æŸä¸€ä½ç½®æ—¶ï¼Œå…¶ä¹‹å‰å’Œä¹‹åçš„ç‰¹å¾é›†åˆå†…éƒ¨ä¹Ÿå­˜åœ¨ä¸åŒçš„æ’åºï¼Œä½†è¿™äº›ä¸åŒçš„æ’åºä¸å½±å“ç‰¹å¾jçš„è´¡çŒ®å€¼ï¼Œè¿™å°±æ˜¯æ±‚å’Œé¡¹ä¸­\\(\\frac{|S|!(|D|-|S|-1)!}{|D|!}\\)çš„å«ä¹‰ã€‚\\((v(S\\cup\\{j\\}-v(S)))\\)å³ç‰¹å¾jåŠ å…¥çš„è¾¹é™…æ•ˆåº”ã€‚ ä»Shapleyå€¼åˆ°SHAP SHAPå¯¹é¢„æµ‹çš„åˆ†è§£ï¼š\\[\\hat{f}(x^*)=\\phi_0+\\sum^{|F|}_{j=1}\\phi^*_j\\] \\(\\hat{f}(x^*)\\)ä¸ºæ¨¡å‹ç»™å‡ºçš„é¢„æµ‹å€¼ï¼Œ\\(\\phi_0\\)ä¸€èˆ¬å–æ¨¡å‹é¢„æµ‹å‡å€¼ï¼Œ\\(\\phi^*_j\\)ä¸ºç¬¬jä¸ªåå˜é‡å¯¹é¢„æµ‹å€¼çš„è´¡çŒ®ã€‚ SHAPä¸­ç‰¹å¾è´¡çŒ®çš„è®¡ç®— \\[\\phi^*_j=\\sum_{S\\in F\\backslash\\{j\\}}\\frac{|S|!(|D|-|S|-1)!}{|D|!}(f_{x^*}(S\\cup\\{j\\}-f_{x^*}(S)))\\] Fä¸ºç‰¹å¾é›†åˆï¼Œ\\(f_{x^*}(Â·)\\)ä¸ºæ¨¡å‹åœ¨æ ·æœ¬\\(x^*\\)ä¸Šçš„é¢„æµ‹ï¼Œ\\(f_{x^*}(S)=E[f(x)|x^*_S]\\)æ˜¯æ¨¡å‹fåŸºäºæ ·æœ¬\\(x^*\\)é‡Œçš„Sä¸­ç‰¹å¾çš„æ¡ä»¶æœŸæœ›è¾“å‡ºã€‚è®¡ç®—åŸç†ä¸Shapleyç›¸åŒï¼Œè¿™é‡ŒShapleyå€¼ä¸­çš„é›†åˆä»·å€¼è¢«æ›¿æ¢ä¸ºç‰¹å¾é›†Så¯¹åº”çš„é¢„æµ‹å€¼ã€‚ SHAPæŠŠé»‘ç®±ä¸­å¤æ‚çš„æ˜ å°„å…³ç³»è¡¨è¾¾æˆä¸€ä¸ªç®€å•çº¿æ€§å’Œï¼Œä¾¿äºæŠŠä¿è´¹çš„æ„æˆè§£é‡Šç»™è¢«ä¿é™©äººã€‚ä½†æ˜¯ï¼Œå¯¹äºä¸åŒè¢«ä¿é™©äººï¼ŒåŒä¸€ä¸ªåå˜é‡å€¼çš„æ•ˆåº”ï¼ˆSHAPï¼‰å¯èƒ½ä¸åŒï¼Œä¾‹å¦‚åŒä¸º45å²çš„è¢«ä¿é™©äººï¼Œå¹´é¾„è¿™ä¸€å˜é‡çš„SHAPå€¼å´ä¸åŒï¼Œè¿™æ˜¯å˜é‡ä¹‹é—´çš„äº¤äº’æ•ˆåº”é€ æˆçš„ã€‚ 8.8.3 Breakdown and approximate SHAP å¯¹äºä¸€ç»„ç»™å®šè¿›å…¥é¡ºåºçš„å˜é‡\\(x_{(1)},x_{(2)},\\ldots,x_{(p)}\\)ï¼Œè§£æ„æ–¹æ³•å¦‚ä¸‹ï¼š åˆå§‹é¢„æµ‹\\(\\hat{y}^{(0)}_i\\)ä¸ºè®­ç»ƒé›†çš„ç´¢èµ”é¢‘ç‡å‡å€¼ å¯¹äº\\(l=1,\\ldots,p\\)ï¼Œè¿›è¡Œä»¥ä¸‹ä¸¤æ­¥ï¼š æŠŠæ‰€æœ‰æ ·æœ¬çš„\\(x_{(l)}\\)å˜ä¸º\\(x_{i,(l)}\\)ï¼Œè®¡ç®—æ‰€æœ‰æ ·æœ¬ä¸Šé¢„æµ‹ç´¢èµ”é¢‘ç‡å‡å€¼\\(\\hat{y}^{(l)}_i\\). è®¡ç®—\\(x_{(l)}\\)çš„æ•ˆåº”ï¼š\\(\\hat{y}^{(l)}_i-\\hat{y}^{(l-1)}_i\\) å¯è§Breakdownæ–¹æ³•å¾—å‡ºçš„ç»“æœæ˜¯å’Œé¡ºåº\\(x_{(1)},x_{(2)},\\ldots,x_{(p)}\\)ç›¸å…³çš„ã€‚å¯¹äº\\(p\\)ä¸ªå˜é‡ï¼Œæœ‰\\(p!\\)ç§æ’åˆ—æ–¹å¼ã€‚ä¸€ç§å¤„ç†æ–¹æ³•æ˜¯é‡‡å–å°‘é‡çš„æ’åˆ—æ–¹å¼ï¼Œå¦‚20ç§ï¼Œå–å¹³å‡ã€‚å¦ä¸€ç§æ˜¯æŒ‰ç…§å˜é‡é‡è¦æ€§é™åºè¿›å…¥ï¼Œè¿™é‡Œå°†å‰è€…ç§°ä¸ºapproximate SHAPï¼Œå°†åè€…ç§°ä¸ºbreakdownã€‚ 8.8.4 From local to global properties å°†å¤šä¸ªæ ·æœ¬ä½¿ç”¨SHAPæˆ–breakdownè¿›è¡Œåˆ†è§£ï¼Œç„¶åä¹Ÿå¯ä»¥å¾—åˆ°ä¸€äº›æ¨¡å‹çš„å…¨å±€æ€§è´¨ï¼š å˜é‡é‡è¦æ€§ï¼šé€šè¿‡æ±‚æ‰€æœ‰æ ·æœ¬ä¸­\\(\\phi_j^*\\)çš„å¹³å‡æ•°ï¼Œå¯ä»¥ç”¨æ¥åº¦é‡å˜é‡\\(x_j\\)çš„é‡è¦æ€§ ä¸»æ•ˆåº”ï¼šå¯¹å˜é‡jçš„å–å€¼åŠå…¶å¯¹æ ·æœ¬içš„è´¡çŒ®å€¼\\(s_{ij}\\)ç”»æ•£ç‚¹å›¾ï¼Œå¯ä»¥å¾—åˆ°ä¸åŒå–å€¼ä¸‹å˜é‡çš„æ•ˆåº”ï¼Œèƒ½å¤Ÿå¾—åˆ°ä¸partial dependence plotç›¸ä¼¼çš„å›¾åƒã€‚ äº¤äº’æ•ˆåº”ï¼šä¸»æ•ˆåº”å›¾ä¸­ï¼Œå˜é‡jçš„åŒä¸€å–å€¼æ°´å¹³ä¸Šï¼Œå¯¹ä¸åŒçš„æ ·æœ¬iå¯èƒ½å¾—åˆ°ä¸åŒçš„è´¡çŒ®åº¦ï¼Œè¿™ä½“ç°äº†å˜é‡jå¯èƒ½ä¸å…¶ä»–å˜é‡ä¹‹é—´å­˜åœ¨äº¤äº’æ•ˆåº”ã€‚ 8.9 Improving the GLM by interpretable machine learning é€šè¿‡ä¹‹å‰çš„ç»“æœæ”¹è¿›GLMæ¨¡å‹ï¼š 1.å»ºç«‹ç®€å•çš„GLMæ¨¡å‹å’Œè°ƒä¼˜çš„MLæ¨¡å‹ 2.æ¯”è¾ƒæ€§èƒ½ï¼Œå¦‚æœGLMæ¨¡å‹æ²¡æœ‰æ”¹è¿›ç©ºé—´å°±åœæ­¢ 3.ç ”ç©¶å˜é‡é‡è¦æ€§å†³å®šä¿ç•™å“ªäº›å˜é‡ 4.é€šè¿‡å¯¹é¢„æµ‹çš„å½±å“æ•ˆæœï¼Œæ‰¾å‡ºæœ€å¼ºçš„é¢„æµ‹å› ç´ ï¼Œä»¥è°ƒæ•´å˜é‡ï¼ˆå¹³æ–¹é¡¹ã€æ ·æ¡ã€åˆ å»éƒ¨åˆ†ç±»ç­‰ï¼‰ 5.æ ¹æ®ç›¸äº’ä½œç”¨å¼ºåº¦æ”¹è¿›äº¤äº’é¡¹ 6.åˆ©ç”¨ä»¥ä¸Šå› ç´ æ”¹è¿›GLMï¼Œç„¶åå›åˆ°ç¬¬äºŒæ­¥ æ ¹æ®Variable importanceï¼Œä¸åˆ é™¤å˜é‡ æ ¹æ®Partial dependence profilesï¼ŒlogDensityç”¨ä¸‰æ¬¡æ ·æ¡å‡½æ•°è¡¨ç¤º æ ¹æ®Interaction strengthï¼Œå¢åŠ VehAge:VehBrandï¼ŒVehBrand:VehGasçš„äº¤äº’ä½œç”¨ åŸºäºä»¥ä¸Šæ”¹è¿›å»ºç«‹æ–°çš„glm 8.10 æ¡ˆä¾‹åˆ†æ 8.10.1 å¯¼å…¥åŒ… library(CASdatasets) # 1.0.6 library(dplyr) # 0.8.5 library(forcats) # 0.5.0 library(reshape2) # 1.4.3 library(corrplot) # 0.84 library(ggplot2) # 3.3.0 library(splines) # 3.6.3 library(splitTools) # 0.2.0 library(xgboost) # 1.0.0.2 library(keras) # 2.2.5.0 library(MetricsWeighted) # 0.5.0 library(flashlight) # 0.7.2 8.10.2 é¢„å¤„ç† data(freMTPL2freq) distinct &lt;- freMTPL2freq %&gt;% distinct_at(vars(-c(IDpol, Exposure, ClaimNb))) %&gt;% mutate(group_id = row_number()) dat &lt;- freMTPL2freq %&gt;% left_join(distinct) %&gt;% mutate(Exposure = pmin(1, Exposure), Freq = pmin(15, ClaimNb / Exposure), VehPower = pmin(12, VehPower), VehAge = pmin(20, VehAge), VehGas = factor(VehGas), DrivAge = pmin(85, DrivAge), logDensity = log(Density), VehBrand = factor(VehBrand, levels = paste0(&quot;B&quot;, c(12, 1:6, 10, 11, 13, 14))), PolicyRegion = relevel(Region, &quot;Ile-de-France&quot;), AreaCode = Area) # Covariables, Response, Weight x &lt;- c(&quot;VehPower&quot;, &quot;VehAge&quot;, &quot;VehBrand&quot;, &quot;VehGas&quot;, &quot;DrivAge&quot;, &quot;logDensity&quot;, &quot;PolicyRegion&quot;) y &lt;- &quot;Freq&quot; w &lt;- &quot;Exposure&quot; 8.10.3 æè¿°æ€§ç»Ÿè®¡ # Univariate description melted &lt;- dat[c(&quot;Freq&quot;, &quot;Exposure&quot;, &quot;DrivAge&quot;, &quot;VehAge&quot;, &quot;VehPower&quot;, &quot;logDensity&quot;)] %&gt;% stack() %&gt;% filter(ind != &quot;Freq&quot; | values &gt; 0) %&gt;% mutate(ind = fct_recode(ind, `Driver&#39;s age` = &quot;DrivAge&quot;, `Vehicle&#39;s age` = &quot;VehAge&quot;, `Vehicle power` = &quot;VehPower&quot;, `Logarithmic density` = &quot;logDensity&quot;)) ggplot(melted, aes(x=values)) + geom_histogram(bins = 19, fill = &quot;#E69F00&quot;) + facet_wrap(~ind, scales = &quot;free&quot;) + labs(x = element_blank(), y = element_blank()) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) # Bivariate description cor_mat &lt;- dat %&gt;% select_at(c(x, &quot;BonusMalus&quot;)) %&gt;% select_if(is.numeric) %&gt;% cor() %&gt;% round(2) corrplot(cor_mat, method = &quot;square&quot;, type = &quot;lower&quot;, diag = FALSE, title = &quot;&quot;, addCoef.col = &quot;black&quot;, tl.col = &quot;black&quot;) # Boxplots th &lt;- theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) # BonusMalus nach DrivAge dat %&gt;% mutate(DrivAge = cut(DrivAge, c(17:24, seq(25, 85, 10)), labels = c(18:25, &quot;26-35&quot;, &quot;36-45&quot;, &quot;46-55&quot;, &quot;56-65&quot;, &quot;66-75&quot;, &quot;76+&quot;), include.lowest = TRUE), DrivAge = fct_recode(DrivAge)) %&gt;% ggplot(aes(x = DrivAge, y = BonusMalus)) + geom_boxplot(outlier.shape = NA, fill = &quot;#E69F00&quot;) + coord_cartesian(ylim = c(50, 125)) # Brand/vehicle age dat %&gt;% ggplot(aes(x = VehBrand, y = VehAge)) + geom_boxplot(outlier.shape = NA, fill = &quot;#E69F00&quot;) + th # Density/Area dat %&gt;% ggplot(aes(x = AreaCode, y = logDensity)) + geom_boxplot(fill = &quot;#E69F00&quot;) + th # Density/Region dat %&gt;% ggplot(aes(x = Region, y = logDensity)) + geom_boxplot(outlier.shape = NA, fill = &quot;#E69F00&quot;) + th Figure 8.2: LIME 8.10.4 å»ºæ¨¡ ind &lt;- partition(dat[[&quot;group_id&quot;]], p = c(train = 0.8, test = 0.2), seed = 22, type = &quot;grouped&quot;) train &lt;- dat[ind$train, ] test &lt;- dat[ind$test, ] 8.10.4.1 glm fit_glm &lt;- glm(Freq ~ VehPower + ns(VehAge, 5) + VehBrand + VehGas + ns(DrivAge, 5) + logDensity + PolicyRegion, data = train, family = quasipoisson(), weights = train[[w]]) 8.10.4.2 XGBoost # Input maker prep_xgb &lt;- function(dat, x) { data.matrix(dat[, x, drop = FALSE]) } # Data interface to XGBoost dtrain &lt;- xgb.DMatrix(prep_xgb(train, x), label = train[[y]], weight = train[[w]]) # Parameters chosen by 5-fold grouped CV params_freq &lt;- list(learning_rate = 0.2, max_depth = 5, alpha = 3, #æƒé‡çš„l1æ­£åˆ™é¡¹ lambda = 0.5, #æƒé‡çš„l2æ­£åˆ™é¡¹ max_delta_step = 2, #æƒé‡æ”¹å˜æœ€å¤§æ­¥é•¿ï¼Œdefaultä¸º0 min_split_loss = 0, #èŠ‚ç‚¹åˆ†è£‚æ‰€éœ€çš„æœ€å°æŸå¤±å‡½æ•°ä¸‹é™å€¼ï¼Œdefaultä¸º0 colsample_bytree = 1, #æ§åˆ¶æ¯æ£µéšæœºé‡‡æ ·çš„åˆ—æ•°çš„å æ¯”(æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾)ï¼Œdefaultä¸º1 subsample = 0.9 #æ§åˆ¶æ ‘çš„æ¯çº§çš„æ¯æ¬¡åˆ†è£‚ï¼Œå¯¹åˆ—æ•°çš„é‡‡æ ·çš„å æ¯”,defaultä¸º1,å’Œcolsample_bytreeåŠŸèƒ½é‡å  ) # Fit set.seed(1) fit_xgb &lt;- xgb.train(params_freq, data = dtrain, nrounds = 580, objective = &quot;count:poisson&quot;, watchlist = list(train = dtrain), print_every_n = 10) 8.10.4.3 ä¸ºåæ–‡è®­ç»ƒå¯å˜å•è°ƒæ€§çº¦æŸå’Œäº¤äº’é¡¹çº¦æŸçš„xgboostï¼Œä½¿ç”¨è¾ƒå°çš„nrounds params_freq_constraints &lt;- list(learning_rate = 0.2, max_depth = 5, alpha = 3, lambda = 0.5, max_delta_step = 2, min_split_loss = 0, monotone_constraints = c(0,-1,0,0,0,0,0), #å¯å˜å•è°ƒæ€§çš„çº¦æŸ colsample_bytree = 1, subsample = 0.9) fit_xgb_constraints &lt;- xgb.train(params_freq_constraints, data = dtrain, nrounds = 80, objective = &quot;count:poisson&quot;, watchlist = list(train = dtrain), print_every_n = 10) params_freq_interaction_constraints &lt;- list(learning_rate = 0.2, max_depth = 5, alpha = 3, lambda = 0.5, max_delta_step = 2, min_split_loss = 0, interaction_constraints = list(4, c(0, 1, 2, 3, 5, 6)), #äº¤äº’é¡¹çº¦æŸï¼Œçº¦æŸä»¥åµŒå¥—åˆ—è¡¨çš„å½¢å¼æŒ‡å®š colsample_bytree = 1, subsample = 0.9) fit_xgb_interaction_constraints &lt;- xgb.train(params_freq_interaction_constraints, data = dtrain, nrounds = 80, objective = &quot;count:poisson&quot;, watchlist = list(train = dtrain), print_every_n = 10) 8.10.4.4 ç¥ç»ç½‘ç»œ prep_nn &lt;- function(dat, x, cat_cols = c(&quot;PolicyRegion&quot;, &quot;VehBrand&quot;)) { dense_cols &lt;- setdiff(x, cat_cols) c(list(dense1 = data.matrix(dat[, dense_cols])), lapply(dat[, cat_cols], function(z) as.integer(z) - 1)) } # Initialize neural net new_neural_net &lt;- function() { k_clear_session() set.seed(1) if (&quot;set_seed&quot; %in% names(tensorflow::tf$random)) { tensorflow::tf$random$set_seed(0) } else if (&quot;set_random_seed&quot; %in% names(tensorflow::tf$random)) { tensorflow::tf$random$set_random_seed(0) } else { print(&quot;Check tf version&quot;) } # Model architecture dense_input &lt;- layer_input(5, name = &quot;dense1&quot;, dtype = &quot;float32&quot;) PolicyRegion_input &lt;- layer_input(1, name = &quot;PolicyRegion&quot;, dtype = &quot;int8&quot;) VehBrand_input &lt;- layer_input(1, name = &quot;VehBrand&quot;, dtype = &quot;int8&quot;) PolicyRegion_emb &lt;- PolicyRegion_input %&gt;% layer_embedding(22, 1) %&gt;% layer_flatten() VehBrand_emb &lt;- VehBrand_input %&gt;% layer_embedding(11, 1) %&gt;% layer_flatten() outputs &lt;- list(dense_input, PolicyRegion_emb, VehBrand_emb) %&gt;% layer_concatenate() %&gt;% layer_dense(20, activation = &quot;tanh&quot;) %&gt;% layer_dense(15, activation = &quot;tanh&quot;) %&gt;% layer_dense(10, activation = &quot;tanh&quot;) %&gt;% layer_dense(1, activation = &quot;exponential&quot;) inputs &lt;- list(dense1 = dense_input, PolicyRegion = PolicyRegion_input, VehBrand = VehBrand_input) model &lt;- keras_model(inputs, outputs) model %&gt;% compile(loss = loss_poisson, optimizer = optimizer_nadam(), weighted_metrics = &quot;poisson&quot;) return(model) } neural_net &lt;- new_neural_net() neural_net %&gt;% summary() history &lt;- neural_net %&gt;% fit(x = prep_nn(train, x), y = train[, y], sample_weight = train[, w], batch_size = 1e4, epochs = 300, verbose = 2) plot(history) åœ¨æœ€åä¸€ä¸ªéšè—å±‚çš„åç»´è¾“å‡ºä¸Šè®­ç»ƒæ³Šæ¾GLM # Calibrate by using last hidden layer activations as GLM input encoder encoder &lt;- keras_model(inputs = neural_net$input, outputs = get_layer(neural_net, &quot;dense_2&quot;)$output) # Creates input for calibration GLM (extends prep_nn) prep_nn_calib &lt;- function(dat, x, cat_cols = c(&quot;PolicyRegion&quot;, &quot;VehBrand&quot;), enc = encoder) { prep_nn(dat, x, cat_cols) %&gt;% predict(enc, ., batch_size = 1e4) %&gt;% data.frame() } # Calibration GLM fit_nn &lt;- glm(Freq ~ ., data = cbind(train[&quot;Freq&quot;], prep_nn_calib(train, x)), family = quasipoisson(), weights = train[[w]]) 8.10.5 è§£é‡Š 8.10.5.1 ä¸ºæ¨¡å‹å»ºç«‹è§£é‡Šå™¨(flashlight)ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆä¸ºmultiflashlight set.seed(1) fillc &lt;- &quot;#E69F00&quot; fl_glm &lt;- flashlight( model = fit_glm, label = &quot;GLM&quot;, predict_function = function(fit, X) predict(fit, X, type = &quot;response&quot;) ) fl_nn &lt;- flashlight( model = fit_nn, label = &quot;NNet&quot;, predict_function = function(fit, X) predict(fit, prep_nn_calib(X, x), type = &quot;response&quot;) ) fl_xgb &lt;- flashlight( model = fit_xgb, label = &quot;XGBoost&quot;, predict_function = function(fit, X) predict(fit, prep_xgb(X, x)) ) fl_xgb_constraints &lt;- flashlight( model = fit_xgb_constraints, label = &quot;XGBoost_constraints&quot;, predict_function = function(fit, X) predict(fit, prep_xgb(X, x)) ) fl_xgb_interaction_constraints &lt;- flashlight( model = fit_xgb_interaction_constraints, label = &quot;XGBoost_interaction_constraints&quot;, predict_function = function(fit, X) predict(fit, prep_xgb(X, x)) ) # Combine them and add common elements like reference data metrics &lt;- list(`Average deviance` = deviance_poisson, `Relative deviance reduction` = r_squared_poisson) fls &lt;- multiflashlight(list(fl_glm, fl_nn, fl_xgb), data = test, y = y, w = w, metrics = metrics) fls_xgb_constraints &lt;- multiflashlight(list(fl_xgb,fl_xgb_constraints), data = test, y = y, w = w, metrics = metrics) fls_xgb_interaction_constraints &lt;- multiflashlight(list(fl_xgb_interaction_constraints), data = test, y = y, w = w, metrics = metrics) fls_interaction_constraints &lt;- multiflashlight(list(fl_glm, fl_nn, fl_xgb,fl_xgb_interaction_constraints), data = test, y = y, w = w, metrics = metrics) # Version on canonical scale fls_log &lt;- multiflashlight(fls, linkinv = log) fls_xgb_interaction_constraints_log &lt;- multiflashlight(fls_xgb_constraints, linkinv = log) fls_interaction_constraints_log &lt;- multiflashlight(fls_interaction_constraints, linkinv = log) 8.10.5.2 å¯¹flashlightåº”ç”¨å¯è§£é‡Šæ€§å‡½æ•° #æ¨¡å‹è¡¨ç° perf &lt;- light_performance(fls) perf plot(perf, geom = &quot;point&quot;) + labs(x = element_blank(), y = element_blank()) #å˜é‡é‡è¦æ€§ imp &lt;- light_importance(fls, v = x) plot(imp, fill = fillc, color = &quot;black&quot;) 8.10.5.3 ICEæ›²çº¿ #driveageåŸºäºä¸­å¿ƒåŒ–ä¸éä¸­å¿ƒåŒ–å’Œæ˜¯å¦ä½¿ç”¨å¯¹æ•°è¢«è§£é‡Šå˜é‡ä½œä¸ºé¢„æµ‹ç»“æœ plot(light_ice(fls, v = &quot;DrivAge&quot;, n_max = 200, seed = 3) , alpha = 0.1) plot(light_ice(fls, v = &quot;DrivAge&quot;, n_max = 200, seed = 3, center = &quot;middle&quot;) , alpha = 0.03) plot(light_ice(fls_log, v = &quot;DrivAge&quot;, n_max = 200, seed = 3) , alpha = 0.1) plot(light_ice(fls_log, v = &quot;DrivAge&quot;, n_max = 200, seed = 3, center = &quot;middle&quot;) , alpha = 0.03) #è€ƒè™‘å•è°ƒæ€§çº¦æŸçš„xgboost plot(light_ice(fls_xgb_constraints, v = &quot;VehAge&quot;, n_max = 200, seed = 3) , alpha = 0.1) plot(light_ice(fls_xgb_constraints, v = &quot;VehAge&quot;, n_max = 200, seed = 3, center = &quot;middle&quot;) , alpha = 0.03) plot(light_ice(fls_xgb_constraints, v = &quot;DrivAge&quot;, n_max = 200, seed = 3) , alpha = 0.1) # Partial dependence curves plot(light_profile(fls, v = &quot;VehAge&quot;, pd_evaluate_at = 0:20)) plot(light_profile(fls, v = &quot;DrivAge&quot;, n_bins = 25)) plot(light_profile(fls, v = &quot;logDensity&quot;)) plot(light_profile(fls, v = &quot;VehGas&quot;)) # ALE versus partial dependence ale_DrivAge &lt;- light_effects(fls, v = &quot;DrivAge&quot;, counts_weighted = TRUE, v_labels = FALSE, n_bins = 20, cut_type = &quot;quantile&quot;) plot(ale_DrivAge, use = c(&quot;pd&quot;, &quot;ale&quot;), show_points = FALSE) # Classic diagnostic plots plot(light_profile(fls, v = &quot;VehAge&quot;, type = &quot;predicted&quot;)) plot(light_profile(fls, v = &quot;VehAge&quot;, type = &quot;residual&quot;)) + geom_hline(yintercept = 0) plot(light_profile(fls, v = &quot;VehAge&quot;, type = &quot;response&quot;)) # Multiple aspects combined eff_DrivAge &lt;- light_effects(fls, v = &quot;DrivAge&quot;, counts_weighted = TRUE) p &lt;- plot(eff_DrivAge, show_points = FALSE) plot_counts(p, eff_DrivAge, alpha = 0.3) # Interaction (relative) interact_rel &lt;- light_interaction( fls_interaction_constraints_log, v = most_important(imp, 4), take_sqrt = FALSE, pairwise = TRUE, use_linkinv = TRUE, seed = 61 ) plot(interact_rel, color = &quot;black&quot;, fill = fillc, rotate_x = TRUE) # Interaction (absolute) interact_abs &lt;- light_interaction( fls_interaction_constraints_log, v = most_important(imp, 4), normalize = FALSE, pairwise = TRUE, use_linkinv = TRUE, seed = 61 ) plot(interact_abs, color = &quot;black&quot;, fill = fillc, rotate_x = TRUE) # Filter on largest three brands sub_data &lt;- test %&gt;% filter(VehBrand %in% c(&quot;B1&quot;, &quot;B2&quot;, &quot;B12&quot;)) # Strong interaction pdp_vehAge_Brand &lt;- light_profile(fls_interaction_constraints_log, v = &quot;VehAge&quot;, by = &quot;VehBrand&quot;, pd_seed = 50, data = sub_data) plot(pdp_vehAge_Brand) # Weak interaction pdp_DrivAge_Gas &lt;- light_profile(fls_interaction_constraints_log, v = &quot;DrivAge&quot;, by = &quot;VehGas&quot;, pd_seed = 50) plot(pdp_DrivAge_Gas) plot(light_ice(fls_xgb_interaction_constraints, v = &quot;DrivAge&quot;, n_max = 200, seed = 3) , alpha = 0.1) 8.10.5.4 å…¨å±€ä»£ç†æ¨¡å‹ surr_nn &lt;- light_global_surrogate(fls_log$NNet, v = x) plot(surr_nn) surr_xgb &lt;- light_global_surrogate(fls_log$XGBoost, v = x) plot(surr_xgb) 8.10.6 å±€éƒ¨æ€§è´¨ #å±€éƒ¨æ€§è´¨ new_obs &lt;- test[1, ] new_obs[, x] unlist(predict(fls, data = new_obs)) # Breakdown bd &lt;- light_breakdown(fls$XGBoost, new_obs = new_obs, v = x, n_max = 1000, seed = 20) plot(bd) # Extract same order of variables for visualization only v &lt;- setdiff(bd$data$variable, c(&quot;baseline&quot;, &quot;prediction&quot;)) # Approximate SHAP shap &lt;- light_breakdown(fls$XGBoost, new_obs, visit_strategy = &quot;permutation&quot;, v = v, n_max = 1000, seed = 20) plot(shap) fl_with_shap &lt;- add_shap(fls$XGBoost, v = x, n_shap = 500, n_perm = 12, n_max = 1000, seed = 100) #ä»¥å±€éƒ¨æ€§è´¨ä¼°è®¡æ¨¡å‹çš„å…¨å±€æ€§è´¨ plot(light_importance(fl_with_shap, v = x, type = &quot;shap&quot;), fill = fillc, color = &quot;black&quot;) plot(light_scatter(fl_with_shap, v = &quot;DrivAge&quot;, type = &quot;shap&quot;), alpha = 0.3) Figure 8.3: LIME 8.10.7 æ”¹è¿›glm fit_glm2 &lt;- glm(Freq ~ VehPower + VehBrand * VehGas + PolicyRegion + ns(DrivAge, 5) + VehBrand * ns(VehAge, 5) + ns(logDensity, 5), data = train, family = quasipoisson(), weights = train[[w]]) # Setting up expainers fl_glm2 &lt;- flashlight( model = fit_glm2, label = &quot;Improved GLM&quot;, predict_function = function(fit, X) predict(fit, X, type = &quot;response&quot;) ) # Combine them and add common elements like reference data fls2 &lt;- multiflashlight(list(fl_glm, fl_glm2, fl_nn, fl_xgb), metrics = metrics, data = test, y = y, w = w) fls2_log &lt;- multiflashlight(fls2, linkinv = log) # Some results plot(light_performance(fls2), geom = &quot;point&quot;, rotate_x = TRUE) plot(light_importance(fls2, v = x), fill = fillc, color = &quot;black&quot;, top_m = 4) plot(light_profile(fls2, v = &quot;logDensity&quot;)) interact_rel_improved &lt;- light_interaction( fls2_log, v = most_important(imp, 4), take_sqrt = FALSE, pairwise = TRUE, use_linkinv = TRUE, seed = 61) plot(interact_rel_improved, color = &quot;black&quot;, fill = fillc, top_m = 4) "],["unsupervised-learning.html", "9 æ— ç›‘ç£å­¦ä¹ æ–¹æ³• 9.1 æ•°æ®é¢„å¤„ç† 9.2 ä¸»æˆåˆ†åˆ†æ 9.3 è‡ªç¼–ç  9.4 K-means clustering 9.5 K-medoids clustering (PAM) 9.6 Gaussian mixture models(GMMs) 9.7 ä¸‰ç§èšç±»æ–¹æ³•è¯„ä»· 9.8 t-SNE 9.9 UMAP 9.10 SOM", " 9 æ— ç›‘ç£å­¦ä¹ æ–¹æ³• å¤§å­¦åŠä»¥åç”Ÿæ´»ä¸­æœ€å¸¸ç”¨çš„å­¦ä¹ æ–¹æ³•ã€‚ åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œ æˆ‘ä»¬å¯ä»¥é™ä½æ•°æ®ï¼ˆåå˜é‡ï¼Œç‰¹å¾ï¼‰ç»´åº¦ã€æ ¹æ®ç‰¹å¾çš„ç›¸ä¼¼åº¦å¯¹æ ·æœ¬è¿›è¡Œèšç±»ã€è®¾è®¡å¯è§†åŒ–å·¥å…·æ­ç¤ºé«˜ç»´æ•°æ®çš„ç‰¹æ€§ã€‚æ— ç›‘ç£å­¦ä¹ ä¸è€ƒè™‘å“åº”å˜é‡ï¼Œä»…è€ƒè™‘ç‰¹å¾çš„ç›¸ä¼¼æ€§ã€‚ æœ¬ç« å°†è€ƒè™‘ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š é™ç»´ï¼š ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ è‡ªç¼–ç ï¼Œç“¶é¢ˆç¥ç»ç½‘ç»œï¼ˆBNNï¼‰ èšç±»ï¼š åˆ†å±‚èšç±»ï¼šä¸éœ€äº‹å…ˆæŒ‡å®šèšç±»ä¸ªæ•° è‡ªä¸‹è€Œä¸Šï¼šåˆå§‹\\(n\\)ç±»ï¼Œå†å°†ç›¸è·æœ€è¿‘çš„ä¸¤ç±»åˆå¹¶ï¼Œå»ºç«‹ä¸€ä¸ªæ–°çš„ç±»ï¼Œç›´åˆ°æœ€ååˆå¹¶æˆ\\(1\\)ç±»ï¼› è‡ªä¸Šè€Œä¸‹ï¼šåˆå§‹\\(1\\)ç±»ï¼Œå†å°†ç›¸è·æœ€è¿œçš„æ ·æœ¬åˆ†è£‚æˆä¸¤ç±»ï¼Œç›´åˆ°æœ€ååˆ†è£‚æˆ\\(n\\)ä¸ªç±»ã€‚ åŸºäºè´¨å¿ƒçš„èšç±»: K-means, K-medoids åŸºäºåˆ†å¸ƒçš„èšç±»: Gaussian mixture models (GMMs) å¯è§†åŒ–é«˜ç»´æ•°æ®ï¼š å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰ \\(t\\)åˆ†å¸ƒéšæœºé‚»è¿‘åµŒå…¥ï¼ˆ\\(t\\)-SNEï¼‰ï¼Œ ç»Ÿä¸€æµå½¢é€¼è¿‘å’ŒæŠ•å½±ï¼ˆUMAPï¼‰ï¼Œ è‡ªç»„ç»‡æ˜ å°„ï¼ˆSOMï¼‰ Kohonenå›¾ã€‚ 9.1 æ•°æ®é¢„å¤„ç† æ•°æ®ä¸­å„ä¸ªå˜é‡çš„è¯´æ˜å¦‚ä¸‹ï¼š å˜é‡ ç±»å‹ è¯´æ˜ brand factor 43ä¸ªæ±½è½¦å“ç‰Œ type factor 96ä¸ªæ°´å¹³ model factor 113ä¸ªæ°´å¹³ seats int åº§ä½æ•° max_power int å‘åŠ¨æœºæœ€å¤§åŠŸç‡(kW),å–å¯¹æ•° max_torque num æœ€å¤§è½¬çŸ©(Nm),å–å¯¹æ•° cubic_capacity int å®¹é‡(cm\\(^3\\)),å–å¯¹æ•° weight int è½¦é‡(kg)ï¼Œå–å¯¹æ•° max_engine_speed int å‘åŠ¨æœºæœ€å¤§è½¬é€Ÿ(rpm) seconds_to_100 int è¾¾åˆ°100km/hæ‰€éœ€è¦ç§’æ•° top_speed int æœ€å¤§è¡Œé©¶é€Ÿåº¦(km/h) sports_car int è·‘è½¦ tau num ä¸“å®¶è¯„åˆ† Figure 9.3 æ˜¾ç¤ºäº†å„ä¸ªå˜é‡ï¼ˆå–å¯¹æ•°åï¼‰çš„æ•£ç‚¹å›¾ï¼ŒQ-Qå›¾ï¼ŒåŠç›¸å…³ç³»æ•°ã€‚ Figure 9.1: æ•£ç‚¹å›¾ \\(\\tau\\)ä¸ºä¸“å®¶æå‡ºçš„è¯„åˆ†æ–¹ç¨‹ï¼Œæ®æ­¤è¯„åˆ†å¯ä»¥å¤§æ¦‚åˆ¤æ–­è¯¥è½¦æ˜¯å¦ä¸ºè·‘è½¦ï¼š \\[\\tau=\\frac{\\text{weight}}{\\frac{\\text{max_power}}{0.735499}}\\text{seats}^{\\frac{1}{3}}\\left(\\frac{\\text{cubic_capacity}}{1000}\\right)^{\\frac{1}{4}}\\] å¦‚æœæŠŠå¸¸æ•°é¡¹æå‡ºï¼Œå¯å¾—åˆ°å¦‚ä¸‹ç­‰ä»·çš„è¯„åˆ†\\(\\tau^+\\): \\[\\tau^+=\\frac{\\text{weight}}{\\text{max_power}}\\text{seats}^{\\frac{1}{3}}\\text{cubic_capacity}^{\\frac{1}{4}}\\] ä¸“å®¶æŠŠ\\(\\tau&lt;17\\)æˆ–\\(\\tau^+&lt;129.9773\\)çš„æ±½è½¦å®šä¹‰ä¸ºè·‘è½¦ã€‚ 9.2 ä¸»æˆåˆ†åˆ†æ Ingenbleek-Lemaire (1988) çš„ç›®æ ‡æ˜¯åˆ©ç”¨ä¸»æˆåˆ†åˆ†ææ¥å¯¹æ•°æ®è¿›è¡Œé™ç»´ï¼Œæ ¹æ®é€‰å–çš„ä¸»æˆåˆ†æ¥åŒºåˆ†è·‘è½¦å’Œæ™®é€šè½¦ï¼Œå¹¶å°è¯•è¾¾åˆ°å’Œä¸“å®¶é€‰æ‹©ä¸€æ ·çš„æ•ˆæœã€‚ PCAé€‚ç”¨äºé«˜æ–¯åˆ†å¸ƒï¼Œè‹¥å˜é‡æ˜¾è‘—ä¸ç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œéœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆæ¯”å¦‚å–å¯¹æ•°æˆ–å…¶ä»–æ–¹æ³•ï¼‰ã€‚ Ingenbleek-Lemaire (1988) æ„é€ äº†ä»¥ä¸‹5ä¸ªè¿‘ä¼¼æœä»é«˜æ–¯åˆ†å¸ƒçš„å˜é‡ä»¥ä¾¿è¿›è¡Œåç»­åˆ†æã€‚ \\[x_1^*=\\ln\\left(\\frac{\\text{weight}}{\\text{max_power}}\\right)\\] \\[x_2^*=\\ln\\left(\\frac{\\text{max_power}}{\\text{cubic_capacity}}\\right)\\] \\[x_3^*=\\ln\\left(\\text{max_torque}\\right)\\] \\[x_4^*=\\ln\\left(\\text{max_engine_speed}\\right)\\] \\[x_5^*=log\\left(\\text{cubic_capacity}\\right)\\] Figure (fig:pairs2) å±•ç¤ºäº†ä»¥ä¸Š5ä¸ªå˜é‡çš„ç›¸å…³æ€§ã€‚ Figure 9.2: æ•£ç‚¹å›¾ ä¸»æˆåˆ†åˆ†æå¯ä»¥é™ä½é«˜ç»´æ•°æ®çš„ç»´æ•°ï¼Œä½¿ç›¸å¯¹äºåŸå§‹æ•°æ®çš„é‡æ„è¯¯å·®æœ€å°ã€‚å¦‚æœåº”ç”¨æˆåŠŸï¼Œå®ƒå‡å°‘äº†ç‰¹å¾ç©ºé—´çš„ç»´æ•°ï¼Œå¹¶ä¸”å®ƒå¯¹äº(ç²¾ç®—)å›å½’å»ºæ¨¡ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒæä¾›äº†å°‘é‡çš„ä¸ç›¸å…³çš„è§£é‡Šå˜é‡ã€‚ å‡è®¾æ ·æœ¬é‡ä¸º\\(n\\)çš„æ ·æœ¬æœ‰\\(q\\)ä¸ªç‰¹å¾\\(\\mathbf{x}_1^*,\\ldots,\\mathbf{x}^*_n\\in\\mathbb{R}^q\\)ã€‚å…¶è®¾è®¡çŸ©é˜µä¸º \\[\\mathbf{X}^*=(\\mathbf{x}_1^*,\\ldots,\\mathbf{x}^*_n)^\\intercal\\in\\mathbb{R}^{n\\times q}.\\] æŠŠè®¾è®¡çŸ©é˜µçš„æ¯åˆ—è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¾—åˆ° \\[\\mathbf{X}=(x_{i,j})_{1\\le i \\le n,1\\le j\\le q}\\in\\mathbb{R}^{n\\times q}.\\] å…¶ä¸­ï¼Œç¬¬\\(i\\)è¡Œæ˜¯æ ·æœ¬\\(i\\)çš„ç‰¹å¾\\(\\mathbf{x}_i\\in\\mathbb{R}^q, 1\\le i\\le n\\), ç¬¬\\(j\\)åˆ—æ˜¯ç¬¬\\(j\\)ä¸ªç‰¹å¾\\(x_j\\in\\mathbb{R}^n\\). çŸ©é˜µ\\(\\mathbf{X}\\)çš„ç§©ä¸º\\(q\\le n\\)ï¼Œå¯ä»¥æ‰¾åˆ°\\(q\\)ä¸ªæ­£äº¤çš„\\(q\\)ç»´åŸºå‘é‡\\(\\mathbf{v}_1,\\ldots,\\mathbf{v}_q\\in\\mathbb{R}^q\\), ä½¿å¾—\\(\\mathbf{v}_1\\)ä¸º\\(\\mathbf{X}\\)æ³¢åŠ¨æœ€å¤§çš„æ–¹å‘ï¼Œ\\(\\mathbf{v}_2\\)ä¸ºä¸\\(\\mathbf{v}_1\\)æ­£äº¤æ–¹å‘ä¸Šçš„\\(\\mathbf{X}\\)æ³¢åŠ¨æœ€å¤§çš„æ–¹å‘ï¼Œä¾æ¬¡ç±»æ¨ã€‚ ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºå¦‚ä¸‹ï¼š \\[\\mathbf{v}_1=\\underset{||\\omega||_2=1}{\\arg \\max}||\\mathbf{X}\\omega||_2^2=\\underset{\\omega^\\intercal\\omega=1}{\\arg \\max} (\\omega^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\omega)\\] \\[\\mathbf{v}_2=\\underset{||\\omega||_2=1}{\\arg \\max}||\\mathbf{X}\\omega||_2^2 ~~~\\text{ subject to } \\mathbf{v}_1^\\intercal\\omega=0.\\] \\[\\ldots\\] ä¸»æˆåˆ†åˆ†æå¯é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼å®ç° æ±‚\\(\\mathbf{X}^\\intercal \\mathbf{X}\\)æˆ–è€…\\(\\mathbf{X}\\)çš„åæ–¹å·®çŸ©é˜µ\\(\\mathbf{\\Sigma}\\)çš„ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ã€‚æ˜“çŸ¥\\(\\mathbf{X}^\\intercal \\mathbf{X}=n\\times\\mathbf{\\Sigma}\\)ï¼Œæ‰€ä»¥å®ƒä»¬çš„ç‰¹å¾å‘é‡ç›¸åŒã€‚ç¬¬ä¸€ä¸ªç‰¹å¾å‘é‡å³ä¸º\\(\\mathbf{v}_1\\)ï¼Œç¬¬äºŒä¸ªç‰¹å¾å‘é‡ä¸º\\(\\mathbf{v}_2\\)ã€‚å‰ä¸¤ä¸ªä¸»æˆåˆ†ä¸º\\(\\mathbf{X}\\mathbf{v}_1,\\mathbf{X}\\mathbf{v}_2\\) å¯¹\\(\\mathbf{X}\\)è¿›è¡Œå¥‡å¼‚å€¼ï¼ˆsingular value decompositionï¼‰åˆ†è§£:\\[\\mathbf{X}=U\\Lambda V^\\intercal.\\]å…¶ä¸­ï¼Œå¯¹è§’çŸ©é˜µ\\(\\Lambda=\\text{diag}(\\lambda_1,\\ldots,\\lambda_q)\\)çš„å…ƒç´ ä¸º\\(\\mathbf{X}^\\intercal \\mathbf{X}\\)çš„ç‰¹å¾å€¼ï¼Œ\\(V\\)ä¸º\\(\\mathbf{X}^\\intercal \\mathbf{X}\\)çš„ç‰¹å¾å‘é‡ã€‚ä¸»æˆåˆ†å¯ä»¥é€šè¿‡\\(\\mathbf{X}V\\)æ±‚å¾—ã€‚ åˆ©ç”¨å‰\\(p\\)ä¸ªä¸»æˆåˆ†å¯ä»¥é‡æ„è®¾è®¡çŸ©é˜µçš„è¿‘ä¼¼å€¼\\[\\mathbf{X}_p=U\\text{diag}(\\lambda_1,\\ldots,\\lambda_p,0,\\ldots,0)V^{\\intercal}.\\] è¯¥è¿‘ä¼¼å€¼ä¸ºä»¥ä¸‹æå€¼é—®é¢˜çš„æ ¹\\[\\underset{B\\in\\mathbb{R}^{n\\times q}}{\\arg \\min}||\\mathbf{X}-B||^2 ~~\\text{subject to rank}(B)\\le q,\\] å³çŸ©é˜µ\\(\\mathbf{X}_p\\)æ˜¯æ‰€æœ‰ç§©ä¸º\\(p\\)çš„çŸ©é˜µä¸­ï¼Œä¸åŸå§‹è®¾è®¡çŸ©é˜µ\\(\\mathbf{X}\\)é‡ç»„å¹³æ–¹è¯¯å·®(FèŒƒæ•°)æœ€å°çš„çŸ©é˜µã€‚ # standardize matrix X &lt;- X01/sqrt(colMeans(X01^2))[col(X01)] # eigenvectors and eigenvalues X1 &lt;- as.matrix(X) nrow(X1) A &lt;- t(X1) %*% X1 A sum(eigen(A)$value)/5 sqrt(eigen(A)$value) # singular values sqrt(eigen(A)$value/nrow(X1)) # scaled eigenvalues eigen(A)$vector A1&lt;-cor(X1) A1*nrow(X1) sqrt(eigen(A1)$value) eigen(A1)$vector eigen(A1)$value # singular value decomposition SVD &lt;- svd(X1) SVD$d # singular values rbind(SVD$v[,1],SVD$v[,2]) # first two right singular vectors # PCA with package PCA t.pca &lt;- princomp(X1,cor=TRUE) t.pca$loadings summary(t.pca) eigen(A1)$value/sum(eigen(A1)$value) é€šè¿‡summary(t.pca)æˆ‘ä»¬å¯ä»¥å¾—åˆ°å‰ä¸¤ä¸ªä¸»æˆåˆ†çš„ç´¯è®¡è´¡çŒ®åº¦å·²ç»åˆ°\\(92%\\)ï¼Œå‰ä¸¤ä¸ªä¸»æˆåˆ†æå–äº†åŸå§‹æ•°æ®çš„ç»å¤§éƒ¨åˆ†ä¿¡æ¯ï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰æ‹©2ä¸ªä¸»æˆåˆ†åº”è¯¥å¯ä»¥è¾ƒå¥½åœ°é‡æ„åŸå§‹æ•°æ®ã€‚ ä»¥ç¬¬ä¸€ä¸»æˆåˆ†ä¸ºä¾‹è¯´æ˜ç¬¬ä¸€ä¸»æˆåˆ†å’Œ\\(x_1^*,\\ldots,x_5^*\\)çš„å…³ç³» # PCA Sports Cars weights alpha &lt;- SVD$v[,1]/sds (alpha_star &lt;- c(alpha[1],alpha[2]-alpha[1], alpha[3], alpha[4], alpha[5]-alpha[2])/alpha[1]) \\(y_1=\\left&lt;\\mathbf{v_1},\\mathbf{x}\\right&gt;=-0.558x_1+0.412x_2+0.539x_3+0.126x_4+0.461x_5\\)å› ä¸ºæ­¤å¤„çš„\\(x_1,\\ldots,x_5\\)æ¥è‡ªæ ‡å‡†è®¾è®¡çŸ©é˜µï¼Œæ‰€ä»¥æˆ‘ä»¬åšé€†å˜æ¢\\(\\alpha_lx_l^*=\\alpha_l\\left(\\hat{\\sigma_l}\\frac{x_l^*-\\hat{\\mu_l}}{\\hat{\\sigma_l}}+\\hat{\\mu_l}\\right)=\\alpha_l\\hat{\\sigma_l}x_l+\\alpha_l\\hat{\\mu_l}\\),å…¶ä¸­\\(\\alpha_l\\hat{\\sigma_l}=\\mathbf{v_{1,l}}\\),ç”±æ­¤å¯å¾—åŸå§‹æ–¹ç¨‹ä¸º\\(\\frac{y^*}{\\alpha_1}=log(\\text{weight})-1.93log(\\text{max_power})-0.65log(\\text{max_torque})-0.64log(\\text{max_engine_speed})+0.25log(\\text{cubic_capacity})\\)å°†æ ·æœ¬å¸¦å…¥è®¡ç®—æˆ‘ä»¬å³å¯å¾—åˆ°ç¬¬ä¸€ä¸»æˆåˆ†çš„å¾—åˆ†ã€‚ç¬¬äºŒä¸»æˆåˆ†è®¡ç®—åŒä¸Šã€‚ # scatter plot switch_sign &lt;- -1 # switch sign of the first component to make svd and princomp compatible tt.pca &lt;- t.pca$scores tt.pca[,1] &lt;- switch_sign *tt.pca[,1] pairs(tt.pca,diag.panel=panel.qq,upper.panel=panel.cor) Figure 9.3: ä¸»æˆåˆ†æ•£ç‚¹å›¾ Figure 5.3:å¯¹è§’çº¿ä¸ºQ-Qå›¾ï¼Œå·¦ä¸‹éƒ¨åˆ†æ•£ç‚¹å›¾ï¼Œå³ä¸Šéƒ¨åˆ†ç›¸å…³ç³»æ•°å›¾ï¼ˆå„ä¸ªä¸»æˆåˆ†ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œæ‰€ä»¥ç›¸å…³ç³»æ•°ä¸º0ï¼‰ # plot first two principal components dat3 &lt;- d.data dat3$v1 &lt;- X1 %*% SVD$v[,1] dat3$v2 &lt;- X1 %*% SVD$v[,2] # png(&quot;./plots/5/pca.png&quot;) plot(x=dat3$v1, y=dat3$v2, col=&quot;blue&quot;,pch=20, ylim=c(-7,7), xlim=c(-7,7), ylab=&quot;2nd principal component&quot;, xlab=&quot;1st principal component&quot;, main=list(&quot;principal components analysis&quot;, cex=1.5), cex.lab=1.5) dat0 &lt;- dat3[which(dat3$tau&lt;21),] points(x=dat0$v1, y=dat0$v2, col=&quot;green&quot;,pch=20) dat0 &lt;- dat3[which(dat3$tau&lt;17),] points(x=dat0$v1, y=dat0$v2, col=&quot;red&quot;,pch=20) legend(&quot;bottomleft&quot;, c(&quot;tau&gt;=21&quot;, &quot;17&lt;=tau&lt;21&quot;, &quot;tau&lt;17 (sports car)&quot;), col=c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;), lty=c(-1,-1,-1), lwd=c(-1,-1,-1), pch=c(20,20,20)) #dev.off() Figure 9.4: ä¸»æˆåˆ†å¾—åˆ†å›¾ Figure 5.4:æ ·æœ¬(n=475)çš„ä¸»æˆåˆ†å¾—åˆ†å›¾ï¼Œå…¶ä¸­è“è‰²ç‚¹å’Œçº¢è‰²ç‚¹ä¹‹é—´æœ‰ä¸€ä¸ªè¶…å¹³é¢å°†è·‘è½¦å’Œæ™®é€šè½¦å¾ˆå¥½çš„åŒºåˆ†å¼€äº†ã€‚ # reconstruction error reconstruction.PCA &lt;- array(NA, c(5)) for (p in 1:5){ Xp &lt;- SVD$v[,1:p] %*% t(SVD$v[,1:p]) %*% t(X) Xp &lt;- t(Xp) reconstruction.PCA[p] &lt;- sqrt(sum(as.matrix((X-Xp)^2))/nrow(X)) } round(reconstruction.PCA,2) # biplot tt.pca &lt;- t.pca tt.pca$scores[,1] &lt;- switch_sign * tt.pca$scores[,1] tt.pca$loadings[1:5,1] &lt;- switch_sign * tt.pca$loadings[1:5,1] biplot(tt.pca,choices=c(1,2),scale=0, expand=2, xlab=&quot;1st principal component&quot;, ylab=&quot;2nd principal component&quot;, cex=c(0.4,1.5), ylim=c(-7,7), xlim=c(-7,7)) Figure 9.5: çŸ¢é‡åˆ†è§£å›¾ Figure 5.5:é»‘ç‚¹ä¹‹é—´çš„è·ç¦»è¡¨ç¤ºç›¸ä¼¼æ€§ã€‚çº¢è‰²çŸ¢é‡æ˜¯å‰ä¸¤ä¸ªæ ‡å‡†æ­£äº¤æƒå€¼å‘é‡\\(\\mathbf{v_1}\\)å’Œ\\(\\mathbf{v_2}\\)çš„åˆ†é‡ã€‚é•¿åº¦åæ˜ äº†å˜é‡çš„æ ‡å‡†å·®ï¼Œå¤¹è§’çš„ä½™å¼¦å€¼ç»™å‡ºäº†ç›¸åº”çš„ç›¸å…³æ€§ã€‚ 9.3 è‡ªç¼–ç  PCAå¯¹å¼‚å¸¸å€¼å¾ˆæ•æ„Ÿï¼Œä¹Ÿæœ‰ç¨³å¥çš„PCAç‰ˆæœ¬ã€‚ä¾‹å¦‚ï¼ŒCrouxç­‰äººç»™å‡ºäº†ä¸€ä¸ªåŸºäºä¸­å€¼ç»å¯¹åå·®(MADs)çš„ç®—æ³•ï¼ŒRåŒ…ï¼špcaPPã€‚ ä¸»æˆåˆ†åˆ†æå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªè‡ªåŠ¨ç¼–ç å™¨ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ›´ä¸€èˆ¬åœ°ä»‹ç»è‡ªåŠ¨ç¼–ç å™¨ï¼Œä¾‹å¦‚BNNã€‚ è‡ªç¼–ç åŒ…å«ç¼–ç å’Œè§£ç ä¸¤ä¸ªé•œé¢å¯¹ç§°çš„æ˜ å°„ï¼š ç¼–ç : \\(\\varphi:\\mathbb{R}^q\\rightarrow\\mathbb{R}^p\\) è§£ç : \\(\\psi:\\mathbb{R}^p\\rightarrow\\mathbb{R}^q\\) å…¶ä¸­\\(p \\le q\\)ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªåº¦é‡å·®å¼‚çš„å‡½æ•°\\(d(Â·,Â·)\\)ï¼Œå½“ä¸”ä»…å½“\\(\\mathbf{x}=\\mathbf{y}\\)æ—¶\\(d(x,y)=0\\)ï¼Œè‡ªç¼–ç å°±æ˜¯æ‰¾åˆ°ä¸€å¯¹\\((\\varphi,\\psi)\\)ä½¿å¾—æœ‰\\(\\pi=\\psi\\circ\\varphi\\)æ»¡è¶³\\(d(\\pi(\\mathbf{x}),\\mathbf{x})\\)æœ€å°ã€‚ åœ¨PCAçš„ä¾‹å­ä¸­ï¼š \\[\\varphi:\\mathbb{R}^q\\rightarrow\\mathbb{R}^p,\\mathbf{x}\\mapsto\\mathbf{y}=\\varphi(\\mathbf{x})=(\\left&lt;\\mathbf{v_1},\\mathbf{x}\\right&gt;,\\ldots,\\left&lt;\\mathbf{v_p},\\mathbf{x}\\right&gt;)^T=(\\mathbf{v_1},\\ldots,\\mathbf{v_p})^T\\mathbf{x}.\\] \\[\\psi:\\mathbb{R}^p\\rightarrow\\mathbb{R}^q,\\mathbf{y}\\mapsto\\psi(\\mathbf{y})=(\\mathbf{v_1},\\ldots,\\mathbf{v_p})\\mathbf{y}.\\] \\[\\pi(\\mathbf{X}^T)=\\psi\\circ\\varphi(\\mathbf{x}^T)=(\\mathbf{v_1},\\ldots,\\mathbf{v_p})(\\mathbf{v_1},\\ldots,\\mathbf{v_p})^T\\mathbf{X}^T=\\mathbf{X_p}^T.\\] \\[\\sum_{i=1}^nd(\\pi(\\mathbf{x}_i),\\mathbf{x}_i)=\\sum_{i=1}^n\\|\\pi(\\mathbf{x}_i-\\mathbf{x}_i)\\|_2^2=\\|\\mathbf{X}_p-\\mathbf{X}\\|_F^2.\\] æ­¤å¤„è¡¡é‡é‡æ„è¯¯å·®çš„ä¸ºé’ˆå¯¹çŸ©é˜µçš„æ¬§æ°è·ç¦»ï¼Œå³FèŒƒæ•°ã€‚ ä½œä¸ºéçº¿æ€§è‡ªç¼–ç å™¨çš„ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬è€ƒè™‘ç“¶é¢ˆç¥ç»ç½‘ç»œ(BNN)ã€‚ä¸ºäº†æˆåŠŸæ ¡å‡†ä¸€ä¸ªBNNï¼Œå®ƒçš„éšè—å±‚æ•°åº”è¯¥æ˜¯å¥‡æ•°\\(d\\) (\\(d\\)ç§°ä¸ºç¥ç»ç½‘ç»œçš„æ·±åº¦)ï¼Œå¹¶ä¸”ä¸­å¿ƒéšè—å±‚åº”è¯¥æ˜¯ä½ç»´çš„ï¼Œæœ‰\\(p\\)ä¸ªéšè—ç¥ç»å…ƒï¼Œæ‰€æœ‰å‰©ä½™çš„éšè—å±‚åº”è¯¥æ˜¯å›´ç»•è¿™ä¸ªä¸­å¿ƒéšè—å±‚å¯¹ç§°çš„ã€‚å› æ­¤å¯¹äºæ·±åº¦\\(d = 3\\)çš„BNNï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å›¾9.6å±•ç¤ºçš„ç¥ç»ç½‘ç»œç»“æ„ Figure 9.6: è‡ªç¼–ç  q=5,p=2 ä¸€èˆ¬çš„ç¥ç»ç½‘ç»œæœ‰å¦‚ä¸‹ç»“æ„ï¼š \\[\\pi:\\mathbb{R}^{q_0}\\rightarrow\\mathbb{R}^{q_{d+1}=q_0},\\mathbf{x}\\mapsto\\pi(\\mathbf{x})=(\\mathbf{z}^\\left(d+1\\right)\\circ\\mathbf{z}^\\left(d\\right)\\circ\\ldots\\circ\\mathbf{z}^\\left(1\\right))(\\mathbf{x})\\] \\[\\mathbf{z}^\\left(m\\right):\\mathbb{R}^{q_{m-1}}\\rightarrow\\mathbb{R}^{q_m},\\mathbf{z}\\mapsto\\mathbf{z}^\\left(m\\right)(\\mathbf{z})=\\left(\\phi\\left(\\left&lt;\\mathbf{w}_1^{\\left(m\\right)},\\mathbf{z}\\right&gt;\\right),\\ldots,\\phi\\left(\\left&lt;\\mathbf{w}_{q_m}^{\\left(m\\right)},\\mathbf{z}\\right&gt;\\right)\\right)^T\\] å…¶ä¸­\\(\\mathbf{w}_l^{\\left(m\\right)}\\in\\mathbb{R}^{q_{m-1}},1 \\le l\\le q_m\\)ä¸ºæƒé‡ï¼Œ\\(\\phi:\\mathbb{R}\\rightarrow\\mathbb{R}\\)ä¸ºæ¿€æ´»å‡½æ•°ã€‚ \\[\\mathbf{z}^\\left(d+1\\right):\\mathbb{R}^{q_{d}}\\rightarrow\\mathbb{R}^{q_{d+1}},\\mathbf{z}\\mapsto\\mathbf{z}^\\left(d+1\\right)(\\mathbf{z})=\\left(\\left&lt;\\mathbf{w}_1^{\\left(d+1\\right)},\\mathbf{z}\\right&gt;,\\ldots,\\left&lt;\\mathbf{w}_{q_{d+1}}^{\\left(d+1\\right)},\\mathbf{z}\\right&gt;\\right)^T\\] æ€»å‚æ•°ä¸ªæ•°ä¸º\\(r=\\sum_{m=1}^{d+1}q_mq_{m-1}\\)ã€‚ ä¸ç»å…¸çš„å‰é¦ˆç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œæˆ‘ä»¬è¿™é‡Œæ²¡æœ‰æˆªè·é¡¹ï¼Œå› ä¸ºç‰¹å¾$_iå·²ç»è¢«æ ‡å‡†åŒ–ã€‚è¿™ç•¥å¾®é™ä½äº†ç½‘ç»œå‚æ•°çš„ç»´æ•°ã€‚ é€‰æ‹©è¾“å‡ºæ¿€æ´»ä¸ºçº¿æ€§æ¿€æ´»ï¼Œæ˜¯å› ä¸ºxçš„æ‰€æœ‰æˆä»½éƒ½åœ¨å®æ•°é¢†åŸŸä¸­ã€‚ä¸‹é¢æˆ‘ä»¬è¿˜å°†åº”ç”¨å…¶ä»–è¾“å‡ºæ¿€æ´»å‡½æ•°ã€‚ ä½œä¸ºéšå±‚çš„æ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬é€šå¸¸é€‰ç”¨åŒæ›²æ­£åˆ‡å‡½æ•°ã€‚å¦‚æœä¸€ä¸ªBNNåªæœ‰çº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œé‚£ä¹ˆä»–æ˜¯ç­‰ä»·äºPCAçš„ã€‚ 9.3.1 æ¨¡å‹è®­ç»ƒ å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š åœ¨æ­£å¼è¿›å…¥ç¥ç»ç½‘ä¹‹å‰ï¼Œæˆ‘ä»¬è¦å…ˆè¿›è¡Œæƒé‡é¢„è®­ç»ƒã€‚ Figure 9.7: è‡ªç¼–ç è®­ç»ƒè¿‡ç¨‹ é¦–å…ˆæˆ‘ä»¬å…ˆå°†ä¸­é—´çš„éšè—å±‚æŠ˜å å¾—åˆ°å¦‚å›¾9.7ä¸­é—´ç»“æ„çš„ç¥ç»ç½‘ç»œï¼Œæ®æ­¤æˆ‘ä»¬å¾—åˆ°æƒé‡ï¼š\\(\\mathbf{w}_1^{(1)}\\ldots,\\mathbf{w}_{q_1=7}^{(1)}\\in\\mathbb{R}^{q_0=5}\\) å’Œ\\(\\mathbf{w}_1^{(d+1)}\\ldots,\\mathbf{w}_{q_{d+1}=5}^{(d+1)}\\in\\mathbb{R}^{q_d=7}\\) å†è®­ç»ƒéšè—å±‚å³å›¾9.7ä¸­å³å›¾çš„æƒé‡ï¼Œå¾—åˆ°ï¼š\\(\\mathbf{w}_1^{(2)}\\ldots,\\mathbf{w}_{q_2=2}^{(2)}\\in\\mathbb{R}^{q_1=7}\\) å’Œ\\(\\mathbf{w}_1^{(3)}\\ldots,\\mathbf{w}_{q_{3}=7}^{(3)}\\in\\mathbb{R}^{q_2=2}\\) bottleneck.1 &lt;- function(q00, q22){ Input &lt;- layer_input(shape = c(q00), dtype = &#39;float32&#39;, name = &#39;Input&#39;) Output = Input %&gt;% layer_dense(units=q22, activation=&#39;tanh&#39;, use_bias=FALSE, name=&#39;Bottleneck&#39;) %&gt;% layer_dense(units=q00, activation=&#39;linear&#39;, use_bias=FALSE, name=&#39;Output&#39;) model &lt;- keras_model(inputs = Input, outputs = Output) model %&gt;% compile(optimizer = optimizer_nadam(), loss = &#39;mean_squared_error&#39;) model } bottleneck.3 &lt;- function(q00, q11, q22){ Input &lt;- layer_input(shape = c(q00), dtype = &#39;float32&#39;, name = &#39;Input&#39;) Encoder = Input %&gt;% layer_dense(units=q11, activation=&#39;tanh&#39;, use_bias=FALSE, name=&#39;Layer1&#39;) %&gt;% layer_dense(units=q22, activation=&#39;tanh&#39;, use_bias=FALSE, name=&#39;Bottleneck&#39;) Decoder = Encoder %&gt;% layer_dense(units=q11, activation=&#39;tanh&#39;, use_bias=FALSE, name=&#39;Layer3&#39;) %&gt;% layer_dense(units=q00, activation=&#39;linear&#39;, use_bias=FALSE, name=&#39;Output&#39;) model &lt;- keras_model(inputs = Input, outputs = Decoder) model %&gt;% compile(optimizer = optimizer_nadam(), loss = &#39;mean_squared_error&#39;) model } # bottleneck architecture q1 &lt;- 7 q2 &lt;- 2 q0 &lt;- ncol(X) # pre-training 1: merging layers 1 and 3 (skipping bottleneck) model.1 &lt;- bottleneck.1(q0, q1) model.1 epochs &lt;- 2000 batch_size &lt;- nrow(X) # fit the merged model {t1 &lt;- proc.time() fit &lt;- model.1 %&gt;% fit(as.matrix(X), as.matrix(X), epochs=epochs, batch_size=batch_size, verbose=0) proc.time()-t1} plot(x=c(1:length(fit[[2]]$loss)), y=sqrt(fit[[2]]$loss*q0), ylim=c(0,max(sqrt(fit[[2]]$loss*q0))),pch=19, cex=.5, xlab=&#39;epochs&#39;, ylab=&#39;Frobenius norm loss&#39;, main=&quot;gradient descent algorithm&quot;) abline(h=c(0.6124), col=&quot;orange&quot;) # neuron activations in the central layer zz &lt;- keras_model(inputs=model.1$input, outputs=get_layer(model.1, &#39;Bottleneck&#39;)$output) yy &lt;- zz %&gt;% predict(as.matrix(X)) # pre-training 2: middlepart model.2 &lt;- bottleneck.1(q1, q2) model.2 epochs &lt;- 2000 # fit the merged model {t1 &lt;- proc.time() fit &lt;- model.2 %&gt;% fit(as.matrix(yy), as.matrix(yy), epochs=epochs, batch_size=batch_size, verbose=0) proc.time()-t1} plot(x=c(1:length(fit[[2]]$loss)), y=sqrt(fit[[2]]$loss*q0), ylim=c(0,max(sqrt(fit[[2]]$loss*q0))),pch=19, cex=.5, xlab=&#39;epochs&#39;, ylab=&#39;Frobenius norm loss&#39;, main=&quot;gradient descent algorithm&quot;) é¢„è®­ç»ƒç»“æŸåï¼Œç”¨è¿™äº›é¢„å…ˆè®­ç»ƒå¥½çš„æƒå€¼å¯¹æ•´ä¸ªBNNè¿›è¡Œé‡æ„ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªé‡æ„è¯¯å·®ï¼ˆå¦‚ä¸‹ï¼‰åœ¨æ­¤æ¡ˆä¾‹ä¸­ï¼Œæ­¤å¤„çš„é‡æ„è¯¯å·®å¤§äºä½¿ç”¨PCAæ—¶å–ä¸¤ä¸ªä¸»æˆåˆ†çš„é‡æ„è¯¯å·®ã€‚ # fitting the full model model.3 &lt;- bottleneck.3(q0, q1, q2) model.3 # set weights weight.3 &lt;- get_weights(model.3) weight.1 &lt;- get_weights(model.1) weight.2 &lt;- get_weights(model.2) weight.3[[1]] &lt;- weight.1[[1]] weight.3[[4]] &lt;- weight.1[[2]] weight.3[[2]] &lt;- weight.2[[1]] weight.3[[3]] &lt;- weight.2[[2]] set_weights(model.3, weight.3) fit0 &lt;- model.3 %&gt;% predict(as.matrix(X)) # reconstruction error of the pre-calibrated network # note that this error may differ from the tutorial because we did not set a seed round(Frobenius.loss(X,fit0),4) ä½¿ç”¨è¿™äº›é¢„å…ˆè®­ç»ƒå¥½çš„æƒå€¼ä½œä¸ºåˆå§‹åŒ–ï¼Œå°†æ¢¯åº¦ä¸‹é™ç®—æ³•åº”ç”¨äºæ•´ä¸ªBNNï¼Œä»¥è·å¾—BNNé™ç»´ã€‚ # calibrate full bottleneck network epochs &lt;- 10000 batch_size &lt;- nrow(X) {t1 &lt;- proc.time() fit &lt;- model.3 %&gt;% fit(as.matrix(X), as.matrix(X), epochs=epochs, batch_size=batch_size, verbose=0) proc.time()-t1} plot(x=c(1:length(fit[[2]]$loss)), y=sqrt(fit[[2]]$loss*q0), col=&quot;blue&quot;, ylim=c(0,max(sqrt(fit[[2]]$loss*q0))),pch=19, cex=.5, xlab=&#39;epochs&#39;, ylab=&#39;Frobenius norm loss&#39;, main=list(&quot;gradient descent algorithm&quot;, cex=1.5), cex.lab=1.5) abline(h=c(0.6124), col=&quot;orange&quot;, lwd=2) legend(&quot;bottomleft&quot;, c(&quot;decrease GDM&quot;, &quot;PCA(p=2)&quot;), col=c(&quot;blue&quot;, &quot;orange&quot;), lty=c(-1,1), lwd=c(-1,2), pch=c(19,-1)) # reconstruction error (slightly differs from 0.5611 because of missing seed) fit0 &lt;- model.3 %&gt;% predict(as.matrix(X)) round(Frobenius.loss(X,fit0),4) # read off the bottleneck activations encoder &lt;- keras_model(inputs=model.3$input, outputs=get_layer(model.3, &#39;Bottleneck&#39;)$output) y&lt;- predict(encoder,as.matrix(X)) Figure 9.8: é‡æ„è¯¯å·®å›¾ æˆ‘ä»¬è¯´æ˜äº†\\(\\mathbf{F}\\)èŒƒæ•°æŸå¤±å‡½æ•°åœ¨è¶…è¿‡10,000æ¬¡è¿­ä»£æ—¶çš„ä¸‹é™è¿‡ç¨‹ã€‚åœ¨å¤§çº¦2000æ¬¡è¿­ä»£åï¼ŒæŸå¤±ä½äº\\(p=2\\)ä¸ªä¸»æˆåˆ†çš„PCA(å›¾9.8ä¸­æ©™è‰²çº¿åœ¨æ°´å¹³0.6124å¤„)ã€‚10,000æ¬¡è¿­ä»£åçš„æœ€ç»ˆé‡æ„è¯¯å·®ä¸º0.5428ã€‚ # note that we may need sign switches to make it comparable to PCA y0 &lt;- max(abs(y))*1.1 plot(x=y[,1], y=y[,2], col=&quot;blue&quot;,pch=20, ylim=c(-y0,y0), xlim=c(-y0,y0), ylab=&quot;2nd bottleneck neuron&quot;, xlab=&quot;1st bottleneck neuron&quot;, main=list(&quot;bottleneck neural network autoencoder&quot;, cex=1.5), cex.lab=1.5) dat0 &lt;- y[which(d.data$tau&lt;21),] points(x=dat0[,1], y=dat0[,2], col=&quot;green&quot;,pch=20) dat0 &lt;- y[which(d.data$tau&lt;17),] points(x=dat0[,1], y=dat0[,2], col=&quot;red&quot;,pch=20) legend(&quot;bottomright&quot;, c(&quot;tau&gt;=21&quot;, &quot;17&lt;=tau&lt;21&quot;, &quot;tau&lt;17 (sports car)&quot;), col=c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;), lty=c(-1,-1,-1), lwd=c(-1,-1,-1), pch=c(20,20,20)) Figure 9.9: BNN å›¾9.9ä¸ºå¯¹æ ·æœ¬\\((n=475)\\)çš„åˆ†ç±»æ•ˆæœï¼Œä¸PCAï¼ˆæ—‹è½¬ï¼‰å¾ˆåƒã€‚ Figure 9.10: BNNä¸PCAé‡æ„è¯¯å·®å¯¹æ¯”å›¾ ç”±å›¾9.10å¾—å‡ºç»“è®ºï¼šä¸¤ç§æ–¹æ³•éƒ½å¾—åˆ°äº†ç›¸ä¼¼çš„ç»“æœã€‚ä½†åœ¨ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒBNNçš„é‡æ„è¯¯å·®è¾ƒå°ã€‚åœ¨å°‘æ•°æƒ…å†µä¸‹ï¼ŒBNNå¯ä»¥å¾—åˆ°æ›´å¥½çš„é‡å»ºç»“æœ(å³ä¸‹è§’çš„æ©™è‰²ç‚¹)ã€‚ é’ˆå¯¹è¿™ä¸ªä¾‹å­è€Œè¨€ï¼Œå±äºä½ç»´æ•°çš„é—®é¢˜ï¼Œä¸»æˆåˆ†åˆ†æé€šå¸¸å°±å¯ä»¥äº†ï¼Œå› ä¸ºéçº¿æ€§çš„éƒ¨åˆ†å¹¶æ²¡æœ‰å‘æŒ¥å…³é”®ä½œç”¨ã€‚ 9.4 K-means clustering K-meansèšç±»æ˜¯ä¸€ç§åŸºäºè´¨å¿ƒï¼ˆcentroid-basedï¼‰çš„èšç±»æ–¹æ³•ï¼Œå®ƒå°†\\(n\\)ä¸ªæ ·æœ¬ç‚¹\\(\\mathbf{x}_i\\in\\mathcal{X}\\subset\\mathbb{R}^q\\)åˆ’åˆ†ä¸º\\(K\\)ä¸ªä¸ç›¸äº¤çš„ç±»:\\[\\mathcal{C}_K:\\mathbb{R}^q\\rightarrow\\mathcal{K}=\\{1,\\ldots,K\\},~~\\mathbf{x}\\mapsto\\mathcal{C}_K(\\mathbf{x})\\]ï¼Œä»¥ä¸Šç»™å‡ºäº†å¯¹ç‰¹å¾ç©ºé—´\\(\\mathcal{X}\\)çš„ä¸€ä¸ªåˆ†å‰²\\((C_1,\\ldots,C_K)\\)ï¼Œå…¶ä¸­\\[C_k=\\{\\mathbf{x}\\in\\mathcal{X};\\mathcal{C}_K(\\mathbf{x})=k\\}\\] ç¡®å®š\\(\\mathcal{C}_K\\)çš„åŸåˆ™æ˜¯ä½¿æ€»ç±»å†…å·®å¼‚æœ€å°ï¼Œè¿™å¯ä»¥è½¬åŒ–ä¸ºè®¡ç®—ä½¿ç±»å†…ç¦»å·®å¹³æ–¹å’Œæ€»å’Œæœ€å°çš„ä¸€ä¸ªåˆ†å‰²ï¼Œæ‰€æ„é€ çš„ç›®æ ‡å‡½æ•°ä¸º ï¼š \\[\\underset{(C_1,\\ldots,C_K)}{\\arg \\min}\\sum_{k=1}^K\\sum_{\\mathbf{x}_i\\in C_k\\cap\\mathcal{X}}d(\\mathbf{\\mu}_k,\\mathbf{x}_i)=\\underset{(C_1,\\ldots,C_K)}{\\arg \\min}\\sum_{k=1}^K\\sum_{\\mathbf{x}_i\\in C_k\\cap\\mathcal{X}}||\\mathbf{\\mu}_k-\\mathbf{x}_i||_2^2\\] å…¶ä¸­\\(\\mathbf{\\mu}_k\\)ä¸ºç±»å‡å€¼å‘é‡ï¼Œå› æ­¤ç›®æ ‡å‡½æ•°è¡¡é‡äº†ç±»å†…æ ·æœ¬ç‚¹å›´ç»•ç±»å‡å€¼å‘é‡çš„ç´§å¯†ç¨‹åº¦ï¼Œå…¶å€¼è¶Šå°æ„å‘³ç€ç±»å†…æ ·æœ¬ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œèšç±»æ•ˆæœè¶Šå¥½ã€‚ä½†æ˜¯ä¸Šè¿°ç›®æ ‡å‡½æ•°å¹¶ä¸å®¹æ˜“æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œè¿™éœ€è¦è€ƒè™‘\\(n\\)ä¸ªæ ·æœ¬ç‚¹æ‰€æœ‰å¯èƒ½çš„ç±»åˆ’åˆ†ï¼Œå› æ­¤K-meansç®—æ³•é‡‡ç”¨äº†è´ªå¿ƒç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–æ¥è¿‘ä¼¼æ±‚è§£ä¸Šè¿°ç›®æ ‡å‡½æ•°ã€‚ K-meansç®—æ³•: é€‰æ‹©åˆå§‹èšç±»ä¸­å¿ƒ\\(\\mathbf{\\mu}_k^{(0)}\\)å’Œèšç±»ä¸ªæ•°\\(K\\)ï¼› è¿­ä»£(ç»ˆæ­¢æ¡ä»¶ï¼šé™¤éç±»å‡å€¼å‘é‡\\(\\mathbf{\\mu}_k^{(t-1)}\\)ä¸å†æ›´æ–°æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°) (1)è®¡ç®—\\(n\\)ä¸ªæ ·æœ¬ç‚¹\\(x_i\\)ä¸å‰ä¸€è½®å‡å€¼å‘é‡\\(\\mathbf{\\mu}_k^{(t)}\\)çš„è·ç¦»ï¼Œæ ¹æ®è·ç¦»æœ€è¿‘åŸåˆ™é‡æ–°åˆ†é…æ‰€æœ‰æ ·æœ¬ç‚¹ï¼Œå³\\[C_k^{(t)}(x_i)=\\underset{\\mathbf{k}\\in\\mathcal{K}}{\\arg\\min}||\\mathbf{\\mu}_k^{(t-1)}-\\mathbf{x}_i||_2^2\\] (2)åŸºäº\\(C_k^{(t)}\\)æ›´æ–°ç±»å‡å€¼å‘é‡\\(\\mathbf{\\mu}_k^{(t)}\\) K-meansä¸­å­˜åœ¨çš„é—®é¢˜ å¯¹åˆå§‹èšç±»ä¸­å¿ƒæ•æ„Ÿã€‚é€‰æ‹©ä¸åŒçš„èšç±»ä¸­å¿ƒä¼šäº§ç”Ÿä¸åŒçš„èšç±»ç»“æœå’Œä¸åŒçš„å‡†ç¡®ç‡ï¼›è‹¥éšæœºæŒ‡å®šåˆå§‹èšç±»ä¸­å¿ƒï¼Œå½“åˆå§‹æŒ‡å®šçš„ä¸¤ä¸ªèšç±»ä¸­å¿ƒåœ¨åŒä¸€ç±»ä¸­æˆ–å¾ˆæ¥è¿‘ï¼Œèšç±»ç»“æœå¾ˆéš¾åŒºåˆ†ã€‚æ­¤å¤„å¯ä»¥ä½¿ç”¨ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼šå…ˆæŒ‡å®šk=2æ‰§è¡Œk-means(åˆå§‹èšç±»ä¸­å¿ƒéšæœºæŒ‡å®š)ï¼Œç»“æœå¾—åˆ°ä¸¤ä¸ªèšç±»ä¸­å¿ƒï¼ŒæŠŠå®ƒä»¬ä½œä¸ºk=3æ—¶çš„å…¶ä¸­ä¸¤ä¸ªåˆå§‹èšç±»ä¸­å¿ƒï¼Œå‰©ä¸‹ä¸€ä¸ªåˆå§‹èšç±»ä¸­å¿ƒéšæœºæŒ‡å®š(å¯ä»¥ä½¿ç”¨åˆ—å‡å€¼)ï¼Œä»¥æ­¤ç±»æ¨ã€‚ K-meansèšç±»ç”±äºä½¿ç”¨ç±»å‡å€¼å‘é‡ä½œä¸ºèšç±»ä¸­å¿ƒï¼Œå› æ­¤å¯¹ç¦»ç¾¤ç‚¹éå¸¸æ•æ„Ÿ å› ä¸ºK-meansç®—æ³•ä¸»è¦é‡‡ç”¨æ¬§å¼è·ç¦»å‡½æ•°åº¦é‡ç±»é—´ç›¸ä¼¼åº¦ï¼Œå¹¶ä¸”é‡‡ç”¨è¯¯å·®å¹³æ–¹å’Œä½œä¸ºç›®æ ‡å‡½æ•°ï¼Œå› æ­¤é€šå¸¸åªèƒ½å‘ç°æ•°æ®åˆ†å¸ƒæ¯”è¾ƒå‡åŒ€çš„çƒçŠ¶ç±»ã€‚ # initialize Kaverage &lt;- colMeans(X) K0 &lt;- 10 TWCD &lt;- array(NA, c(K0)) # total within-cluster dissimilarity Classifier &lt;- array(1, c(K0, nrow(X))) (TWCD[1] &lt;- sum(colSums(as.matrix(X^2)))) # run K-means algorithm set.seed(100) for (K in 2:K0){ if (K==2){(K_res &lt;- kmeans(X,K) )} if (K&gt;2){(K_res &lt;- kmeans(X,K_centers) )} TWCD[K] &lt;- sum(K_res$withins) Classifier[K,] &lt;- K_res$cluster K_centers &lt;- array(NA, c(K+1, ncol(X))) K_centers[K+1,] &lt;- Kaverage K_centers[1:K,] &lt;- K_res$centers } # plot losses xtitle &lt;- &quot;decrease in total within-cluster dissimilarity &quot; plot(x=c(1:K0), y=TWCD, ylim=c(0, max(TWCD)), main=list(xtitle, cex=1.5), col=&quot;blue&quot;, cex=1.5, pch=20, ylab=&quot;total within-cluster dissimilarity&quot;, xlab=&quot;hyperparameter K&quot;, cex.lab=1.5) lines(x=c(1:K0), y=TWCD, col=&quot;blue&quot;, lty=3) # singular value decomposition SVD &lt;- svd(as.matrix(X)) pca &lt;- c(1,2) dat3 &lt;- d.data dat3$v1 &lt;- as.matrix(X) %*% SVD$v[,pca[1]] dat3$v2 &lt;- as.matrix(X) %*% SVD$v[,pca[2]] lim0 &lt;- 7 plot(x=dat3$v1, y=dat3$v2, col=&quot;orange&quot;,pch=20, ylim=c(-lim0,lim0), xlim=c(-lim0,lim0), ylab=paste(&quot;principal component &quot;, pca[2], sep=&quot;&quot;),xlab=paste(&quot;principal component &quot;, pca[1], sep=&quot;&quot;),, main=list(&quot;K-means vs. PCA&quot;, cex=1.5), cex.lab=1.5) dat0 &lt;- dat3[which(Classifier[4,]==4),] points(x=dat0$v1, y=dat0$v2, col=&quot;blue&quot;,pch=20) dat0 &lt;- dat3[which(Classifier[4,]==1),] points(x=dat0$v1, y=dat0$v2, col=&quot;red&quot;,pch=20) dat0 &lt;- dat3[which(Classifier[4,]==3),] points(x=dat0$v1, y=dat0$v2, col=&quot;magenta&quot;,pch=20) legend(&quot;bottomleft&quot;, c(&quot;cluster 1&quot;, &quot;cluster 2&quot;, &quot;cluster 3&quot;, &quot;cluster 4&quot;), col=c(&quot;red&quot;, &quot;orange&quot;, &quot;magenta&quot;, &quot;blue&quot;), lty=c(-1,-1,-1,-1), lwd=c(-1,-1,-1,-1), pch=c(20,20,20,20)) 9.5 K-medoids clustering (PAM) K-meansç®—æ³•ä½¿ç”¨ç±»å‡å€¼å‘é‡ä½œä¸ºèšç±»ä¸­å¿ƒï¼Œå› æ­¤å¯¹ç¦»ç¾¤ç‚¹éå¸¸æ•æ„Ÿï¼Œå¦‚æœå…·æœ‰æå¤§å€¼ï¼Œå¯èƒ½å¤§å¹…åº¦åœ°æ‰­æ›²æ•°æ®çš„åˆ†å¸ƒã€‚K-mediodsç®—æ³•ä½¿ç”¨æ ·æœ¬ç‚¹ä½œä¸ºèšç±»ä¸­å¿ƒï¼Œä¿®æ­£èšç±»ä¸­å¿ƒæ˜¯è®¡ç®—å½“å‰ç±»éèšç±»ä¸­å¿ƒç‚¹åˆ°å…¶ä»–æ‰€æœ‰ç‚¹çš„æœ€å°å€¼æ¥æ›´æ–°èšç±»ä¸­å¿ƒï¼ŒåŒæ—¶å¯ä»¥ä½¿ç”¨Manhattanè·ç¦»ï¼Œå¯ä»¥æœ‰æ•ˆå‰Šå¼±ç¦»ç¾¤ç‚¹çš„å½±å“ã€‚æœ¬ä¾‹ä¸­K-medoidsèšç±»ä½¿ç”¨Manhattanè·ç¦»,(\\(L1\\)èŒƒå¼å¯¹ç¦»ç¾¤ç‚¹çš„æƒ©ç½šæƒé‡æ¯”æ¬§å¼è·ç¦»å°)ï¼Œç»“æœæ˜¾ç¤ºå…¶èšç±»ä¸­å¿ƒè¦æ¯”K-meansèšç±»æ›´åŠ èšé›†ä¸€äº›ã€‚ PAMç®—æ³•ï¼š é€‰æ‹©åˆå§‹èšç±»ä¸­å¿ƒ\\(c_1,c_2,\\ldots,c_k\\in\\mathcal{X}\\)å’Œèšç±»ä¸ªæ•°\\(K\\)ï¼Œè®¡ç®—\\(TWCD\\)ï¼› \\[TWCD=\\sum_{k=1}^K\\sum_{x_i\\in{C_k}\\cap\\mathcal{X}}d(c_k),x_i\\] è¿­ä»£(ç»ˆæ­¢æ¡ä»¶ï¼š\\(TWCD\\)ä¸å†å‡å°‘) (1)éå†æ¯ä¸ªéèšç±»ä¸­å¿ƒ\\(x_i\\)æ›¿æ¢èšç±»ä¸­å¿ƒç‚¹\\(c_k^{t-1}\\)ï¼Œæ ¹æ®è·ç¦»æœ€è¿‘åŸåˆ™é‡æ–°åˆ†é…å„æ ·æœ¬ç‚¹ï¼Œè®¡ç®—\\(TWCD\\); (2)æ ¹æ®\\(TWCD\\)æœ€å°åŸåˆ™æ›´æ–°èšç±»ä¸­å¿ƒ\\(c_k^{t}\\)ã€‚ set.seed(100) (K_res &lt;- pam(X, k=4, metric=&quot;manhattan&quot;, diss=FALSE)) # plot K-medoids versus PCA plot(x=dat3$v1, y=dat3$v2, col=&quot;orange&quot;,pch=20, ylim=c(-lim0,lim0), xlim=c(-lim0,lim0), ylab=paste(&quot;principal component &quot;, pca[2], sep=&quot;&quot;),xlab=paste(&quot;principal component &quot;, pca[1], sep=&quot;&quot;),, main=list(&quot;K-medoids vs. PCA&quot;, cex=1.5), cex.lab=1.5) dat0 &lt;- dat3[which(K_res$cluster==4),] points(x=dat0$v1, y=dat0$v2, col=&quot;red&quot;,pch=20) dat0 &lt;- dat3[which(K_res$cluster==3),] points(x=dat0$v1, y=dat0$v2, col=&quot;blue&quot;,pch=20) dat0 &lt;- dat3[which(K_res$cluster==2),] points(x=dat0$v1, y=dat0$v2, col=&quot;magenta&quot;,pch=20) points(x=dat3[K_res$id.med,&quot;v1&quot;],y=dat3[K_res$id.med,&quot;v2&quot;], col=&quot;black&quot;,pch=20, cex=2) legend(&quot;bottomleft&quot;, c(&quot;cluster 1&quot;, &quot;cluster 2&quot;, &quot;cluster 3&quot;, &quot;cluster 4&quot;), col=c(&quot;red&quot;, &quot;orange&quot;, &quot;magenta&quot;, &quot;blue&quot;), lty=c(-1,-1,-1,-1), lwd=c(-1,-1,-1,-1), pch=c(20,20,20,20)) 9.6 Gaussian mixture models(GMMs) K-meanså‡è®¾æ•°æ®ç‚¹æ˜¯çƒçŠ¶çš„ï¼ŒGMMså‡è®¾æ•°æ®ç‚¹æ˜¯å‘ˆé«˜æ–¯åˆ†å¸ƒï¼Œæä¾›äº†æ›´å¤šçš„å¯èƒ½æ€§ã€‚å¯¹\\(n\\)ç»´æ ·æœ¬ç©ºé—´\\(\\mathcal{X}\\)ä¸­çš„éšæœºå‘é‡\\(x\\)ï¼Œè‹¥\\(x\\)æœä»é«˜æ–¯åˆ†å¸ƒï¼Œ\\(\\mu\\)æ˜¯\\(n\\)ç»´å‡å€¼å‘é‡ï¼Œ\\(\\Sigma\\)æ˜¯\\(n\\times n\\) çš„åæ–¹å·®çŸ©é˜µï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š \\[p(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)\\Sigma^{-1}(x-\\mu)}\\] ç”±æ­¤å¯å®šä¹‰é«˜æ–¯æ··åˆåˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒç”±\\(k\\)ä¸ªæ··åˆæˆåˆ†ç»„æˆï¼Œæ¯ä¸ªæ··åˆæˆåˆ†å¯¹åº”ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­\\(\\mu_i\\)ä¸\\(\\Sigma_i\\)æ˜¯ç¬¬\\(i\\)ä¸ªé«˜æ–¯æ··åˆæˆåˆ†çš„å‚æ•°ï¼Œ\\(\\alpha_i\\)ä¸ºç›¸åº”çš„æ··åˆç³»æ•°(\\(\\alpha_i&gt;0,\\sum_{i=1}^k\\alpha_i=1\\)) \\[p_{\\mathcal{M}}=\\sum_{i=1}^k\\alpha_i\\times p(x|\\mu_i,\\Sigma_i)\\] è‹¥è®­ç»ƒé›†\\(D={x_1,x_2,\\dots,x_m}\\)ç”±ä¸Šè¿°è¿‡ç¨‹ç”Ÿæˆï¼Œä»¤éšæœºå˜é‡\\(z_j\\in{1,2,\\dots,k}\\)è¡¨ç¤ºç”Ÿæˆæ ·æœ¬\\(x_j\\)çš„é«˜æ–¯æ··åˆæˆåˆ†ï¼Œæ¢å¥è¯è¯´ï¼Œæ ·æœ¬\\(x_j\\)å±äºç¬¬\\(z_j\\)ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œ\\(z_j\\)çš„å…ˆéªŒæ¦‚ç‡\\(p(x_j=i)=\\alpha_i(i=1,2,\\dots,k)\\)ã€‚æ ¹æ®è´å¶æ–¯å®šç†ï¼Œåˆ™\\(z_j\\)çš„åéªŒåˆ†å¸ƒ(è¡¨ç¤ºæ ·æœ¬\\(x_j\\)ç”±ç¬¬\\(i\\)ä¸ªé«˜æ–¯æ··åˆæˆåˆ†ç”Ÿæˆçš„åéªŒæ¦‚ç‡\\(\\gamma_ji\\))ä¸ºï¼š \\[\\gamma_{ji}=p_{\\mathcal{M}}(z_j=i|x_j)=\\frac{p(z_j=i)\\times p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{a_i\\times p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{i=1}^k\\alpha_l\\times p(x_j|\\mu_l,\\Sigma_l)}\\] å½“é«˜æ–¯æ··åˆæˆä¸å·²çŸ¥æ—¶ï¼Œé«˜æ–¯æ··åˆèšç±»å°†æŠŠæ ·æœ¬é›†\\(D\\)åˆ’åˆ†ä¸º\\(k\\)ä¸ªç±»\\(C=\\{C_1,C_2,\\dots,C_k\\}\\)ï¼Œåˆ™æ¯ä¸ªæ ·æœ¬\\(x_j\\)çš„ç±»æ ‡è®°\\(\\gamma_j\\)ä½œå¦‚ä¸‹ç¡®å®šï¼š \\[\\gamma_j=\\underset{i\\in\\{1,2,\\dots,k\\}}{\\arg \\min}\\gamma_{ji}\\] å¯¹ç»™å®šæ ·æœ¬é›†\\(D\\)ï¼Œå¯é‡‡ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ±‚è§£æ¨¡å‹å‚æ•°\\(\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1 \\le i \\le k\\}\\)ï¼Œä¼¼ç„¶å‡½æ•° \\[LL(D)=ln\\left(\\prod_{j=1}^m p_{\\mathcal{M}(x_j)}\\right)=\\sum_{j=1}^m ln\\left(\\sum_{i=1}^k \\alpha_i \\times p(x_j|\\mu_i,\\Sigma_i) \\right)\\] ä»¤\\(\\frac{\\partial{LL(D)}}{\\partial(\\mu_i)}=0\\)ï¼Œå¾—åˆ°\\(\\mu_i=\\frac{\\sum_{j=1}^{m}\\gamma_{ji}x_j}{\\sum_{j=1}^m\\gamma_{ji}}\\) ä»¤\\(\\frac{\\partial{LL(D)}}{\\partial(\\Sigma_i)}=0\\)ï¼Œå¾—åˆ°\\(\\Sigma_i=\\frac{\\sum_{j=1}^{m}\\gamma_{ji}(x_j-\\mu_i)(x_j-\\mu_i)^T}{\\sum_{j=1}^m\\gamma_{ji}}\\) å¯ä»¥çœ‹åˆ°å„æ··åˆæˆåˆ†çš„å‡å€¼\\(\\mu_i\\)å’Œåæ–¹å·®é˜µ\\(\\Sigma_i\\)å¯é€šè¿‡æ ·æœ¬åŠ æƒå¹³å‡æ¥ä¼°è®¡ï¼Œæ ·æœ¬æƒé‡æ˜¯æ¯ä¸ªæ ·æœ¬å±äºè¯¥æˆåˆ†çš„åéªŒæ¦‚ç‡ã€‚ å¯¹äºæ··åˆç³»æ•°\\(\\alpha_i\\)ï¼Œé™¤äº†è¦æœ€å¤§åŒ–\\(LL(D)\\)ï¼Œè¿˜éœ€è¦æ»¡è¶³\\(\\alpha_i&gt;0,\\sum_{i=1}^k\\alpha_i=1\\)ï¼Œå¯ä»¥ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥æ¡ä»¶æå€¼æ±‚è§£ï¼Œå…¶ä¸­\\(\\lambda\\)ä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜å­ã€‚ \\[LLF(D)=LL(D)=\\lambda\\left(\\sum_{i=1}^k\\alpha_i-1\\right)\\] ä»¤\\(\\frac{\\partial{LLF(D)}}{\\partial(\\alpha_i)}=0\\)ï¼Œå¾—åˆ°\\(\\alpha_i=\\frac{\\sum_{j=1}^{m}\\gamma_{ji}}{m}\\) å¯ä»¥çœ‹åˆ°å„é«˜æ–¯æˆåˆ†\\(\\alpha_i\\)çš„æ··åˆç³»æ•°ç”±æ ·æœ¬å±äºè¯¥æˆåˆ†çš„å¹³å‡åéªŒæ¦‚ç‡ç¡®å®šã€‚ å› ä¸º\\(z_j\\in\\{1,2\\dots,k\\}\\)æ˜¯éšå˜é‡(LatentVariable)ï¼Œæ— æ³•è§‚æµ‹ï¼Œä¸€èˆ¬é‡‡ç”¨EMç®—æ³•è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚EMç®—æ³•ï¼š ç¡®å®šåˆå§‹é«˜æ–¯æ··åˆåˆ†å¸ƒçš„æ¨¡å‹å‚æ•°\\(\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1 \\le i \\le k\\}\\)å’Œèšç±»ä¸ªæ•°\\(K\\). è¿­ä»£(ç»ˆæ­¢æ¡ä»¶ï¼šä¼¼ç„¶å‡½æ•°\\(LL(D)\\)å¢é•¿å¾ˆå°‘ç”šè‡³ä¸å†å¢é•¿æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°) (1)æ ¹æ®å‚æ•°\\(\\Theta^{t-1}=\\{(\\alpha_i^{t-1},\\mu_i^{t-1},\\Sigma_i^{t-1})|1 \\le i \\le k\\}\\)ç¡®å®š\\(\\gamma_{ji}\\); (2)æ ¹æ®\\(\\gamma_{ji}\\)æ›´æ–°å‚æ•°\\(\\Theta^{t}=\\{(\\alpha_i^t,\\mu_i^t,\\Sigma_i^t)|1 \\le i \\le k\\}\\) æœ€åæ ¹æ®\\(\\gamma_j=\\underset{i\\in\\{1,2,\\dots,k\\}}{\\arg \\min}\\gamma_{ji}\\)ç¡®å®šå„æ ·æœ¬\\(x_j\\)æ‰€å±ç±» seed &lt;- 100 set.seed(seed) K_res &lt;- GMM(X, gaussian_comps=4, dist_mode=&quot;eucl_dist&quot;, seed_mode=&quot;random_subset&quot;, em_iter=5, seed=seed) summary(K_res) clust &lt;- predict_GMM(X, K_res$centroids, K_res$covariance_matrices, K_res$weights)$cluster_labels pred &lt;- predict_GMM(X, K_res$centroids, K_res$covariance_matrices, K_res$weights) names(pred) pred$cluster_labels[1:5] pred$cluster_proba[1:5,] K_res$centroids # singular value decomposition SVD &lt;- svd(as.matrix(X)) pca &lt;- c(1,2) dat3 &lt;- d.data dat3$v1 &lt;- as.matrix(X) %*% SVD$v[,pca[1]] dat3$v2 &lt;- as.matrix(X) %*% SVD$v[,pca[2]] (kk1 &lt;- K_res$centroids %*% SVD$v[,pca[1]]) (kk2 &lt;- K_res$centroids %*% SVD$v[,pca[2]]) lim0 &lt;- 7 plot(x=dat3$v1, y=dat3$v2, col=&quot;orange&quot;,pch=20, ylim=c(-lim0,lim0), xlim=c(-lim0,lim0), ylab=paste(&quot;principal component &quot;, pca[2], sep=&quot;&quot;),xlab=paste(&quot;principal component &quot;, pca[1], sep=&quot;&quot;),, main=list(&quot;GMM(diagonal) vs. PCA&quot;, cex=1.5), cex.lab=1.5) dat0 &lt;- dat3[which(clust==0),] points(x=dat0$v1, y=dat0$v2, col=&quot;red&quot;,pch=20) dat0 &lt;- dat3[which(clust==3),] points(x=dat0$v1, y=dat0$v2, col=&quot;blue&quot;,pch=20) dat0 &lt;- dat3[which(clust==1),] points(x=dat0$v1, y=dat0$v2, col=&quot;magenta&quot;,pch=20) points(x=kk1,y=kk2, col=&quot;black&quot;,pch=20, cex=2) legend(&quot;bottomleft&quot;, c(&quot;cluster 1&quot;, &quot;cluster 2&quot;, &quot;cluster 3&quot;, &quot;cluster 4&quot;), col=c(&quot;red&quot;, &quot;orange&quot;, &quot;magenta&quot;, &quot;blue&quot;), lty=c(-1,-1,-1,-1), lwd=c(-1,-1,-1,-1), pch=c(20,20,20,20)) 9.7 ä¸‰ç§èšç±»æ–¹æ³•è¯„ä»· å›¾9.11å±•ç¤ºäº†èšç±»çš„ç»“æœï¼ŒGMMsæœ€å¥½ã€‚ set.seed(100) #K-meansèšç±»(æ¬§å¼è·ç¦») K_means &lt;- kmeans(X, 4) #çš„K-medoidsèšç±»(æ›¼å“ˆé¡¿è·ç¦») K_medoids &lt;- pam(X, k = 4, metric = &quot;manhattan&quot;) #GMMèšç±»(æ¬§å¼è·ç¦») K_gmm &lt;- GMM(X, gaussian_comps = 4, dist_mode = &quot;eucl_dist&quot;, seed_mode = &quot;random_subset&quot;, em_iter= 5,seed = 100) clust &lt;- predict_GMM(X, K_gmm$centroids, K_gmm$covariance_matrices, K_gmm$weights)$cluster_labels #k-means cluster &lt;- c(&#39;cluster1&#39;,&#39;cluster2&#39;,&#39;cluster3&#39;,&#39;cluster4&#39;) cars &lt;- function(x,y) nrow(dat3[which(y == x),]) sports &lt;- function(x,y) nrow(dat3[which(y == x &amp; dat3$sports_car == 1),]) K_means &lt;- cbind.data.frame(cluster, &quot;cars&quot; = c(cars(K_means$cluster,2), cars(K_means$cluster,4), cars(K_means$cluster,3), cars(K_means$cluster,1)), &quot;sports cars&quot; = c(sports(K_means$cluster,2), sports(K_means$cluster,4), sports(K_means$cluster,3), sports(K_means$cluster,1))) K_medoids &lt;- cbind.data.frame(cluster, &quot;cars&quot; = c(cars(K_medoids$clustering,4), cars(K_medoids$clustering,4), cars(K_medoids$clustering,2), cars(K_medoids$clustering,1)), &quot;sports cars&quot; = c(sports(K_medoids$clustering,4), sports(K_medoids$clustering,3), sports(K_medoids$clustering,2), sports(K_medoids$clustering,1))) GMMs &lt;- cbind.data.frame(cluster, &quot;cars&quot; = c(cars(clust,0),cars(clust,3), cars(clust,1),cars(clust,2)), &quot;sports cars&quot; = c(sports(clust,0),sports(clust,3), sports(clust,1),sports(clust,2))) k_means &lt;- reshape2::melt(K_means, id.vars = &quot;cluster&quot;, variable.name = &quot;k_means&quot;) k_medoids &lt;- reshape2::melt(K_medoids, id = &quot;cluster&quot;, variable.name = &quot;k_medoids&quot;) GMMs &lt;- reshape2::melt(GMMs, id = &quot;cluster&quot;, variable.name = &quot;GMMs&quot;) #ç»˜åˆ¶ä¸‰ç§èšç±»æ–¹æ³•èšç±»ç»“æœä¸­è·‘è½¦æ ·æœ¬ä¸çœŸå®è·‘è½¦æ ·æœ¬å¯¹æ¯”å›¾ myggplot &lt;- function(mydf,myxcol,myycol,myfill) { ggplot2::ggplot(data=mydf,aes(x = {{myxcol}}, y = {{myycol}}, fill = {{myfill}}))+ geom_bar(stat=&quot;identity&quot;,position = &quot;dodge&quot;)+ geom_text(aes(label = value), position = position_dodge((1)), size = 3, vjust = -0.5)+ theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.title.x = element_blank())+ labs(y = &quot;number&quot;)+ ylim(0,250) } #ç»„åˆå›¾å½¢ multiplot(myggplot(k_means,cluster,value,k_means), myggplot(k_medoids,cluster,value,k_medoids), myggplot(GMMs,cluster,value,GMMs)) Figure 9.11: èšç±»æ¯”è¾ƒ 9.8 t-SNE ç®€ä»‹ tåˆ†å¸ƒ-éšæœºé‚»è¿‘åµŒå…¥(t-SNE, t-distributed stochastic neighbor embedding)ç”± Laurens van der Maatenå’ŒGeoffrey Hintonåœ¨2008å¹´æå‡ºã€‚t-SNEæœ¬è´¨æ˜¯ä¸€ç§åµŒå…¥æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†é«˜ç»´ç©ºé—´ä¸­çš„æ•°æ®æ˜ å°„åˆ°ä½ç»´ç©ºé—´ä¸­ï¼Œå¹¶ä¿ç•™æ•°æ®é›†çš„å±€éƒ¨ç‰¹æ€§ã€‚ åŸºæœ¬åŸç† t-SNEå°†æ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦è½¬åŒ–ä¸ºæ¡ä»¶æ¦‚ç‡ï¼ŒåŸå§‹ç©ºé—´ä¸­æ•°æ®ç‚¹çš„ç›¸ä¼¼åº¦ç”±é«˜æ–¯è”åˆåˆ†å¸ƒè¡¨ç¤ºï¼ŒåµŒå…¥ç©ºé—´ä¸­æ•°æ®ç‚¹çš„ç›¸ä¼¼åº¦ç”±tåˆ†å¸ƒè¡¨ç¤ºã€‚ å°†åŸå§‹ç©ºé—´å’ŒåµŒå…¥ç©ºé—´çš„è”åˆæ¦‚ç‡åˆ†å¸ƒçš„KLæ•£åº¦ä½œä¸ºæŸå¤±å‡½æ•°(loss function)ï¼Œè¯„ä¼°åµŒå…¥æ•ˆæœçš„å¥½åã€‚é€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œæœ€ç»ˆè·å¾—æ”¶æ•›ç»“æœã€‚ å…·ä½“è¿‡ç¨‹ åŸå§‹ç©ºé—´ æ„å»ºä¸€ä¸ªé«˜ç»´å¯¹è±¡é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿å¾—ç›¸ä¼¼çš„å¯¹è±¡æœ‰æ›´é«˜çš„æ¦‚ç‡è¢«é€‰æ‹©ï¼Œè€Œä¸ç›¸ä¼¼çš„å¯¹è±¡æœ‰è¾ƒä½çš„æ¦‚ç‡è¢«é€‰æ‹©ã€‚\\(q\\)ç»´ç©ºé—´ä¸­ç»™å®šä¸€ç»„æ•°æ®\\(x_1,â€¦,x_n\\)ã€‚ å®šä¹‰æ¡ä»¶æ¦‚ç‡ï¼š \\[ q_{j|i}=\\frac{exp\\left\\{-\\frac{1}{2\\sigma_i^2}||x_i-x_j||_2^2\\right\\}}{\\sum_{k\\ne i}exp\\left\\{-\\frac{1}{2\\sigma_i^2}||x_i-x_j||_2^2\\right\\}},\\ for\\ i\\ne j \\] å®šä¹‰è”åˆæ¦‚ç‡åˆ†å¸ƒï¼š \\[ q_{i,j}=\\frac{1}{2n}(q_{j|i}+q_{i|j}),\\ for\\ i\\ne j \\] æ­¤å¼æ—¢ä¿è¯äº†å¯¹ç§°æ€§ï¼Œåˆä½¿å¾—\\(\\sum_jq_{i,j}&gt;\\frac{1}{2n}\\)for all \\(i\\) å›°æƒ‘åº¦(Perplexity) \\(\\sigma_i\\)çš„é€‰æ‹©å¿…é¡»æ»¡è¶³ï¼šåœ¨æ•°æ®å¯†é›†çš„åœ°æ–¹è¦å°ï¼Œæ•°æ®ç¨€ç–çš„åœ°æ–¹è¦å¤§ã€‚ å¯¹äº\\(\\sigma_i\\)ï¼Œä¸€ä¸ªå¥½çš„åˆ†é…åº”ä½¿å¾—å›°æƒ‘åº¦ä¸ºå¸¸æ•°ã€‚ \\[ Perp(q_{.|i})=exp\\left\\{H(q_{.|i})\\right\\}=exp\\left\\{-\\sum_{j\\ne i}q_{j|i}log_2(q_{j|i})\\right\\} \\] å›°æƒ‘åº¦å¯ç†è§£ä¸ºå¯¹æ¯ä¸ªç‚¹é‚»å±…æ•°é‡çš„çŒœæµ‹ï¼Œå¯¹æœ€ç»ˆæˆå›¾æœ‰ç€å¤æ‚çš„å½±å“ã€‚ä½å›°æƒ‘åº¦å¯¹åº”çš„æ˜¯å±€éƒ¨è§†è§’ï¼›é«˜å›°æƒ‘åº¦å¯¹åº”çš„æ˜¯å…¨å±€è§†è§’ã€‚ åµŒå…¥ç©ºé—´ åœ¨\\(p\\)ç»´ç©ºé—´ä¸­(\\(p&lt;q\\))æ‰¾åˆ°ä¸€ç»„æ•°æ®ç‚¹\\(y_1,â€¦,y_n\\)ï¼Œä½¿å¾—è¿™ç»„æ•°æ®ç‚¹æ„å»ºçš„è”åˆæ¦‚ç‡åˆ†å¸ƒ\\(p\\)å°½å¯èƒ½åœ°ä¸é«˜ç»´ç©ºé—´ä¸­çš„è”åˆæ¦‚ç‡åˆ†å¸ƒ\\(q\\)ç›¸ä¼¼ã€‚ \\[ p_{i,j}=\\frac{(1+||y_i-y_j||_2^2)^{-1}}{\\sum_{k\\ne l}(1+||y_k-y_l||_2^2)^{-1}},\\ for\\ i\\ne j \\] t(1)åˆ†å¸ƒçš„é€‰æ‹©ï¼šè§£å†³æ‹¥æŒ¤é—®é¢˜ï¼ˆé«˜ç»´ç©ºé—´ä¸­åˆ†ç¦»çš„ç°‡ï¼Œåœ¨ä½ç»´ä¸­è¢«åˆ†çš„ä¸æ˜æ˜¾ï¼‰ Figure 9.12: t-distribution å›¾9.12å±•ç¤ºäº†ä¸åŒè‡ªç”±åº¦ä¸‹çš„tåˆ†å¸ƒçš„å¯†åº¦å‡½æ•°å›¾åƒï¼Œå¯ä»¥çœ‹å‡ºï¼Œè‡ªç”±åº¦è¶Šå°ï¼Œtåˆ†å¸ƒçš„å°¾éƒ¨è¶Šåšã€‚ \\[ p_{i,j}\\approx||y_i-y_j||^{-2}_2\\ for\\ ||y_i-y_j||_2\\rightarrow\\infty \\] é™ç»´çš„æ•ˆæœç”¨ä¸¤åˆ†å¸ƒé—´çš„KLæ•£åº¦(Kullback-Leibler divergences)åº¦é‡ \\[ D_{KL}(q||p)=\\sum_{j=1}^Jq_jlog\\frac{q_j}{p_j} \\] library(tsne) perp = c(10,20,30,40,50,60) #å›°æƒ‘åº¦ par(mfrow=c(2,3)) for (i in 1:6){ set.seed(100) tsne = tsne(X, k=2, initial_dim=ncol(X), perplexity=perp[i]) tsne1 = tsne[,c(2,1)] plot(tsne1,col=&quot;blue&quot;,pch=20, ylab=&quot;component 1&quot;,xlab=&quot;component 2&quot;, main=list(paste(&quot;t-SNE with perplexity &quot;, perp[i], sep=&quot;&quot;))) points(tsne1[which(d.data$tau&lt;21),], col=&quot;green&quot;,pch=20) points(tsne1[which(d.data$tau&lt;17),], col=&quot;red&quot;,pch=20) } Figure 9.13: t-SNE å›¾9.13å±•ç¤ºäº†å›°æƒ‘åº¦ï¼ˆ10-60ï¼‰å¯¹é™ç»´ç»“æœçš„å½±å“ï¼Œå¯ä»¥çœ‹å‡ºï¼Œå›°æƒ‘åº¦ä¸º30æ—¶ï¼Œé™ç»´çš„æ•ˆæœæœ€ä½³ã€‚ 9.9 UMAP ç®€ä»‹ ç»Ÿä¸€æµå½¢é€¼è¿‘ä¸æŠ•å½±(UMAP, Uniform Manifold Approximation and Projection)æ˜¯å»ºç«‹åœ¨é»æ›¼å‡ ä½•å’Œä»£æ•°æ‹“æ‰‘ç†è®ºæ¡†æ¶ä¸Šçš„æ–°çš„é™ç»´æµå½¢å­¦ä¹ æŠ€æœ¯ã€‚åœ¨å¯è§†åŒ–è´¨é‡æ–¹é¢ï¼ŒUMAPç®—æ³•ä¸t-SNEå…·æœ‰ç«äº‰ä¼˜åŠ¿ï¼Œä½†æ˜¯å®ƒä¿ç•™äº†æ›´å¤šå…¨å±€ç»“æ„ã€å…·æœ‰ä¼˜è¶Šçš„è¿è¡Œæ€§èƒ½ã€æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚ Figure 9.14: manifold å›¾9.14ä¸­ä¸¤ä¸ªé»‘ç‚¹ï¼Œè‹¥è€ƒè™‘ç›´çº¿è·ç¦»ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªé»‘ç‚¹ä¹‹é—´è·ç¦»å¾ˆç›¸è¿‘ï¼›å¦‚æœæ”¾åˆ°æµå½¢å­¦ä¸Šï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªç‚¹è·ç¦»å°±å¾—æ²¿ç€å›¾ä¸­æ›²çº¿ç»•ä¸¤åœˆã€‚ åŸºæœ¬åŸç† è®¡ç®—é«˜ç»´çš„æµå½¢ç»“æ„ç‰¹å¾ï¼Œç¡®å®šé«˜ç»´ç©ºé—´ä¸­å„ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œä»è€Œæ„é€ é«˜ç»´çš„æ•°æ®åˆ†å¸ƒç»“æ„ã€‚ å°†å®ƒä»¬æŠ•å½±åˆ°ä½ç»´ç©ºé—´ï¼Œæ ¹æ®é«˜ç»´ç©ºé—´ç‚¹ä¸ç‚¹ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ï¼Œæå–ç‰¹å¾å€¼ï¼Œåœ¨ä½ç»´ç©ºé—´ä¸­é‡æ„è¿™ç§è·ç¦»å…³ç³»ï¼Œå¹¶è®¡ç®—ä½ç»´ç©ºé—´ä¸­å„ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ã€‚ ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ¥æœ€å°åŒ–è¿™äº›è·ç¦»ä¹‹é—´çš„å·®å¼‚ã€‚ å…·ä½“è¿‡ç¨‹ æ„å»ºé«˜ç»´ç©ºé—´çš„æ¨¡ç³Šæ‹“æ‰‘è¡¨ç¤º \\(q\\)ç»´ç©ºé—´ä¸­ç»™å®šä¸€ç»„æ•°æ®\\(x_1,â€¦,x_n\\)ã€‚å®šä¹‰ï¼š \\[ d:dissimilarity\\ measure\\\\ X_i=\\left\\{x_{i_1},â€¦,x_{i_k}\\right\\}:k\\ nearest\\ neighbors\\ of\\ x_i \\] å¯¹äºæ¯ä¸ª\\(x_i\\)ï¼Œç¡®å®š\\(\\rho_i\\)å’Œ\\(\\sigma_i\\) \\[ \\rho_i=min\\ d(x_i,x_{i_j}),1\\le j\\le k\\\\ \\sum_{j=1}^kexp\\left\\{âˆ’\\frac{d(x_i,x_{i_j})-\\rho_i}{\\sigma_i}\\right\\} =log_2k \\] \\(\\rho_i\\)æ§åˆ¶åµŒå…¥çš„ç´§å¯†ç¨‹åº¦ï¼Œå€¼è¶Šå°ç‚¹è¶Šèšé›†ï¼›\\(\\sigma_i\\)æ§åˆ¶æœ‰æ•ˆçš„åµŒå…¥é™ç»´èŒƒå›´ã€‚ åˆ†å¸ƒè®¡ç®—ï¼š \\[ q_{i|j}=exp\\left\\{âˆ’\\frac{d(x_i,x_{i_j})-\\rho_i}{\\sigma_i}\\right\\}\\\\ q_{i,j}=q_{i|j}+q_{j|i}-q_{i|j}q_{j|i} \\] ç®€å•åœ°ä¼˜åŒ–ä½ç»´è¡¨ç¤ºï¼Œä½¿å…¶å…·æœ‰å°½å¯èƒ½æ¥è¿‘çš„æ¨¡ç³Šæ‹“æ‰‘è¡¨ç¤ºï¼Œå¹¶ç”¨äº¤å‰ç†µæ¥åº¦é‡ã€‚ ä½ç»´ç©ºé—´çš„åˆ†å¸ƒ \\[ p_{ij}=(1+a(y_i-y_j)^{2b})^{âˆ’1} \\] where \\(a\\approx1.93\\) and \\(b\\approx0.79\\) for default UMAP hyperparameters äº¤å‰ç†µä½œä¸ºä»£ä»·å‡½æ•° \\[ CE(X,Y)=\\sum_i\\sum_j\\left\\{q_{ij}(X)ln\\frac{q_{ij}(X)}{p_{ij}(Y)}+(1-q_{ij}(X))ln\\frac{1-q_{ij}(X)}{1-p_{ij}(Y)}\\right\\} \\] å‚æ•°è¯´æ˜ umap configuration parameters value note n_neighbors 15 ç¡®å®šç›¸é‚»ç‚¹çš„æ•°é‡ï¼Œé€šå¸¸å–2-100 n_components 2 é™ç»´çš„ç»´æ•°ï¼Œé»˜è®¤æ˜¯2 metric euclidean è·ç¦»çš„è®¡ç®—æ–¹æ³•ï¼Œå¯é€‰ï¼šeuclidean,manhattan,chebyshev,minkowski,correlation,hammingç­‰ n_epochs 200 æ¨¡å‹è®­ç»ƒè¿­ä»£æ¬¡æ•°ã€‚æ•°æ®é‡å¤§æ—¶200ï¼Œå°æ—¶500 input data æ•°æ®ç±»å‹ï¼Œå¦‚æœæ˜¯dataå°±ä¼šæŒ‰ç…§æ•°æ®è®¡ç®—ï¼›å¦‚æœæ˜¯distå°±ä¼šæŒ‰è·ç¦»çŸ©é˜µè®¡ç®— init spectral åˆå§‹åŒ–ï¼Œæœ‰ä¸‰ç§æ–¹å¼ï¼šspectral,random,è‡ªå®šä¹‰ min_dist 0.1 æ§åˆ¶åµŒå…¥çš„ç´§å¯†ç¨‹åº¦ï¼Œå€¼è¶Šå°ç‚¹è¶Šèšé›†ï¼Œé»˜è®¤0.1 set_op_mix_ratio 1 é™ç»´è¿‡ç¨‹ä¸­ç‰¹å¾çš„ç»“åˆæ–¹å¼ï¼Œå–0-1ã€‚0ä»£è¡¨å–äº¤é›†ï¼Œ1ä»£è¡¨å–åˆé›†ï¼›ä¸­é—´å°±æ˜¯æ¯”ä¾‹ local_connectivity 1 å±€éƒ¨è¿æ¥çš„ç‚¹ä¹‹é—´å€¼ï¼Œé»˜è®¤1ï¼Œå…¶å€¼è¶Šå¤§å±€éƒ¨è¿æ¥è¶Šå¤šï¼Œå¯¼è‡´çš„ç»“æœå°±æ˜¯è¶…è¶Šå›ºæœ‰çš„æµå½¢ç»´æ•°å‡ºç°æ”¹å˜ bandwidth 1 ç”¨äºæ„é€ å­é›†å‚æ•° alpha 1 å­¦ä¹ ç‡ gamma 1 å¸ƒå±€æœ€ä¼˜çš„å­¦ä¹ ç‡ negative_sample_rate 5 æ¯ä¸€ä¸ªé˜³æ€§æ ·æœ¬å¯¼è‡´çš„é˜´æ€§ç‡ã€‚å…¶å€¼è¶Šå¤§å¯¼è‡´é«˜çš„ä¼˜åŒ–ä¹Ÿå°±æ˜¯è¿‡æ‹Ÿåˆï¼Œé¢„æµ‹å‡†ç¡®åº¦ä¸‹é™ã€‚é»˜è®¤æ˜¯5 a NA b NA spread 1 æ§åˆ¶æœ‰æ•ˆçš„åµŒå…¥é™ç»´èŒƒå›´ï¼Œä¸min_distè”åˆä½¿ç”¨ random_state NA éšæœºç§å­ï¼Œç¡®ä¿æ¨¡å‹çš„å¯é‡å¤æ€§ transform_state NA ç”¨äºæ•°å€¼è½¬æ¢æ“ä½œã€‚é»˜è®¤å€¼42 knn NA knn_repeats 1 verbose FALSE æ§åˆ¶å·¥ä½œæ—¥å¿—ï¼Œé˜²æ­¢å­˜å‚¨è¿‡å¤š umap_learn_args NA è°ƒç”¨pythonåŸºäºumap-learnè®­ç»ƒå¥½çš„å‚æ•° library(umap) min_dist = c(0.1,0.5,0.9) k = c(15,50,75,100) sign = matrix(c(1,-1,-1,1,1,1,-1,-1),4,2,byrow = F) par(mfrow=c(3,4)) for (i in 1:3){ for (j in 1:4){ umap.param = umap.defaults umap.param$n_components = 2 umap.param$random_state = 100 umap.param$min_dist = min_dist[i] umap.param$n_neighbors = k[j] umap = umap(X, config=umap.param, method=&quot;naive&quot;) umap1 = matrix() umap$layout[,1] = sign[j,1]*umap$layout[,1] umap$layout[,2] = sign[j,2]*umap$layout[,2] umap1 = umap$layout[,c(2,1)] plot(umap1,col=&quot;blue&quot;,pch=20, ylab=&quot;component 1&quot;, xlab=&quot;component 2&quot;, main=list(paste(&quot;UMAP (k=&quot;,k[j],&quot;, min_dist= &quot;,min_dist[i], &quot;)&quot;,sep=&quot;&quot;))) points(umap1[which(d.data$tau&lt;21),], col=&quot;green&quot;,pch=20) points(umap1[which(d.data$tau&lt;17),], col=&quot;red&quot;,pch=20) } } Figure 9.15: UMAP å›¾9.15å±•ç¤ºäº†é™ç»´ç»“æœä»¥åŠä¸¤å‚æ•°kå’Œmin_distå¯¹é™ç»´ç»“æœçš„å½±å“ - ç«–çœ‹ï¼šmin_distè¶Šå¤§ï¼Œå›¾å½¢ä¸­çš„ç‚¹åˆ†æ•£åœ°è¶Šå‡åŒ€ - æ¨ªçœ‹ï¼škè¶Šå°ï¼Œæ•°æ®çš„æµè¡Œç»“æ„æ˜¾ç¤ºåœ°è¶Šæ˜æ˜¾ 9.10 SOM è‡ªç»„ç»‡æ˜ å°„(Self-organizing map, SOM)æ˜¯ä¸€ç§ç«äº‰å‹ç¥ç»ç½‘ç»œï¼Œç”±è¾“å…¥å±‚å’Œç«äº‰å±‚ï¼ˆå¸¸è§2ç»´ï¼‰æ„æˆã€‚ å›¾9.16å±•ç¤ºäº†SOMçš„ç»“æ„ã€‚ Figure 9.16: SOM è¾“å…¥å±‚ç¥ç»å…ƒçš„æ•°é‡ç”±è¾“å…¥å‘é‡çš„ç»´åº¦å†³å®šï¼Œä¸€ä¸ªç¥ç»å…ƒå¯¹åº”ä¸€ä¸ªç‰¹å¾ ç«äº‰å±‚çš„å¸¸è§ç»“æ„ï¼šçŸ©å½¢(Rectangular)ã€å…­è¾¹å½¢(Hexagonal)ï¼Œå‚è§å›¾9.17ã€‚ Figure 9.17: SOM-com ç«äº‰å±‚ç¥ç»å…ƒçš„æ•°é‡å†³å®šäº†æœ€ç»ˆæ¨¡å‹çš„ç²’åº¦ä¸è§„æ¨¡ï¼Œå¯¹æœ€ç»ˆæ¨¡å‹çš„å‡†ç¡®æ€§ä¸æ³›åŒ–èƒ½åŠ›å½±å“å¾ˆå¤§ã€‚ ç»éªŒå…¬å¼ï¼š\\(N\\ge5\\sqrt{m}\\)ï¼Œ\\(m\\)ä¸ºè®­ç»ƒæ ·æœ¬æ•° åŸºæœ¬åŸç† è¿ç”¨ç«äº‰å­¦ä¹ (competitive learning)ç­–ç•¥ï¼Œç«äº‰å±‚å„ç¥ç»å…ƒç«äº‰å¯¹è¾“å…¥å±‚å“åº”çš„æœºä¼šï¼Œæœ€åä»…æœ‰ä¸€ä¸ªç¥ç»å…ƒè·èƒœï¼Œä»£è¡¨å¯¹è¾“å…¥å±‚çš„åˆ†ç±»ã€‚å¦‚æ­¤è¿­ä»£ï¼Œé€æ­¥ä¼˜åŒ–ç½‘ç»œã€‚ å…·ä½“è¿‡ç¨‹ åˆå§‹åŒ–ï¼šåˆå§‹åŒ–è¿æ¥æƒé‡\\(\\omega_{j}^{(0)}=\\left(\\omega_{j 1}, \\ldots, \\omega_{j n}\\right), j=1, \\ldots, N\\) ç«äº‰ï¼šå¯¹äºè¾“å…¥æ ·æœ¬\\(x_i\\)ï¼Œéå†ç«äº‰å±‚ä¸­æ¯ä¸€ä¸ªç¥ç»å…ƒï¼Œè®¡ç®—\\(x_i\\)ä¸æ¯ä¸ªç¥ç»å…ƒçš„è¿æ¥æƒé‡\\(\\omega_j\\)ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆé€šå¸¸ä½¿ç”¨æ¬§å¼è·ç¦»æˆ–å¹³æ–¹æ¬§å¼è·ç¦»ï¼‰ã€‚è·ç¦»æœ€å°çš„ç¥ç»å…ƒèŠ‚ç‚¹èƒœå‡ºï¼Œç§°ä¸ºBMN(best matching neuron) \\[ j^{*}=j^{*}(i)=\\underset{j \\in \\mathcal{J}}{\\arg \\min } d\\left(\\boldsymbol{w}_{j}, \\boldsymbol{x}_{i}\\right) \\] åˆä½œï¼šè·èƒœç¥ç»å…ƒå†³å®šäº†å…´å¥‹ç¥ç»å…ƒæ‹“æ‰‘é‚»åŸŸçš„ç©ºé—´ä½ç½®ï¼Œä»è€Œä¸ºç›¸é‚»ç¥ç»å…ƒä¹‹é—´çš„åˆä½œæä¾›äº†åŸºç¡€ã€‚ ç¥ç»ç”Ÿç‰©å­¦çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸€ç»„å…´å¥‹ç¥ç»å…ƒå†…å­˜åœ¨æ¨ªå‘çš„ç›¸äº’ä½œç”¨ã€‚å› æ­¤å½“ä¸€ä¸ªç¥ç»å…ƒè¢«æ¿€æ´»æ—¶ï¼Œè¿‘é‚»èŠ‚ç‚¹å¾€å¾€æ¯”è¿œç¦»çš„èŠ‚ç‚¹æ›´å…´å¥‹ã€‚å®šä¹‰é‚»åŸŸå‡½æ•°\\(\\theta\\)ï¼Œè¡¨ç¤ºè·èƒœç¥ç»å…ƒå¯¹è¿‘é‚»ç¥ç»å…ƒçš„å½±å“å¼ºå¼±ï¼Œä¹Ÿå³ä¼˜èƒœé‚»åŸŸä¸­æ¯ä¸ªç¥ç»å…ƒçš„æ›´æ–°å¹…åº¦ã€‚å¸¸è§çš„é€‰æ‹©æ˜¯é«˜æ–¯å‡½æ•°ï¼š \\[ \\theta\\left(j^{*}(i), j ; t\\right)=\\exp \\left\\{-\\frac{1}{2 \\sigma(t)^{2}}\\left\\|j-j^{*}(i)\\right\\|_{2}^{2} / J^{2}\\right\\} \\] é‚»åŸŸåŠå¾„\\(\\sigma(t)\\)éšç€æ—¶é—´çš„æ¨ç§»è€Œå‡å°‘ã€‚ è¶Šé è¿‘ä¼˜èƒœç¥ç»å…ƒï¼Œæ›´æ–°å¹…åº¦è¶Šå¤§ï¼›è¶Šè¿œç¦»ä¼˜èƒœç¥ç»å…ƒï¼Œæ›´æ–°å¹…åº¦è¶Šå° é€‚åº”/å­¦ä¹ ï¼šæ›´æ–°ä¼˜èƒœé‚»åŸŸå†…ç¥ç»å…ƒçš„è¿æ¥æƒé‡ \\[ \\boldsymbol{w}_{j}^{(t)}=\\boldsymbol{w}_{j}^{(t-1)}+\\theta\\left(j^{*}(i), j ; t\\right) \\alpha(t)\\left(\\boldsymbol{x}_{i}-\\boldsymbol{w}_{j}^{(t-1)}\\right) \\] å­¦ä¹ ç‡\\(\\alpha(t)\\)éšç€æ—¶é—´çš„æ¨ç§»è€Œå‡å°‘ã€‚ è¿­ä»£ï¼šè¿”å›ç¬¬äºŒæ­¥ï¼Œè¿­ä»£è‡³æ”¶æ•›æˆ–è¾¾åˆ°è®¾å®šæ¬¡æ•° library(kohonen) p = ceiling(sqrt(5*sqrt(nrow(X)))) #æ ¹æ®ç»éªŒå…¬å¼ç¡®å®šæ­£æ–¹å½¢çš„è¾¹é•¿ set.seed(100) som1 = som(as.matrix(X),grid=somgrid(xdim=p,ydim=p,topo=&quot;rectangular&quot;),rlen=500,dist.fcts=&quot;euclidean&quot;) summary(som1) head(som1$unit.classif,100) #å„æ•°æ®ç‚¹çš„è·èƒœç¥ç»å…ƒ tail(som1$codes[[1]]) #è¿æ¥æƒé‡å‘é‡ set.seed(100) som2 = som(as.matrix(X),grid=somgrid(xdim=p,ydim=p,topo=&quot;hexagonal&quot;),rlen=500,dist.fcts=&quot;euclidean&quot;) par(mfrow=c(2,3)) plot(som1,c(&quot;changes&quot;)) plot(som1,c(&quot;counts&quot;), main=&quot;allocation counts to neurons&quot;) d.data$tau2 = d.data$sports_car+as.integer(d.data$tau&lt;21)+1 plot(som1,c(&quot;mapping&quot;),classif=predict(som1),col=c(&quot;blue&quot;,&quot;green&quot;,&quot;red&quot;)[d.data$tau2], pch=19, main=&quot;allocation of cases to neurons&quot;) plot(som2,c(&quot;changes&quot;)) plot(som2,c(&quot;counts&quot;), main=&quot;allocation counts to neurons&quot;) d.data$tau2 = d.data$sports_car+as.integer(d.data$tau&lt;21)+1 plot(som2,c(&quot;mapping&quot;),classif=predict(som2),col=c(&quot;blue&quot;,&quot;green&quot;,&quot;red&quot;)[d.data$tau2], pch=19, main=&quot;allocation of cases to neurons&quot;) Figure 9.18: SOM å›¾9.18çš„ç¬¬ä¸€è¡Œå±•ç¤ºçš„æ˜¯çŸ©å½¢ç«äº‰å±‚è¾“å‡ºçš„é™ç»´ç»“æœï¼Œç¬¬äºŒè¡Œå±•ç¤ºçš„æ˜¯å…­è¾¹å½¢ç«äº‰å±‚è¾“å‡ºçš„é™ç»´ç»“æœï¼Œsports carsï¼ˆçº¢è‰²ï¼‰å¯ä»¥è¢«è¾ƒä¸ºæ˜æ˜¾åœ°åŒºåˆ†å¼€æ¥ã€‚ "]]
