[["index.html", "现代精算统计模型  欢迎  答疑  课程安排", " 现代精算统计模型 Modern Actuarial Models 2021-01-10 17:43:54  欢迎 《现代精算统计模型》主要讲述如何使用统计学习和机器学习算法，提升传统的精算统计模型或者解决新的精算问题。这门课主要参考瑞士精算师协会发布的“精算数据科学”，该教程的主要目的是“为精算师提供一个对数据科学全面且易懂的介绍”，该教程提供了多篇方法性文章并开源代码，这样“读者可以相对容易地把这些数据科学方法用在自己的数据上”。 我们建议大家仔细阅读以下文献，尝试并理解所有代码。此网站将作为该课程的辅助，为大家答疑，总结文献，并对文献中的方法做扩展。该网站由授课老师高光远和助教张玮钰管理，欢迎大家反馈意见到助教、微信群、或邮箱 guangyuan.gao@ruc.edu.cn。  答疑 我定期把同学们的普遍疑问在这里解答，欢迎提问！  Tensorflow for Apple M1 (2020/12/23) 购买Apple M1的同学需要用这个pre-release tensorflow，从pypi下载的tensorflow暂不支持Apple M1  NLP (2020/12/18) 数据 这个数据第\\(i\\)行\\(j\\)列表示，在第\\(i\\)个评论中第\\(j\\)个词的排名(依照总出现频率)，所以每一行还保持了句子中词语的先后顺序。每一行都是一个时间序列数据（样本）。 LSTM input维度是batch size * length * 1，即以上所示的.csv矩阵文档。 embedding_3 作用就是把input的最后一个维度爆炸到256，参数个数为vocab_size* embedding dimension，可以看作把400个高频词映射到256维空间。 embedding_3和lstm_2输出维度中，有两个none,其中第一个表示batch size, 第二个表示sequence length。因为LSTM在时间维度上循环使用参数，所以sequence length不影响参数的个数。 sequence length不影响参数个数，对于不同的句子长度如100或者150，该模型都不需要调整，(应该)可以直接载入数据训练。 lstm_3 只有一个none, 表示batch size, 我们要求lstm_3不返回整个sequence只看最近的状态。  Reproducible results using Keras (2020/12/11) 使用Keras复现结果的方法。 https://cran.r-project.org/web/packages/keras/vignettes/faq.html  为什么不直接用relu解决vanishing gradient 而设计复杂的lstm gru (2020/12/11) relu值域在0到无穷，不如tanh和sigmoid稳健，后两种可以认为把极大极小值都截断了。 relu是线性变换，可能描述不了非线性效应。我最常用tanh。 当然，使用relu会明显提升计算速度，因为relu的导数容易计算。 另参见stackexchange  xaringan (2020/12/06) html格式的slides： https://slides.yihui.org/xaringan/zh-CN.html#1  samme.r (2020/11/27) 关于samme.r算法, 请参考下面文章中的exponential loss function. https://web.stanford.edu/~hastie/Papers/samme.pdf 算法samme.r仅在以上draft中出现，正式发表时samme.r被删掉了，推测审稿人有异议。正式文章参考 http://ww.web.stanford.edu/~hastie/Papers/SII-2-3-A8-Zhu.pdf  随机种子数 (2020/11/20) 输入RNGversion(\"3.5.0\"); set.seed(100)，使得你的随机种子数和paper的相同，模型结果相近。  MAC OS, Linux, WIN (2020/11/16) 据观察，在MAC OS和Linux系统下安装keras成功的比例较高。WIN系统下，Python各个包的依赖以及和R包的匹配有一定的问题，今天是通过更换镜像源解决了R中无法加载tensorflow.keras模块的问题，推测是TUNA源中WIN包依赖关系没有及时更新。 为了解决镜像源更新延迟、或者tensorflow版本过低的问题，这里共享WIN下经测试的conda环境配置。下载该文档，从该文档所在文件夹启动命令行，使用命令conda env create --name &lt;env&gt; --file filename.yaml，安装该conda环境。在R中使用reticulate::use_condaenv(\"&lt;env&gt;\",required=T)关联该环境。 另外，可下载MAC OS系统下经测试的conda环境配置。可通过conda env create --name &lt;env&gt; --file filename.yaml安装。  CASdatasets (2020/11/13) 源文件在http://cas.uqam.ca/，但下载速度很慢，我把它放在坚果云共享。下载后选择install from local archive file。  微信群 (2020/11/08)  课程安排 以下安排为初步计划，根据大家的需求和背景，我们可能要花更多的时间在某些重要的方法及其在精算上的应用。 第10周： 准备工作。 第11周: 1 - French Motor Third-Party Liability Claims https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764 机动 2 - Inisghts from Inside Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3226852 3 - Nesting Classical Actuarial Models into Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525 第12周： 4 - On Boosting: Theory and Applications https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3402687 第13周： 5 - Unsupervised Learning: What is a Sports Car https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3439358 第14周： 6 - Lee and Carter go Machine Learning: Recurrent Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3441030 第15周： 7 - The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887 第16周： 8 - Peeking into the Black Box: An Actuarial Case Study for Interpretable Machine Learning https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944 第17周： 9 - Convolutional neural network case studies: (1) Anomalies in Mortality Rates (2) Image Recognition https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656210 "],["cnn.html", "1 卷积神经网络 1.1 卷积层 (Convolution) 1.2 池化层 (Pooling) 1.3 批标准化层 (Batch Normalization) 1.4 其他组件 1.5 特性 1.6 Human Mortality Database (HMD) 1.7 MNIST dataset", " 1 卷积神经网络 万淇、蔡清扬、高光远 深度学习之所以这么热，大部分归功于卷积神经网络在计算机视觉上取得的巨大成功。卷积神经网络还可以用在自然语言处理、时间序列分析、异常检测、可穿戴设备与健康检测、GO。 大型预先训练的CNNs库可用于图像识别：AlexNet，GoogLeNet，ResNet, Inception, MobileNet,，VGG， DenseNet,，NASNet 等。它们可以直接使用，将某一图像分类至已知的类别之中 也可以应用于迁移学习。 Figure 1.1: Transfer learning 1.1 卷积层 (Convolution) 作用：特征提取，一般想要多少特征，就设置多少个卷积核(filter)。不同的卷积核相当于不同的特征提取器. 计算过程如下图所示： Figure 1.2: Convolution 1.1.1 超参数 一个卷积层主要有以下超参数 Channels: 黑白图像一般只有一个通道，彩色图像一般有三个通道，即RGB. Filters: 一般想要多少特征，就设置多少个卷积核。不同的卷积核相当于不同的特征提取器. Padding: 补零。作用：保持图像大小，使之减小不会太快；还能照顾到边缘特征。 Figure 1.3: Padding Dilation: 膨胀卷积（Dilated Convolution）也称为空洞卷积（Atrous Convolution）是一种不增加参数数量同时增加输出单元感受野的一种方法。空洞卷积通过给卷积核插入“空洞”来变相地增加其大小 Figure 1.4: Dilation Figure 1.5: Dilation Strides: 步长。卷积核每次滑动的步幅。 假设第\\(k-1\\)层输出图像的维度为\\(n_1^{(k-1)}\\times n_2^{(k-1)}\\times m^{(k-1)}\\), 经过第\\(k\\)层的卷积运算，得到的图片维度为\\(n_1^{(k)}\\times n_2^{(k)}\\times m^{(k)}\\)。相关超参数总结如下： 特征 超参数 输入大小 \\(n_1^{(k-1)}\\times n_2^{(k-1)}\\) 输入特征 \\(m^{(k-1)}\\) 输出特征(卷积核) \\(m^{(k)}\\) 补零 \\(p_1^{(k)},p_2^{(k)}\\) 膨胀 \\(d_1^{(k)},d_2^{(k)}\\) 步长 \\(s_1^{(k)},s_2^{(k)}\\) 输出大小 \\(n_1^{(k)}\\times n_2^{(k)}\\) 其中，输出大小\\(n_1^{(k)}\\times n_2^{(k)}\\)由输入大小\\(n_1^{(k-1)}\\times n_2^{(k-1)}\\)和补零、膨胀、步长决定。具体计算为？ 1.2 池化层 (Pooling) 也称下采样层，其作用是进行特征选择，降低特征数量，从而减少参数数量。在图像中，最主要作用就是压缩图像。池化层一般分为平均池化和最大池化。 Figure 1.6: Pooling 1.3 批标准化层 (Batch Normalization) 在batch上进行标准化后再送入下一层，它可以防止梯度消失和梯度爆炸问题，加快收敛速度。主要分为两步： 通过训练期间各批次的参数平均值和方差对输入进行移位和缩放。 通过训练期间学习的后两个（可学习）参数进行移位和缩放。 ？？Detailed Algorithm 1.4 其他组件 1.4.1 全连接层 (Dense) 全连接层中的每个神经元与其前一层的所有神经元进行全连接。 CNN中因为图像是二维的，所以在进入全连接层的时候需要经过一个Flatten（扁平化）的操作。 Flatten层作用就是通过重新排列维度并保留所有值的简单变换. Figure 1.7: Dense layer 1.4.2 输出神经元 即我们最后输出的结果，一般接在全连接层后。 案例一是生存率问题，结果取值在\\([0,1]\\)中，所以使用sigmoid的函数对最后的值进行缩放。案例二是多分类的输出结果，所以使用softmax函数进行输出。 Figure 1.8: Activation Functions Figure 1.9: Activation Functions Figure 1.10: Activation Functions 1.4.3 激活函数 (Activation) 在上面讨论的网络层（卷积层、池化层和全连接层）中，所有的操作其实都是线性的，但只有使用非线性激活，网络建模的全部威力才会发挥出来。常用的非线性激活函数有：ReLU、sigmoid、tanh等 Figure 1.11: Activation Functions Figure 1.12: Activation Functions Figure 1.13: Activation Functions 1.5 特性 1.5.1 平移 1.5.2 旋转 1.5.3 尺度 1.6 Human Mortality Database (HMD) 目标: 根据死亡率表的局部特征(\\(10\\times10\\))，检测该局部异常死亡率强度。 1.6.1 输入和标签 死亡率\\(q_{x,t,c,g}\\)、人口数量\\(E_{x,t,c,g}\\) 年龄\\(x\\), 日历年\\(t\\), 国家\\(c\\), 性别\\(g\\). 由于领土的变化，某些年的数据会出现变更前与变更后的的两个数据，处理方式是取平均值作为最后的研究数据。 某些死亡率数据存在缺失：如果相邻(以年龄\\(x\\)和日历年\\(t\\))值可用，我们线性插补，否则使用最近邻的值进行插补。 假定没有人口的迁移以及其他的误差：\\[E_{x,t,c,g}=E_{x-1,t-1,c,g}(1-q_{{x-1},{t-1},c,g})\\] 定义标准化残差: \\[r_{x,t,c,g}=\\frac{E_{x,t,c,g}-E_{x-1,t-1,c,g}(1-q_{x-1,t-1,c,g})}{E_{x,t,c,g}}\\] \\(r_{x,t,c,g}&lt;0\\)表明可能有人口迁出或者数据错误， \\(r_{x,t,c,g}&gt;0\\)表明可能有人口迁入或者数据错误。 经过预处理HMD，对每个国家每个性别我们得到一个关于死亡率\\(q_{x,t,c,g}\\)的二维数组，其中行代表不同日历年，列代表不同年龄。为了检测死亡率的异常值，我们考虑死亡率的局部变化特征，使用大小为\\(10\\times10\\)的窗口在死亡率二维数组上进行移动，并设置步长为\\(5\\)。可以得到死亡率的局部矩阵\\((q_{x,t,c,g})_{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}\\), 其中\\(x_i:=20+5i,t_i=1950+5i\\)。我们定义如下(原始)输入特征\\(W_{i,c}\\in\\mathbb{R}^{10\\times 10\\times 3}\\): \\[ \\begin{aligned} W_{i,c,\\cdot,\\cdot,1}:=&amp;(\\text{logit}(q_{x,t,c,males}))_{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}\\\\ W_{i,c,\\cdot,\\cdot,2}:=&amp;(\\text{logit}(q_{x,t,c,females}))_{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}\\\\ W_{i,c,\\cdot,\\cdot,3}:=&amp;W_{i,c,\\cdot,\\cdot,1}-W_{i,c,\\cdot,\\cdot,2} \\end{aligned} \\] 其中, $q = $ Figure 1.14: Mortality Window 然后分别对三个通道进行正则化, 得到\\(\\boldsymbol{X}_{i,c}\\in[0,1]^{10\\times10\\times3}\\). 通过对所有国家进行如上处理, 可以得到大约\\(4000\\)张图像. 接下来, 我们定义每张图的“标签”. 首先, 对\\(r_{x,t,c,g}\\)进行MinMax正则化处理, 得到\\(\\bar{r}_{x,t,c,g}\\in[0,1].\\) 然后, 定义标签为异常强度 \\[Y_{i,c}:=\\underset{x_i&lt;x\\le x_i+10, t_i&lt;t\\le t_i+10}{\\max} \\left|\\frac{\\bar{r}_{x,t,c,males}+\\bar{r}_{x,t,c,females}}{2} \\right|\\in[0,1].\\] 我们的目标是基于死亡率在大小为\\(10\\times10\\)上的局部特征, 预测该范围内死亡率的异常强度. 在训练神经网络时, 选取均方误差损失函数\\[\\mathcal{L}(Y,\\hat{\\mu}(\\boldsymbol{X});\\mathcal{I}):=\\frac{1}{|\\mathcal{I}|}\\sum_{(i,c)\\in\\mathcal{I}}(Y_{i,c}-\\hat{\\mu}(\\boldsymbol{X}_{i,c}))^2.\\] 1.6.2 评估指标 在评估模型时, 我们通过如下步骤定义二分类AOC指标: 定义伯努利随机变量 \\[ b_{i,c}:= \\begin{cases} 1, Y_{i,c}\\geq q_{0.95}(Y), \\\\ 0, \\text{otherwise}, \\end{cases} \\] 其中, \\(q_{0.95}(Y)\\)为所有因变量\\(Y_{i,c}\\)的0.95分位数, 即\\(b_{i,c}\\)为“非常异常”指示标量. 把神经网络的输出结果\\(\\hat{\\mu}(\\boldsymbol{X}_{i,c})\\)当作概率\\(\\Pr (b_{i,c}=1)\\)的预测. 画出该二分类问题的receiver operating characteristic curve (ROC), 并计算 area under the curve (AUC). 利用以上模型评估方法, 我们可以对国家按照“异常强度”的相似性进行分类, 具体步骤如下: 对每个国家\\(c\\)分别建立CNN模型, 并使用该模型对其他国家\\(c^*\\)的数据进行预测, 计算AUC \\(A_{c,c^*}\\). 并建立矩阵\\(A=(A_{c,c^*})_{c,c*\\in\\mathcal{C}}\\), 其中\\(\\mathcal{C}\\)为所有国家的集合. 对\\(A\\)进行列标准化, 并进行奇异值分解, 得到前两个主成分\\(P_{j,c}, j=1,2\\). 对主成分\\(P_{j,c}, j=1,2\\)进行聚类, 得到4个簇. 1.7 MNIST dataset 目标: 对手写\\(0-9\\)进行分类。 MNIST 全称为 Modified National Institute of Standards and Technology. 修改过后的MNIST数据集，它是一个由不同的人的手写体数字组成的图片数据集，包含了\\(7\\)万张关于手写数字\\(0,1,\\ldots,9\\)的图像，格式为\\(28×28\\)的灰度像素。 神经网络的输入为由灰度像素构成的\\(28\\times28\\)数组\\(\\boldsymbol{X}\\in[0,1]^{28\\times28}\\), 输出为在\\(\\{0,1,\\ldots,9\\}\\)上的离散分布\\((p_0,\\ldots,p_9)^T\\), 其中\\(\\sum_{j=0}^9p_j=1\\). 图像的标签为实际数字的one-hot编码\\(Y\\in\\{0,1\\}^{10}\\). 损失函数为交叉熵(cross-entropy) \\[\\mathcal{L}(Y,\\hat{p}(\\boldsymbol{X});\\mathcal{I}):=-\\sum_{i\\in\\mathcal{I}}\\sum_{j=0}^9Y_{i,j}\\log\\hat{p}_j(\\boldsymbol{X}_i).\\] "]]
