<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 无监督学习方法 | 现代精算统计模型</title>
  <meta name="description" content="The output format is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5 无监督学习方法 | 现代精算统计模型" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The output format is bookdown::gitbook." />
  <meta name="github-repo" content="sxpyggy/Modern-Actuarial-Models" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 无监督学习方法 | 现代精算统计模型" />
  
  <meta name="twitter:description" content="The output format is bookdown::gitbook." />
  

<meta name="author" content="Modern Actuarial Models" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="boosting.html"/>
<link rel="next" href="rnn.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">现代精算统计模型</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>👨‍🏫 欢迎</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#答疑"><i class="fa fa-check"></i>🤔 答疑</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#课程安排"><i class="fa fa-check"></i>🗓️ 课程安排</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i>简介</a></li>
<li class="chapter" data-level="1" data-path="pre.html"><a href="pre.html"><i class="fa fa-check"></i><b>1</b> 准备工作</a><ul>
<li class="chapter" data-level="1.1" data-path="pre.html"><a href="pre.html#常用链接"><i class="fa fa-check"></i><b>1.1</b> 常用链接</a></li>
<li class="chapter" data-level="1.2" data-path="pre.html"><a href="pre.html#克隆代码"><i class="fa fa-check"></i><b>1.2</b> 克隆代码</a></li>
<li class="chapter" data-level="1.3" data-path="pre.html"><a href="pre.html#r-interface-to-keras"><i class="fa fa-check"></i><b>1.3</b> R interface to Keras</a><ul>
<li class="chapter" data-level="1.3.1" data-path="pre.html"><a href="pre.html#r自动安装"><i class="fa fa-check"></i><b>1.3.1</b> R自动安装</a></li>
<li class="chapter" data-level="1.3.2" data-path="pre.html"><a href="pre.html#使用reticulate关联conda环境"><i class="fa fa-check"></i><b>1.3.2</b> 使用reticulate关联conda环境</a></li>
<li class="chapter" data-level="1.3.3" data-path="pre.html"><a href="pre.html#指定conda安装"><i class="fa fa-check"></i><b>1.3.3</b> 指定conda安装</a></li>
<li class="chapter" data-level="1.3.4" data-path="pre.html"><a href="pre.html#使用reticulate安装"><i class="fa fa-check"></i><b>1.3.4</b> 使用reticulate安装</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="pre.html"><a href="pre.html#r-interface-to-python"><i class="fa fa-check"></i><b>1.4</b> R interface to Python</a><ul>
<li class="chapter" data-level="1.4.1" data-path="pre.html"><a href="pre.html#reticulate-常见命令"><i class="fa fa-check"></i><b>1.4.1</b> reticulate 常见命令</a></li>
<li class="chapter" data-level="1.4.2" data-path="pre.html"><a href="pre.html#切换r关联的conda环境"><i class="fa fa-check"></i><b>1.4.2</b> 切换R关联的conda环境</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="pre.html"><a href="pre.html#python"><i class="fa fa-check"></i><b>1.5</b> Python</a><ul>
<li class="chapter" data-level="1.5.1" data-path="pre.html"><a href="pre.html#conda环境"><i class="fa fa-check"></i><b>1.5.1</b> Conda环境</a></li>
<li class="chapter" data-level="1.5.2" data-path="pre.html"><a href="pre.html#常用的conda命令"><i class="fa fa-check"></i><b>1.5.2</b> 常用的Conda命令</a></li>
<li class="chapter" data-level="1.5.3" data-path="pre.html"><a href="pre.html#tensorflowpytorch-gpu-version"><i class="fa fa-check"></i><b>1.5.3</b> Tensorflow/Pytorch GPU version</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="french.html"><a href="french.html"><i class="fa fa-check"></i><b>2</b> 车险索赔频率预测</a><ul>
<li class="chapter" data-level="2.1" data-path="french.html"><a href="french.html#背景介绍"><i class="fa fa-check"></i><b>2.1</b> 背景介绍</a></li>
<li class="chapter" data-level="2.2" data-path="french.html"><a href="french.html#预测模型概述"><i class="fa fa-check"></i><b>2.2</b> 预测模型概述</a></li>
<li class="chapter" data-level="2.3" data-path="french.html"><a href="french.html#特征工程"><i class="fa fa-check"></i><b>2.3</b> 特征工程</a><ul>
<li class="chapter" data-level="2.3.1" data-path="french.html"><a href="french.html#截断"><i class="fa fa-check"></i><b>2.3.1</b> 截断</a></li>
<li class="chapter" data-level="2.3.2" data-path="french.html"><a href="french.html#离散化"><i class="fa fa-check"></i><b>2.3.2</b> 离散化</a></li>
<li class="chapter" data-level="2.3.3" data-path="french.html"><a href="french.html#设定基础水平"><i class="fa fa-check"></i><b>2.3.3</b> 设定基础水平</a></li>
<li class="chapter" data-level="2.3.4" data-path="french.html"><a href="french.html#协变量变形"><i class="fa fa-check"></i><b>2.3.4</b> 协变量变形</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="french.html"><a href="french.html#训练集-验证集-测试集"><i class="fa fa-check"></i><b>2.4</b> 训练集-验证集-测试集</a></li>
<li class="chapter" data-level="2.5" data-path="french.html"><a href="french.html#泊松偏差损失函数"><i class="fa fa-check"></i><b>2.5</b> 泊松偏差损失函数</a></li>
<li class="chapter" data-level="2.6" data-path="french.html"><a href="french.html#泊松回归模型"><i class="fa fa-check"></i><b>2.6</b> 泊松回归模型</a></li>
<li class="chapter" data-level="2.7" data-path="french.html"><a href="french.html#泊松可加模型"><i class="fa fa-check"></i><b>2.7</b> 泊松可加模型</a></li>
<li class="chapter" data-level="2.8" data-path="french.html"><a href="french.html#泊松回归树"><i class="fa fa-check"></i><b>2.8</b> 泊松回归树</a></li>
<li class="chapter" data-level="2.9" data-path="french.html"><a href="french.html#随机森林"><i class="fa fa-check"></i><b>2.9</b> 随机森林</a></li>
<li class="chapter" data-level="2.10" data-path="french.html"><a href="french.html#泊松提升树"><i class="fa fa-check"></i><b>2.10</b> 泊松提升树</a></li>
<li class="chapter" data-level="2.11" data-path="french.html"><a href="french.html#模型比较"><i class="fa fa-check"></i><b>2.11</b> 模型比较</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nn.html"><a href="nn.html"><i class="fa fa-check"></i><b>3</b> 神经网络</a><ul>
<li class="chapter" data-level="3.1" data-path="nn.html"><a href="nn.html#建立神经网络的一般步骤"><i class="fa fa-check"></i><b>3.1</b> 建立神经网络的一般步骤</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nn.html"><a href="nn.html#明确目标和数据类型"><i class="fa fa-check"></i><b>3.1.1</b> 明确目标和数据类型</a></li>
<li class="chapter" data-level="3.1.2" data-path="nn.html"><a href="nn.html#数据预处理"><i class="fa fa-check"></i><b>3.1.2</b> 数据预处理</a></li>
<li class="chapter" data-level="3.1.3" data-path="nn.html"><a href="nn.html#选取合适的神经网络类型"><i class="fa fa-check"></i><b>3.1.3</b> 选取合适的神经网络类型</a></li>
<li class="chapter" data-level="3.1.4" data-path="nn.html"><a href="nn.html#建立神经网络全连接神经网络"><i class="fa fa-check"></i><b>3.1.4</b> 建立神经网络（全连接神经网络）</a></li>
<li class="chapter" data-level="3.1.5" data-path="nn.html"><a href="nn.html#训练神经网络"><i class="fa fa-check"></i><b>3.1.5</b> 训练神经网络</a></li>
<li class="chapter" data-level="3.1.6" data-path="nn.html"><a href="nn.html#调参"><i class="fa fa-check"></i><b>3.1.6</b> 调参</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="nn.html"><a href="nn.html#数据预处理-1"><i class="fa fa-check"></i><b>3.2</b> 数据预处理</a></li>
<li class="chapter" data-level="3.3" data-path="nn.html"><a href="nn.html#神经网络提升模型-combined-actuarial-neural-network"><i class="fa fa-check"></i><b>3.3</b> 神经网络提升模型 （combined actuarial neural network）</a></li>
<li class="chapter" data-level="3.4" data-path="nn.html"><a href="nn.html#神经网络结构"><i class="fa fa-check"></i><b>3.4</b> 神经网络结构</a><ul>
<li class="chapter" data-level="3.4.1" data-path="nn.html"><a href="nn.html#结构参数"><i class="fa fa-check"></i><b>3.4.1</b> 结构参数</a></li>
<li class="chapter" data-level="3.4.2" data-path="nn.html"><a href="nn.html#输入层"><i class="fa fa-check"></i><b>3.4.2</b> 输入层</a></li>
<li class="chapter" data-level="3.4.3" data-path="nn.html"><a href="nn.html#embedding-layer"><i class="fa fa-check"></i><b>3.4.3</b> Embedding layer</a></li>
<li class="chapter" data-level="3.4.4" data-path="nn.html"><a href="nn.html#隐藏层"><i class="fa fa-check"></i><b>3.4.4</b> 隐藏层</a></li>
<li class="chapter" data-level="3.4.5" data-path="nn.html"><a href="nn.html#输出层"><i class="fa fa-check"></i><b>3.4.5</b> 输出层</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="nn.html"><a href="nn.html#训练神经网络-1"><i class="fa fa-check"></i><b>3.5</b> 训练神经网络</a></li>
<li class="chapter" data-level="3.6" data-path="nn.html"><a href="nn.html#总结"><i class="fa fa-check"></i><b>3.6</b> 总结</a></li>
<li class="chapter" data-level="3.7" data-path="nn.html"><a href="nn.html#其它模型"><i class="fa fa-check"></i><b>3.7</b> 其它模型</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>4</b> 提升方法 (Boosting)</a><ul>
<li class="chapter" data-level="4.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>4.1</b> AdaBoost</a></li>
<li class="chapter" data-level="4.2" data-path="boosting.html"><a href="boosting.html#logit-boost-real-discrete-gentle-adaboost"><i class="fa fa-check"></i><b>4.2</b> Logit Boost (real, discrete, gentle AdaBoost)</a></li>
<li class="chapter" data-level="4.3" data-path="boosting.html"><a href="boosting.html#adaboost.m1"><i class="fa fa-check"></i><b>4.3</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="4.4" data-path="boosting.html"><a href="boosting.html#samme-stage-wise-additive-modeling-using-a-multi-class-exponential-loss-function"><i class="fa fa-check"></i><b>4.4</b> SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function)</a></li>
<li class="chapter" data-level="4.5" data-path="boosting.html"><a href="boosting.html#samme.r-multi-class-real-adaboost"><i class="fa fa-check"></i><b>4.5</b> SAMME.R (multi-class real AdaBoost)</a></li>
<li class="chapter" data-level="4.6" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>4.6</b> Gradient Boosting</a></li>
<li class="chapter" data-level="4.7" data-path="boosting.html"><a href="boosting.html#newton-boosting"><i class="fa fa-check"></i><b>4.7</b> Newton Boosting</a></li>
<li class="chapter" data-level="4.8" data-path="boosting.html"><a href="boosting.html#xgboost"><i class="fa fa-check"></i><b>4.8</b> XGBoost</a></li>
<li class="chapter" data-level="4.9" data-path="boosting.html"><a href="boosting.html#case-study"><i class="fa fa-check"></i><b>4.9</b> Case study</a><ul>
<li class="chapter" data-level="4.9.1" data-path="boosting.html"><a href="boosting.html#数据描述"><i class="fa fa-check"></i><b>4.9.1</b> 数据描述</a></li>
<li class="chapter" data-level="4.9.2" data-path="boosting.html"><a href="boosting.html#数据预处理-2"><i class="fa fa-check"></i><b>4.9.2</b> 数据预处理</a></li>
<li class="chapter" data-level="4.9.3" data-path="boosting.html"><a href="boosting.html#特征工程-1"><i class="fa fa-check"></i><b>4.9.3</b> 特征工程</a></li>
<li class="chapter" data-level="4.9.4" data-path="boosting.html"><a href="boosting.html#建模流程"><i class="fa fa-check"></i><b>4.9.4</b> 建模流程</a></li>
<li class="chapter" data-level="4.9.5" data-path="boosting.html"><a href="boosting.html#模型度量gini系数"><i class="fa fa-check"></i><b>4.9.5</b> 模型度量——Gini系数</a></li>
<li class="chapter" data-level="4.9.6" data-path="boosting.html"><a href="boosting.html#建立adaboost模型"><i class="fa fa-check"></i><b>4.9.6</b> 建立AdaBoost模型</a></li>
<li class="chapter" data-level="4.9.7" data-path="boosting.html"><a href="boosting.html#建立xgboost模型"><i class="fa fa-check"></i><b>4.9.7</b> 建立XGBoost模型</a></li>
<li class="chapter" data-level="4.9.8" data-path="boosting.html"><a href="boosting.html#结论"><i class="fa fa-check"></i><b>4.9.8</b> 结论</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="boosting.html"><a href="boosting.html#appendix-commonly-used-python-code-for-py-beginners"><i class="fa fa-check"></i><b>4.10</b> Appendix: Commonly used Python code (for py-beginners)</a><ul>
<li class="chapter" data-level="4.10.1" data-path="boosting.html"><a href="boosting.html#python标准数据类型"><i class="fa fa-check"></i><b>4.10.1</b> Python标准数据类型</a></li>
<li class="chapter" data-level="4.10.2" data-path="boosting.html"><a href="boosting.html#python内置函数"><i class="fa fa-check"></i><b>4.10.2</b> Python内置函数</a></li>
<li class="chapter" data-level="4.10.3" data-path="boosting.html"><a href="boosting.html#numpy包"><i class="fa fa-check"></i><b>4.10.3</b> numpy包</a></li>
<li class="chapter" data-level="4.10.4" data-path="boosting.html"><a href="boosting.html#pandas包"><i class="fa fa-check"></i><b>4.10.4</b> pandas包</a></li>
<li class="chapter" data-level="4.10.5" data-path="boosting.html"><a href="boosting.html#matplotlib包"><i class="fa fa-check"></i><b>4.10.5</b> Matplotlib包</a></li>
<li class="chapter" data-level="4.10.6" data-path="boosting.html"><a href="boosting.html#常用教程网址"><i class="fa fa-check"></i><b>4.10.6</b> 常用教程网址</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>5</b> 无监督学习方法</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#数据预处理-3"><i class="fa fa-check"></i><b>5.1</b> 数据预处理</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#主成分分析"><i class="fa fa-check"></i><b>5.2</b> 主成分分析</a></li>
<li class="chapter" data-level="5.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#自编码"><i class="fa fa-check"></i><b>5.3</b> 自编码</a><ul>
<li class="chapter" data-level="5.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#模型训练"><i class="fa fa-check"></i><b>5.3.1</b> 模型训练</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>5.4</b> K-means clustering</a></li>
<li class="chapter" data-level="5.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-medoids-clustering-pam"><i class="fa fa-check"></i><b>5.5</b> K-medoids clustering (PAM)</a></li>
<li class="chapter" data-level="5.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#gaussian-mixture-modelsgmms"><i class="fa fa-check"></i><b>5.6</b> Gaussian mixture models(GMMs)</a></li>
<li class="chapter" data-level="5.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#三种聚类方法评价"><i class="fa fa-check"></i><b>5.7</b> 三种聚类方法评价</a></li>
<li class="chapter" data-level="5.8" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#t-sne"><i class="fa fa-check"></i><b>5.8</b> t-SNE</a></li>
<li class="chapter" data-level="5.9" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#umap"><i class="fa fa-check"></i><b>5.9</b> UMAP</a></li>
<li class="chapter" data-level="5.10" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#som"><i class="fa fa-check"></i><b>5.10</b> SOM</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rnn.html"><a href="rnn.html"><i class="fa fa-check"></i><b>6</b> 循环神经网络与死亡率预测</a><ul>
<li class="chapter" data-level="6.1" data-path="rnn.html"><a href="rnn.html#lee-carter-model"><i class="fa fa-check"></i><b>6.1</b> Lee-Carter Model</a></li>
<li class="chapter" data-level="6.2" data-path="rnn.html"><a href="rnn.html#普通循环神经网络recurrent-neural-network"><i class="fa fa-check"></i><b>6.2</b> 普通循环神经网络（recurrent neural network）</a></li>
<li class="chapter" data-level="6.3" data-path="rnn.html"><a href="rnn.html#长短期记忆神经网络long-short-term-memory"><i class="fa fa-check"></i><b>6.3</b> 长短期记忆神经网络（Long short-term memory）</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rnn.html"><a href="rnn.html#激活函数activation-functions"><i class="fa fa-check"></i><b>6.3.1</b> 激活函数（Activation functions）</a></li>
<li class="chapter" data-level="6.3.2" data-path="rnn.html"><a href="rnn.html#gates-and-cell-state"><i class="fa fa-check"></i><b>6.3.2</b> Gates and cell state</a></li>
<li class="chapter" data-level="6.3.3" data-path="rnn.html"><a href="rnn.html#output-function"><i class="fa fa-check"></i><b>6.3.3</b> Output Function</a></li>
<li class="chapter" data-level="6.3.4" data-path="rnn.html"><a href="rnn.html#time-distributed-layer"><i class="fa fa-check"></i><b>6.3.4</b> Time-distributed Layer</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="rnn.html"><a href="rnn.html#门控循环神经网络gated-recurrent-unit"><i class="fa fa-check"></i><b>6.4</b> 门控循环神经网络（Gated Recurrent Unit）</a><ul>
<li class="chapter" data-level="6.4.1" data-path="rnn.html"><a href="rnn.html#gates"><i class="fa fa-check"></i><b>6.4.1</b> Gates</a></li>
<li class="chapter" data-level="6.4.2" data-path="rnn.html"><a href="rnn.html#neuron-activations"><i class="fa fa-check"></i><b>6.4.2</b> Neuron Activations</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rnn.html"><a href="rnn.html#案例分析case-study"><i class="fa fa-check"></i><b>6.5</b> 案例分析（Case study）</a><ul>
<li class="chapter" data-level="6.5.1" data-path="rnn.html"><a href="rnn.html#数据描述-1"><i class="fa fa-check"></i><b>6.5.1</b> 数据描述</a></li>
<li class="chapter" data-level="6.5.2" data-path="rnn.html"><a href="rnn.html#死亡率热力图"><i class="fa fa-check"></i><b>6.5.2</b> 死亡率热力图</a></li>
<li class="chapter" data-level="6.5.3" data-path="rnn.html"><a href="rnn.html#lee-carter-模型"><i class="fa fa-check"></i><b>6.5.3</b> Lee-Carter 模型</a></li>
<li class="chapter" data-level="6.5.4" data-path="rnn.html"><a href="rnn.html#初试rnn"><i class="fa fa-check"></i><b>6.5.4</b> 初试RNN</a></li>
<li class="chapter" data-level="6.5.5" data-path="rnn.html"><a href="rnn.html#rnn-1"><i class="fa fa-check"></i><b>6.5.5</b> RNN</a></li>
<li class="chapter" data-level="6.5.6" data-path="rnn.html"><a href="rnn.html#引入性别协变量"><i class="fa fa-check"></i><b>6.5.6</b> 引入性别协变量</a></li>
<li class="chapter" data-level="6.5.7" data-path="rnn.html"><a href="rnn.html#稳健性"><i class="fa fa-check"></i><b>6.5.7</b> 稳健性</a></li>
<li class="chapter" data-level="6.5.8" data-path="rnn.html"><a href="rnn.html#预测结果图"><i class="fa fa-check"></i><b>6.5.8</b> 预测结果图</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nlp.html"><a href="nlp.html"><i class="fa fa-check"></i><b>7</b> 自然语言处理</a><ul>
<li class="chapter" data-level="7.1" data-path="nlp.html"><a href="nlp.html#预处理"><i class="fa fa-check"></i><b>7.1</b> 预处理</a></li>
<li class="chapter" data-level="7.2" data-path="nlp.html"><a href="nlp.html#bag-of-words"><i class="fa fa-check"></i><b>7.2</b> Bag of words</a></li>
<li class="chapter" data-level="7.3" data-path="nlp.html"><a href="nlp.html#bag-of-part-of-speech"><i class="fa fa-check"></i><b>7.3</b> Bag of part-of-speech</a></li>
<li class="chapter" data-level="7.4" data-path="nlp.html"><a href="nlp.html#word-embeddings"><i class="fa fa-check"></i><b>7.4</b> Word embeddings</a><ul>
<li class="chapter" data-level="7.4.1" data-path="nlp.html"><a href="nlp.html#neural-probabilistic-language-model"><i class="fa fa-check"></i><b>7.4.1</b> Neural probabilistic language model</a></li>
<li class="chapter" data-level="7.4.2" data-path="nlp.html"><a href="nlp.html#word2vec"><i class="fa fa-check"></i><b>7.4.2</b> word2vec</a></li>
<li class="chapter" data-level="7.4.3" data-path="nlp.html"><a href="nlp.html#global-vectors-for-word-representationglove"><i class="fa fa-check"></i><b>7.4.3</b> Global vectors for word representation(Glove)</a></li>
<li class="chapter" data-level="7.4.4" data-path="nlp.html"><a href="nlp.html#pre-trained-word-embeddings"><i class="fa fa-check"></i><b>7.4.4</b> Pre-trained word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="nlp.html"><a href="nlp.html#机器学习算法"><i class="fa fa-check"></i><b>7.5</b> 机器学习算法</a></li>
<li class="chapter" data-level="7.6" data-path="nlp.html"><a href="nlp.html#神经网络"><i class="fa fa-check"></i><b>7.6</b> 神经网络</a><ul>
<li class="chapter" data-level="7.6.1" data-path="nlp.html"><a href="nlp.html#数据预处理-4"><i class="fa fa-check"></i><b>7.6.1</b> 数据预处理</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="nlp.html"><a href="nlp.html#case-study-1"><i class="fa fa-check"></i><b>7.7</b> Case study</a><ul>
<li class="chapter" data-level="7.7.1" data-path="nlp.html"><a href="nlp.html#函数说明"><i class="fa fa-check"></i><b>7.7.1</b> 函数说明</a></li>
<li class="chapter" data-level="7.7.2" data-path="nlp.html"><a href="nlp.html#可能遇到的问题"><i class="fa fa-check"></i><b>7.7.2</b> 可能遇到的问题</a></li>
<li class="chapter" data-level="7.7.3" data-path="nlp.html"><a href="nlp.html#结果比较"><i class="fa fa-check"></i><b>7.7.3</b> 结果比较</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="nlp.html"><a href="nlp.html#结论-1"><i class="fa fa-check"></i><b>7.8</b> 结论</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flashlight.html"><a href="flashlight.html"><i class="fa fa-check"></i><b>8</b> 通用模型解释方法</a><ul>
<li class="chapter" data-level="8.1" data-path="flashlight.html"><a href="flashlight.html#数据"><i class="fa fa-check"></i><b>8.1</b> 数据</a></li>
<li class="chapter" data-level="8.2" data-path="flashlight.html"><a href="flashlight.html#模型"><i class="fa fa-check"></i><b>8.2</b> 模型</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flashlight.html"><a href="flashlight.html#glm"><i class="fa fa-check"></i><b>8.2.1</b> GLM</a></li>
<li class="chapter" data-level="8.2.2" data-path="flashlight.html"><a href="flashlight.html#xgboost-1"><i class="fa fa-check"></i><b>8.2.2</b> XGBoost</a></li>
<li class="chapter" data-level="8.2.3" data-path="flashlight.html"><a href="flashlight.html#神经网络-1"><i class="fa fa-check"></i><b>8.2.3</b> 神经网络</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flashlight.html"><a href="flashlight.html#模型整体表现-model-performance"><i class="fa fa-check"></i><b>8.3</b> 模型整体表现 （model performance）</a></li>
<li class="chapter" data-level="8.4" data-path="flashlight.html"><a href="flashlight.html#变量重要性variable-importance"><i class="fa fa-check"></i><b>8.4</b> 变量重要性（variable importance）</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flashlight.html"><a href="flashlight.html#permutation-importance"><i class="fa fa-check"></i><b>8.4.1</b> Permutation importance</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flashlight.html"><a href="flashlight.html#边缘效应主效应"><i class="fa fa-check"></i><b>8.5</b> 边缘效应（主效应）</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flashlight.html"><a href="flashlight.html#individual-conditional-expectationsice"><i class="fa fa-check"></i><b>8.5.1</b> Individual conditional expectations（ICE）</a></li>
<li class="chapter" data-level="8.5.2" data-path="flashlight.html"><a href="flashlight.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>8.5.2</b> Partial dependence profiles</a></li>
<li class="chapter" data-level="8.5.3" data-path="flashlight.html"><a href="flashlight.html#accumulated-local-effects-profiles-ale"><i class="fa fa-check"></i><b>8.5.3</b> Accumulated local effects profiles (ALE)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="flashlight.html"><a href="flashlight.html#交互效应"><i class="fa fa-check"></i><b>8.6</b> 交互效应</a></li>
<li class="chapter" data-level="8.7" data-path="flashlight.html"><a href="flashlight.html#全局代理模型global-surrogate-models"><i class="fa fa-check"></i><b>8.7</b> 全局代理模型（Global surrogate models）</a></li>
<li class="chapter" data-level="8.8" data-path="flashlight.html"><a href="flashlight.html#局部解释样本解释"><i class="fa fa-check"></i><b>8.8</b> 局部解释（样本解释？）</a><ul>
<li class="chapter" data-level="8.8.1" data-path="flashlight.html"><a href="flashlight.html#lime和live"><i class="fa fa-check"></i><b>8.8.1</b> LIME和LIVE</a></li>
<li class="chapter" data-level="8.8.2" data-path="flashlight.html"><a href="flashlight.html#shapshapley-additive-explanations"><i class="fa fa-check"></i><b>8.8.2</b> SHAP(Shapley Additive Explanations)</a></li>
<li class="chapter" data-level="8.8.3" data-path="flashlight.html"><a href="flashlight.html#breakdown-and-approximate-shap"><i class="fa fa-check"></i><b>8.8.3</b> Breakdown and approximate SHAP</a></li>
<li class="chapter" data-level="8.8.4" data-path="flashlight.html"><a href="flashlight.html#from-local-to-global-properties"><i class="fa fa-check"></i><b>8.8.4</b> From local to global properties</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="flashlight.html"><a href="flashlight.html#improving-the-glm-by-interpretable-machine-learning"><i class="fa fa-check"></i><b>8.9</b> Improving the GLM by interpretable machine learning</a></li>
<li class="chapter" data-level="8.10" data-path="flashlight.html"><a href="flashlight.html#案例分析"><i class="fa fa-check"></i><b>8.10</b> 案例分析</a><ul>
<li class="chapter" data-level="8.10.1" data-path="flashlight.html"><a href="flashlight.html#导入包"><i class="fa fa-check"></i><b>8.10.1</b> 导入包</a></li>
<li class="chapter" data-level="8.10.2" data-path="flashlight.html"><a href="flashlight.html#预处理-1"><i class="fa fa-check"></i><b>8.10.2</b> 预处理</a></li>
<li class="chapter" data-level="8.10.3" data-path="flashlight.html"><a href="flashlight.html#描述性统计"><i class="fa fa-check"></i><b>8.10.3</b> 描述性统计</a></li>
<li class="chapter" data-level="8.10.4" data-path="flashlight.html"><a href="flashlight.html#建模"><i class="fa fa-check"></i><b>8.10.4</b> 建模</a></li>
<li class="chapter" data-level="8.10.5" data-path="flashlight.html"><a href="flashlight.html#解释"><i class="fa fa-check"></i><b>8.10.5</b> 解释</a></li>
<li class="chapter" data-level="8.10.6" data-path="flashlight.html"><a href="flashlight.html#局部性质"><i class="fa fa-check"></i><b>8.10.6</b> 局部性质</a></li>
<li class="chapter" data-level="8.10.7" data-path="flashlight.html"><a href="flashlight.html#改进glm"><i class="fa fa-check"></i><b>8.10.7</b> 改进glm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>9</b> 卷积神经网络</a><ul>
<li class="chapter" data-level="9.1" data-path="cnn.html"><a href="cnn.html#卷积层-convolution"><i class="fa fa-check"></i><b>9.1</b> 卷积层 (Convolution)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="cnn.html"><a href="cnn.html#超参数"><i class="fa fa-check"></i><b>9.1.1</b> 超参数</a></li>
<li class="chapter" data-level="9.1.2" data-path="cnn.html"><a href="cnn.html#参数个数计算"><i class="fa fa-check"></i><b>9.1.2</b> 参数个数计算</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="cnn.html"><a href="cnn.html#池化层-pooling"><i class="fa fa-check"></i><b>9.2</b> 池化层 (Pooling)</a></li>
<li class="chapter" data-level="9.3" data-path="cnn.html"><a href="cnn.html#批标准化层-batch-normalization"><i class="fa fa-check"></i><b>9.3</b> 批标准化层 (Batch Normalization)</a></li>
<li class="chapter" data-level="9.4" data-path="cnn.html"><a href="cnn.html#其他组件"><i class="fa fa-check"></i><b>9.4</b> 其他组件</a><ul>
<li class="chapter" data-level="9.4.1" data-path="cnn.html"><a href="cnn.html#全连接层-dense"><i class="fa fa-check"></i><b>9.4.1</b> 全连接层 (Dense)</a></li>
<li class="chapter" data-level="9.4.2" data-path="cnn.html"><a href="cnn.html#输出神经元"><i class="fa fa-check"></i><b>9.4.2</b> 输出神经元</a></li>
<li class="chapter" data-level="9.4.3" data-path="cnn.html"><a href="cnn.html#激活函数-activation"><i class="fa fa-check"></i><b>9.4.3</b> 激活函数 (Activation)</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="cnn.html"><a href="cnn.html#特性"><i class="fa fa-check"></i><b>9.5</b> 特性</a><ul>
<li class="chapter" data-level="9.5.1" data-path="cnn.html"><a href="cnn.html#平移不变性"><i class="fa fa-check"></i><b>9.5.1</b> 平移不变性</a></li>
<li class="chapter" data-level="9.5.2" data-path="cnn.html"><a href="cnn.html#旋转不变性"><i class="fa fa-check"></i><b>9.5.2</b> 旋转不变性</a></li>
<li class="chapter" data-level="9.5.3" data-path="cnn.html"><a href="cnn.html#尺度不变性"><i class="fa fa-check"></i><b>9.5.3</b> 尺度不变性</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="cnn.html"><a href="cnn.html#隐藏层可视化"><i class="fa fa-check"></i><b>9.6</b> 隐藏层可视化</a></li>
<li class="chapter" data-level="9.7" data-path="cnn.html"><a href="cnn.html#逆卷积"><i class="fa fa-check"></i><b>9.7</b> 逆卷积</a></li>
<li class="chapter" data-level="9.8" data-path="cnn.html"><a href="cnn.html#human-mortality-database-hmd"><i class="fa fa-check"></i><b>9.8</b> <span>Human Mortality Database (HMD)</span></a><ul>
<li class="chapter" data-level="9.8.1" data-path="cnn.html"><a href="cnn.html#输入和标签"><i class="fa fa-check"></i><b>9.8.1</b> 输入和标签</a></li>
<li class="chapter" data-level="9.8.2" data-path="cnn.html"><a href="cnn.html#评估指标"><i class="fa fa-check"></i><b>9.8.2</b> 评估指标</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="cnn.html"><a href="cnn.html#mnist-dataset"><i class="fa fa-check"></i><b>9.9</b> MNIST dataset</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/sxpyggy/Modern-Actuarial-Models/tree/modern-actuarial-models" target="blank">GitHub 仓库</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">现代精算统计模型</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-learning" class="section level1">
<h1><span class="header-section-number">5</span> 无监督学习方法</h1>
<p><em>梁译中、方明慧、王晗、高光远</em></p>
<p>大学及以后生活中最常用的学习方法。</p>
<p>在无监督学习中， 我们可以降低数据（协变量，特征）维度、根据特征的相似度对样本进行聚类、设计可视化工具揭示高维数据的特性。无监督学习不考虑响应变量，仅考虑特征的相似性。</p>
<p>本章将考虑以下几种方法：</p>
<ul>
<li><p><strong>降维</strong>：</p>
<ul>
<li><p>主成分分析（PCA）</p></li>
<li><p>自编码，瓶颈神经网络（BNN）</p></li>
</ul></li>
<li><p><strong>聚类</strong>：</p>
<ul>
<li><p>分层聚类：不需事先指定聚类个数</p>
<ul>
<li><p>自下而上：初始<span class="math inline">\(n\)</span>类，再将相距最近的两类合并，建立一个新的类，直到最后合并成<span class="math inline">\(1\)</span>类；</p></li>
<li><p>自上而下：初始<span class="math inline">\(1\)</span>类，再将相距最远的样本分裂成两类，直到最后分裂成<span class="math inline">\(n\)</span>个类。</p></li>
</ul></li>
<li><p>基于质心的聚类: K-means, K-medoids</p></li>
<li><p>基于分布的聚类: Gaussian mixture models (GMMs)</p></li>
</ul></li>
<li><p><strong>可视化高维数据</strong>：</p>
<ul>
<li><p>变分自动编码器（VAE）</p></li>
<li><p><span class="math inline">\(t\)</span>分布随机邻近嵌入（<span class="math inline">\(t\)</span>-SNE），</p></li>
<li><p>统一流形逼近和投影（UMAP），</p></li>
<li><p>自组织映射（SOM）</p></li>
<li><p>Kohonen图。</p></li>
</ul></li>
</ul>
<div id="数据预处理-3" class="section level2">
<h2><span class="header-section-number">5.1</span> 数据预处理</h2>
<p>数据中各个变量的说明如下：</p>
<table>
<thead>
<tr class="header">
<th align="center">变量</th>
<th align="center">类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">brand</td>
<td align="center">factor</td>
<td>43个汽车品牌</td>
</tr>
<tr class="even">
<td align="center">type</td>
<td align="center">factor</td>
<td>96个水平</td>
</tr>
<tr class="odd">
<td align="center">model</td>
<td align="center">factor</td>
<td>113个水平</td>
</tr>
<tr class="even">
<td align="center">seats</td>
<td align="center">int</td>
<td>座位数</td>
</tr>
<tr class="odd">
<td align="center">max_power</td>
<td align="center">int</td>
<td>发动机最大功率(kW),取对数</td>
</tr>
<tr class="even">
<td align="center">max_torque</td>
<td align="center">num</td>
<td>最大转矩(Nm),取对数</td>
</tr>
<tr class="odd">
<td align="center">cubic_capacity</td>
<td align="center">int</td>
<td>容量(cm<span class="math inline">\(^3\)</span>),取对数</td>
</tr>
<tr class="even">
<td align="center">weight</td>
<td align="center">int</td>
<td>车重(kg)，取对数</td>
</tr>
<tr class="odd">
<td align="center">max_engine_speed</td>
<td align="center">int</td>
<td>发动机最大转速(rpm)</td>
</tr>
<tr class="even">
<td align="center">seconds_to_100</td>
<td align="center">int</td>
<td>达到100km/h所需要秒数</td>
</tr>
<tr class="odd">
<td align="center">top_speed</td>
<td align="center">int</td>
<td>最大行驶速度(km/h)</td>
</tr>
<tr class="even">
<td align="center">sports_car</td>
<td align="center">int</td>
<td>跑车</td>
</tr>
<tr class="odd">
<td align="center">tau</td>
<td align="center">num</td>
<td>专家评分</td>
</tr>
</tbody>
</table>
<p>Figure <a href="unsupervised-learning.html#fig:pairs">5.3</a> 显示了各个变量（取对数后）的散点图，Q-Q图，及相关系数。</p>
<div class="figure" style="text-align: center"><span id="fig:pairs1"></span>
<img src="plots/5/log.png" alt="散点图" width="40%" />
<p class="caption">
Figure 5.1: 散点图
</p>
</div>
<p><span class="math inline">\(\tau\)</span>为专家提出的评分方程，据此评分可以大概判断该车是否为跑车：</p>
<p><span class="math display">\[\tau=\frac{\text{weight}}{\frac{\text{max_power}}{0.735499}}\text{seats}^{\frac{1}{3}}\left(\frac{\text{cubic_capacity}}{1000}\right)^{\frac{1}{4}}\]</span>
如果把常数项提出，可得到如下等价的评分<span class="math inline">\(\tau^+\)</span>:</p>
<p><span class="math display">\[\tau^+=\frac{\text{weight}}{\text{max_power}}\text{seats}^{\frac{1}{3}}\text{cubic_capacity}^{\frac{1}{4}}\]</span></p>
<p>专家把<span class="math inline">\(\tau&lt;17\)</span>或<span class="math inline">\(\tau^+&lt;129.9773\)</span>的汽车定义为跑车。</p>
</div>
<div id="主成分分析" class="section level2">
<h2><span class="header-section-number">5.2</span> 主成分分析</h2>
<p>Ingenbleek-Lemaire (1988) 的目标是利用主成分分析来对数据进行降维，根据选取的主成分来区分跑车和普通车，并尝试达到和专家选择一样的效果。</p>
<p>PCA适用于高斯分布，若变量显著不符合高斯分布，需要对数据进行预处理（比如取对数或其他方法）。
Ingenbleek-Lemaire (1988) 构造了以下5个近似服从高斯分布的变量以便进行后续分析。</p>
<p><span class="math display">\[x_1^*=\ln\left(\frac{\text{weight}}{\text{max_power}}\right)\]</span>
<span class="math display">\[x_2^*=\ln\left(\frac{\text{max_power}}{\text{cubic_capacity}}\right)\]</span>
<span class="math display">\[x_3^*=\ln\left(\text{max_torque}\right)\]</span>
<span class="math display">\[x_4^*=\ln\left(\text{max_engine_speed}\right)\]</span> <span class="math display">\[x_5^*=log\left(\text{cubic_capacity}\right)\]</span></p>
<p>Figure (fig:pairs2) 展示了以上5个变量的相关性。</p>
<div class="figure" style="text-align: center"><span id="fig:pairs2"></span>
<img src="plots/5/x1-x5scatter.png" alt="散点图" width="40%"  />
<p class="caption">
Figure 5.2: 散点图
</p>
</div>
<p>主成分分析可以降低高维数据的维数，使相对于原始数据的重构误差最小。如果应用成功，它减少了特征空间的维数，并且它对于(精算)回归建模特别有用，因为它提供了少量的不相关的解释变量。</p>
<p>假设样本量为<span class="math inline">\(n\)</span>的样本有<span class="math inline">\(q\)</span>个特征<span class="math inline">\(\mathbf{x}_1^*,\ldots,\mathbf{x}^*_n\in\mathbb{R}^q\)</span>。其设计矩阵为
<span class="math display">\[\mathbf{X}^*=(\mathbf{x}_1^*,\ldots,\mathbf{x}^*_n)^\intercal\in\mathbb{R}^{n\times q}.\]</span>
把设计矩阵的每列进行标准化，得到
<span class="math display">\[\mathbf{X}=(x_{i,j})_{1\le i \le n,1\le j\le q}\in\mathbb{R}^{n\times q}.\]</span>
其中，第<span class="math inline">\(i\)</span>行是样本<span class="math inline">\(i\)</span>的特征<span class="math inline">\(\mathbf{x}_i\in\mathbb{R}^q, 1\le i\le n\)</span>, 第<span class="math inline">\(j\)</span>列是第<span class="math inline">\(j\)</span>个特征<span class="math inline">\(x_j\in\mathbb{R}^n\)</span>.</p>
<p>矩阵<span class="math inline">\(\mathbf{X}\)</span>的秩为<span class="math inline">\(q\le n\)</span>，可以找到<span class="math inline">\(q\)</span>个正交的<span class="math inline">\(q\)</span>维基向量<span class="math inline">\(\mathbf{v}_1,\ldots,\mathbf{v}_q\in\mathbb{R}^q\)</span>, 使得<span class="math inline">\(\mathbf{v}_1\)</span>为<span class="math inline">\(\mathbf{X}\)</span>波动最大的方向，<span class="math inline">\(\mathbf{v}_2\)</span>为与<span class="math inline">\(\mathbf{v}_1\)</span>正交方向上的<span class="math inline">\(\mathbf{X}\)</span>波动最大的方向，依次类推。</p>
<p>用数学公式表示如下：
<span class="math display">\[\mathbf{v}_1=\underset{||\omega||_2=1}{\arg \max}||\mathbf{X}\omega||_2^2=\underset{\omega^\intercal\omega=1}{\arg \max} (\omega^\intercal\mathbf{X}^\intercal\mathbf{X}\omega)\]</span></p>
<p><span class="math display">\[\mathbf{v}_2=\underset{||\omega||_2=1}{\arg \max}||\mathbf{X}\omega||_2^2 ~~~\text{ subject to } \mathbf{v}_1^\intercal\omega=0.\]</span></p>
<p><span class="math display">\[\ldots\]</span></p>
<p>主成分分析可通过以下两种方式实现</p>
<ul>
<li><p>求<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>或者<span class="math inline">\(\mathbf{X}\)</span>的协方差矩阵<span class="math inline">\(\mathbf{\Sigma}\)</span>的特征向量和特征值。易知<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}=n\times\mathbf{\Sigma}\)</span>，所以它们的特征向量相同。第一个特征向量即为<span class="math inline">\(\mathbf{v}_1\)</span>，第二个特征向量为<span class="math inline">\(\mathbf{v}_2\)</span>。前两个主成分为<span class="math inline">\(\mathbf{X}\mathbf{v}_1,\mathbf{X}\mathbf{v}_2\)</span></p></li>
<li><p>对<span class="math inline">\(\mathbf{X}\)</span>进行奇异值（singular value decomposition）分解:<span class="math display">\[\mathbf{X}=U\Lambda V^\intercal.\]</span>其中，对角矩阵<span class="math inline">\(\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_q)\)</span>的元素为<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>的特征值，<span class="math inline">\(V\)</span>为<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>的特征向量。主成分可以通过<span class="math inline">\(\mathbf{X}V\)</span>求得。</p></li>
</ul>
<p>利用前<span class="math inline">\(p\)</span>个主成分可以重构设计矩阵的近似值<span class="math display">\[\mathbf{X}_p=U\text{diag}(\lambda_1,\ldots,\lambda_p,0,\ldots,0)V^{\intercal}.\]</span></p>
<p>该近似值为以下极值问题的根<span class="math display">\[\underset{B\in\mathbb{R}^{n\times q}}{\arg \min}||\mathbf{X}-B||^2 ~~\text{subject to rank}(B)\le q,\]</span></p>
<p>即矩阵<span class="math inline">\(\mathbf{X}_p\)</span>是所有秩为<span class="math inline">\(p\)</span>的矩阵中，与原始设计矩阵<span class="math inline">\(\mathbf{X}\)</span>重组平方误差(F范数)最小的矩阵。</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="unsupervised-learning.html#cb65-1"></a><span class="co"># standardize matrix</span></span>
<span id="cb65-2"><a href="unsupervised-learning.html#cb65-2"></a>X &lt;-<span class="st"> </span>X01<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">colMeans</span>(X01<span class="op">^</span><span class="dv">2</span>))[<span class="kw">col</span>(X01)]</span>
<span id="cb65-3"><a href="unsupervised-learning.html#cb65-3"></a></span>
<span id="cb65-4"><a href="unsupervised-learning.html#cb65-4"></a><span class="co"># eigenvectors and eigenvalues</span></span>
<span id="cb65-5"><a href="unsupervised-learning.html#cb65-5"></a>X1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X)</span>
<span id="cb65-6"><a href="unsupervised-learning.html#cb65-6"></a><span class="kw">nrow</span>(X1)</span>
<span id="cb65-7"><a href="unsupervised-learning.html#cb65-7"></a>A &lt;-<span class="st">  </span><span class="kw">t</span>(X1) <span class="op">%*%</span><span class="st"> </span>X1</span>
<span id="cb65-8"><a href="unsupervised-learning.html#cb65-8"></a>A</span>
<span id="cb65-9"><a href="unsupervised-learning.html#cb65-9"></a><span class="kw">sum</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value)<span class="op">/</span><span class="dv">5</span></span>
<span id="cb65-10"><a href="unsupervised-learning.html#cb65-10"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value)      <span class="co"># singular values</span></span>
<span id="cb65-11"><a href="unsupervised-learning.html#cb65-11"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value<span class="op">/</span><span class="kw">nrow</span>(X1))   <span class="co"># scaled eigenvalues</span></span>
<span id="cb65-12"><a href="unsupervised-learning.html#cb65-12"></a><span class="kw">eigen</span>(A)<span class="op">$</span>vector</span>
<span id="cb65-13"><a href="unsupervised-learning.html#cb65-13"></a>A1&lt;-<span class="kw">cor</span>(X1)</span>
<span id="cb65-14"><a href="unsupervised-learning.html#cb65-14"></a>A1<span class="op">*</span><span class="kw">nrow</span>(X1)</span>
<span id="cb65-15"><a href="unsupervised-learning.html#cb65-15"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A1)<span class="op">$</span>value)  </span>
<span id="cb65-16"><a href="unsupervised-learning.html#cb65-16"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>vector</span>
<span id="cb65-17"><a href="unsupervised-learning.html#cb65-17"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>value</span>
<span id="cb65-18"><a href="unsupervised-learning.html#cb65-18"></a></span>
<span id="cb65-19"><a href="unsupervised-learning.html#cb65-19"></a><span class="co"># singular value decomposition</span></span>
<span id="cb65-20"><a href="unsupervised-learning.html#cb65-20"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(X1)</span>
<span id="cb65-21"><a href="unsupervised-learning.html#cb65-21"></a>SVD<span class="op">$</span>d                       <span class="co"># singular values</span></span>
<span id="cb65-22"><a href="unsupervised-learning.html#cb65-22"></a><span class="kw">rbind</span>(SVD<span class="op">$</span>v[,<span class="dv">1</span>],SVD<span class="op">$</span>v[,<span class="dv">2</span>])  <span class="co"># first two right singular vectors</span></span>
<span id="cb65-23"><a href="unsupervised-learning.html#cb65-23"></a></span>
<span id="cb65-24"><a href="unsupervised-learning.html#cb65-24"></a><span class="co"># PCA with package PCA</span></span>
<span id="cb65-25"><a href="unsupervised-learning.html#cb65-25"></a>t.pca &lt;-<span class="st"> </span><span class="kw">princomp</span>(X1,<span class="dt">cor=</span><span class="ot">TRUE</span>)</span>
<span id="cb65-26"><a href="unsupervised-learning.html#cb65-26"></a>t.pca<span class="op">$</span>loadings          </span>
<span id="cb65-27"><a href="unsupervised-learning.html#cb65-27"></a><span class="kw">summary</span>(t.pca)</span>
<span id="cb65-28"><a href="unsupervised-learning.html#cb65-28"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>value<span class="op">/</span><span class="kw">sum</span>(<span class="kw">eigen</span>(A1)<span class="op">$</span>value)</span></code></pre></div>
<p>通过<code>summary(t.pca)</code>我们可以得到前两个主成分的累计贡献度已经到<span class="math inline">\(92%\)</span>，前两个主成分提取了原始数据的绝大部分信息，所以我们选择2个主成分应该可以较好地重构原始数据。</p>
<p>以第一主成分为例说明第一主成分和<span class="math inline">\(x_1^*,\ldots,x_5^*\)</span>的关系</p>
<pre class="{r，eval=f}"><code># PCA Sports Cars weights
alpha &lt;- SVD$v[,1]/sds
(alpha_star &lt;- c(alpha[1],alpha[2]-alpha[1], alpha[3], alpha[4], alpha[5]-alpha[2])/alpha[1])</code></pre>
<p><span class="math inline">\(y_1=\left&lt;\mathbf{v_1},\mathbf{x}\right&gt;=-0.558x_1+0.412x_2+0.539x_3+0.126x_4+0.461x_5\)</span>因为此处的<span class="math inline">\(x_1,\ldots,x_5\)</span>来自标准设计矩阵，所以我们做逆变换<span class="math inline">\(\alpha_lx_l^*=\alpha_l\left(\hat{\sigma_l}\frac{x_l^*-\hat{\mu_l}}{\hat{\sigma_l}}+\hat{\mu_l}\right)=\alpha_l\hat{\sigma_l}x_l+\alpha_l\hat{\mu_l}\)</span>,其中<span class="math inline">\(\alpha_l\hat{\sigma_l}=\mathbf{v_{1,l}}\)</span>,由此可得原始方程为<span class="math inline">\(\frac{y^*}{\alpha_1}=log(\text{weight})-1.93log(\text{max_power})-0.65log(\text{max_torque})-0.64log(\text{max_engine_speed})+0.25log(\text{cubic_capacity})\)</span>将样本带入计算我们即可得到第一主成分的得分。第二主成分计算同上。</p>
<pre class="{r，eval=f}"><code># scatter plot
switch_sign &lt;- -1           # switch sign of the first component to make svd and princomp compatible
tt.pca &lt;- t.pca$scores
tt.pca[,1] &lt;- switch_sign *tt.pca[,1]
pairs(tt.pca,diag.panel=panel.qq,upper.panel=panel.cor)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:pairs"></span>
<img src="plots/5/pcascatter.png" alt="主成分散点图" width="40%"  />
<p class="caption">
Figure 5.3: 主成分散点图
</p>
</div>
<p>Figure 5.3:对角线为Q-Q图，左下部分散点图，右上部分相关系数图（各个主成分之间相互独立，所以相关系数为0）</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="unsupervised-learning.html#cb68-1"></a><span class="co"># plot first two principal components</span></span>
<span id="cb68-2"><a href="unsupervised-learning.html#cb68-2"></a>dat3 &lt;-<span class="st"> </span>d.data </span>
<span id="cb68-3"><a href="unsupervised-learning.html#cb68-3"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span>X1 <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">1</span>]</span>
<span id="cb68-4"><a href="unsupervised-learning.html#cb68-4"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span>X1 <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">2</span>]</span>
<span id="cb68-5"><a href="unsupervised-learning.html#cb68-5"></a></span>
<span id="cb68-6"><a href="unsupervised-learning.html#cb68-6"></a><span class="co"># png(&quot;./plots/5/pca.png&quot;)</span></span>
<span id="cb68-7"><a href="unsupervised-learning.html#cb68-7"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">ylab=</span><span class="st">&quot;2nd principal component&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;1st principal component&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;principal components analysis&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb68-8"><a href="unsupervised-learning.html#cb68-8"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(dat3<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb68-9"><a href="unsupervised-learning.html#cb68-9"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb68-10"><a href="unsupervised-learning.html#cb68-10"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(dat3<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb68-11"><a href="unsupervised-learning.html#cb68-11"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb68-12"><a href="unsupervised-learning.html#cb68-12"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb68-13"><a href="unsupervised-learning.html#cb68-13"></a><span class="co">#dev.off()</span></span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:2pcas"></span>
<img src="plots/5/pca.png" alt="主成分得分图" width="40%"  />
<p class="caption">
Figure 5.4: 主成分得分图
</p>
</div>
<p>Figure 5.4:样本(n=475)的主成分得分图，其中蓝色点和红色点之间有一个超平面将跑车和普通车很好的区分开了。</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="unsupervised-learning.html#cb69-1"></a><span class="co"># reconstruction error</span></span>
<span id="cb69-2"><a href="unsupervised-learning.html#cb69-2"></a>reconstruction.PCA &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(<span class="dv">5</span>))</span>
<span id="cb69-3"><a href="unsupervised-learning.html#cb69-3"></a></span>
<span id="cb69-4"><a href="unsupervised-learning.html#cb69-4"></a><span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>){</span>
<span id="cb69-5"><a href="unsupervised-learning.html#cb69-5"></a>  Xp &lt;-<span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>p] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(SVD<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>p]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X)</span>
<span id="cb69-6"><a href="unsupervised-learning.html#cb69-6"></a>  Xp &lt;-<span class="st"> </span><span class="kw">t</span>(Xp)</span>
<span id="cb69-7"><a href="unsupervised-learning.html#cb69-7"></a>  reconstruction.PCA[p] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(<span class="kw">as.matrix</span>((X<span class="op">-</span>Xp)<span class="op">^</span><span class="dv">2</span>))<span class="op">/</span><span class="kw">nrow</span>(X))</span>
<span id="cb69-8"><a href="unsupervised-learning.html#cb69-8"></a>               }</span>
<span id="cb69-9"><a href="unsupervised-learning.html#cb69-9"></a><span class="kw">round</span>(reconstruction.PCA,<span class="dv">2</span>)               </span>
<span id="cb69-10"><a href="unsupervised-learning.html#cb69-10"></a></span>
<span id="cb69-11"><a href="unsupervised-learning.html#cb69-11"></a><span class="co"># biplot</span></span>
<span id="cb69-12"><a href="unsupervised-learning.html#cb69-12"></a>tt.pca &lt;-<span class="st"> </span>t.pca</span>
<span id="cb69-13"><a href="unsupervised-learning.html#cb69-13"></a>tt.pca<span class="op">$</span>scores[,<span class="dv">1</span>] &lt;-<span class="st">  </span>switch_sign <span class="op">*</span><span class="st"> </span>tt.pca<span class="op">$</span>scores[,<span class="dv">1</span>]</span>
<span id="cb69-14"><a href="unsupervised-learning.html#cb69-14"></a>tt.pca<span class="op">$</span>loadings[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span>] &lt;-<span class="st"> </span>switch_sign <span class="op">*</span><span class="st"> </span>tt.pca<span class="op">$</span>loadings[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span>] </span>
<span id="cb69-15"><a href="unsupervised-learning.html#cb69-15"></a><span class="kw">biplot</span>(tt.pca,<span class="dt">choices=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dt">scale=</span><span class="dv">0</span>, <span class="dt">expand=</span><span class="dv">2</span>, <span class="dt">xlab=</span><span class="st">&quot;1st principal component&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;2nd principal component&quot;</span>, <span class="dt">cex=</span><span class="kw">c</span>(<span class="fl">0.4</span>,<span class="fl">1.5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:2pcas-2"></span>
<img src="plots/5/biplot.png" alt="矢量分解图" width="40%"  />
<p class="caption">
Figure 5.5: 矢量分解图
</p>
</div>
<p>Figure 5.5:黑点之间的距离表示相似性。红色矢量是前两个标准正交权值向量<span class="math inline">\(\mathbf{v_1}\)</span>和<span class="math inline">\(\mathbf{v_2}\)</span>的分量。长度反映了变量的标准差，夹角的余弦值给出了相应的相关性。</p>
</div>
<div id="自编码" class="section level2">
<h2><span class="header-section-number">5.3</span> 自编码</h2>
<p>PCA对异常值很敏感，也有稳健的PCA版本。例如，Croux等人给出了一个基于中值绝对偏差(MADs)的算法，R包：pcaPP。
主成分分析可以看作是一个自动编码器。接下来，我们将更一般地介绍自动编码器，例如BNN。</p>
<p>自编码包含编码和解码两个镜面对称的映射：</p>
<ul>
<li><p>编码: <span class="math inline">\(\varphi:\mathbb{R}^q\rightarrow\mathbb{R}^p\)</span></p></li>
<li><p>解码: <span class="math inline">\(\psi:\mathbb{R}^p\rightarrow\mathbb{R}^q\)</span>
其中<span class="math inline">\(p \le q\)</span>，我们选择一个度量差异的函数<span class="math inline">\(d(·,·)\)</span>，当且仅当<span class="math inline">\(\mathbf{x}=\mathbf{y}\)</span>时<span class="math inline">\(d(x,y)=0\)</span>，自编码就是找到一对<span class="math inline">\((\varphi,\psi)\)</span>使得有<span class="math inline">\(\pi=\psi\circ\varphi\)</span>满足<span class="math inline">\(d(\pi(\mathbf{x}),\mathbf{x})\)</span>最小。</p></li>
</ul>
<p>在PCA的例子中：</p>
<p><span class="math display">\[\varphi:\mathbb{R}^q\rightarrow\mathbb{R}^p,\mathbf{x}\mapsto\mathbf{y}=\varphi(\mathbf{x})=(\left&lt;\mathbf{v_1},\mathbf{x}\right&gt;,\ldots,\left&lt;\mathbf{v_p},\mathbf{x}\right&gt;)^T=(\mathbf{v_1},\ldots,\mathbf{v_p})^T\mathbf{x}.\]</span></p>
<p><span class="math display">\[\psi:\mathbb{R}^p\rightarrow\mathbb{R}^q,\mathbf{y}\mapsto\psi(\mathbf{y})=(\mathbf{v_1},\ldots,\mathbf{v_p})\mathbf{y}.\]</span></p>
<p><span class="math display">\[\pi(\mathbf{X}^T)=\psi\circ\varphi(\mathbf{x}^T)=(\mathbf{v_1},\ldots,\mathbf{v_p})(\mathbf{v_1},\ldots,\mathbf{v_p})^T\mathbf{X}^T=\mathbf{X_p}^T.\]</span></p>
<p><span class="math display">\[\sum_{i=1}^nd(\pi(\mathbf{x}_i),\mathbf{x}_i)=\sum_{i=1}^n\|\pi(\mathbf{x}_i-\mathbf{x}_i)\|_2^2=\|\mathbf{X}_p-\mathbf{X}\|_F^2.\]</span></p>
<p>此处衡量重构误差的为针对矩阵的欧氏距离，即F范数。</p>
<p>作为非线性自编码器的一个例子，我们考虑瓶颈神经网络(BNN)。为了成功校准一个BNN，它的隐藏层数应该是奇数<span class="math inline">\(d\)</span> (<span class="math inline">\(d\)</span>称为神经网络的深度)，并且中心隐藏层应该是低维的，有<span class="math inline">\(p\)</span>个隐藏神经元，所有剩余的隐藏层应该是围绕这个中心隐藏层对称的。因此对于深度<span class="math inline">\(d = 3\)</span>的BNN，我们可以选择图<a href="unsupervised-learning.html#fig:bnn-structure">5.6</a>展示的神经网络结构</p>
<div class="figure" style="text-align: center"><span id="fig:bnn-structure"></span>
<img src="plots/5/bnn.png" alt="自编码 q=5,p=2" width="40%"  />
<p class="caption">
Figure 5.6: 自编码 q=5,p=2
</p>
</div>
<p>一般的神经网络有如下结构：</p>
<p><span class="math display">\[\pi:\mathbb{R}^{q_0}\rightarrow\mathbb{R}^{q_{d+1}=q_0},\mathbf{x}\mapsto\pi(\mathbf{x})=(\mathbf{z}^\left(d+1\right)\circ\mathbf{z}^\left(d\right)\circ\ldots\circ\mathbf{z}^\left(1\right))(\mathbf{x})\]</span></p>
<p><span class="math display">\[\mathbf{z}^\left(m\right):\mathbb{R}^{q_{m-1}}\rightarrow\mathbb{R}^{q_m},\mathbf{z}\mapsto\mathbf{z}^\left(m\right)(\mathbf{z})=\left(\phi\left(\left&lt;\mathbf{w}_1^{\left(m\right)},\mathbf{z}\right&gt;\right),\ldots,\phi\left(\left&lt;\mathbf{w}_{q_m}^{\left(m\right)},\mathbf{z}\right&gt;\right)\right)^T\]</span></p>
<p>其中<span class="math inline">\(\mathbf{w}_l^{\left(m\right)}\in\mathbb{R}^{q_{m-1}},1 \le l\le q_m\)</span>为权重，<span class="math inline">\(\phi:\mathbb{R}\rightarrow\mathbb{R}\)</span>为激活函数。</p>
<p><span class="math display">\[\mathbf{z}^\left(d+1\right):\mathbb{R}^{q_{d}}\rightarrow\mathbb{R}^{q_{d+1}},\mathbf{z}\mapsto\mathbf{z}^\left(d+1\right)(\mathbf{z})=\left(\left&lt;\mathbf{w}_1^{\left(d+1\right)},\mathbf{z}\right&gt;,\ldots,\left&lt;\mathbf{w}_{q_{d+1}}^{\left(d+1\right)},\mathbf{z}\right&gt;\right)^T\]</span></p>
<p>总参数个数为<span class="math inline">\(r=\sum_{m=1}^{d+1}q_mq_{m-1}\)</span>。</p>
<ul>
<li><p>与经典的前馈神经网络相比，我们这里没有截距项，因为特征$_i已经被标准化。这略微降低了网络参数的维数。</p></li>
<li><p>选择输出激活为线性激活，是因为x的所有成份都在实数领域中。下面我们还将应用其他输出激活函数。</p></li>
<li><p>作为隐层的激活函数，我们通常选用双曲正切函数。如果一个BNN只有线性激活函数，那么他是等价于PCA的。</p></li>
</ul>
<div id="模型训练" class="section level3">
<h3><span class="header-section-number">5.3.1</span> 模型训练</h3>
<p>具体过程如下：</p>
<p>在正式进入神经网之前，我们要先进行权重预训练。</p>
<div class="figure" style="text-align: center"><span id="fig:bnn-train"></span>
<img src="plots/5/bnn_train.png" alt="自编码训练过程" width="60%"  />
<p class="caption">
Figure 5.7: 自编码训练过程
</p>
</div>
<ul>
<li><p>首先我们先将中间的隐藏层折叠得到如图<a href="unsupervised-learning.html#fig:bnn-train">5.7</a>中间结构的神经网络，据此我们得到权重：<span class="math inline">\(\mathbf{w}_1^{(1)}\ldots,\mathbf{w}_{q_1=7}^{(1)}\in\mathbb{R}^{q_0=5}\)</span>
和<span class="math inline">\(\mathbf{w}_1^{(d+1)}\ldots,\mathbf{w}_{q_{d+1}=5}^{(d+1)}\in\mathbb{R}^{q_d=7}\)</span></p></li>
<li><p>再训练隐藏层即图<a href="unsupervised-learning.html#fig:bnn-train">5.7</a>中右图的权重，得到：<span class="math inline">\(\mathbf{w}_1^{(2)}\ldots,\mathbf{w}_{q_2=2}^{(2)}\in\mathbb{R}^{q_1=7}\)</span>
和<span class="math inline">\(\mathbf{w}_1^{(3)}\ldots,\mathbf{w}_{q_{3}=7}^{(3)}\in\mathbb{R}^{q_2=2}\)</span></p></li>
</ul>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="unsupervised-learning.html#cb70-1"></a>bottleneck<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="cf">function</span>(q00, q22){</span>
<span id="cb70-2"><a href="unsupervised-learning.html#cb70-2"></a>   Input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(q00), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Input&#39;</span>)</span>
<span id="cb70-3"><a href="unsupervised-learning.html#cb70-3"></a>   </span>
<span id="cb70-4"><a href="unsupervised-learning.html#cb70-4"></a>   Output =<span class="st"> </span>Input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-5"><a href="unsupervised-learning.html#cb70-5"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q22, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Bottleneck&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-6"><a href="unsupervised-learning.html#cb70-6"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q00, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Output&#39;</span>)</span>
<span id="cb70-7"><a href="unsupervised-learning.html#cb70-7"></a></span>
<span id="cb70-8"><a href="unsupervised-learning.html#cb70-8"></a>   model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> Input, <span class="dt">outputs =</span> Output)</span>
<span id="cb70-9"><a href="unsupervised-learning.html#cb70-9"></a>   </span>
<span id="cb70-10"><a href="unsupervised-learning.html#cb70-10"></a>   model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb70-11"><a href="unsupervised-learning.html#cb70-11"></a>   model</span>
<span id="cb70-12"><a href="unsupervised-learning.html#cb70-12"></a>   }</span>
<span id="cb70-13"><a href="unsupervised-learning.html#cb70-13"></a></span>
<span id="cb70-14"><a href="unsupervised-learning.html#cb70-14"></a>bottleneck<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="cf">function</span>(q00, q11, q22){   </span>
<span id="cb70-15"><a href="unsupervised-learning.html#cb70-15"></a>   Input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(q00), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Input&#39;</span>)</span>
<span id="cb70-16"><a href="unsupervised-learning.html#cb70-16"></a>   </span>
<span id="cb70-17"><a href="unsupervised-learning.html#cb70-17"></a>   Encoder =<span class="st"> </span>Input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-18"><a href="unsupervised-learning.html#cb70-18"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q11, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Layer1&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb70-19"><a href="unsupervised-learning.html#cb70-19"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q22, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Bottleneck&#39;</span>) </span>
<span id="cb70-20"><a href="unsupervised-learning.html#cb70-20"></a></span>
<span id="cb70-21"><a href="unsupervised-learning.html#cb70-21"></a>   Decoder =<span class="st"> </span>Encoder <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-22"><a href="unsupervised-learning.html#cb70-22"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q11, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Layer3&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-23"><a href="unsupervised-learning.html#cb70-23"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q00, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Output&#39;</span>)</span>
<span id="cb70-24"><a href="unsupervised-learning.html#cb70-24"></a></span>
<span id="cb70-25"><a href="unsupervised-learning.html#cb70-25"></a>   model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> Input, <span class="dt">outputs =</span> Decoder)</span>
<span id="cb70-26"><a href="unsupervised-learning.html#cb70-26"></a>   model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb70-27"><a href="unsupervised-learning.html#cb70-27"></a>   model</span>
<span id="cb70-28"><a href="unsupervised-learning.html#cb70-28"></a>   }</span></code></pre></div>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="unsupervised-learning.html#cb71-1"></a><span class="co"># bottleneck architecture</span></span>
<span id="cb71-2"><a href="unsupervised-learning.html#cb71-2"></a>q1 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb71-3"><a href="unsupervised-learning.html#cb71-3"></a>q2 &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb71-4"><a href="unsupervised-learning.html#cb71-4"></a>q0 &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)</span>
<span id="cb71-5"><a href="unsupervised-learning.html#cb71-5"></a></span>
<span id="cb71-6"><a href="unsupervised-learning.html#cb71-6"></a><span class="co"># pre-training 1: merging layers 1 and 3 (skipping bottleneck)</span></span>
<span id="cb71-7"><a href="unsupervised-learning.html#cb71-7"></a>model<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.1</span>(q0, q1)</span>
<span id="cb71-8"><a href="unsupervised-learning.html#cb71-8"></a>model<span class="fl">.1</span></span>
<span id="cb71-9"><a href="unsupervised-learning.html#cb71-9"></a>epochs &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb71-10"><a href="unsupervised-learning.html#cb71-10"></a>batch_size &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb71-11"><a href="unsupervised-learning.html#cb71-11"></a></span>
<span id="cb71-12"><a href="unsupervised-learning.html#cb71-12"></a><span class="co"># fit the merged model</span></span>
<span id="cb71-13"><a href="unsupervised-learning.html#cb71-13"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb71-14"><a href="unsupervised-learning.html#cb71-14"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.1</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(X), <span class="kw">as.matrix</span>(X), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb71-15"><a href="unsupervised-learning.html#cb71-15"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb71-16"><a href="unsupervised-learning.html#cb71-16"></a></span>
<span id="cb71-17"><a href="unsupervised-learning.html#cb71-17"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0),  <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="st">&quot;gradient descent algorithm&quot;</span>) </span>
<span id="cb71-18"><a href="unsupervised-learning.html#cb71-18"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.6124</span>), <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>)</span>
<span id="cb71-19"><a href="unsupervised-learning.html#cb71-19"></a> </span>
<span id="cb71-20"><a href="unsupervised-learning.html#cb71-20"></a><span class="co"># neuron activations in the central layer </span></span>
<span id="cb71-21"><a href="unsupervised-learning.html#cb71-21"></a>zz &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs=</span>model<span class="fl">.1</span><span class="op">$</span>input, <span class="dt">outputs=</span><span class="kw">get_layer</span>(model<span class="fl">.1</span>, <span class="st">&#39;Bottleneck&#39;</span>)<span class="op">$</span>output)</span>
<span id="cb71-22"><a href="unsupervised-learning.html#cb71-22"></a>yy &lt;-<span class="st"> </span>zz <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb71-23"><a href="unsupervised-learning.html#cb71-23"></a></span>
<span id="cb71-24"><a href="unsupervised-learning.html#cb71-24"></a><span class="co"># pre-training 2: middlepart</span></span>
<span id="cb71-25"><a href="unsupervised-learning.html#cb71-25"></a>model<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.1</span>(q1, q2)</span>
<span id="cb71-26"><a href="unsupervised-learning.html#cb71-26"></a>model<span class="fl">.2</span></span>
<span id="cb71-27"><a href="unsupervised-learning.html#cb71-27"></a>epochs &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb71-28"><a href="unsupervised-learning.html#cb71-28"></a></span>
<span id="cb71-29"><a href="unsupervised-learning.html#cb71-29"></a><span class="co"># fit the merged model</span></span>
<span id="cb71-30"><a href="unsupervised-learning.html#cb71-30"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb71-31"><a href="unsupervised-learning.html#cb71-31"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.2</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(yy), <span class="kw">as.matrix</span>(yy), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb71-32"><a href="unsupervised-learning.html#cb71-32"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb71-33"><a href="unsupervised-learning.html#cb71-33"></a></span>
<span id="cb71-34"><a href="unsupervised-learning.html#cb71-34"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0),  <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="st">&quot;gradient descent algorithm&quot;</span>) </span></code></pre></div>
<p>预训练结束后，用这些预先训练好的权值对整个BNN进行重构，我们会得到一个重构误差（如下）在此案例中，此处的重构误差大于使用PCA时取两个主成分的重构误差。</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="unsupervised-learning.html#cb72-1"></a><span class="co"># fitting the full model</span></span>
<span id="cb72-2"><a href="unsupervised-learning.html#cb72-2"></a>model<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.3</span>(q0, q1, q2)</span>
<span id="cb72-3"><a href="unsupervised-learning.html#cb72-3"></a>model<span class="fl">.3</span></span>
<span id="cb72-4"><a href="unsupervised-learning.html#cb72-4"></a></span>
<span id="cb72-5"><a href="unsupervised-learning.html#cb72-5"></a><span class="co"># set weights</span></span>
<span id="cb72-6"><a href="unsupervised-learning.html#cb72-6"></a>weight<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.3</span>)</span>
<span id="cb72-7"><a href="unsupervised-learning.html#cb72-7"></a>weight<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.1</span>)</span>
<span id="cb72-8"><a href="unsupervised-learning.html#cb72-8"></a>weight<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.2</span>)</span>
<span id="cb72-9"><a href="unsupervised-learning.html#cb72-9"></a>weight<span class="fl">.3</span>[[<span class="dv">1</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.1</span>[[<span class="dv">1</span>]]</span>
<span id="cb72-10"><a href="unsupervised-learning.html#cb72-10"></a>weight<span class="fl">.3</span>[[<span class="dv">4</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.1</span>[[<span class="dv">2</span>]]</span>
<span id="cb72-11"><a href="unsupervised-learning.html#cb72-11"></a>weight<span class="fl">.3</span>[[<span class="dv">2</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.2</span>[[<span class="dv">1</span>]]</span>
<span id="cb72-12"><a href="unsupervised-learning.html#cb72-12"></a>weight<span class="fl">.3</span>[[<span class="dv">3</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.2</span>[[<span class="dv">2</span>]]</span>
<span id="cb72-13"><a href="unsupervised-learning.html#cb72-13"></a><span class="kw">set_weights</span>(model<span class="fl">.3</span>, weight<span class="fl">.3</span>)</span>
<span id="cb72-14"><a href="unsupervised-learning.html#cb72-14"></a>fit0 &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb72-15"><a href="unsupervised-learning.html#cb72-15"></a></span>
<span id="cb72-16"><a href="unsupervised-learning.html#cb72-16"></a><span class="co"># reconstruction error of the pre-calibrated network</span></span>
<span id="cb72-17"><a href="unsupervised-learning.html#cb72-17"></a><span class="co"># note that this error may differ from the tutorial because we did not set a seed</span></span>
<span id="cb72-18"><a href="unsupervised-learning.html#cb72-18"></a><span class="kw">round</span>(<span class="kw">Frobenius.loss</span>(X,fit0),<span class="dv">4</span>)</span></code></pre></div>
<p>使用这些预先训练好的权值作为初始化，将梯度下降算法应用于整个BNN，以获得BNN降维。</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="unsupervised-learning.html#cb73-1"></a><span class="co"># calibrate full bottleneck network</span></span>
<span id="cb73-2"><a href="unsupervised-learning.html#cb73-2"></a>epochs &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb73-3"><a href="unsupervised-learning.html#cb73-3"></a>batch_size &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb73-4"><a href="unsupervised-learning.html#cb73-4"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb73-5"><a href="unsupervised-learning.html#cb73-5"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(X), <span class="kw">as.matrix</span>(X), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb73-6"><a href="unsupervised-learning.html#cb73-6"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb73-7"><a href="unsupervised-learning.html#cb73-7"></a></span>
<span id="cb73-8"><a href="unsupervised-learning.html#cb73-8"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;gradient descent algorithm&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>) </span>
<span id="cb73-9"><a href="unsupervised-learning.html#cb73-9"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.6124</span>), <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>) </span>
<span id="cb73-10"><a href="unsupervised-learning.html#cb73-10"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;decrease GDM&quot;</span>, <span class="st">&quot;PCA(p=2)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;orange&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb73-11"><a href="unsupervised-learning.html#cb73-11"></a></span>
<span id="cb73-12"><a href="unsupervised-learning.html#cb73-12"></a><span class="co"># reconstruction error (slightly differs from 0.5611 because of missing seed)</span></span>
<span id="cb73-13"><a href="unsupervised-learning.html#cb73-13"></a>fit0 &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb73-14"><a href="unsupervised-learning.html#cb73-14"></a><span class="kw">round</span>(<span class="kw">Frobenius.loss</span>(X,fit0),<span class="dv">4</span>)</span>
<span id="cb73-15"><a href="unsupervised-learning.html#cb73-15"></a></span>
<span id="cb73-16"><a href="unsupervised-learning.html#cb73-16"></a><span class="co"># read off the bottleneck activations</span></span>
<span id="cb73-17"><a href="unsupervised-learning.html#cb73-17"></a>encoder &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs=</span>model<span class="fl">.3</span><span class="op">$</span>input, <span class="dt">outputs=</span><span class="kw">get_layer</span>(model<span class="fl">.3</span>, <span class="st">&#39;Bottleneck&#39;</span>)<span class="op">$</span>output)</span>
<span id="cb73-18"><a href="unsupervised-learning.html#cb73-18"></a>y&lt;-<span class="st"> </span><span class="kw">predict</span>(encoder,<span class="kw">as.matrix</span>(X))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gda"></span>
<img src="plots/5/gda.png" alt="重构误差图" width="60%"  />
<p class="caption">
Figure 5.8: 重构误差图
</p>
</div>
<p>我们说明了<span class="math inline">\(\mathbf{F}\)</span>范数损失函数在超过10,000次迭代时的下降过程。在大约2000次迭代后，损失低于<span class="math inline">\(p=2\)</span>个主成分的PCA(图<a href="unsupervised-learning.html#fig:gda">5.8</a>中橙色线在水平0.6124处)。10,000次迭代后的最终重构误差为0.5428。</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="unsupervised-learning.html#cb74-1"></a><span class="co"># note that we may need sign switches to make it comparable to PCA</span></span>
<span id="cb74-2"><a href="unsupervised-learning.html#cb74-2"></a>y0 &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(y))<span class="op">*</span><span class="fl">1.1</span></span>
<span id="cb74-3"><a href="unsupervised-learning.html#cb74-3"></a><span class="kw">plot</span>(<span class="dt">x=</span>y[,<span class="dv">1</span>], <span class="dt">y=</span>y[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>y0,y0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>y0,y0), <span class="dt">ylab=</span><span class="st">&quot;2nd bottleneck neuron&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;1st bottleneck neuron&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;bottleneck neural network autoencoder&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb74-4"><a href="unsupervised-learning.html#cb74-4"></a>dat0 &lt;-<span class="st"> </span>y[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb74-5"><a href="unsupervised-learning.html#cb74-5"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0[,<span class="dv">1</span>], <span class="dt">y=</span>dat0[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb74-6"><a href="unsupervised-learning.html#cb74-6"></a>dat0 &lt;-<span class="st"> </span>y[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb74-7"><a href="unsupervised-learning.html#cb74-7"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0[,<span class="dv">1</span>], <span class="dt">y=</span>dat0[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb74-8"><a href="unsupervised-learning.html#cb74-8"></a><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:bnns"></span>
<img src="plots/5/bnns.png" alt="BNN" width="60%"  />
<p class="caption">
Figure 5.9: BNN
</p>
</div>
<p>图<a href="unsupervised-learning.html#fig:bnns">5.9</a>为对样本<span class="math inline">\((n=475)\)</span>的分类效果，与PCA（旋转）很像。</p>
<div class="figure" style="text-align: center"><span id="fig:BPcamparison"></span>
<img src="plots/5/BPcamparison.png" alt="BNN与PCA重构误差对比图" width="60%"  />
<p class="caption">
Figure 5.10: BNN与PCA重构误差对比图
</p>
</div>
<p>由图<a href="unsupervised-learning.html#fig:BPcamparison">5.10</a>得出结论：两种方法都得到了相似的结果。但在一般情况下，BNN的重构误差较小。在少数情况下，BNN可以得到更好的重建结果(右下角的橙色点)。
针对这个例子而言，属于低维数的问题，主成分分析通常就可以了，因为非线性的部分并没有发挥关键作用。</p>
</div>
</div>
<div id="k-means-clustering" class="section level2">
<h2><span class="header-section-number">5.4</span> K-means clustering</h2>
<p>K-means聚类是一种基于质心（centroid-based）的聚类方法，它将<span class="math inline">\(n\)</span>个样本点<span class="math inline">\(\mathbf{x}_i\in\mathcal{X}\subset\mathbb{R}^q\)</span>划分为<span class="math inline">\(K\)</span>个不相交的类:<span class="math display">\[\mathcal{C}_K:\mathbb{R}^q\rightarrow\mathcal{K}=\{1,\ldots,K\},~~\mathbf{x}\mapsto\mathcal{C}_K(\mathbf{x})\]</span>，以上给出了对特征空间<span class="math inline">\(\mathcal{X}\)</span>的一个分割<span class="math inline">\((C_1,\ldots,C_K)\)</span>，其中<span class="math display">\[C_k=\{\mathbf{x}\in\mathcal{X};\mathcal{C}_K(\mathbf{x})=k\}\]</span></p>
<p>确定<span class="math inline">\(\mathcal{C}_K\)</span>的原则是使总类内差异最小，这可以转化为计算使类内离差平方和总和最小的一个分割，所构造的目标函数为 ：
<span class="math display">\[\underset{(C_1,\ldots,C_K)}{\arg \min}\sum_{k=1}^K\sum_{\mathbf{x}_i\in C_k\cap\mathcal{X}}d(\mathbf{\mu}_k,\mathbf{x}_i)=\underset{(C_1,\ldots,C_K)}{\arg \min}\sum_{k=1}^K\sum_{\mathbf{x}_i\in C_k\cap\mathcal{X}}||\mathbf{\mu}_k-\mathbf{x}_i||_2^2\]</span></p>
<p>其中<span class="math inline">\(\mathbf{\mu}_k\)</span>为类均值向量，因此目标函数衡量了类内样本点围绕类均值向量的紧密程度，其值越小意味着类内样本相似度越高，聚类效果越好。但是上述目标函数并不容易找到最优解，这需要考虑<span class="math inline">\(n\)</span>个样本点所有可能的类划分，因此K-means算法采用了贪心策略，通过迭代优化来近似求解上述目标函数。</p>
<p>K-means算法:</p>
<ol style="list-style-type: decimal">
<li><p>选择初始聚类中心<span class="math inline">\(\mathbf{\mu}_k^{(0)}\)</span>和聚类个数<span class="math inline">\(K\)</span>；</p></li>
<li><p>迭代(终止条件：除非类均值向量<span class="math inline">\(\mathbf{\mu}_k^{(t-1)}\)</span>不再更新或达到最大迭代次数)</p>
<p>(1)计算<span class="math inline">\(n\)</span>个样本点<span class="math inline">\(x_i\)</span>与前一轮均值向量<span class="math inline">\(\mathbf{\mu}_k^{(t)}\)</span>的距离，根据距离最近原则重新分配所有样本点，即<span class="math display">\[C_k^{(t)}(x_i)=\underset{\mathbf{k}\in\mathcal{K}}{\arg\min}||\mathbf{\mu}_k^{(t-1)}-\mathbf{x}_i||_2^2\]</span>
(2)基于<span class="math inline">\(C_k^{(t)}\)</span>更新类均值向量<span class="math inline">\(\mathbf{\mu}_k^{(t)}\)</span></p></li>
</ol>
<p>K-means中存在的问题</p>
<ol style="list-style-type: decimal">
<li><p>对初始聚类中心敏感。选择不同的聚类中心会产生不同的聚类结果和不同的准确率；若随机指定初始聚类中心，当初始指定的两个聚类中心在同一类中或很接近，聚类结果很难区分。此处可以使用一种优化算法：先指定k=2执行k-means(初始聚类中心随机指定)，结果得到两个聚类中心，把它们作为k=3时的其中两个初始聚类中心，剩下一个初始聚类中心随机指定(可以使用列均值)，以此类推。</p></li>
<li><p>K-means聚类由于使用类均值向量作为聚类中心，因此对离群点非常敏感</p></li>
<li><p>因为K-means算法主要采用欧式距离函数度量类间相似度，并且采用误差平方和作为目标函数，因此通常只能发现数据分布比较均匀的球状类。</p></li>
</ol>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="unsupervised-learning.html#cb75-1"></a><span class="co"># initialize</span></span>
<span id="cb75-2"><a href="unsupervised-learning.html#cb75-2"></a>Kaverage &lt;-<span class="st"> </span><span class="kw">colMeans</span>(X)</span>
<span id="cb75-3"><a href="unsupervised-learning.html#cb75-3"></a>K0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb75-4"><a href="unsupervised-learning.html#cb75-4"></a>TWCD &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(K0))  <span class="co"># total within-cluster dissimilarity</span></span>
<span id="cb75-5"><a href="unsupervised-learning.html#cb75-5"></a>Classifier &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span>, <span class="kw">c</span>(K0, <span class="kw">nrow</span>(X)))</span>
<span id="cb75-6"><a href="unsupervised-learning.html#cb75-6"></a>(TWCD[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">colSums</span>(<span class="kw">as.matrix</span>(X<span class="op">^</span><span class="dv">2</span>))))</span>
<span id="cb75-7"><a href="unsupervised-learning.html#cb75-7"></a></span>
<span id="cb75-8"><a href="unsupervised-learning.html#cb75-8"></a><span class="co"># run K-means algorithm</span></span>
<span id="cb75-9"><a href="unsupervised-learning.html#cb75-9"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb75-10"><a href="unsupervised-learning.html#cb75-10"></a><span class="cf">for</span> (K <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>K0){ </span>
<span id="cb75-11"><a href="unsupervised-learning.html#cb75-11"></a>   <span class="cf">if</span> (K<span class="op">==</span><span class="dv">2</span>){(K_res &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X,K) )}</span>
<span id="cb75-12"><a href="unsupervised-learning.html#cb75-12"></a>   <span class="cf">if</span> (K<span class="op">&gt;</span><span class="dv">2</span>){(K_res  &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X,K_centers) )}</span>
<span id="cb75-13"><a href="unsupervised-learning.html#cb75-13"></a>   TWCD[K] &lt;-<span class="st"> </span><span class="kw">sum</span>(K_res<span class="op">$</span>withins)</span>
<span id="cb75-14"><a href="unsupervised-learning.html#cb75-14"></a>   Classifier[K,] &lt;-<span class="st"> </span>K_res<span class="op">$</span>cluster</span>
<span id="cb75-15"><a href="unsupervised-learning.html#cb75-15"></a>   K_centers &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(K<span class="op">+</span><span class="dv">1</span>, <span class="kw">ncol</span>(X)))</span>
<span id="cb75-16"><a href="unsupervised-learning.html#cb75-16"></a>   K_centers[K<span class="op">+</span><span class="dv">1</span>,] &lt;-<span class="st"> </span>Kaverage</span>
<span id="cb75-17"><a href="unsupervised-learning.html#cb75-17"></a>   K_centers[<span class="dv">1</span><span class="op">:</span>K,] &lt;-<span class="st"> </span>K_res<span class="op">$</span>centers </span>
<span id="cb75-18"><a href="unsupervised-learning.html#cb75-18"></a>                }</span>
<span id="cb75-19"><a href="unsupervised-learning.html#cb75-19"></a></span>
<span id="cb75-20"><a href="unsupervised-learning.html#cb75-20"></a><span class="co"># plot losses                </span></span>
<span id="cb75-21"><a href="unsupervised-learning.html#cb75-21"></a>xtitle &lt;-<span class="st"> &quot;decrease in total within-cluster dissimilarity &quot;</span></span>
<span id="cb75-22"><a href="unsupervised-learning.html#cb75-22"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>K0), <span class="dt">y=</span>TWCD, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">max</span>(TWCD)), <span class="dt">main=</span><span class="kw">list</span>(xtitle, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylab=</span><span class="st">&quot;total within-cluster dissimilarity&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;hyperparameter K&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb75-23"><a href="unsupervised-learning.html#cb75-23"></a><span class="kw">lines</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>K0), <span class="dt">y=</span>TWCD, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span>
<span id="cb75-24"><a href="unsupervised-learning.html#cb75-24"></a></span>
<span id="cb75-25"><a href="unsupervised-learning.html#cb75-25"></a><span class="co"># singular value decomposition</span></span>
<span id="cb75-26"><a href="unsupervised-learning.html#cb75-26"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb75-27"><a href="unsupervised-learning.html#cb75-27"></a>pca &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb75-28"><a href="unsupervised-learning.html#cb75-28"></a>dat3 &lt;-<span class="st"> </span>d.data</span>
<span id="cb75-29"><a href="unsupervised-learning.html#cb75-29"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]]</span>
<span id="cb75-30"><a href="unsupervised-learning.html#cb75-30"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]]</span>
<span id="cb75-31"><a href="unsupervised-learning.html#cb75-31"></a></span>
<span id="cb75-32"><a href="unsupervised-learning.html#cb75-32"></a>lim0 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb75-33"><a href="unsupervised-learning.html#cb75-33"></a></span>
<span id="cb75-34"><a href="unsupervised-learning.html#cb75-34"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;K-means vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb75-35"><a href="unsupervised-learning.html#cb75-35"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">4</span>),]</span>
<span id="cb75-36"><a href="unsupervised-learning.html#cb75-36"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb75-37"><a href="unsupervised-learning.html#cb75-37"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">1</span>),]</span>
<span id="cb75-38"><a href="unsupervised-learning.html#cb75-38"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb75-39"><a href="unsupervised-learning.html#cb75-39"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb75-40"><a href="unsupervised-learning.html#cb75-40"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb75-41"><a href="unsupervised-learning.html#cb75-41"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="k-medoids-clustering-pam" class="section level2">
<h2><span class="header-section-number">5.5</span> K-medoids clustering (PAM)</h2>
<p>K-means算法使用类均值向量作为聚类中心，因此对离群点非常敏感，如果具有极大值，可能大幅度地扭曲数据的分布。K-mediods算法使用样本点作为聚类中心，修正聚类中心是计算当前类非聚类中心点到其他所有点的最小值来更新聚类中心，同时可以使用Manhattan距离，可以有效削弱离群点的影响。本例中K-medoids聚类使用Manhattan距离,(<span class="math inline">\(L1\)</span>范式对离群点的惩罚权重比欧式距离小)，结果显示其聚类中心要比K-means聚类更加聚集一些。</p>
<p>PAM算法：</p>
<ol style="list-style-type: decimal">
<li><p>选择初始聚类中心<span class="math inline">\(c_1,c_2,\ldots,c_k\in\mathcal{X}\)</span>和聚类个数<span class="math inline">\(K\)</span>，计算<span class="math inline">\(TWCD\)</span>；
<span class="math display">\[TWCD=\sum_{k=1}^K\sum_{x_i\in{C_k}\cap\mathcal{X}}d(c_k),x_i\]</span></p></li>
<li><p>迭代(终止条件：<span class="math inline">\(TWCD\)</span>不再减少)</p>
<p>(1)遍历每个非聚类中心<span class="math inline">\(x_i\)</span>替换聚类中心点<span class="math inline">\(c_k^{t-1}\)</span>，根据距离最近原则重新分配各样本点，计算<span class="math inline">\(TWCD\)</span>;</p>
<p>(2)根据<span class="math inline">\(TWCD\)</span>最小原则更新聚类中心<span class="math inline">\(c_k^{t}\)</span>。</p></li>
</ol>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="unsupervised-learning.html#cb76-1"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb76-2"><a href="unsupervised-learning.html#cb76-2"></a>(K_res &lt;-<span class="st"> </span><span class="kw">pam</span>(X, <span class="dt">k=</span><span class="dv">4</span>, <span class="dt">metric=</span><span class="st">&quot;manhattan&quot;</span>, <span class="dt">diss=</span><span class="ot">FALSE</span>))</span>
<span id="cb76-3"><a href="unsupervised-learning.html#cb76-3"></a></span>
<span id="cb76-4"><a href="unsupervised-learning.html#cb76-4"></a><span class="co"># plot K-medoids versus PCA</span></span>
<span id="cb76-5"><a href="unsupervised-learning.html#cb76-5"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;K-medoids vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb76-6"><a href="unsupervised-learning.html#cb76-6"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">4</span>),]</span>
<span id="cb76-7"><a href="unsupervised-learning.html#cb76-7"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb76-8"><a href="unsupervised-learning.html#cb76-8"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb76-9"><a href="unsupervised-learning.html#cb76-9"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb76-10"><a href="unsupervised-learning.html#cb76-10"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">2</span>),]</span>
<span id="cb76-11"><a href="unsupervised-learning.html#cb76-11"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb76-12"><a href="unsupervised-learning.html#cb76-12"></a><span class="kw">points</span>(<span class="dt">x=</span>dat3[K_res<span class="op">$</span>id.med,<span class="st">&quot;v1&quot;</span>],<span class="dt">y=</span>dat3[K_res<span class="op">$</span>id.med,<span class="st">&quot;v2&quot;</span>], <span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb76-13"><a href="unsupervised-learning.html#cb76-13"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="gaussian-mixture-modelsgmms" class="section level2">
<h2><span class="header-section-number">5.6</span> Gaussian mixture models(GMMs)</h2>
<p>K-means假设数据点是球状的，GMMs假设数据点是呈高斯分布，提供了更多的可能性。对<span class="math inline">\(n\)</span>维样本空间<span class="math inline">\(\mathcal{X}\)</span>中的随机向量<span class="math inline">\(x\)</span>，若<span class="math inline">\(x\)</span>服从高斯分布，<span class="math inline">\(\mu\)</span>是<span class="math inline">\(n\)</span>维均值向量，<span class="math inline">\(\Sigma\)</span>是<span class="math inline">\(n\times n\)</span> 的协方差矩阵，其概率密度函数为：
<span class="math display">\[p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)\Sigma^{-1}(x-\mu)}\]</span></p>
<p>由此可定义高斯混合分布，该分布由<span class="math inline">\(k\)</span>个混合成分组成，每个混合成分对应一个高斯分布，其中<span class="math inline">\(\mu_i\)</span>与<span class="math inline">\(\Sigma_i\)</span>是第<span class="math inline">\(i\)</span>个高斯混合成分的参数，<span class="math inline">\(\alpha_i\)</span>为相应的混合系数(<span class="math inline">\(\alpha_i&gt;0,\sum_{i=1}^k\alpha_i=1\)</span>)
<span class="math display">\[p_{\mathcal{M}}=\sum_{i=1}^k\alpha_i\times p(x|\mu_i,\Sigma_i)\]</span></p>
<p>若训练集<span class="math inline">\(D={x_1,x_2,\dots,x_m}\)</span>由上述过程生成，令随机变量<span class="math inline">\(z_j\in{1,2,\dots,k}\)</span>表示生成样本<span class="math inline">\(x_j\)</span>的高斯混合成分，换句话说，样本<span class="math inline">\(x_j\)</span>属于第<span class="math inline">\(z_j\)</span>个高斯分布，<span class="math inline">\(z_j\)</span>的先验概率<span class="math inline">\(p(x_j=i)=\alpha_i(i=1,2,\dots,k)\)</span>。根据贝叶斯定理，则<span class="math inline">\(z_j\)</span>的后验分布(表示样本<span class="math inline">\(x_j\)</span>由第<span class="math inline">\(i\)</span>个高斯混合成分生成的后验概率<span class="math inline">\(\gamma_ji\)</span>)为：
<span class="math display">\[\gamma_{ji}=p_{\mathcal{M}}(z_j=i|x_j)=\frac{p(z_j=i)\times p_{\mathcal{M}}(x_j|z_j=i)}{p_{\mathcal{M}}(x_j)}=\frac{a_i\times p(x_j|\mu_i,\Sigma_i)}{\sum_{i=1}^k\alpha_l\times p(x_j|\mu_l,\Sigma_l)}\]</span></p>
<p>当高斯混合成不已知时，高斯混合聚类将把样本集<span class="math inline">\(D\)</span>划分为<span class="math inline">\(k\)</span>个类<span class="math inline">\(C=\{C_1,C_2,\dots,C_k\}\)</span>，则每个样本<span class="math inline">\(x_j\)</span>的类标记<span class="math inline">\(\gamma_j\)</span>作如下确定：
<span class="math display">\[\gamma_j=\underset{i\in\{1,2,\dots,k\}}{\arg \min}\gamma_{ji}\]</span></p>
<p>对给定样本集<span class="math inline">\(D\)</span>，可采用极大似然估计求解模型参数<span class="math inline">\(\{(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k\}\)</span>，似然函数
<span class="math display">\[LL(D)=ln\left(\prod_{j=1}^m p_{\mathcal{M}(x_j)}\right)=\sum_{j=1}^m ln\left(\sum_{i=1}^k \alpha_i \times p(x_j|\mu_i,\Sigma_i) \right)\]</span></p>
<p>令<span class="math inline">\(\frac{\partial{LL(D)}}{\partial(\mu_i)}=0\)</span>，得到<span class="math inline">\(\mu_i=\frac{\sum_{j=1}^{m}\gamma_{ji}x_j}{\sum_{j=1}^m\gamma_{ji}}\)</span>
令<span class="math inline">\(\frac{\partial{LL(D)}}{\partial(\Sigma_i)}=0\)</span>，得到<span class="math inline">\(\Sigma_i=\frac{\sum_{j=1}^{m}\gamma_{ji}(x_j-\mu_i)(x_j-\mu_i)^T}{\sum_{j=1}^m\gamma_{ji}}\)</span>
可以看到各混合成分的均值<span class="math inline">\(\mu_i\)</span>和协方差阵<span class="math inline">\(\Sigma_i\)</span>可通过样本加权平均来估计，样本权重是每个样本属于该成分的后验概率。</p>
<p>对于混合系数<span class="math inline">\(\alpha_i\)</span>，除了要最大化<span class="math inline">\(LL(D)\)</span>，还需要满足<span class="math inline">\(\alpha_i&gt;0,\sum_{i=1}^k\alpha_i=1\)</span>，可以使用拉格朗日条件极值求解，其中<span class="math inline">\(\lambda\)</span>为拉格朗日乘子。
<span class="math display">\[LLF(D)=LL(D)=\lambda\left(\sum_{i=1}^k\alpha_i-1\right)\]</span></p>
<p>令<span class="math inline">\(\frac{\partial{LLF(D)}}{\partial(\alpha_i)}=0\)</span>，得到<span class="math inline">\(\alpha_i=\frac{\sum_{j=1}^{m}\gamma_{ji}}{m}\)</span>
可以看到各高斯成分<span class="math inline">\(\alpha_i\)</span>的混合系数由样本属于该成分的平均后验概率确定。</p>
<p>因为<span class="math inline">\(z_j\in\{1,2\dots,k\}\)</span>是隐变量(LatentVariable)，无法观测，一般采用EM算法进行迭代优化。EM算法：</p>
<ol style="list-style-type: decimal">
<li><p>确定初始高斯混合分布的模型参数<span class="math inline">\(\{(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k\}\)</span>和聚类个数<span class="math inline">\(K\)</span>.</p></li>
<li><p>迭代(终止条件：似然函数<span class="math inline">\(LL(D)\)</span>增长很少甚至不再增长或达到最大迭代次数)</p>
<p>(1)根据参数<span class="math inline">\(\Theta^{t-1}=\{(\alpha_i^{t-1},\mu_i^{t-1},\Sigma_i^{t-1})|1 \le i \le k\}\)</span>确定<span class="math inline">\(\gamma_{ji}\)</span>;</p>
<p>(2)根据<span class="math inline">\(\gamma_{ji}\)</span>更新参数<span class="math inline">\(\Theta^{t}=\{(\alpha_i^t,\mu_i^t,\Sigma_i^t)|1 \le i \le k\}\)</span></p></li>
<li><p>最后根据<span class="math inline">\(\gamma_j=\underset{i\in\{1,2,\dots,k\}}{\arg \min}\gamma_{ji}\)</span>确定各样本<span class="math inline">\(x_j\)</span>所属类</p></li>
</ol>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="unsupervised-learning.html#cb77-1"></a>seed &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb77-2"><a href="unsupervised-learning.html#cb77-2"></a><span class="kw">set.seed</span>(seed)</span>
<span id="cb77-3"><a href="unsupervised-learning.html#cb77-3"></a>K_res &lt;-<span class="st"> </span><span class="kw">GMM</span>(X, <span class="dt">gaussian_comps=</span><span class="dv">4</span>, <span class="dt">dist_mode=</span><span class="st">&quot;eucl_dist&quot;</span>, <span class="dt">seed_mode=</span><span class="st">&quot;random_subset&quot;</span>, <span class="dt">em_iter=</span><span class="dv">5</span>, <span class="dt">seed=</span>seed)</span>
<span id="cb77-4"><a href="unsupervised-learning.html#cb77-4"></a><span class="kw">summary</span>(K_res)</span>
<span id="cb77-5"><a href="unsupervised-learning.html#cb77-5"></a>clust &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_res<span class="op">$</span>centroids, K_res<span class="op">$</span>covariance_matrices, K_res<span class="op">$</span>weights)<span class="op">$</span>cluster_labels</span>
<span id="cb77-6"><a href="unsupervised-learning.html#cb77-6"></a></span>
<span id="cb77-7"><a href="unsupervised-learning.html#cb77-7"></a>pred &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_res<span class="op">$</span>centroids, K_res<span class="op">$</span>covariance_matrices, K_res<span class="op">$</span>weights)</span>
<span id="cb77-8"><a href="unsupervised-learning.html#cb77-8"></a><span class="kw">names</span>(pred)</span>
<span id="cb77-9"><a href="unsupervised-learning.html#cb77-9"></a></span>
<span id="cb77-10"><a href="unsupervised-learning.html#cb77-10"></a>pred<span class="op">$</span>cluster_labels[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</span>
<span id="cb77-11"><a href="unsupervised-learning.html#cb77-11"></a>pred<span class="op">$</span>cluster_proba[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</span>
<span id="cb77-12"><a href="unsupervised-learning.html#cb77-12"></a></span>
<span id="cb77-13"><a href="unsupervised-learning.html#cb77-13"></a>K_res<span class="op">$</span>centroids</span>
<span id="cb77-14"><a href="unsupervised-learning.html#cb77-14"></a></span>
<span id="cb77-15"><a href="unsupervised-learning.html#cb77-15"></a><span class="co"># singular value decomposition</span></span>
<span id="cb77-16"><a href="unsupervised-learning.html#cb77-16"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb77-17"><a href="unsupervised-learning.html#cb77-17"></a></span>
<span id="cb77-18"><a href="unsupervised-learning.html#cb77-18"></a>pca &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb77-19"><a href="unsupervised-learning.html#cb77-19"></a>dat3 &lt;-<span class="st"> </span>d.data</span>
<span id="cb77-20"><a href="unsupervised-learning.html#cb77-20"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]]</span>
<span id="cb77-21"><a href="unsupervised-learning.html#cb77-21"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]]</span>
<span id="cb77-22"><a href="unsupervised-learning.html#cb77-22"></a></span>
<span id="cb77-23"><a href="unsupervised-learning.html#cb77-23"></a>(kk1 &lt;-<span class="st"> </span>K_res<span class="op">$</span>centroids <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]])</span>
<span id="cb77-24"><a href="unsupervised-learning.html#cb77-24"></a>(kk2 &lt;-<span class="st"> </span>K_res<span class="op">$</span>centroids <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]])</span>
<span id="cb77-25"><a href="unsupervised-learning.html#cb77-25"></a></span>
<span id="cb77-26"><a href="unsupervised-learning.html#cb77-26"></a>lim0 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb77-27"><a href="unsupervised-learning.html#cb77-27"></a></span>
<span id="cb77-28"><a href="unsupervised-learning.html#cb77-28"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;GMM(diagonal) vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb77-29"><a href="unsupervised-learning.html#cb77-29"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">0</span>),]</span>
<span id="cb77-30"><a href="unsupervised-learning.html#cb77-30"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb77-31"><a href="unsupervised-learning.html#cb77-31"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb77-32"><a href="unsupervised-learning.html#cb77-32"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb77-33"><a href="unsupervised-learning.html#cb77-33"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">1</span>),]</span>
<span id="cb77-34"><a href="unsupervised-learning.html#cb77-34"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb77-35"><a href="unsupervised-learning.html#cb77-35"></a><span class="kw">points</span>(<span class="dt">x=</span>kk1,<span class="dt">y=</span>kk2, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb77-36"><a href="unsupervised-learning.html#cb77-36"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="三种聚类方法评价" class="section level2">
<h2><span class="header-section-number">5.7</span> 三种聚类方法评价</h2>
<p>图<a href="unsupervised-learning.html#fig:cluster-results">5.11</a>展示了聚类的结果，GMMs最好。</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="unsupervised-learning.html#cb78-1"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb78-2"><a href="unsupervised-learning.html#cb78-2"></a><span class="co">#K-means聚类(欧式距离)</span></span>
<span id="cb78-3"><a href="unsupervised-learning.html#cb78-3"></a>K_means &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dv">4</span>)</span>
<span id="cb78-4"><a href="unsupervised-learning.html#cb78-4"></a><span class="co">#的K-medoids聚类(曼哈顿距离)</span></span>
<span id="cb78-5"><a href="unsupervised-learning.html#cb78-5"></a>K_medoids &lt;-<span class="st"> </span><span class="kw">pam</span>(X, <span class="dt">k =</span> <span class="dv">4</span>, <span class="dt">metric =</span> <span class="st">&quot;manhattan&quot;</span>)</span>
<span id="cb78-6"><a href="unsupervised-learning.html#cb78-6"></a><span class="co">#GMM聚类(欧式距离)</span></span>
<span id="cb78-7"><a href="unsupervised-learning.html#cb78-7"></a>K_gmm &lt;-<span class="st"> </span><span class="kw">GMM</span>(X, <span class="dt">gaussian_comps =</span> <span class="dv">4</span>, <span class="dt">dist_mode =</span> <span class="st">&quot;eucl_dist&quot;</span>, </span>
<span id="cb78-8"><a href="unsupervised-learning.html#cb78-8"></a>             <span class="dt">seed_mode =</span> <span class="st">&quot;random_subset&quot;</span>, </span>
<span id="cb78-9"><a href="unsupervised-learning.html#cb78-9"></a>             <span class="dt">em_iter=</span> <span class="dv">5</span>,<span class="dt">seed =</span> <span class="dv">100</span>)</span>
<span id="cb78-10"><a href="unsupervised-learning.html#cb78-10"></a>clust &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_gmm<span class="op">$</span>centroids, </span>
<span id="cb78-11"><a href="unsupervised-learning.html#cb78-11"></a>                     K_gmm<span class="op">$</span>covariance_matrices, K_gmm<span class="op">$</span>weights)<span class="op">$</span>cluster_labels</span>
<span id="cb78-12"><a href="unsupervised-learning.html#cb78-12"></a><span class="co">#k-means</span></span>
<span id="cb78-13"><a href="unsupervised-learning.html#cb78-13"></a>cluster &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;cluster1&#39;</span>,<span class="st">&#39;cluster2&#39;</span>,<span class="st">&#39;cluster3&#39;</span>,<span class="st">&#39;cluster4&#39;</span>)</span>
<span id="cb78-14"><a href="unsupervised-learning.html#cb78-14"></a>cars &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) <span class="kw">nrow</span>(dat3[<span class="kw">which</span>(y <span class="op">==</span><span class="st"> </span>x),])</span>
<span id="cb78-15"><a href="unsupervised-learning.html#cb78-15"></a>sports &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) <span class="kw">nrow</span>(dat3[<span class="kw">which</span>(y <span class="op">==</span><span class="st"> </span>x <span class="op">&amp;</span><span class="st"> </span>dat3<span class="op">$</span>sports_car <span class="op">==</span><span class="st"> </span><span class="dv">1</span>),])</span>
<span id="cb78-16"><a href="unsupervised-learning.html#cb78-16"></a>K_means &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(cluster,</span>
<span id="cb78-17"><a href="unsupervised-learning.html#cb78-17"></a>                            <span class="st">&quot;cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">2</span>),</span>
<span id="cb78-18"><a href="unsupervised-learning.html#cb78-18"></a>                                     <span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">4</span>),</span>
<span id="cb78-19"><a href="unsupervised-learning.html#cb78-19"></a>                                     <span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">3</span>),</span>
<span id="cb78-20"><a href="unsupervised-learning.html#cb78-20"></a>                                     <span class="kw">cars</span>(K_means<span class="op">$</span>cluster,<span class="dv">1</span>)),</span>
<span id="cb78-21"><a href="unsupervised-learning.html#cb78-21"></a>                            <span class="st">&quot;sports cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">2</span>),</span>
<span id="cb78-22"><a href="unsupervised-learning.html#cb78-22"></a>                                            <span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">4</span>),</span>
<span id="cb78-23"><a href="unsupervised-learning.html#cb78-23"></a>                                            <span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">3</span>),</span>
<span id="cb78-24"><a href="unsupervised-learning.html#cb78-24"></a>                                            <span class="kw">sports</span>(K_means<span class="op">$</span>cluster,<span class="dv">1</span>)))</span>
<span id="cb78-25"><a href="unsupervised-learning.html#cb78-25"></a>K_medoids &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(cluster,</span>
<span id="cb78-26"><a href="unsupervised-learning.html#cb78-26"></a>                              <span class="st">&quot;cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">4</span>),</span>
<span id="cb78-27"><a href="unsupervised-learning.html#cb78-27"></a>                                       <span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">4</span>),</span>
<span id="cb78-28"><a href="unsupervised-learning.html#cb78-28"></a>                                       <span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">2</span>),</span>
<span id="cb78-29"><a href="unsupervised-learning.html#cb78-29"></a>                                       <span class="kw">cars</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">1</span>)),</span>
<span id="cb78-30"><a href="unsupervised-learning.html#cb78-30"></a>                              <span class="st">&quot;sports cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">4</span>),</span>
<span id="cb78-31"><a href="unsupervised-learning.html#cb78-31"></a>                                              <span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">3</span>),</span>
<span id="cb78-32"><a href="unsupervised-learning.html#cb78-32"></a>                                              <span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">2</span>),</span>
<span id="cb78-33"><a href="unsupervised-learning.html#cb78-33"></a>                                              <span class="kw">sports</span>(K_medoids<span class="op">$</span>clustering,<span class="dv">1</span>)))</span>
<span id="cb78-34"><a href="unsupervised-learning.html#cb78-34"></a>GMMs &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(cluster,</span>
<span id="cb78-35"><a href="unsupervised-learning.html#cb78-35"></a>                         <span class="st">&quot;cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cars</span>(clust,<span class="dv">0</span>),<span class="kw">cars</span>(clust,<span class="dv">3</span>),</span>
<span id="cb78-36"><a href="unsupervised-learning.html#cb78-36"></a>                                  <span class="kw">cars</span>(clust,<span class="dv">1</span>),<span class="kw">cars</span>(clust,<span class="dv">2</span>)),</span>
<span id="cb78-37"><a href="unsupervised-learning.html#cb78-37"></a>                         <span class="st">&quot;sports cars&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">sports</span>(clust,<span class="dv">0</span>),<span class="kw">sports</span>(clust,<span class="dv">3</span>),</span>
<span id="cb78-38"><a href="unsupervised-learning.html#cb78-38"></a>                                         <span class="kw">sports</span>(clust,<span class="dv">1</span>),<span class="kw">sports</span>(clust,<span class="dv">2</span>)))</span>
<span id="cb78-39"><a href="unsupervised-learning.html#cb78-39"></a>k_means &lt;-<span class="st"> </span>reshape2<span class="op">::</span><span class="kw">melt</span>(K_means, <span class="dt">id.vars =</span> <span class="st">&quot;cluster&quot;</span>,</span>
<span id="cb78-40"><a href="unsupervised-learning.html#cb78-40"></a>                          <span class="dt">variable.name =</span> <span class="st">&quot;k_means&quot;</span>)</span>
<span id="cb78-41"><a href="unsupervised-learning.html#cb78-41"></a>k_medoids &lt;-<span class="st"> </span>reshape2<span class="op">::</span><span class="kw">melt</span>(K_medoids, <span class="dt">id =</span> <span class="st">&quot;cluster&quot;</span>, </span>
<span id="cb78-42"><a href="unsupervised-learning.html#cb78-42"></a>                            <span class="dt">variable.name =</span> <span class="st">&quot;k_medoids&quot;</span>)</span>
<span id="cb78-43"><a href="unsupervised-learning.html#cb78-43"></a>GMMs &lt;-<span class="st"> </span>reshape2<span class="op">::</span><span class="kw">melt</span>(GMMs, <span class="dt">id =</span> <span class="st">&quot;cluster&quot;</span>, <span class="dt">variable.name =</span> <span class="st">&quot;GMMs&quot;</span>)</span>
<span id="cb78-44"><a href="unsupervised-learning.html#cb78-44"></a><span class="co">#绘制三种聚类方法聚类结果中跑车样本与真实跑车样本对比图</span></span>
<span id="cb78-45"><a href="unsupervised-learning.html#cb78-45"></a>myggplot &lt;-<span class="st"> </span><span class="cf">function</span>(mydf,myxcol,myycol,myfill) {</span>
<span id="cb78-46"><a href="unsupervised-learning.html#cb78-46"></a>   ggplot2<span class="op">::</span><span class="kw">ggplot</span>(<span class="dt">data=</span>mydf,<span class="kw">aes</span>(<span class="dt">x =</span> {{myxcol}},</span>
<span id="cb78-47"><a href="unsupervised-learning.html#cb78-47"></a>                                 <span class="dt">y =</span> {{myycol}},</span>
<span id="cb78-48"><a href="unsupervised-learning.html#cb78-48"></a>                                 <span class="dt">fill =</span> {{myfill}}))<span class="op">+</span></span>
<span id="cb78-49"><a href="unsupervised-learning.html#cb78-49"></a><span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>,<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>)<span class="op">+</span></span>
<span id="cb78-50"><a href="unsupervised-learning.html#cb78-50"></a><span class="st">      </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> value),</span>
<span id="cb78-51"><a href="unsupervised-learning.html#cb78-51"></a>                <span class="dt">position =</span> <span class="kw">position_dodge</span>((<span class="dv">1</span>)),</span>
<span id="cb78-52"><a href="unsupervised-learning.html#cb78-52"></a>                <span class="dt">size =</span> <span class="dv">3</span>,</span>
<span id="cb78-53"><a href="unsupervised-learning.html#cb78-53"></a>                <span class="dt">vjust =</span> <span class="fl">-0.5</span>)<span class="op">+</span></span>
<span id="cb78-54"><a href="unsupervised-learning.html#cb78-54"></a><span class="st">      </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>),</span>
<span id="cb78-55"><a href="unsupervised-learning.html#cb78-55"></a>            <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb78-56"><a href="unsupervised-learning.html#cb78-56"></a>            <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb78-57"><a href="unsupervised-learning.html#cb78-57"></a>            <span class="dt">panel.background =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb78-58"><a href="unsupervised-learning.html#cb78-58"></a>            <span class="dt">axis.title.x =</span> <span class="kw">element_blank</span>())<span class="op">+</span></span>
<span id="cb78-59"><a href="unsupervised-learning.html#cb78-59"></a><span class="st">      </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;number&quot;</span>)<span class="op">+</span></span>
<span id="cb78-60"><a href="unsupervised-learning.html#cb78-60"></a><span class="st">      </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">250</span>)</span>
<span id="cb78-61"><a href="unsupervised-learning.html#cb78-61"></a>   </span>
<span id="cb78-62"><a href="unsupervised-learning.html#cb78-62"></a>}</span>
<span id="cb78-63"><a href="unsupervised-learning.html#cb78-63"></a><span class="co">#组合图形</span></span>
<span id="cb78-64"><a href="unsupervised-learning.html#cb78-64"></a><span class="kw">multiplot</span>(<span class="kw">myggplot</span>(k_means,cluster,value,k_means),</span>
<span id="cb78-65"><a href="unsupervised-learning.html#cb78-65"></a>          <span class="kw">myggplot</span>(k_medoids,cluster,value,k_medoids),</span>
<span id="cb78-66"><a href="unsupervised-learning.html#cb78-66"></a>          <span class="kw">myggplot</span>(GMMs,cluster,value,GMMs))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:cluster-results"></span>
<img src="plots/5/cluster.png" alt="聚类比较" width="60%"  />
<p class="caption">
Figure 5.11: 聚类比较
</p>
</div>
</div>
<div id="t-sne" class="section level2">
<h2><span class="header-section-number">5.8</span> t-SNE</h2>
<p><strong>简介</strong></p>
<p><strong>t分布-随机邻近嵌入</strong>(t-SNE, t-distributed stochastic neighbor embedding)由 Laurens van der Maaten和Geoffrey Hinton在2008年提出。t-SNE本质是一种嵌入模型，能够将高维空间中的数据映射到低维空间中，并保留数据集的局部特性。</p>
<p><strong>基本原理</strong></p>
<p>t-SNE将数据点之间的<strong>相似度转化为条件概率</strong>，<strong>原始空间</strong>中数据点的相似度由<strong>高斯联合分布</strong>表示，<strong>嵌入空间</strong>中数据点的相似度由<strong>t分布</strong>表示。</p>
<p>将原始空间和嵌入空间的联合概率分布的<strong>KL散度</strong>作为损失函数(loss function)，评估嵌入效果的好坏。通过<strong>梯度下降算法</strong>最小化损失函数，最终获得收敛结果。</p>
<p><strong>具体过程</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>原始空间</strong><br />
构建一个高维对象间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。<span class="math inline">\(q\)</span>维空间中给定一组数据<span class="math inline">\(x_1,…,x_n\)</span>。<br />
定义条件概率：
<span class="math display">\[
q_{j|i}=\frac{exp\left\{-\frac{1}{2\sigma_i^2}||x_i-x_j||_2^2\right\}}{\sum_{k\ne i}exp\left\{-\frac{1}{2\sigma_i^2}||x_i-x_j||_2^2\right\}},\ for\ i\ne j
\]</span>
定义联合概率分布：
<span class="math display">\[
q_{i,j}=\frac{1}{2n}(q_{j|i}+q_{i|j}),\ for\ i\ne j
\]</span>
此式既保证了对称性，又使得<span class="math inline">\(\sum_jq_{i,j}&gt;\frac{1}{2n}\)</span>for all <span class="math inline">\(i\)</span><br />
<strong>困惑度(Perplexity)</strong><br />
<span class="math inline">\(\sigma_i\)</span>的选择必须满足：在数据密集的地方要小，数据稀疏的地方要大。<br />
对于<span class="math inline">\(\sigma_i\)</span>，一个好的分配应使得困惑度为常数。
<span class="math display">\[
Perp(q_{.|i})=exp\left\{H(q_{.|i})\right\}=exp\left\{-\sum_{j\ne i}q_{j|i}log_2(q_{j|i})\right\} 
\]</span>
困惑度可理解为对每个点邻居数量的猜测，对最终成图有着复杂的影响。低困惑度对应的是局部视角；高困惑度对应的是全局视角。</p></li>
<li><p><strong>嵌入空间</strong><br />
在<span class="math inline">\(p\)</span>维空间中(<span class="math inline">\(p&lt;q\)</span>)找到一组数据点<span class="math inline">\(y_1,…,y_n\)</span>，使得这组数据点构建的联合概率分布<span class="math inline">\(p\)</span>尽可能地与高维空间中的联合概率分布<span class="math inline">\(q\)</span>相似。
<span class="math display">\[
p_{i,j}=\frac{(1+||y_i-y_j||_2^2)^{-1}}{\sum_{k\ne l}(1+||y_k-y_l||_2^2)^{-1}},\ for\ i\ne j
\]</span>
<strong>t(1)分布的选择</strong>：解决拥挤问题（高维空间中分离的簇，在低维中被分的不明显）</p>
<div class="figure" style="text-align: center"><span id="fig:tdist"></span>
<img src="plots/5/t-distribution.png" alt="t-distribution" width="60%"  />
<p class="caption">
Figure 5.12: t-distribution
</p>
</div>
<p>图<a href="unsupervised-learning.html#fig:tdist">5.12</a>展示了不同自由度下的t分布的密度函数图像，可以看出，自由度越小，t分布的尾部越厚。
<span class="math display">\[
p_{i,j}\approx||y_i-y_j||^{-2}_2\ for\ ||y_i-y_j||_2\rightarrow\infty 
\]</span>
降维的效果用两分布间的KL散度(Kullback-Leibler divergences)度量
<span class="math display">\[
D_{KL}(q||p)=\sum_{j=1}^Jq_jlog\frac{q_j}{p_j}
\]</span></p></li>
</ol>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="unsupervised-learning.html#cb79-1"></a><span class="kw">library</span>(tsne)</span>
<span id="cb79-2"><a href="unsupervised-learning.html#cb79-2"></a>perp =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">30</span>,<span class="dv">40</span>,<span class="dv">50</span>,<span class="dv">60</span>) <span class="co">#困惑度</span></span>
<span id="cb79-3"><a href="unsupervised-learning.html#cb79-3"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb79-4"><a href="unsupervised-learning.html#cb79-4"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>){</span>
<span id="cb79-5"><a href="unsupervised-learning.html#cb79-5"></a>    <span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb79-6"><a href="unsupervised-learning.html#cb79-6"></a>    tsne =<span class="st"> </span><span class="kw">tsne</span>(X, <span class="dt">k=</span><span class="dv">2</span>, <span class="dt">initial_dim=</span><span class="kw">ncol</span>(X), <span class="dt">perplexity=</span>perp[i])</span>
<span id="cb79-7"><a href="unsupervised-learning.html#cb79-7"></a>    tsne1 =<span class="st"> </span>tsne[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>)]</span>
<span id="cb79-8"><a href="unsupervised-learning.html#cb79-8"></a>    <span class="kw">plot</span>(tsne1,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>,</span>
<span id="cb79-9"><a href="unsupervised-learning.html#cb79-9"></a>         <span class="dt">ylab=</span><span class="st">&quot;component 1&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;component 2&quot;</span>,</span>
<span id="cb79-10"><a href="unsupervised-learning.html#cb79-10"></a>         <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;t-SNE with perplexity &quot;</span>, perp[i], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)))</span>
<span id="cb79-11"><a href="unsupervised-learning.html#cb79-11"></a>    <span class="kw">points</span>(tsne1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),], <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb79-12"><a href="unsupervised-learning.html#cb79-12"></a>    <span class="kw">points</span>(tsne1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb79-13"><a href="unsupervised-learning.html#cb79-13"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:t-SNE"></span>
<img src="plots/5/t-SNE.png" alt="t-SNE" width="80%"  />
<p class="caption">
Figure 5.13: t-SNE
</p>
</div>
<p>图<a href="unsupervised-learning.html#fig:t-SNE">5.13</a>展示了困惑度（10-60）对降维结果的影响，可以看出，困惑度为30时，降维的效果最佳。</p>
</div>
<div id="umap" class="section level2">
<h2><span class="header-section-number">5.9</span> UMAP</h2>
<p><strong>简介</strong></p>
<strong>统一流形逼近与投影</strong>(UMAP, Uniform Manifold Approximation and Projection)是建立在黎曼几何和代数拓扑理论框架上的新的降维<strong>流形学习技术</strong>。在可视化质量方面，UMAP算法与t-SNE具有竞争优势，但是它保留了更多全局结构、具有优越的运行性能、更好的可扩展性。
<div class="figure" style="text-align: center"><span id="fig:manifold"></span>
<img src="plots/5/manifold.png" alt="manifold" width="60%"  />
<p class="caption">
Figure 5.14: manifold
</p>
</div>
<p>图<a href="unsupervised-learning.html#fig:manifold">5.14</a>中两个黑点，若考虑直线距离，那么这两个黑点之间距离很相近；如果放到流形学上，那么这两个点距离就得沿着图中曲线绕两圈。</p>
<p><strong>基本原理</strong></p>
<ol style="list-style-type: decimal">
<li><p>计算<strong>高维</strong>的<strong>流形结构特征</strong>，确定高维空间中各个点之间的距离，从而构造高维的数据分布结构。</p></li>
<li><p>将它们<strong>投影到低维空间</strong>，根据高维空间点与点之间的相对关系，提取特征值，在低维空间中<strong>重构</strong>这种距离关系，并计算低维空间中各个点之间的距离。</p></li>
<li><p>使用<strong>随机梯度下降</strong>来最小化这些距离之间的差异。</p></li>
</ol>
<p><strong>具体过程</strong></p>
<ol style="list-style-type: decimal">
<li><p>构建<strong>高维空间</strong>的模糊拓扑表示<br />
<span class="math inline">\(q\)</span>维空间中给定一组数据<span class="math inline">\(x_1,…,x_n\)</span>。定义：
<span class="math display">\[
d:dissimilarity\ measure\\
X_i=\left\{x_{i_1},…,x_{i_k}\right\}:k\ nearest\ neighbors\ of\ x_i
\]</span>
对于每个<span class="math inline">\(x_i\)</span>，确定<span class="math inline">\(\rho_i\)</span>和<span class="math inline">\(\sigma_i\)</span>
<span class="math display">\[
\rho_i=min\ d(x_i,x_{i_j}),1\le j\le k\\
\sum_{j=1}^kexp\left\{−\frac{d(x_i,x_{i_j})-\rho_i}{\sigma_i}\right\} =log_2k
\]</span>
<span class="math inline">\(\rho_i\)</span>控制嵌入的紧密程度，值越小点越聚集；<span class="math inline">\(\sigma_i\)</span>控制有效的嵌入降维范围。<br />
分布计算：
<span class="math display">\[
q_{i|j}=exp\left\{−\frac{d(x_i,x_{i_j})-\rho_i}{\sigma_i}\right\}\\
q_{i,j}=q_{i|j}+q_{j|i}-q_{i|j}q_{j|i}
\]</span></p></li>
<li><p>简单地优化低维表示，使其具有尽可能接近的模糊拓扑表示，并用交叉熵来度量。<br />
低维空间的分布<br />
<span class="math display">\[
p_{ij}=(1+a(y_i-y_j)^{2b})^{−1}
\]</span>
where <span class="math inline">\(a\approx1.93\)</span> and <span class="math inline">\(b\approx0.79\)</span> for default UMAP hyperparameters<br />
交叉熵作为代价函数<br />
<span class="math display">\[
CE(X,Y)=\sum_i\sum_j\left\{q_{ij}(X)ln\frac{q_{ij}(X)}{p_{ij}(Y)}+(1-q_{ij}(X))ln\frac{1-q_{ij}(X)}{1-p_{ij}(Y)}\right\}
\]</span></p></li>
</ol>
<p><strong>参数说明</strong></p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>umap configuration parameters</th>
<th>value</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>n_neighbors</strong></td>
<td><strong>15</strong></td>
<td><strong>确定相邻点的数量，通常取2-100</strong></td>
</tr>
<tr class="even">
<td><strong>n_components</strong></td>
<td><strong>2</strong></td>
<td><strong>降维的维数，默认是2</strong></td>
</tr>
<tr class="odd">
<td><strong>metric</strong></td>
<td><strong>euclidean</strong></td>
<td><strong>距离的计算方法，可选：euclidean,manhattan,chebyshev,minkowski,correlation,hamming等</strong></td>
</tr>
<tr class="even">
<td><strong>n_epochs</strong></td>
<td><strong>200</strong></td>
<td><strong>模型训练迭代次数。数据量大时200，小时500</strong></td>
</tr>
<tr class="odd">
<td>input</td>
<td>data</td>
<td>数据类型，如果是data就会按照数据计算；如果是dist就会按距离矩阵计算</td>
</tr>
<tr class="even">
<td>init</td>
<td>spectral</td>
<td>初始化，有三种方式：spectral,random,自定义</td>
</tr>
<tr class="odd">
<td><strong>min_dist</strong></td>
<td><strong>0.1</strong></td>
<td><strong>控制嵌入的紧密程度，值越小点越聚集，默认0.1</strong></td>
</tr>
<tr class="even">
<td>set_op_mix_ratio</td>
<td>1</td>
<td>降维过程中特征的结合方式，取0-1。0代表取交集，1代表取合集；中间就是比例</td>
</tr>
<tr class="odd">
<td>local_connectivity</td>
<td>1</td>
<td>局部连接的点之间值，默认1，其值越大局部连接越多，导致的结果就是超越固有的流形维数出现改变</td>
</tr>
<tr class="even">
<td>bandwidth</td>
<td>1</td>
<td>用于构造子集参数</td>
</tr>
<tr class="odd">
<td>alpha</td>
<td>1</td>
<td>学习率</td>
</tr>
<tr class="even">
<td>gamma</td>
<td>1</td>
<td>布局最优的学习率</td>
</tr>
<tr class="odd">
<td>negative_sample_rate</td>
<td>5</td>
<td>每一个阳性样本导致的阴性率。其值越大导致高的优化也就是过拟合，预测准确度下降。默认是5</td>
</tr>
<tr class="even">
<td>a</td>
<td>NA</td>
<td></td>
</tr>
<tr class="odd">
<td>b</td>
<td>NA</td>
<td></td>
</tr>
<tr class="even">
<td><strong>spread</strong></td>
<td><strong>1</strong></td>
<td><strong>控制有效的嵌入降维范围，与min_dist联合使用</strong></td>
</tr>
<tr class="odd">
<td><strong>random_state</strong></td>
<td><strong>NA</strong></td>
<td><strong>随机种子，确保模型的可重复性</strong></td>
</tr>
<tr class="even">
<td>transform_state</td>
<td>NA</td>
<td>用于数值转换操作。默认值42</td>
</tr>
<tr class="odd">
<td>knn</td>
<td>NA</td>
<td></td>
</tr>
<tr class="even">
<td>knn_repeats</td>
<td>1</td>
<td></td>
</tr>
<tr class="odd">
<td>verbose</td>
<td>FALSE</td>
<td>控制工作日志，防止存储过多</td>
</tr>
<tr class="even">
<td>umap_learn_args</td>
<td>NA</td>
<td>调用python基于umap-learn训练好的参数</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="unsupervised-learning.html#cb80-1"></a><span class="kw">library</span>(umap)</span>
<span id="cb80-2"><a href="unsupervised-learning.html#cb80-2"></a>min_dist =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.5</span>,<span class="fl">0.9</span>)</span>
<span id="cb80-3"><a href="unsupervised-learning.html#cb80-3"></a>k =<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">50</span>,<span class="dv">75</span>,<span class="dv">100</span>)</span>
<span id="cb80-4"><a href="unsupervised-learning.html#cb80-4"></a>sign =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>),<span class="dv">4</span>,<span class="dv">2</span>,<span class="dt">byrow =</span> F)</span>
<span id="cb80-5"><a href="unsupervised-learning.html#cb80-5"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">4</span>))</span>
<span id="cb80-6"><a href="unsupervised-learning.html#cb80-6"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>){</span>
<span id="cb80-7"><a href="unsupervised-learning.html#cb80-7"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){</span>
<span id="cb80-8"><a href="unsupervised-learning.html#cb80-8"></a>        umap.param =<span class="st"> </span>umap.defaults</span>
<span id="cb80-9"><a href="unsupervised-learning.html#cb80-9"></a>        umap.param<span class="op">$</span>n_components =<span class="st"> </span><span class="dv">2</span></span>
<span id="cb80-10"><a href="unsupervised-learning.html#cb80-10"></a>        umap.param<span class="op">$</span>random_state =<span class="st"> </span><span class="dv">100</span></span>
<span id="cb80-11"><a href="unsupervised-learning.html#cb80-11"></a>        umap.param<span class="op">$</span>min_dist =<span class="st"> </span>min_dist[i]</span>
<span id="cb80-12"><a href="unsupervised-learning.html#cb80-12"></a>        umap.param<span class="op">$</span>n_neighbors =<span class="st"> </span>k[j]</span>
<span id="cb80-13"><a href="unsupervised-learning.html#cb80-13"></a>        umap =<span class="st"> </span><span class="kw">umap</span>(X, <span class="dt">config=</span>umap.param, <span class="dt">method=</span><span class="st">&quot;naive&quot;</span>)</span>
<span id="cb80-14"><a href="unsupervised-learning.html#cb80-14"></a>        umap1 =<span class="st"> </span><span class="kw">matrix</span>()</span>
<span id="cb80-15"><a href="unsupervised-learning.html#cb80-15"></a>        umap<span class="op">$</span>layout[,<span class="dv">1</span>] =<span class="st"> </span>sign[j,<span class="dv">1</span>]<span class="op">*</span>umap<span class="op">$</span>layout[,<span class="dv">1</span>]</span>
<span id="cb80-16"><a href="unsupervised-learning.html#cb80-16"></a>        umap<span class="op">$</span>layout[,<span class="dv">2</span>] =<span class="st"> </span>sign[j,<span class="dv">2</span>]<span class="op">*</span>umap<span class="op">$</span>layout[,<span class="dv">2</span>]</span>
<span id="cb80-17"><a href="unsupervised-learning.html#cb80-17"></a>        umap1 =<span class="st"> </span>umap<span class="op">$</span>layout[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>)]</span>
<span id="cb80-18"><a href="unsupervised-learning.html#cb80-18"></a>        <span class="kw">plot</span>(umap1,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>,</span>
<span id="cb80-19"><a href="unsupervised-learning.html#cb80-19"></a>             <span class="dt">ylab=</span><span class="st">&quot;component 1&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;component 2&quot;</span>,</span>
<span id="cb80-20"><a href="unsupervised-learning.html#cb80-20"></a>             <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;UMAP (k=&quot;</span>,k[j],<span class="st">&quot;, min_dist= &quot;</span>,min_dist[i], <span class="st">&quot;)&quot;</span>,<span class="dt">sep=</span><span class="st">&quot;&quot;</span>)))</span>
<span id="cb80-21"><a href="unsupervised-learning.html#cb80-21"></a>        <span class="kw">points</span>(umap1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),], <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb80-22"><a href="unsupervised-learning.html#cb80-22"></a>        <span class="kw">points</span>(umap1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb80-23"><a href="unsupervised-learning.html#cb80-23"></a>    }</span>
<span id="cb80-24"><a href="unsupervised-learning.html#cb80-24"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:UMAP"></span>
<img src="plots/5/UMAP.png" alt="UMAP" width="80%"  />
<p class="caption">
Figure 5.15: UMAP
</p>
</div>
<p>图<a href="unsupervised-learning.html#fig:UMAP">5.15</a>展示了降维结果以及两参数<code>k</code>和<code>min_dist</code>对降维结果的影响<br />
- 竖看：<code>min_dist</code>越大，图形中的点分散地越均匀<br />
- 横看：<code>k</code>越小，数据的流行结构显示地越明显</p>
</div>
<div id="som" class="section level2">
<h2><span class="header-section-number">5.10</span> SOM</h2>
<p><strong>自组织映射</strong>(Self-organizing map, SOM)是一种竞争型神经网络，由输入层和竞争层（常见2维）构成。</p>
图<a href="unsupervised-learning.html#fig:som-ill">5.16</a>展示了SOM的结构。
<div class="figure" style="text-align: center"><span id="fig:som-ill"></span>
<img src="plots/5/som.png" alt="SOM" width="60%"  />
<p class="caption">
Figure 5.16: SOM
</p>
</div>
<ul>
<li><p><strong>输入层</strong>神经元的数量由输入向量的维度决定，一个神经元对应一个特征</p></li>
<li><p><strong>竞争层</strong>的常见结构：矩形(Rectangular)、六边形(Hexagonal)，参见图<a href="unsupervised-learning.html#fig:SOM-com">5.17</a>。</p>
<div class="figure" style="text-align: center"><span id="fig:SOM-com"></span>
<img src="plots/5/som-com.png" alt="SOM-com" width="80%"  />
<p class="caption">
Figure 5.17: SOM-com
</p>
</div>
<p>竞争层神经元的数量决定了最终模型的粒度与规模，对最终模型的准确性与泛化能力影响很大。<br />
经验公式：<span class="math inline">\(N\ge5\sqrt{m}\)</span>，<span class="math inline">\(m\)</span>为训练样本数</p></li>
</ul>
<p><strong>基本原理</strong></p>
<p>运用竞争学习(competitive learning)策略，竞争层各神经元竞争对输入层响应的机会，最后仅有一个神经元获胜，代表对输入层的分类。如此迭代，逐步优化网络。</p>
<p><strong>具体过程</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>初始化</strong>：初始化连接权重<span class="math inline">\(\omega_{j}^{(0)}=\left(\omega_{j 1}, \ldots, \omega_{j n}\right), j=1, \ldots, N\)</span></p></li>
<li><p><strong>竞争</strong>：对于输入样本<span class="math inline">\(x_i\)</span>，遍历竞争层中每一个神经元，计算<span class="math inline">\(x_i\)</span>与每个神经元的连接权重<span class="math inline">\(\omega_j\)</span>之间的相似度（通常使用欧式距离或平方欧式距离）。距离最小的神经元节点胜出，称为BMN(best matching neuron)</p></li>
</ol>
<p><span class="math display">\[
j^{*}=j^{*}(i)=\underset{j \in \mathcal{J}}{\arg \min } d\left(\boldsymbol{w}_{j}, \boldsymbol{x}_{i}\right)
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p><strong>合作</strong>：获胜神经元决定了兴奋神经元拓扑邻域的空间位置，从而为相邻神经元之间的合作提供了基础。
神经生物学的研究表明，一组兴奋神经元内存在横向的相互作用。因此当一个神经元被激活时，近邻节点往往比远离的节点更兴奋。定义邻域函数<span class="math inline">\(\theta\)</span>，表示获胜神经元对近邻神经元的影响强弱，也即优胜邻域中每个神经元的更新幅度。常见的选择是高斯函数：
<span class="math display">\[
\theta\left(j^{*}(i), j ; t\right)=\exp \left\{-\frac{1}{2 \sigma(t)^{2}}\left\|j-j^{*}(i)\right\|_{2}^{2} / J^{2}\right\}
\]</span>
邻域半径<span class="math inline">\(\sigma(t)\)</span>随着时间的推移而减少。<br />
越靠近优胜神经元，更新幅度越大；越远离优胜神经元，更新幅度越小</p></li>
<li><p><strong>适应/学习</strong>：更新优胜邻域内神经元的连接权重
<span class="math display">\[
\boldsymbol{w}_{j}^{(t)}=\boldsymbol{w}_{j}^{(t-1)}+\theta\left(j^{*}(i), j ; t\right) \alpha(t)\left(\boldsymbol{x}_{i}-\boldsymbol{w}_{j}^{(t-1)}\right)
\]</span>
学习率<span class="math inline">\(\alpha(t)\)</span>随着时间的推移而减少。</p></li>
<li><p><strong>迭代</strong>：返回第二步，迭代至收敛或达到设定次数</p></li>
</ol>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="unsupervised-learning.html#cb81-1"></a><span class="kw">library</span>(kohonen)</span>
<span id="cb81-2"><a href="unsupervised-learning.html#cb81-2"></a>p =<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">sqrt</span>(<span class="dv">5</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">nrow</span>(X)))) <span class="co">#根据经验公式确定正方形的边长</span></span>
<span id="cb81-3"><a href="unsupervised-learning.html#cb81-3"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb81-4"><a href="unsupervised-learning.html#cb81-4"></a>som1 =<span class="st"> </span><span class="kw">som</span>(<span class="kw">as.matrix</span>(X),<span class="dt">grid=</span><span class="kw">somgrid</span>(<span class="dt">xdim=</span>p,<span class="dt">ydim=</span>p,<span class="dt">topo=</span><span class="st">&quot;rectangular&quot;</span>),<span class="dt">rlen=</span><span class="dv">500</span>,<span class="dt">dist.fcts=</span><span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb81-5"><a href="unsupervised-learning.html#cb81-5"></a><span class="kw">summary</span>(som1)</span>
<span id="cb81-6"><a href="unsupervised-learning.html#cb81-6"></a><span class="kw">head</span>(som1<span class="op">$</span>unit.classif,<span class="dv">100</span>) <span class="co">#各数据点的获胜神经元</span></span>
<span id="cb81-7"><a href="unsupervised-learning.html#cb81-7"></a><span class="kw">tail</span>(som1<span class="op">$</span>codes[[<span class="dv">1</span>]]) <span class="co">#连接权重向量</span></span>
<span id="cb81-8"><a href="unsupervised-learning.html#cb81-8"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb81-9"><a href="unsupervised-learning.html#cb81-9"></a>som2 =<span class="st"> </span><span class="kw">som</span>(<span class="kw">as.matrix</span>(X),<span class="dt">grid=</span><span class="kw">somgrid</span>(<span class="dt">xdim=</span>p,<span class="dt">ydim=</span>p,<span class="dt">topo=</span><span class="st">&quot;hexagonal&quot;</span>),<span class="dt">rlen=</span><span class="dv">500</span>,<span class="dt">dist.fcts=</span><span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb81-10"><a href="unsupervised-learning.html#cb81-10"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb81-11"><a href="unsupervised-learning.html#cb81-11"></a><span class="kw">plot</span>(som1,<span class="kw">c</span>(<span class="st">&quot;changes&quot;</span>))</span>
<span id="cb81-12"><a href="unsupervised-learning.html#cb81-12"></a><span class="kw">plot</span>(som1,<span class="kw">c</span>(<span class="st">&quot;counts&quot;</span>), <span class="dt">main=</span><span class="st">&quot;allocation counts to neurons&quot;</span>)</span>
<span id="cb81-13"><a href="unsupervised-learning.html#cb81-13"></a>d.data<span class="op">$</span>tau2 =<span class="st"> </span>d.data<span class="op">$</span>sports_car<span class="op">+</span><span class="kw">as.integer</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb81-14"><a href="unsupervised-learning.html#cb81-14"></a><span class="kw">plot</span>(som1,<span class="kw">c</span>(<span class="st">&quot;mapping&quot;</span>),<span class="dt">classif=</span><span class="kw">predict</span>(som1),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;red&quot;</span>)[d.data<span class="op">$</span>tau2], <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">main=</span><span class="st">&quot;allocation of cases to neurons&quot;</span>)</span>
<span id="cb81-15"><a href="unsupervised-learning.html#cb81-15"></a><span class="kw">plot</span>(som2,<span class="kw">c</span>(<span class="st">&quot;changes&quot;</span>))</span>
<span id="cb81-16"><a href="unsupervised-learning.html#cb81-16"></a><span class="kw">plot</span>(som2,<span class="kw">c</span>(<span class="st">&quot;counts&quot;</span>), <span class="dt">main=</span><span class="st">&quot;allocation counts to neurons&quot;</span>)</span>
<span id="cb81-17"><a href="unsupervised-learning.html#cb81-17"></a>d.data<span class="op">$</span>tau2 =<span class="st"> </span>d.data<span class="op">$</span>sports_car<span class="op">+</span><span class="kw">as.integer</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb81-18"><a href="unsupervised-learning.html#cb81-18"></a><span class="kw">plot</span>(som2,<span class="kw">c</span>(<span class="st">&quot;mapping&quot;</span>),<span class="dt">classif=</span><span class="kw">predict</span>(som2),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;red&quot;</span>)[d.data<span class="op">$</span>tau2], <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">main=</span><span class="st">&quot;allocation of cases to neurons&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SOM"></span>
<img src="plots/5/som2.png" alt="SOM" width="80%"  />
<p class="caption">
Figure 5.18: SOM
</p>
</div>
<p>图<a href="unsupervised-learning.html#fig:SOM">5.18</a>的第一行展示的是矩形竞争层输出的降维结果，第二行展示的是六边形竞争层输出的降维结果，sports cars（红色）可以被较为明显地区分开来。</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="boosting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="rnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": "github"
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
