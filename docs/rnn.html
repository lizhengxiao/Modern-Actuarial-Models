<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 循环神经网络与死亡率预测 | 现代精算统计模型</title>
  <meta name="description" content="The output format is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6 循环神经网络与死亡率预测 | 现代精算统计模型" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The output format is bookdown::gitbook." />
  <meta name="github-repo" content="sxpyggy/Modern-Actuarial-Models" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 循环神经网络与死亡率预测 | 现代精算统计模型" />
  
  <meta name="twitter:description" content="The output format is bookdown::gitbook." />
  

<meta name="author" content="Modern Actuarial Models" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="unsupervised-learning.html"/>
<link rel="next" href="nlp.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">现代精算统计模型</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>👨‍🏫 欢迎</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#答疑"><i class="fa fa-check"></i>🤔 答疑</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#课程安排"><i class="fa fa-check"></i>🗓️ 课程安排</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i>简介</a></li>
<li class="chapter" data-level="1" data-path="pre.html"><a href="pre.html"><i class="fa fa-check"></i><b>1</b> 准备工作</a><ul>
<li class="chapter" data-level="1.1" data-path="pre.html"><a href="pre.html#常用链接"><i class="fa fa-check"></i><b>1.1</b> 常用链接</a></li>
<li class="chapter" data-level="1.2" data-path="pre.html"><a href="pre.html#克隆代码"><i class="fa fa-check"></i><b>1.2</b> 克隆代码</a></li>
<li class="chapter" data-level="1.3" data-path="pre.html"><a href="pre.html#r-interface-to-keras"><i class="fa fa-check"></i><b>1.3</b> R interface to Keras</a><ul>
<li class="chapter" data-level="1.3.1" data-path="pre.html"><a href="pre.html#r自动安装"><i class="fa fa-check"></i><b>1.3.1</b> R自动安装</a></li>
<li class="chapter" data-level="1.3.2" data-path="pre.html"><a href="pre.html#使用reticulate关联conda环境"><i class="fa fa-check"></i><b>1.3.2</b> 使用reticulate关联conda环境</a></li>
<li class="chapter" data-level="1.3.3" data-path="pre.html"><a href="pre.html#指定conda安装"><i class="fa fa-check"></i><b>1.3.3</b> 指定conda安装</a></li>
<li class="chapter" data-level="1.3.4" data-path="pre.html"><a href="pre.html#使用reticulate安装"><i class="fa fa-check"></i><b>1.3.4</b> 使用reticulate安装</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="pre.html"><a href="pre.html#r-interface-to-python"><i class="fa fa-check"></i><b>1.4</b> R interface to Python</a><ul>
<li class="chapter" data-level="1.4.1" data-path="pre.html"><a href="pre.html#reticulate-常见命令"><i class="fa fa-check"></i><b>1.4.1</b> reticulate 常见命令</a></li>
<li class="chapter" data-level="1.4.2" data-path="pre.html"><a href="pre.html#切换r关联的conda环境"><i class="fa fa-check"></i><b>1.4.2</b> 切换R关联的conda环境</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="pre.html"><a href="pre.html#python"><i class="fa fa-check"></i><b>1.5</b> Python</a><ul>
<li class="chapter" data-level="1.5.1" data-path="pre.html"><a href="pre.html#conda环境"><i class="fa fa-check"></i><b>1.5.1</b> Conda环境</a></li>
<li class="chapter" data-level="1.5.2" data-path="pre.html"><a href="pre.html#常用的conda命令"><i class="fa fa-check"></i><b>1.5.2</b> 常用的Conda命令</a></li>
<li class="chapter" data-level="1.5.3" data-path="pre.html"><a href="pre.html#tensorflowpytorch-gpu-version"><i class="fa fa-check"></i><b>1.5.3</b> Tensorflow/Pytorch GPU version</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="french.html"><a href="french.html"><i class="fa fa-check"></i><b>2</b> 车险索赔频率预测</a><ul>
<li class="chapter" data-level="2.1" data-path="french.html"><a href="french.html#背景介绍"><i class="fa fa-check"></i><b>2.1</b> 背景介绍</a></li>
<li class="chapter" data-level="2.2" data-path="french.html"><a href="french.html#预测模型概述"><i class="fa fa-check"></i><b>2.2</b> 预测模型概述</a></li>
<li class="chapter" data-level="2.3" data-path="french.html"><a href="french.html#特征工程"><i class="fa fa-check"></i><b>2.3</b> 特征工程</a><ul>
<li class="chapter" data-level="2.3.1" data-path="french.html"><a href="french.html#截断"><i class="fa fa-check"></i><b>2.3.1</b> 截断</a></li>
<li class="chapter" data-level="2.3.2" data-path="french.html"><a href="french.html#离散化"><i class="fa fa-check"></i><b>2.3.2</b> 离散化</a></li>
<li class="chapter" data-level="2.3.3" data-path="french.html"><a href="french.html#设定基础水平"><i class="fa fa-check"></i><b>2.3.3</b> 设定基础水平</a></li>
<li class="chapter" data-level="2.3.4" data-path="french.html"><a href="french.html#协变量变形"><i class="fa fa-check"></i><b>2.3.4</b> 协变量变形</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="french.html"><a href="french.html#训练集-验证集-测试集"><i class="fa fa-check"></i><b>2.4</b> 训练集-验证集-测试集</a></li>
<li class="chapter" data-level="2.5" data-path="french.html"><a href="french.html#泊松偏差损失函数"><i class="fa fa-check"></i><b>2.5</b> 泊松偏差损失函数</a></li>
<li class="chapter" data-level="2.6" data-path="french.html"><a href="french.html#泊松回归模型"><i class="fa fa-check"></i><b>2.6</b> 泊松回归模型</a></li>
<li class="chapter" data-level="2.7" data-path="french.html"><a href="french.html#泊松可加模型"><i class="fa fa-check"></i><b>2.7</b> 泊松可加模型</a></li>
<li class="chapter" data-level="2.8" data-path="french.html"><a href="french.html#泊松回归树"><i class="fa fa-check"></i><b>2.8</b> 泊松回归树</a></li>
<li class="chapter" data-level="2.9" data-path="french.html"><a href="french.html#随机森林"><i class="fa fa-check"></i><b>2.9</b> 随机森林</a></li>
<li class="chapter" data-level="2.10" data-path="french.html"><a href="french.html#泊松提升树"><i class="fa fa-check"></i><b>2.10</b> 泊松提升树</a></li>
<li class="chapter" data-level="2.11" data-path="french.html"><a href="french.html#模型比较"><i class="fa fa-check"></i><b>2.11</b> 模型比较</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nn.html"><a href="nn.html"><i class="fa fa-check"></i><b>3</b> 神经网络</a><ul>
<li class="chapter" data-level="3.1" data-path="nn.html"><a href="nn.html#建立神经网络的一般步骤"><i class="fa fa-check"></i><b>3.1</b> 建立神经网络的一般步骤</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nn.html"><a href="nn.html#明确目标和数据类型"><i class="fa fa-check"></i><b>3.1.1</b> 明确目标和数据类型</a></li>
<li class="chapter" data-level="3.1.2" data-path="nn.html"><a href="nn.html#数据预处理"><i class="fa fa-check"></i><b>3.1.2</b> 数据预处理</a></li>
<li class="chapter" data-level="3.1.3" data-path="nn.html"><a href="nn.html#选取合适的神经网络类型"><i class="fa fa-check"></i><b>3.1.3</b> 选取合适的神经网络类型</a></li>
<li class="chapter" data-level="3.1.4" data-path="nn.html"><a href="nn.html#建立神经网络全连接神经网络"><i class="fa fa-check"></i><b>3.1.4</b> 建立神经网络（全连接神经网络）</a></li>
<li class="chapter" data-level="3.1.5" data-path="nn.html"><a href="nn.html#训练神经网络"><i class="fa fa-check"></i><b>3.1.5</b> 训练神经网络</a></li>
<li class="chapter" data-level="3.1.6" data-path="nn.html"><a href="nn.html#调参"><i class="fa fa-check"></i><b>3.1.6</b> 调参</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="nn.html"><a href="nn.html#数据预处理-1"><i class="fa fa-check"></i><b>3.2</b> 数据预处理</a></li>
<li class="chapter" data-level="3.3" data-path="nn.html"><a href="nn.html#神经网络提升模型-combined-actuarial-neural-network"><i class="fa fa-check"></i><b>3.3</b> 神经网络提升模型 （combined actuarial neural network）</a></li>
<li class="chapter" data-level="3.4" data-path="nn.html"><a href="nn.html#神经网络结构"><i class="fa fa-check"></i><b>3.4</b> 神经网络结构</a><ul>
<li class="chapter" data-level="3.4.1" data-path="nn.html"><a href="nn.html#结构参数"><i class="fa fa-check"></i><b>3.4.1</b> 结构参数</a></li>
<li class="chapter" data-level="3.4.2" data-path="nn.html"><a href="nn.html#输入层"><i class="fa fa-check"></i><b>3.4.2</b> 输入层</a></li>
<li class="chapter" data-level="3.4.3" data-path="nn.html"><a href="nn.html#embedding-layer"><i class="fa fa-check"></i><b>3.4.3</b> Embedding layer</a></li>
<li class="chapter" data-level="3.4.4" data-path="nn.html"><a href="nn.html#隐藏层"><i class="fa fa-check"></i><b>3.4.4</b> 隐藏层</a></li>
<li class="chapter" data-level="3.4.5" data-path="nn.html"><a href="nn.html#输出层"><i class="fa fa-check"></i><b>3.4.5</b> 输出层</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="nn.html"><a href="nn.html#训练神经网络-1"><i class="fa fa-check"></i><b>3.5</b> 训练神经网络</a></li>
<li class="chapter" data-level="3.6" data-path="nn.html"><a href="nn.html#总结"><i class="fa fa-check"></i><b>3.6</b> 总结</a></li>
<li class="chapter" data-level="3.7" data-path="nn.html"><a href="nn.html#其它模型"><i class="fa fa-check"></i><b>3.7</b> 其它模型</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>4</b> 提升方法 (Boosting)</a><ul>
<li class="chapter" data-level="4.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>4.1</b> AdaBoost</a></li>
<li class="chapter" data-level="4.2" data-path="boosting.html"><a href="boosting.html#logit-boost-real-discrete-gentle-adaboost"><i class="fa fa-check"></i><b>4.2</b> Logit Boost (real, discrete, gentle AdaBoost)</a></li>
<li class="chapter" data-level="4.3" data-path="boosting.html"><a href="boosting.html#adaboost.m1"><i class="fa fa-check"></i><b>4.3</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="4.4" data-path="boosting.html"><a href="boosting.html#samme-stage-wise-additive-modeling-using-a-multi-class-exponential-loss-function"><i class="fa fa-check"></i><b>4.4</b> SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function)</a></li>
<li class="chapter" data-level="4.5" data-path="boosting.html"><a href="boosting.html#samme.r-multi-class-real-adaboost"><i class="fa fa-check"></i><b>4.5</b> SAMME.R (multi-class real AdaBoost)</a></li>
<li class="chapter" data-level="4.6" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>4.6</b> Gradient Boosting</a></li>
<li class="chapter" data-level="4.7" data-path="boosting.html"><a href="boosting.html#newton-boosting"><i class="fa fa-check"></i><b>4.7</b> Newton Boosting</a></li>
<li class="chapter" data-level="4.8" data-path="boosting.html"><a href="boosting.html#xgboost"><i class="fa fa-check"></i><b>4.8</b> XGBoost</a></li>
<li class="chapter" data-level="4.9" data-path="boosting.html"><a href="boosting.html#case-study"><i class="fa fa-check"></i><b>4.9</b> Case study</a><ul>
<li class="chapter" data-level="4.9.1" data-path="boosting.html"><a href="boosting.html#数据描述"><i class="fa fa-check"></i><b>4.9.1</b> 数据描述</a></li>
<li class="chapter" data-level="4.9.2" data-path="boosting.html"><a href="boosting.html#数据预处理-2"><i class="fa fa-check"></i><b>4.9.2</b> 数据预处理</a></li>
<li class="chapter" data-level="4.9.3" data-path="boosting.html"><a href="boosting.html#特征工程-1"><i class="fa fa-check"></i><b>4.9.3</b> 特征工程</a></li>
<li class="chapter" data-level="4.9.4" data-path="boosting.html"><a href="boosting.html#建模流程"><i class="fa fa-check"></i><b>4.9.4</b> 建模流程</a></li>
<li class="chapter" data-level="4.9.5" data-path="boosting.html"><a href="boosting.html#模型度量gini系数"><i class="fa fa-check"></i><b>4.9.5</b> 模型度量——Gini系数</a></li>
<li class="chapter" data-level="4.9.6" data-path="boosting.html"><a href="boosting.html#建立adaboost模型"><i class="fa fa-check"></i><b>4.9.6</b> 建立AdaBoost模型</a></li>
<li class="chapter" data-level="4.9.7" data-path="boosting.html"><a href="boosting.html#建立xgboost模型"><i class="fa fa-check"></i><b>4.9.7</b> 建立XGBoost模型</a></li>
<li class="chapter" data-level="4.9.8" data-path="boosting.html"><a href="boosting.html#结论"><i class="fa fa-check"></i><b>4.9.8</b> 结论</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="boosting.html"><a href="boosting.html#appendix-commonly-used-python-code-for-py-beginners"><i class="fa fa-check"></i><b>4.10</b> Appendix: Commonly used Python code (for py-beginners)</a><ul>
<li class="chapter" data-level="4.10.1" data-path="boosting.html"><a href="boosting.html#python标准数据类型"><i class="fa fa-check"></i><b>4.10.1</b> Python标准数据类型</a></li>
<li class="chapter" data-level="4.10.2" data-path="boosting.html"><a href="boosting.html#python内置函数"><i class="fa fa-check"></i><b>4.10.2</b> Python内置函数</a></li>
<li class="chapter" data-level="4.10.3" data-path="boosting.html"><a href="boosting.html#numpy包"><i class="fa fa-check"></i><b>4.10.3</b> numpy包</a></li>
<li class="chapter" data-level="4.10.4" data-path="boosting.html"><a href="boosting.html#pandas包"><i class="fa fa-check"></i><b>4.10.4</b> pandas包</a></li>
<li class="chapter" data-level="4.10.5" data-path="boosting.html"><a href="boosting.html#matplotlib包"><i class="fa fa-check"></i><b>4.10.5</b> Matplotlib包</a></li>
<li class="chapter" data-level="4.10.6" data-path="boosting.html"><a href="boosting.html#常用教程网址"><i class="fa fa-check"></i><b>4.10.6</b> 常用教程网址</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>5</b> 无监督学习方法</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#数据预处理-3"><i class="fa fa-check"></i><b>5.1</b> 数据预处理</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#主成分分析"><i class="fa fa-check"></i><b>5.2</b> 主成分分析</a></li>
<li class="chapter" data-level="5.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#自编码"><i class="fa fa-check"></i><b>5.3</b> 自编码</a><ul>
<li class="chapter" data-level="5.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#模型训练"><i class="fa fa-check"></i><b>5.3.1</b> 模型训练</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>5.4</b> K-means clustering</a></li>
<li class="chapter" data-level="5.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-medoids-clustering-pam"><i class="fa fa-check"></i><b>5.5</b> K-medoids clustering (PAM)</a></li>
<li class="chapter" data-level="5.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#gaussian-mixture-modelsgmms"><i class="fa fa-check"></i><b>5.6</b> Gaussian mixture models(GMMs)</a></li>
<li class="chapter" data-level="5.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#三种聚类方法评价"><i class="fa fa-check"></i><b>5.7</b> 三种聚类方法评价</a></li>
<li class="chapter" data-level="5.8" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#t-sne"><i class="fa fa-check"></i><b>5.8</b> t-SNE</a></li>
<li class="chapter" data-level="5.9" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#umap"><i class="fa fa-check"></i><b>5.9</b> UMAP</a></li>
<li class="chapter" data-level="5.10" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#som"><i class="fa fa-check"></i><b>5.10</b> SOM</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rnn.html"><a href="rnn.html"><i class="fa fa-check"></i><b>6</b> 循环神经网络与死亡率预测</a><ul>
<li class="chapter" data-level="6.1" data-path="rnn.html"><a href="rnn.html#lee-carter-model"><i class="fa fa-check"></i><b>6.1</b> Lee-Carter Model</a></li>
<li class="chapter" data-level="6.2" data-path="rnn.html"><a href="rnn.html#普通循环神经网络recurrent-neural-network"><i class="fa fa-check"></i><b>6.2</b> 普通循环神经网络（recurrent neural network）</a></li>
<li class="chapter" data-level="6.3" data-path="rnn.html"><a href="rnn.html#长短期记忆神经网络long-short-term-memory"><i class="fa fa-check"></i><b>6.3</b> 长短期记忆神经网络（Long short-term memory）</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rnn.html"><a href="rnn.html#激活函数activation-functions"><i class="fa fa-check"></i><b>6.3.1</b> 激活函数（Activation functions）</a></li>
<li class="chapter" data-level="6.3.2" data-path="rnn.html"><a href="rnn.html#gates-and-cell-state"><i class="fa fa-check"></i><b>6.3.2</b> Gates and cell state</a></li>
<li class="chapter" data-level="6.3.3" data-path="rnn.html"><a href="rnn.html#output-function"><i class="fa fa-check"></i><b>6.3.3</b> Output Function</a></li>
<li class="chapter" data-level="6.3.4" data-path="rnn.html"><a href="rnn.html#time-distributed-layer"><i class="fa fa-check"></i><b>6.3.4</b> Time-distributed Layer</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="rnn.html"><a href="rnn.html#门控循环神经网络gated-recurrent-unit"><i class="fa fa-check"></i><b>6.4</b> 门控循环神经网络（Gated Recurrent Unit）</a><ul>
<li class="chapter" data-level="6.4.1" data-path="rnn.html"><a href="rnn.html#gates"><i class="fa fa-check"></i><b>6.4.1</b> Gates</a></li>
<li class="chapter" data-level="6.4.2" data-path="rnn.html"><a href="rnn.html#neuron-activations"><i class="fa fa-check"></i><b>6.4.2</b> Neuron Activations</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rnn.html"><a href="rnn.html#案例分析case-study"><i class="fa fa-check"></i><b>6.5</b> 案例分析（Case study）</a><ul>
<li class="chapter" data-level="6.5.1" data-path="rnn.html"><a href="rnn.html#数据描述-1"><i class="fa fa-check"></i><b>6.5.1</b> 数据描述</a></li>
<li class="chapter" data-level="6.5.2" data-path="rnn.html"><a href="rnn.html#死亡率热力图"><i class="fa fa-check"></i><b>6.5.2</b> 死亡率热力图</a></li>
<li class="chapter" data-level="6.5.3" data-path="rnn.html"><a href="rnn.html#lee-carter-模型"><i class="fa fa-check"></i><b>6.5.3</b> Lee-Carter 模型</a></li>
<li class="chapter" data-level="6.5.4" data-path="rnn.html"><a href="rnn.html#初试rnn"><i class="fa fa-check"></i><b>6.5.4</b> 初试RNN</a></li>
<li class="chapter" data-level="6.5.5" data-path="rnn.html"><a href="rnn.html#rnn-1"><i class="fa fa-check"></i><b>6.5.5</b> RNN</a></li>
<li class="chapter" data-level="6.5.6" data-path="rnn.html"><a href="rnn.html#引入性别协变量"><i class="fa fa-check"></i><b>6.5.6</b> 引入性别协变量</a></li>
<li class="chapter" data-level="6.5.7" data-path="rnn.html"><a href="rnn.html#稳健性"><i class="fa fa-check"></i><b>6.5.7</b> 稳健性</a></li>
<li class="chapter" data-level="6.5.8" data-path="rnn.html"><a href="rnn.html#预测结果图"><i class="fa fa-check"></i><b>6.5.8</b> 预测结果图</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nlp.html"><a href="nlp.html"><i class="fa fa-check"></i><b>7</b> 自然语言处理</a><ul>
<li class="chapter" data-level="7.1" data-path="nlp.html"><a href="nlp.html#预处理"><i class="fa fa-check"></i><b>7.1</b> 预处理</a></li>
<li class="chapter" data-level="7.2" data-path="nlp.html"><a href="nlp.html#bag-of-words"><i class="fa fa-check"></i><b>7.2</b> Bag of words</a></li>
<li class="chapter" data-level="7.3" data-path="nlp.html"><a href="nlp.html#bag-of-part-of-speech"><i class="fa fa-check"></i><b>7.3</b> Bag of part-of-speech</a></li>
<li class="chapter" data-level="7.4" data-path="nlp.html"><a href="nlp.html#word-embeddings"><i class="fa fa-check"></i><b>7.4</b> Word embeddings</a><ul>
<li class="chapter" data-level="7.4.1" data-path="nlp.html"><a href="nlp.html#neural-probabilistic-language-model"><i class="fa fa-check"></i><b>7.4.1</b> Neural probabilistic language model</a></li>
<li class="chapter" data-level="7.4.2" data-path="nlp.html"><a href="nlp.html#word2vec"><i class="fa fa-check"></i><b>7.4.2</b> word2vec</a></li>
<li class="chapter" data-level="7.4.3" data-path="nlp.html"><a href="nlp.html#global-vectors-for-word-representationglove"><i class="fa fa-check"></i><b>7.4.3</b> Global vectors for word representation(Glove)</a></li>
<li class="chapter" data-level="7.4.4" data-path="nlp.html"><a href="nlp.html#pre-trained-word-embeddings"><i class="fa fa-check"></i><b>7.4.4</b> Pre-trained word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="nlp.html"><a href="nlp.html#机器学习算法"><i class="fa fa-check"></i><b>7.5</b> 机器学习算法</a></li>
<li class="chapter" data-level="7.6" data-path="nlp.html"><a href="nlp.html#神经网络"><i class="fa fa-check"></i><b>7.6</b> 神经网络</a><ul>
<li class="chapter" data-level="7.6.1" data-path="nlp.html"><a href="nlp.html#数据预处理-4"><i class="fa fa-check"></i><b>7.6.1</b> 数据预处理</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="nlp.html"><a href="nlp.html#case-study-1"><i class="fa fa-check"></i><b>7.7</b> Case study</a><ul>
<li class="chapter" data-level="7.7.1" data-path="nlp.html"><a href="nlp.html#函数说明"><i class="fa fa-check"></i><b>7.7.1</b> 函数说明</a></li>
<li class="chapter" data-level="7.7.2" data-path="nlp.html"><a href="nlp.html#可能遇到的问题"><i class="fa fa-check"></i><b>7.7.2</b> 可能遇到的问题</a></li>
<li class="chapter" data-level="7.7.3" data-path="nlp.html"><a href="nlp.html#结果比较"><i class="fa fa-check"></i><b>7.7.3</b> 结果比较</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="nlp.html"><a href="nlp.html#结论-1"><i class="fa fa-check"></i><b>7.8</b> 结论</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flashlight.html"><a href="flashlight.html"><i class="fa fa-check"></i><b>8</b> 通用模型解释方法</a><ul>
<li class="chapter" data-level="8.1" data-path="flashlight.html"><a href="flashlight.html#数据"><i class="fa fa-check"></i><b>8.1</b> 数据</a></li>
<li class="chapter" data-level="8.2" data-path="flashlight.html"><a href="flashlight.html#模型"><i class="fa fa-check"></i><b>8.2</b> 模型</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flashlight.html"><a href="flashlight.html#glm"><i class="fa fa-check"></i><b>8.2.1</b> GLM</a></li>
<li class="chapter" data-level="8.2.2" data-path="flashlight.html"><a href="flashlight.html#xgboost-1"><i class="fa fa-check"></i><b>8.2.2</b> XGBoost</a></li>
<li class="chapter" data-level="8.2.3" data-path="flashlight.html"><a href="flashlight.html#神经网络-1"><i class="fa fa-check"></i><b>8.2.3</b> 神经网络</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flashlight.html"><a href="flashlight.html#模型整体表现-model-performance"><i class="fa fa-check"></i><b>8.3</b> 模型整体表现 （model performance）</a></li>
<li class="chapter" data-level="8.4" data-path="flashlight.html"><a href="flashlight.html#变量重要性variable-importance"><i class="fa fa-check"></i><b>8.4</b> 变量重要性（variable importance）</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flashlight.html"><a href="flashlight.html#permutation-importance"><i class="fa fa-check"></i><b>8.4.1</b> Permutation importance</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flashlight.html"><a href="flashlight.html#边缘效应主效应"><i class="fa fa-check"></i><b>8.5</b> 边缘效应（主效应）</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flashlight.html"><a href="flashlight.html#individual-conditional-expectationsice"><i class="fa fa-check"></i><b>8.5.1</b> Individual conditional expectations（ICE）</a></li>
<li class="chapter" data-level="8.5.2" data-path="flashlight.html"><a href="flashlight.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>8.5.2</b> Partial dependence profiles</a></li>
<li class="chapter" data-level="8.5.3" data-path="flashlight.html"><a href="flashlight.html#accumulated-local-effects-profiles-ale"><i class="fa fa-check"></i><b>8.5.3</b> Accumulated local effects profiles (ALE)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="flashlight.html"><a href="flashlight.html#交互效应"><i class="fa fa-check"></i><b>8.6</b> 交互效应</a></li>
<li class="chapter" data-level="8.7" data-path="flashlight.html"><a href="flashlight.html#全局代理模型global-surrogate-models"><i class="fa fa-check"></i><b>8.7</b> 全局代理模型（Global surrogate models）</a></li>
<li class="chapter" data-level="8.8" data-path="flashlight.html"><a href="flashlight.html#局部解释样本解释"><i class="fa fa-check"></i><b>8.8</b> 局部解释（样本解释？）</a><ul>
<li class="chapter" data-level="8.8.1" data-path="flashlight.html"><a href="flashlight.html#lime和live"><i class="fa fa-check"></i><b>8.8.1</b> LIME和LIVE</a></li>
<li class="chapter" data-level="8.8.2" data-path="flashlight.html"><a href="flashlight.html#shapshapley-additive-explanations"><i class="fa fa-check"></i><b>8.8.2</b> SHAP(Shapley Additive Explanations)</a></li>
<li class="chapter" data-level="8.8.3" data-path="flashlight.html"><a href="flashlight.html#breakdown-and-approximate-shap"><i class="fa fa-check"></i><b>8.8.3</b> Breakdown and approximate SHAP</a></li>
<li class="chapter" data-level="8.8.4" data-path="flashlight.html"><a href="flashlight.html#from-local-to-global-properties"><i class="fa fa-check"></i><b>8.8.4</b> From local to global properties</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="flashlight.html"><a href="flashlight.html#improving-the-glm-by-interpretable-machine-learning"><i class="fa fa-check"></i><b>8.9</b> Improving the GLM by interpretable machine learning</a></li>
<li class="chapter" data-level="8.10" data-path="flashlight.html"><a href="flashlight.html#案例分析"><i class="fa fa-check"></i><b>8.10</b> 案例分析</a><ul>
<li class="chapter" data-level="8.10.1" data-path="flashlight.html"><a href="flashlight.html#导入包"><i class="fa fa-check"></i><b>8.10.1</b> 导入包</a></li>
<li class="chapter" data-level="8.10.2" data-path="flashlight.html"><a href="flashlight.html#预处理-1"><i class="fa fa-check"></i><b>8.10.2</b> 预处理</a></li>
<li class="chapter" data-level="8.10.3" data-path="flashlight.html"><a href="flashlight.html#描述性统计"><i class="fa fa-check"></i><b>8.10.3</b> 描述性统计</a></li>
<li class="chapter" data-level="8.10.4" data-path="flashlight.html"><a href="flashlight.html#建模"><i class="fa fa-check"></i><b>8.10.4</b> 建模</a></li>
<li class="chapter" data-level="8.10.5" data-path="flashlight.html"><a href="flashlight.html#解释"><i class="fa fa-check"></i><b>8.10.5</b> 解释</a></li>
<li class="chapter" data-level="8.10.6" data-path="flashlight.html"><a href="flashlight.html#局部性质"><i class="fa fa-check"></i><b>8.10.6</b> 局部性质</a></li>
<li class="chapter" data-level="8.10.7" data-path="flashlight.html"><a href="flashlight.html#改进glm"><i class="fa fa-check"></i><b>8.10.7</b> 改进glm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>9</b> 卷积神经网络</a><ul>
<li class="chapter" data-level="9.1" data-path="cnn.html"><a href="cnn.html#卷积层-convolution"><i class="fa fa-check"></i><b>9.1</b> 卷积层 (Convolution)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="cnn.html"><a href="cnn.html#超参数"><i class="fa fa-check"></i><b>9.1.1</b> 超参数</a></li>
<li class="chapter" data-level="9.1.2" data-path="cnn.html"><a href="cnn.html#参数个数计算"><i class="fa fa-check"></i><b>9.1.2</b> 参数个数计算</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="cnn.html"><a href="cnn.html#池化层-pooling"><i class="fa fa-check"></i><b>9.2</b> 池化层 (Pooling)</a></li>
<li class="chapter" data-level="9.3" data-path="cnn.html"><a href="cnn.html#批标准化层-batch-normalization"><i class="fa fa-check"></i><b>9.3</b> 批标准化层 (Batch Normalization)</a></li>
<li class="chapter" data-level="9.4" data-path="cnn.html"><a href="cnn.html#其他组件"><i class="fa fa-check"></i><b>9.4</b> 其他组件</a><ul>
<li class="chapter" data-level="9.4.1" data-path="cnn.html"><a href="cnn.html#全连接层-dense"><i class="fa fa-check"></i><b>9.4.1</b> 全连接层 (Dense)</a></li>
<li class="chapter" data-level="9.4.2" data-path="cnn.html"><a href="cnn.html#输出神经元"><i class="fa fa-check"></i><b>9.4.2</b> 输出神经元</a></li>
<li class="chapter" data-level="9.4.3" data-path="cnn.html"><a href="cnn.html#激活函数-activation"><i class="fa fa-check"></i><b>9.4.3</b> 激活函数 (Activation)</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="cnn.html"><a href="cnn.html#特性"><i class="fa fa-check"></i><b>9.5</b> 特性</a><ul>
<li class="chapter" data-level="9.5.1" data-path="cnn.html"><a href="cnn.html#平移不变性"><i class="fa fa-check"></i><b>9.5.1</b> 平移不变性</a></li>
<li class="chapter" data-level="9.5.2" data-path="cnn.html"><a href="cnn.html#旋转不变性"><i class="fa fa-check"></i><b>9.5.2</b> 旋转不变性</a></li>
<li class="chapter" data-level="9.5.3" data-path="cnn.html"><a href="cnn.html#尺度不变性"><i class="fa fa-check"></i><b>9.5.3</b> 尺度不变性</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="cnn.html"><a href="cnn.html#隐藏层可视化"><i class="fa fa-check"></i><b>9.6</b> 隐藏层可视化</a></li>
<li class="chapter" data-level="9.7" data-path="cnn.html"><a href="cnn.html#逆卷积"><i class="fa fa-check"></i><b>9.7</b> 逆卷积</a></li>
<li class="chapter" data-level="9.8" data-path="cnn.html"><a href="cnn.html#human-mortality-database-hmd"><i class="fa fa-check"></i><b>9.8</b> <span>Human Mortality Database (HMD)</span></a><ul>
<li class="chapter" data-level="9.8.1" data-path="cnn.html"><a href="cnn.html#输入和标签"><i class="fa fa-check"></i><b>9.8.1</b> 输入和标签</a></li>
<li class="chapter" data-level="9.8.2" data-path="cnn.html"><a href="cnn.html#评估指标"><i class="fa fa-check"></i><b>9.8.2</b> 评估指标</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="cnn.html"><a href="cnn.html#mnist-dataset"><i class="fa fa-check"></i><b>9.9</b> MNIST dataset</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/sxpyggy/Modern-Actuarial-Models/tree/modern-actuarial-models" target="blank">GitHub 仓库</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">现代精算统计模型</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rnn" class="section level1">
<h1><span class="header-section-number">6</span> 循环神经网络与死亡率预测</h1>
<p><em>方玉昕、鲁瑶、高光远</em></p>
<p>😷 新冠肺炎死亡率数据：<a href="https://mpidr.shinyapps.io/stmortality/" class="uri">https://mpidr.shinyapps.io/stmortality/</a></p>
<div id="lee-carter-model" class="section level2">
<h2><span class="header-section-number">6.1</span> Lee-Carter Model</h2>
<p>Lee Carter模型中，死亡力（force of mortality）的定义如下：<br />
<span class="math display">\[\log \left(m_{t, x}\right)=a_{x}+b_{x} k_{t}\]</span>
其中，</p>
<ul>
<li><p><span class="math inline">\(m_{t, x}&gt;0\)</span> 是 <span class="math inline">\(x\)</span> 岁的人在日历年 <span class="math inline">\(t\)</span> 的死亡率（mortality rate）,</p></li>
<li><p><span class="math inline">\(a_{x}\)</span> 是 <span class="math inline">\(x\)</span> 岁的人的平均对数死亡率,</p></li>
<li><p><span class="math inline">\(b_{x}\)</span> 是死亡率变化的年龄因素,</p></li>
<li><p><span class="math inline">\(\left(k_{t}\right)_{t}\)</span> 是死亡率变化的日历年因素.</p></li>
</ul>
<p>用 <span class="math inline">\(M_{t, x}\)</span> 表示某一性别死亡率的观察值（raw mortality rates）.<br />
我们对对数死亡率 <span class="math inline">\(\log \left(M_{t, x}\right)\)</span> 中心化处理：<br />
<span class="math display">\[\log \left(M_{t, x}^{\circ}\right)=\log \left(M_{t, x}\right)-\widehat{a}_{x}=\log \left(M_{t, x}\right)-\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)\]</span></p>
<p>其中，</p>
<ul>
<li><p><span class="math inline">\(\mathcal{T}\)</span> 为训练集中日历年的集合,</p></li>
<li><p><span class="math inline">\(\widehat{a}_{x}=\frac{1}{|\mathcal{T}|} \sum_{s \in \mathcal{T}} \log \left(M_{s, x}\right)\)</span> 是平均对数死亡率 <span class="math inline">\(a_{x}\)</span> 的估计.</p></li>
</ul>
<p>对于<span class="math inline">\(b_x,k_t\)</span> 我们的目标是求如下最优化问题：<br />
<span class="math display">\[\underset{\left(b_{x}\right)_{x},\left(k_{t}\right)_{t}}{\arg \min } \sum_{t, x}\left(\log \left(M_{t, x}^{\circ}\right)-b_{x} k_{t}\right)^{2}。\]</span></p>
<p>定义矩阵 <span class="math inline">\(A=\left(\log \left(M_{t, x}^{\circ}\right)\right)_{x, t}\)</span>。上述最优化问题可以通过对<span class="math inline">\(A\)</span>进行奇异值分解（SVD）解决<span class="math display">\[A=U\Lambda V^\intercal,\]</span>
其中<span class="math inline">\(U\)</span>称为左奇异矩阵，对角矩阵<span class="math inline">\(\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_T)\)</span>中的对角元素<span class="math inline">\(\lambda_1\geq\lambda_2\geq\ldots\geq\lambda_T\geq0\)</span>称为奇异值，<span class="math inline">\(V\)</span>称为右奇异矩阵。</p>
<ul>
<li><p><span class="math inline">\(A\)</span> 的第一个左奇异向量<span class="math inline">\(U_{\cdot,1}\)</span>与第一个奇异值<span class="math inline">\(\lambda_1\)</span>相乘，可以得到 <span class="math inline">\(\left(b_{x}\right)_{x}\)</span> 的一个估计 <span class="math inline">\(\left(\widehat{b}_{x}\right)_{x}\)</span>。</p></li>
<li><p><span class="math inline">\(A\)</span> 的第一个右奇异向量<span class="math inline">\(V_{\cdot,1}\)</span>给出了 <span class="math inline">\(\left(k_{t}\right)_{t}\)</span> 的一个估计 <span class="math inline">\(\left(\widehat{k}_{t}\right)_{t}\)</span>。</p></li>
</ul>
<p>为了求解结果的唯一性，增加约束：<br />
<span class="math display">\[\sum_{x} \hat{b}_{x}=1 \quad \text { and } \quad \sum_{t \in \mathcal{T}} \hat{k}_{t}=0\]</span>
至此即可解出唯一的 <span class="math inline">\(\left(\hat{a}_{x}, \hat{b}_{x}\right)_{x}, \left(\hat{k}_{t}\right)_{t}\)</span> . 这就是经典的LC模型构建方法.</p>
</div>
<div id="普通循环神经网络recurrent-neural-network" class="section level2">
<h2><span class="header-section-number">6.2</span> 普通循环神经网络（recurrent neural network）</h2>
<p><strong>输入变量（Input）</strong> : <span class="math inline">\(\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\)</span> with components <span class="math inline">\(\boldsymbol{x}_{t} \in \mathbb{R}^{\tau_{0}}\)</span> at times <span class="math inline">\(t=1, \ldots, T\)</span> (in time series structure).</p>
<p><strong>输出变量（Output）</strong>: <span class="math inline">\(y \in \mathcal{Y} \subset \mathbb{R}\)</span> .</p>
<p>首先看一个具有 <span class="math inline">\(\tau_{1} \in \mathbb{N}\)</span> 个隐层神经元（hidden neurons）和单一隐层（hidden layer）的RNN. 隐层由如下映射（mapping）定义：
<span class="math display">\[\boldsymbol{z}^{(1)}: \mathbb{R}^{\tau_{0} \times \tau_{1}} \rightarrow \mathbb{R}^{\tau_{1}}, \quad\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) \mapsto \boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right)\]</span>
其中下标 <span class="math inline">\(t\)</span> 表示时间,上标 (1) 表示第一隐层（本例中也是唯一隐层）.</p>
<p>隐层结构构造如下：<br />
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}\right) =&amp;\left(\phi\left(\left\langle\boldsymbol{w}_{1}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{1}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right),  \ldots, \phi\left(\left\langle\boldsymbol{w}_{\tau_{1}}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{\tau_{1}}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)\right)^{\top} \\
\stackrel{\text { def. }}{=} &amp;\phi\left(\left\langle W^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle U^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)
\end{aligned}
\]</span>
其中第 <span class="math inline">\(1 \leq j \leq \tau_{1}\)</span> 个神经元的结构为：<br />
<span class="math display">\[\phi\left(\left\langle\boldsymbol{w}_{j}^{(1)}, \boldsymbol{x}_{t}\right\rangle+\left\langle\boldsymbol{u}_{j}^{(1)}, \boldsymbol{z}_{t-1}\right\rangle\right)=\phi\left(w_{j, 0}^{(1)}+\sum_{l=1}^{\tau_{0}} w_{j, l}^{(1)} x_{t, l}+\sum_{l=1}^{\tau_{1}} u_{j, l}^{(1)} z_{t-1, l}\right)\]</span></p>
<ul>
<li><span class="math inline">\(\phi: \mathbb{R} \rightarrow \mathbb{R}\)</span> 是非线性激活函数（activation function）</li>
<li>网络参数（network parameters）为 <span class="math display">\[W^{(1)}=\left(\boldsymbol{w}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau \times\left(\tau_{0}+1\right)} \text{(including an intercept)}\]</span> <span class="math display">\[U^{(1)}=\left(\boldsymbol{u}_{j}^{(1)}\right)_{1 \leq j \leq \tau_{1}}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} \text{(excluding an intercept)}\]</span></li>
</ul>
<p>除了上述单隐层的结构，我们还可以轻松地设计多隐层的RNN.</p>
<p>例如，双隐层的RNN结构可以为:</p>
<ul>
<li><p><strong>1st variant</strong> : 仅允许同级隐层之间的循环
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &amp;=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right) \\
\boldsymbol{z}_{t}^{(2)} &amp;=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
\]</span></p></li>
<li><p><strong>2nd variant</strong> : 允许跨级隐层循环
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &amp;=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &amp;=\boldsymbol{z}^{(2)}\left(\boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
\]</span></p></li>
<li><p><strong>3rd variant</strong> : 允许二级隐层与输入层 <span class="math inline">\(\boldsymbol{x}_{t}\)</span> 进行循环
<span class="math display">\[
\begin{aligned}
\boldsymbol{z}_{t}^{(1)} &amp;=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right) \\
\boldsymbol{z}_{t}^{(2)} &amp;=\boldsymbol{z}^{(2)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t}^{(1)}, \boldsymbol{z}_{t-1}^{(2)}\right)
\end{aligned}
\]</span></p></li>
</ul>
</div>
<div id="长短期记忆神经网络long-short-term-memory" class="section level2">
<h2><span class="header-section-number">6.3</span> 长短期记忆神经网络（Long short-term memory）</h2>
<p>以上plain vanilla RNN 无法处理长距离依赖和且有梯度消散的问题。为此，Hochreiter-Schmidhuber (1997)提出了长短期记忆神经网络(Long Short Term Memory Network, LSTM)。</p>
<div id="激活函数activation-functions" class="section level3">
<h3><span class="header-section-number">6.3.1</span> 激活函数（Activation functions）</h3>
<p>LSTM 用到3种不同的 <strong>激活函数（activation functions）</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>Sigmoid函数（Sigmoid function）<br />
<span class="math display">\[\phi_{\sigma}(x)=\frac{1}{1+e^{-x}} \in(0,1)\]</span></p></li>
<li><p>双曲正切函数（Hyberbolic tangent function）
<span class="math display">\[\phi_{\tanh }(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=2 \phi_{\sigma}(2 x)-1 \in(-1,1)\]</span></p></li>
<li><p>一般的激活函数（General activation function）
<span class="math display">\[\phi: \mathbb{R} \rightarrow \mathbb{R}\]</span></p></li>
</ol>
</div>
<div id="gates-and-cell-state" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Gates and cell state</h3>
<p>令 <span class="math inline">\(\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span> 表示时间 <span class="math inline">\((t-1)\)</span> 时的<strong>活化状态（neuron activations）</strong>. 我们定义3中不同的 <strong>门（gates）</strong>, 用来决定传播到下一个时间的信息量：</p>
<ul>
<li><p><strong>遗忘门（Forget gate）</strong> (loss of memory rate):
<span class="math display">\[\boldsymbol{f}_{t}^{(1)}=\boldsymbol{f}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{f}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{f}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept) <span class="math inline">\(, U_{f}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span> (excluding an intercept <span class="math inline">\(),\)</span> and where the activation function is evaluated element wise.</p></li>
<li><p><strong>输入门（Input gate）</strong> (memory update rate):
<span class="math display">\[\boldsymbol{i}_{t}^{(1)}=\boldsymbol{i}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{i}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{i}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{i}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span>.</p></li>
<li><p><strong>输出门（Output gate）</strong> (release of memory information rate):
<span class="math display">\[\boldsymbol{o}_{t}^{(1)}=\boldsymbol{o}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{o}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{o}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept) <span class="math inline">\(, U_{o}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span>.</p></li>
</ul>
<p>注意：以上三种门的名字并不代表着它们在实际中的作用，它们的作用由网络参数决定，而网络参数是从数据中学到的。</p>
<p>令 <span class="math inline">\(\left(\boldsymbol{c}_{t}^{(1)}\right)_{t}\)</span> 表示 <strong>细胞状态（cell state）</strong> , 用以储存已获得的相关信息.</p>
<p>细胞状态的更新规则如下：<br />
<span class="math display">\[\begin{aligned}
\boldsymbol{c}_{t}^{(1)}&amp;=\boldsymbol{c}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)\\&amp;=\boldsymbol{f}_{t}^{(1)} \circ \boldsymbol{c}_{t-1}^{(1)}+\boldsymbol{i}_{t}^{(1)} \circ \phi_{\tanh }\left(\left\langle W_{c}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{c}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}
\end{aligned}\]</span>
for network parameters <span class="math inline">\(W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},\)</span> and <span class="math inline">\(\circ\)</span>
denotes the Hadamard product (element wise product).</p>
<p>最后，我们更新时刻 <span class="math inline">\(t\)</span> 时的活化状态 <span class="math inline">\(\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span>.<br />
<span class="math display">\[\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}, \boldsymbol{c}_{t-1}^{(1)}\right)=\boldsymbol{o}_{t}^{(1)} \circ \phi\left(\boldsymbol{c}_{t}^{(1)}\right) \in \mathbb{R}^{\tau_{1}}\]</span></p>
<p>至此，</p>
<ul>
<li><p>涉及的全部网络参数有: <span class="math display">\[W_{f}^{\top}, W_{i}^{\top}, W_{o}^{\top}, W_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}，~~ U_{f}^{\top}, U_{i}^{\top}, U_{o}^{\top}, U_{c}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}} .\]</span></p></li>
<li><p>一个LSTM层需要 <span class="math inline">\(4\left(\left(\tau_{0}+1\right) \tau_{1}+\tau_{1}^{2}\right)\)</span> 个网络参数。</p></li>
<li><p>以上定义的复杂映射在keras通过函数<code>layer_lstm()</code>即可实现。</p></li>
<li><p>这些参数均由梯度下降的变式算法（a variant of the gradient descent algorithm）学习得.</p></li>
</ul>
</div>
<div id="output-function" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Output Function</h3>
<p>基于 <span class="math inline">\(\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\)</span> , 我们来预测定义在 <span class="math inline">\(\mathcal{Y} \subset \mathbb{R}\)</span> 的随机变量 <span class="math inline">\(Y_{T}\)</span> .</p>
<p><span class="math display">\[\widehat{Y}_{T}=\widehat{Y}_{T}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{T}^{(1)}\right\rangle \in \mathcal{Y}\]</span>
其中，</p>
<ul>
<li><p><span class="math inline">\(z_{T}^{(1)}\)</span> 是最新的隐层神经元活化状态（hidden neuron activation）</p></li>
<li><p><span class="math inline">\(\boldsymbol{w} \in \mathbb{R}^{\tau_{1}+1}\)</span> 是输出权重(again including an intercept component)</p></li>
<li><p><span class="math inline">\(\varphi: \mathbb{R} \rightarrow \mathcal{Y}\)</span> 是一个恰当的输出激活函数，选择时需要考虑<span class="math inline">\(y\)</span>的取值范围。</p></li>
</ul>
</div>
<div id="time-distributed-layer" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Time-distributed Layer</h3>
<p>以上只考虑了根据最新的状态 <span class="math inline">\(\boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\)</span> 所确定的单一的输出 <span class="math inline">\(Y_{T}\)</span>.</p>
<p>但是我们可以考虑 <strong>所有</strong> 隐层神经元状态:<br />
<span class="math display">\[\boldsymbol{z}_{1}^{(1)}\left(\boldsymbol{x}_{1}\right), \boldsymbol{z}_{2}^{(1)}\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right), \boldsymbol{z}_{3}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{3}\right), \ldots, \boldsymbol{z}_{T}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{T}\right)\]</span>
每一个状态 <span class="math inline">\(\boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)\)</span> 都可以作为解释变量，用以估计 <span class="math inline">\(t\)</span> 时所对应的 <span class="math inline">\(Y_{t}\)</span> :</p>
<p><span class="math display">\[\widehat{Y}_{t}=\widehat{Y}_{t}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\right\rangle=\varphi\left\langle\boldsymbol{w}, \boldsymbol{z}_{t}^{(1)}\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{t}\right)\right\rangle\]</span>
其中过滤器（filter） <span class="math inline">\(\varphi\langle\boldsymbol{w}, \cdot\rangle\)</span> 对所有时间 <span class="math inline">\(t\)</span> 取相同函数.</p>
<p><strong>小结：LSTM的优势</strong></p>
<ul>
<li><p>时间序列结构和因果关系都可以得到正确的反应</p></li>
<li><p>由于参数不依赖时间，LSTM可以很容易地拓展到未来时间段</p></li>
</ul>
</div>
</div>
<div id="门控循环神经网络gated-recurrent-unit" class="section level2">
<h2><span class="header-section-number">6.4</span> 门控循环神经网络（Gated Recurrent Unit）</h2>
<p>另一个比较热门的RNN结构是：门控循环单元（gated recurrent unit, GRU), 由Cho et al. (2014) 提出，它比LSTM更加简洁，但同样可以缓解plain vanilla RNN中梯度消散的问题。</p>
<div id="gates" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Gates</h3>
<p>GRU只使用2个不同的<strong>门（gates）</strong>. 令 <span class="math inline">\(\boldsymbol{z}_{t-1}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span> 表示 <span class="math inline">\((t-1)\)</span> 时神经元活化状态.</p>
<ul>
<li><p><strong>Reset gate</strong>: 类似于LSTM中的遗忘门
<span class="math display">\[\boldsymbol{r}_{t}^{(1)}=\boldsymbol{r}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{r}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{r}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{r}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span>.</p></li>
<li><p><strong>Update gate</strong>: 类似于LSTM中的输入门
<span class="math display">\[\boldsymbol{u}_{t}^{(1)}=\boldsymbol{u}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\phi_{\sigma}\left(\left\langle W_{u}, \boldsymbol{x}_{t}\right\rangle+\left\langle U_{u}, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in(0,1)^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept), <span class="math inline">\(U_{u}^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}}\)</span></p></li>
</ul>
</div>
<div id="neuron-activations" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Neuron Activations</h3>
<p>以上门变量的作用是，已知 <span class="math inline">\(t-1\)</span> 时神经元活化状态 <span class="math inline">\(\boldsymbol{z}_{t-1}^{(1)}\)</span>, 计算 <span class="math inline">\(t\)</span> 时神经元活化状态 <span class="math inline">\(\boldsymbol{z}_{t}^{(1)} \in \mathbb{R}^{\tau_{1}}\)</span> . 我们选用如下结构：
<span class="math display">\[\boldsymbol{z}_{t}^{(1)}=\boldsymbol{z}^{(1)}\left(\boldsymbol{x}_{t}, \boldsymbol{z}_{t-1}^{(1)}\right)=\boldsymbol{r}_{t}^{(1)} \circ \boldsymbol{z}_{t-1}^{(1)}+\left(\mathbf{1}-\boldsymbol{r}_{t}^{(1)}\right) \circ \phi\left(\left\langle W, \boldsymbol{x}_{t}\right\rangle+\boldsymbol{u}_{t} \circ\left\langle U, \boldsymbol{z}_{t-1}^{(1)}\right\rangle\right) \in \mathbb{R}^{\tau_{1}}\]</span>
for network parameters <span class="math inline">\(W^{\top} \in \mathbb{R}^{\tau_{1} \times\left(\tau_{0}+1\right)}\)</span> (including an intercept) <span class="math inline">\(, U^{\top} \in \mathbb{R}^{\tau_{1} \times \tau_{1}},\)</span> and where <span class="math inline">\(\circ\)</span> denotes the Hadamard product.</p>
<p>GRU网络比LSTM网络的结构更简洁，而且会产生相近的结果。
但是，GRU在稳健性上有较大缺陷，因此现阶段LSTM的使用更为广泛.</p>
</div>
</div>
<div id="案例分析case-study" class="section level2">
<h2><span class="header-section-number">6.5</span> 案例分析（Case study）</h2>
<p>本案例的数据来源于Human Mortality Database (HMD)中的数据，选择瑞士人口数据(HMD中标记为“CHE”)作为示例。</p>
<div id="数据描述-1" class="section level3">
<h3><span class="header-section-number">6.5.1</span> 数据描述</h3>
<p>数据包含7个变量，各变量说明如下：</p>
<table>
<thead>
<tr class="header">
<th align="center">变量</th>
<th align="center">类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Gender</td>
<td align="center">factor</td>
<td>两种性别——男性和女性</td>
</tr>
<tr class="even">
<td align="center">Year</td>
<td align="center">int</td>
<td>日历年，1950年到2016年</td>
</tr>
<tr class="odd">
<td align="center">Age</td>
<td align="center">int</td>
<td>年龄范围0-99岁</td>
</tr>
<tr class="even">
<td align="center">Country</td>
<td align="center">chr</td>
<td>“CHE”，代表瑞士</td>
</tr>
<tr class="odd">
<td align="center">imputed_flag</td>
<td align="center">logi</td>
<td>原始死亡率为0，用HMD中其余国家同日历年同年龄的平均死亡率代替，则该变量为TRUE</td>
</tr>
<tr class="even">
<td align="center">mx</td>
<td align="center">num</td>
<td>死亡率</td>
</tr>
<tr class="odd">
<td align="center">logmx</td>
<td align="center">num</td>
<td>对数死亡率</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="rnn.html#cb82-1"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb82-2"><a href="rnn.html#cb82-2"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb82-3"><a href="rnn.html#cb82-3"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb82-4"><a href="rnn.html#cb82-4"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb82-5"><a href="rnn.html#cb82-5"></a><span class="kw">length</span>(<span class="kw">unique</span>(all_mort<span class="op">$</span>Age))</span>
<span id="cb82-6"><a href="rnn.html#cb82-6"></a><span class="kw">length</span>(<span class="kw">unique</span>(all_mort<span class="op">$</span>Year))</span>
<span id="cb82-7"><a href="rnn.html#cb82-7"></a><span class="dv">67</span><span class="op">*</span><span class="dv">2</span><span class="op">*</span><span class="dv">100</span></span></code></pre></div>
</div>
<div id="死亡率热力图" class="section level3">
<h3><span class="header-section-number">6.5.2</span> 死亡率热力图</h3>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="rnn.html#cb83-1"></a>gender &lt;-<span class="st"> &quot;Male&quot;</span></span>
<span id="cb83-2"><a href="rnn.html#cb83-2"></a><span class="co">#gender &lt;- &quot;Female&quot;</span></span>
<span id="cb83-3"><a href="rnn.html#cb83-3"></a>m0 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(all_mort<span class="op">$</span>logmx), <span class="kw">max</span>(all_mort<span class="op">$</span>logmx))</span>
<span id="cb83-4"><a href="rnn.html#cb83-4"></a><span class="co"># rows are calendar year t, columns are ages x</span></span>
<span id="cb83-5"><a href="rnn.html#cb83-5"></a>logmx &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">matrix</span>(<span class="kw">as.matrix</span>(all_mort[<span class="kw">which</span>(all_mort<span class="op">$</span>Gender<span class="op">==</span>gender),<span class="st">&quot;logmx&quot;</span>]), <span class="dt">nrow=</span><span class="dv">100</span>, <span class="dt">ncol=</span><span class="dv">67</span>))</span>
<span id="cb83-6"><a href="rnn.html#cb83-6"></a><span class="co"># png(&quot;./plots/6/heat.png&quot;)</span></span>
<span id="cb83-7"><a href="rnn.html#cb83-7"></a><span class="kw">image</span>(<span class="dt">z=</span>logmx, <span class="dt">useRaster=</span><span class="ot">TRUE</span>,  <span class="dt">zlim=</span>m0, <span class="dt">col=</span><span class="kw">rev</span>(<span class="kw">rainbow</span>(<span class="dt">n=</span><span class="dv">60</span>, <span class="dt">start=</span><span class="dv">0</span>, <span class="dt">end=</span>.<span class="dv">72</span>)), <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;Swiss &quot;</span>,gender, <span class="st">&quot; raw log-mortality rates&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">ylab=</span><span class="st">&quot;age x&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;calendar year t&quot;</span>)</span>
<span id="cb83-8"><a href="rnn.html#cb83-8"></a><span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span>(<span class="dv">2016-1950</span>))<span class="op">/</span>(<span class="dv">2016-1950</span>), <span class="kw">c</span>(<span class="dv">1950</span><span class="op">:</span><span class="dv">2016</span>))                   </span>
<span id="cb83-9"><a href="rnn.html#cb83-9"></a><span class="kw">axis</span>(<span class="dv">2</span>, <span class="dt">at=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">49</span>)<span class="op">/</span><span class="dv">50</span>, <span class="dt">labels=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">49</span>)<span class="op">*</span><span class="dv">2</span>)                   </span>
<span id="cb83-10"><a href="rnn.html#cb83-10"></a><span class="kw">lines</span>(<span class="dt">x=</span><span class="kw">rep</span>((<span class="dv">1999-1950</span><span class="fl">+0.5</span>)<span class="op">/</span>(<span class="dv">2016-1950</span>), <span class="dv">2</span>), <span class="dt">y=</span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb83-11"><a href="rnn.html#cb83-11"></a><span class="kw">dev.off</span>()</span></code></pre></div>
<p>图<a href="rnn.html#fig:heatplot">6.1</a>显示了男女性对数死亡率随时间的改善:</p>
<ul>
<li><p>左右两幅图的色标相同，蓝色表示死亡率小，红色表示死亡率大</p></li>
<li><p>该图显示过去几十年典型的死亡率改善——热图中颜色呈略微向上的对角线结构</p></li>
<li><p>平均而言，女性死亡率低于男性</p></li>
<li><p>图中位于2000年的垂直黑线表示对于训练数据<span class="math inline">\(\mathcal{T}\)</span>和验证数据<span class="math inline">\(\mathcal{V}\)</span>的划分:后续模型将使用日历年<span class="math inline">\(t=1950, \ldots, 1999\)</span>作为训练数据<span class="math inline">\(\mathcal{T}\)</span>进行学习，用<span class="math inline">\(2000, \ldots, 2016\)</span>作为验证数据<span class="math inline">\(\mathcal{V}\)</span>对死亡率做样本外验证。</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:heatplot"></span>
<img src="plots/6/heat.png" alt="瑞士男女性死亡率热力图" width="60%" />
<p class="caption">
Figure 6.1: 瑞士男女性死亡率热力图
</p>
</div>
</div>
<div id="lee-carter-模型" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Lee-Carter 模型</h3>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="rnn.html#cb84-1"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">1999</span></span>
<span id="cb84-2"><a href="rnn.html#cb84-2"></a>gender &lt;-<span class="st"> &quot;Female&quot;</span></span>
<span id="cb84-3"><a href="rnn.html#cb84-3"></a>train &lt;-<span class="st"> </span>all_mort[Year<span class="op">&lt;=</span>ObsYear][Gender <span class="op">==</span><span class="st"> </span>gender]</span>
<span id="cb84-4"><a href="rnn.html#cb84-4"></a><span class="kw">min</span>(train<span class="op">$</span>Year)</span>
<span id="cb84-5"><a href="rnn.html#cb84-5"></a>    </span>
<span id="cb84-6"><a href="rnn.html#cb84-6"></a><span class="co">### fit via SVD</span></span>
<span id="cb84-7"><a href="rnn.html#cb84-7"></a>train[,ax<span class="op">:</span><span class="er">=</span><span class="st"> </span><span class="kw">mean</span>(logmx), by =<span class="st"> </span>(Age)]</span>
<span id="cb84-8"><a href="rnn.html#cb84-8"></a>train[,mx_adj<span class="op">:</span><span class="er">=</span><span class="st"> </span>logmx<span class="op">-</span>ax]  </span>
<span id="cb84-9"><a href="rnn.html#cb84-9"></a>rates_mat &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">dcast.data.table</span>(Age<span class="op">~</span>Year, <span class="dt">value.var =</span> <span class="st">&quot;mx_adj&quot;</span>, sum))[,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb84-10"><a href="rnn.html#cb84-10"></a><span class="kw">dim</span>(rates_mat)</span>
<span id="cb84-11"><a href="rnn.html#cb84-11"></a>svd_fit &lt;-<span class="st"> </span><span class="kw">svd</span>(rates_mat)</span>
<span id="cb84-12"><a href="rnn.html#cb84-12"></a>    </span>
<span id="cb84-13"><a href="rnn.html#cb84-13"></a>ax &lt;-<span class="st"> </span>train[,<span class="kw">unique</span>(ax)]</span>
<span id="cb84-14"><a href="rnn.html#cb84-14"></a>bx &lt;-<span class="st"> </span>svd_fit<span class="op">$</span>u[,<span class="dv">1</span>]<span class="op">*</span>svd_fit<span class="op">$</span>d[<span class="dv">1</span>]</span>
<span id="cb84-15"><a href="rnn.html#cb84-15"></a>kt &lt;-<span class="st"> </span>svd_fit<span class="op">$</span>v[,<span class="dv">1</span>]</span>
<span id="cb84-16"><a href="rnn.html#cb84-16"></a>      </span>
<span id="cb84-17"><a href="rnn.html#cb84-17"></a>c1 &lt;-<span class="st"> </span><span class="kw">mean</span>(kt)</span>
<span id="cb84-18"><a href="rnn.html#cb84-18"></a>c2 &lt;-<span class="st"> </span><span class="kw">sum</span>(bx)</span>
<span id="cb84-19"><a href="rnn.html#cb84-19"></a>ax &lt;-<span class="st"> </span>ax<span class="op">+</span>c1<span class="op">*</span>bx</span>
<span id="cb84-20"><a href="rnn.html#cb84-20"></a>bx &lt;-<span class="st"> </span>bx<span class="op">/</span>c2</span>
<span id="cb84-21"><a href="rnn.html#cb84-21"></a>kt &lt;-<span class="st"> </span>(kt<span class="op">-</span>c1)<span class="op">*</span>c2</span>
<span id="cb84-22"><a href="rnn.html#cb84-22"></a>    </span>
<span id="cb84-23"><a href="rnn.html#cb84-23"></a><span class="co">### extrapolation and forecast</span></span>
<span id="cb84-24"><a href="rnn.html#cb84-24"></a>vali  &lt;-<span class="st"> </span>all_mort[Year<span class="op">&gt;</span>ObsYear][Gender <span class="op">==</span><span class="st"> </span>gender]    </span>
<span id="cb84-25"><a href="rnn.html#cb84-25"></a>t_forecast &lt;-<span class="st"> </span>vali[,<span class="kw">unique</span>(Year)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">length</span>()</span>
<span id="cb84-26"><a href="rnn.html#cb84-26"></a>forecast_kt  =kt <span class="op">%&gt;%</span><span class="st"> </span>forecast<span class="op">::</span><span class="kw">rwf</span>(t_forecast, <span class="dt">drift =</span> T)</span>
<span id="cb84-27"><a href="rnn.html#cb84-27"></a>kt_forecast =<span class="st"> </span>forecast_kt<span class="op">$</span>mean</span>
<span id="cb84-28"><a href="rnn.html#cb84-28"></a> </span>
<span id="cb84-29"><a href="rnn.html#cb84-29"></a><span class="co"># illustration selected drift</span></span>
<span id="cb84-30"><a href="rnn.html#cb84-30"></a>plot_data &lt;-<span class="st"> </span><span class="kw">c</span>(kt, kt_forecast)</span>
<span id="cb84-31"><a href="rnn.html#cb84-31"></a><span class="kw">plot</span>(plot_data, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">cex=</span><span class="dv">2</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">ylab=</span><span class="st">&quot;values k_t&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;calendar year t&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;estimated process k_t for &quot;</span>,gender, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), <span class="dt">cex=</span><span class="fl">1.5</span>)) </span>
<span id="cb84-32"><a href="rnn.html#cb84-32"></a><span class="kw">points</span>(kt, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb84-33"><a href="rnn.html#cb84-33"></a><span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(plot_data)), <span class="dt">labels=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(plot_data))<span class="op">+</span><span class="dv">1949</span>)                   </span>
<span id="cb84-34"><a href="rnn.html#cb84-34"></a><span class="kw">abline</span>(<span class="dt">v=</span>(<span class="kw">length</span>(kt)<span class="op">+</span><span class="fl">0.5</span>), <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb84-35"><a href="rnn.html#cb84-35"></a><span class="co"># in-sample and out-of-sample analysis    </span></span>
<span id="cb84-36"><a href="rnn.html#cb84-36"></a>fitted =<span class="st"> </span>(ax<span class="op">+</span>(bx)<span class="op">%*%</span><span class="kw">t</span>(kt)) <span class="op">%&gt;%</span><span class="st"> </span>melt</span>
<span id="cb84-37"><a href="rnn.html#cb84-37"></a>train<span class="op">$</span>pred_LC_svd =<span class="st"> </span>fitted<span class="op">$</span>value <span class="op">%&gt;%</span><span class="st"> </span>exp</span>
<span id="cb84-38"><a href="rnn.html#cb84-38"></a>fitted_vali =<span class="st"> </span>(ax<span class="op">+</span>(bx)<span class="op">%*%</span><span class="kw">t</span>(kt_forecast)) <span class="op">%&gt;%</span><span class="st"> </span>melt</span>
<span id="cb84-39"><a href="rnn.html#cb84-39"></a>vali<span class="op">$</span>pred_LC_svd =<span class="st">   </span>fitted_vali<span class="op">$</span>value <span class="op">%&gt;%</span><span class="st"> </span>exp</span>
<span id="cb84-40"><a href="rnn.html#cb84-40"></a><span class="kw">round</span>(<span class="kw">c</span>((<span class="kw">mean</span>((train<span class="op">$</span>mx<span class="op">-</span>train<span class="op">$</span>pred_LC_svd)<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>) , (<span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali<span class="op">$</span>pred_LC_svd)<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>)),<span class="dv">4</span>)</span></code></pre></div>
<p>用带漂移项的随机游走预测<span class="math inline">\(t \in \mathcal{V}=\{2000, \ldots, 2016\}\)</span>的<span class="math inline">\(\hat{k}_{t}\)</span>，下图说明了结果。</p>
<div class="figure" style="text-align: center"><span id="fig:kt"></span>
<img src="plots/6/kt.png" alt="瑞士男女性kt的估计与预测值" width="60%"  />
<p class="caption">
Figure 6.2: 瑞士男女性kt的估计与预测值
</p>
</div>
<p>图<a href="rnn.html#fig:kt">6.2</a>显示对于女性来说预测结果是相对合理的，但是对于男性而言，由此产生的漂移可能需要进一步的探索，下面<strong>男女性样本内外的MSE损失</strong>结果也表明了这一点：男性样本外损失较大</p>
<p><span class="math display">\[
\begin{array}{|c|cc|cc|}
\hline &amp; \ {\text { in-sample loss }} &amp; \ {\text { in-sample loss }}  &amp; \ {\text { out-of-sample loss }} &amp; \ {\text { out-of-sample loss }}\\
&amp; \text { female } &amp; \text { male } &amp; \text { female } &amp; \text { male } \\
\hline \text { LC model with SVD } &amp; 3.7573 &amp; 8.8110 &amp; 0.6045 &amp; 1.8152 \\
\hline
\end{array}
\]</span></p>
</div>
<div id="初试rnn" class="section level3">
<h3><span class="header-section-number">6.5.4</span> 初试RNN</h3>
<ol style="list-style-type: decimal">
<li>数据说明</li>
</ol>
<ul>
<li><p>选择性别为“女性”，提取<span class="math inline">\(1990, \ldots, 2001\)</span>年的对数死亡率，年龄为<span class="math inline">\(0 \leq x \leq 99\)</span></p></li>
<li><p>超参数设置：回顾周期<span class="math inline">\(T=10\)</span>；<span class="math inline">\(\tau_{0}=3\)</span></p></li>
<li><p>定义解释变量和响应变量：</p>
<p>对于<span class="math inline">\(1 \leq x \leq 98\)</span>，<span class="math inline">\(1 \leq t \leq T\)</span>，有</p>
<p><strong>解释变量</strong><span class="math inline">\(\boldsymbol{x}_{t, x}=\left(\log \left(M_{1999-(T-t), x-1}\right), \log \left(M_{1999-(T-t), x}\right), \log \left(M_{1999-(T-t), x+1}\right)\right)^{\top} \in \mathbb{R}^{\tau_{0}}\)</span></p>
<p><strong>响应变量</strong><span class="math inline">\(\boldsymbol{Y}_{T, x}=\log(M_{2000,x}) =\log \left(M_{1999-(T-T)+1, x}\right) \in \mathbb{R}_{-}\)</span></p>
<p>同时考虑<span class="math inline">\((x-1,x,x+1)\)</span>目的是用邻近的年龄来平滑输入。</p></li>
<li><p>选择训练数据和验证数据：</p>
<p><strong>训练数据</strong><span class="math inline">\(\mathcal{T}=\{(\boldsymbol{x}_{1,x}, \ldots,\boldsymbol{x}_{T,x};\boldsymbol{Y}_{T, x});1 \leq x \leq 98\}\)</span></p>
<p><strong>验证数据</strong><span class="math inline">\(\mathcal{V}=\{(\boldsymbol{x}_{2,x}, \ldots,\boldsymbol{x}_{T+1,x};\boldsymbol{Y}_{T+1, x});1 \leq x \leq 98\}\)</span>，在训练数据基础上时移了一个日历年</p></li>
<li><p>数据如下图所示：</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="plots/6/datatoy.png" alt="RNN初试中选择的数据" width="60%"  />
<p class="caption">
Figure 6.3: RNN初试中选择的数据
</p>
</div>
<p>黑线表示选定的解释变量<span class="math inline">\(\boldsymbol{x}_{t, x}\)</span>;蓝色的点是训练数据中的的响应变量<span class="math inline">\(\boldsymbol{Y}_{T, x}\)</span>；验证数据中响应变量<span class="math inline">\(\boldsymbol{Y}_{T+1, x}=\log(M_{2001,x})\)</span>用红色的点表示</p>
<ol start="2" style="list-style-type: decimal">
<li>数据预处理</li>
</ol>
<ul>
<li><p>对解释变量应用MinMaxScaler进行标准化处理</p></li>
<li><p>切换响应变量的符号</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>比较LSTMs和GRUs</li>
</ol>
<ul>
<li><p>在验证集<span class="math inline">\(\mathcal{V}\)</span>上跟踪过拟合</p></li>
<li><p>梯度下降优化算法选用的是<code>nadam</code></p></li>
<li><p>下图显示了5个模型的收敛行为</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:loss1"></span>
<img src="plots/6/loss1.png" alt="模型的样本内外损失" width="60%"  />
<p class="caption">
Figure 6.4: 模型的样本内外损失
</p>
</div>
<ul>
<li>根据过拟合确定的停止时间的模型校准结果如下表所示：</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|ccc|cc|}
\hline &amp; \# \text { param. } &amp; \text { epochs } &amp; \text { run time } &amp; \text { in-sample loss } &amp; \text { out-of-sample loss } \\
\hline \text { LSTM1 } &amp; 186 &amp; 150 &amp; 8 \mathrm{sec} &amp; 0.0655 &amp; 0.0936 \\
\text { LSTM2 } &amp; 345 &amp; 200 &amp; 15 \mathrm{sec} &amp; 0.0603 &amp; 0.0918 \\
\hline \text { GRU1 } &amp; 141 &amp; 100 &amp; 5 \mathrm{sec} &amp; 0.0671 &amp; 0.0860 \\
\text { GRU2 } &amp; 260 &amp; 200 &amp; 14 \mathrm{sec} &amp; 0.0651 &amp; 0.0958 \\
\hline \text { deep FNN } &amp; 184 &amp; 200 &amp; 5 \mathrm{sec} &amp; 0.0485 &amp; 0.1577 \\
\hline
\end{array}
\]</span></p>
<ul>
<li>表中所示模型的超参数设置</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>LSTM1和GRU1表示只有一个隐藏层的RNN，该隐藏层的神经元个数<span class="math inline">\(\tau_{1}=5\)</span></p></li>
<li><p>LSTM2和GRU2表示有两个隐藏层的RNN，第一个隐藏层神经元个数<span class="math inline">\(\tau_{1}=5\)</span>；第二个隐藏层神经元个数<span class="math inline">\(\tau_{2}=4\)</span></p></li>
<li><p>deep FNN表示有两个隐藏层的前馈神经网络结构，其中<span class="math inline">\((q1,q2)=(5,4)\)</span></p></li>
</ol>
<ul>
<li>结论</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>LSTM2与LSTM1模型预测质量相当，但LSTM2有更多参数及更长的运行时间</p></li>
<li><p>LTSM与GRU超参数选择相同时，普遍的观察结果是GRU比LSTM更快的过拟合，但GRU不稳定</p></li>
<li><p>前馈神经网络与RNN相比没有竞争力</p></li>
</ol>
<ul>
<li>在本文建模中<strong>未引入年龄变量的原因</strong>：</li>
</ul>
<p>协变量标准化到（-1,1）的过程是在所有年龄上同时进行的，因此协变量信息保留了死亡率水平，这和引入年龄变量具有相同的信息质量。</p>
<ol start="4" style="list-style-type: decimal">
<li>超参数选择</li>
</ol>
<ul>
<li>分别改变<span class="math inline">\(T、\tau_{0}、\tau_{1}\)</span>的值，得到结果如下表所示：</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|ccc|cc|}
\hline &amp; \text { # param. } &amp; \text { epochs } &amp; \text { run time } &amp; \text { in-sample } &amp; \text { out-of-sample } \\
\hline \text { base case: } &amp; &amp; &amp; &amp; &amp; \\
\text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=5\right) &amp; 186 &amp; 150 &amp; 8 \mathrm{sec} &amp; 0.0655 &amp; 0.0936 \\
\hline \text { LSTM1 }\left(T=10, \tau_{0}=1, \tau_{1}=5\right) &amp; 146 &amp; 100 &amp; 5 \mathrm{sec} &amp; 0.0647 &amp; 0.1195 \\
\text { LSTM1 }\left(T=10, \tau_{0}=5, \tau_{1}=5\right) &amp; 226 &amp; 150 &amp; 15 \mathrm{sec} &amp; 0.0583 &amp; 0.0798 \\
\hline \text { LSTM1 }\left(T=5, \tau_{0}=3, \tau_{1}=5\right) &amp; 186 &amp; 100 &amp; 4 \mathrm{sec} &amp; 0.0753 &amp; 0.1028 \\
\text { LSTM1 }\left(T=20, \tau_{0}=3, \tau_{1}=5\right) &amp; 186 &amp; 200 &amp; 16 \mathrm{sec} &amp; 0.0626 &amp; 0.0968 \\
\hline \text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=3\right) &amp; 88 &amp; 200 &amp; 10 \mathrm{sec} &amp; 0.0694 &amp; 0.0987 \\
\text { LSTM1 }\left(T=10, \tau_{0}=3, \tau_{1}=10\right) &amp; 571 &amp; 100 &amp; 5 \mathrm{sec} &amp; 0.0626 &amp; 0.0883 \\
\hline
\end{array}
\]</span></p>
<ul>
<li>结论</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>分别令<span class="math inline">\(\tau_{0}=1,3,5\)</span>,需要更长的运行时间并提供更好的样本外结果；</p></li>
<li><p>分别令<span class="math inline">\(T=5,10,20\)</span>，结论同上；</p></li>
<li><p>分别令<span class="math inline">\(\tau_{1}=3,5,10\)</span>，导致更快的收敛，因为梯度下降算法有更多的自由度</p></li>
<li><p>最大的影响是通过设定一个更大的<span class="math inline">\(\tau_{0}\)</span>而产生的，因此后面RNN示例中设定<span class="math inline">\(\tau_{0}=5\)</span></p></li>
</ol>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="rnn.html#cb85-1"></a><span class="co"># load corresponding data</span></span>
<span id="cb85-2"><a href="rnn.html#cb85-2"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb85-3"><a href="rnn.html#cb85-3"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb85-4"><a href="rnn.html#cb85-4"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb85-5"><a href="rnn.html#cb85-5"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb85-6"><a href="rnn.html#cb85-6"></a><span class="co"># LSTMs and GRUs</span></span>
<span id="cb85-7"><a href="rnn.html#cb85-7"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;</span>)</span>
<span id="cb85-8"><a href="rnn.html#cb85-8"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb85-9"><a href="rnn.html#cb85-9"></a>tau0 &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb85-10"><a href="rnn.html#cb85-10"></a>tau1 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb85-11"><a href="rnn.html#cb85-11"></a>tau2 &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb85-12"><a href="rnn.html#cb85-12"></a><span class="kw">summary</span>(<span class="kw">LSTM1</span>(T0, tau0, tau1, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-13"><a href="rnn.html#cb85-13"></a><span class="kw">summary</span>(<span class="kw">LSTM2</span>(T0, tau0, tau1, tau2, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-14"><a href="rnn.html#cb85-14"></a><span class="kw">summary</span>(<span class="kw">LSTM_TD</span>(T0, tau0, tau1, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-15"><a href="rnn.html#cb85-15"></a><span class="kw">summary</span>(<span class="kw">GRU1</span>(T0, tau0, tau1, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-16"><a href="rnn.html#cb85-16"></a><span class="kw">summary</span>(<span class="kw">GRU2</span>(T0, tau0, tau1, tau2, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-17"><a href="rnn.html#cb85-17"></a><span class="kw">summary</span>(<span class="kw">FNN</span>(T0, tau0, tau1, tau2, <span class="dv">0</span>, <span class="st">&quot;nadam&quot;</span>))</span>
<span id="cb85-18"><a href="rnn.html#cb85-18"></a><span class="co"># Bringing the data in the right structure for a toy example</span></span>
<span id="cb85-19"><a href="rnn.html#cb85-19"></a>gender &lt;-<span class="st"> &quot;Female&quot;</span></span>
<span id="cb85-20"><a href="rnn.html#cb85-20"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb85-21"><a href="rnn.html#cb85-21"></a>mort_rates &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>(all_mort<span class="op">$</span>Gender<span class="op">==</span>gender), <span class="kw">c</span>(<span class="st">&quot;Year&quot;</span>, <span class="st">&quot;Age&quot;</span>, <span class="st">&quot;logmx&quot;</span>)] </span>
<span id="cb85-22"><a href="rnn.html#cb85-22"></a>mort_rates &lt;-<span class="st"> </span><span class="kw">dcast</span>(mort_rates, Year <span class="op">~</span><span class="st"> </span>Age, <span class="dt">value.var=</span><span class="st">&quot;logmx&quot;</span>)</span>
<span id="cb85-23"><a href="rnn.html#cb85-23"></a><span class="kw">dim</span>(mort_rates)</span>
<span id="cb85-24"><a href="rnn.html#cb85-24"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span>     <span class="co"># lookback period</span></span>
<span id="cb85-25"><a href="rnn.html#cb85-25"></a>tau0 &lt;-<span class="st"> </span><span class="dv">3</span>    <span class="co"># dimension of x_t (should be odd for our application)</span></span>
<span id="cb85-26"><a href="rnn.html#cb85-26"></a>delta0 &lt;-<span class="st"> </span>(tau0<span class="dv">-1</span>)<span class="op">/</span><span class="dv">2</span></span>
<span id="cb85-27"><a href="rnn.html#cb85-27"></a>toy_rates &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(mort_rates[<span class="kw">which</span>(mort_rates<span class="op">$</span>Year <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>((ObsYear<span class="op">-</span>T0)<span class="op">:</span>(ObsYear<span class="op">+</span><span class="dv">1</span>))),])</span>
<span id="cb85-28"><a href="rnn.html#cb85-28"></a><span class="kw">dim</span>(toy_rates)</span>
<span id="cb85-29"><a href="rnn.html#cb85-29"></a>xt &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(<span class="dv">2</span>,<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0, T0, tau0))</span>
<span id="cb85-30"><a href="rnn.html#cb85-30"></a>YT &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(<span class="dv">2</span>,<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0))</span>
<span id="cb85-31"><a href="rnn.html#cb85-31"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>){<span class="cf">for</span> (a0 <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0)){ </span>
<span id="cb85-32"><a href="rnn.html#cb85-32"></a>    xt[i,a0,,] &lt;-<span class="st"> </span>toy_rates[<span class="kw">c</span>(i<span class="op">:</span>(T0<span class="op">+</span>i<span class="dv">-1</span>)),<span class="kw">c</span>((a0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(a0<span class="op">+</span>tau0))]</span>
<span id="cb85-33"><a href="rnn.html#cb85-33"></a>    YT[i,a0] &lt;-<span class="st"> </span>toy_rates[T0<span class="op">+</span>i,a0<span class="op">+</span><span class="dv">1</span><span class="op">+</span>delta0]</span>
<span id="cb85-34"><a href="rnn.html#cb85-34"></a>}}</span>
<span id="cb85-35"><a href="rnn.html#cb85-35"></a><span class="kw">dim</span>(xt)</span>
<span id="cb85-36"><a href="rnn.html#cb85-36"></a><span class="kw">dim</span>(YT)</span>
<span id="cb85-37"><a href="rnn.html#cb85-37"></a><span class="kw">plot</span>(<span class="dt">x=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;white&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;calendar years&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;raw log-mortality rates&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;data toy example&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">xlim=</span><span class="kw">range</span>(toy_rates[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(toy_rates[,<span class="op">-</span><span class="dv">1</span>]), <span class="dt">type=</span><span class="st">&#39;l&#39;</span>)</span>
<span id="cb85-38"><a href="rnn.html#cb85-38"></a><span class="cf">for</span> (a0 <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(toy_rates)){</span>
<span id="cb85-39"><a href="rnn.html#cb85-39"></a>  <span class="cf">if</span> (a0 <span class="op">%in%</span><span class="st"> </span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>)<span class="op">*</span><span class="dv">3</span>)){</span>
<span id="cb85-40"><a href="rnn.html#cb85-40"></a>    <span class="kw">lines</span>(<span class="dt">x=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[<span class="dv">1</span><span class="op">:</span>T0,a0])    </span>
<span id="cb85-41"><a href="rnn.html#cb85-41"></a>    <span class="kw">points</span>(<span class="dt">x=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),a0], <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb85-42"><a href="rnn.html#cb85-42"></a>    <span class="kw">lines</span>(<span class="dt">x=</span>toy_rates[(T0)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">1</span>),<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[(T0)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">1</span>),a0], <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb85-43"><a href="rnn.html#cb85-43"></a>    <span class="kw">lines</span>(<span class="dt">x=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),<span class="dv">1</span>], <span class="dt">y=</span>toy_rates[(T0<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(T0<span class="op">+</span><span class="dv">2</span>),a0], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb85-44"><a href="rnn.html#cb85-44"></a>    }}</span>
<span id="cb85-45"><a href="rnn.html#cb85-45"></a><span class="co"># LSTMs and GRUs</span></span>
<span id="cb85-46"><a href="rnn.html#cb85-46"></a>x.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(xt[<span class="dv">1</span>,,,]<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">/</span>(<span class="kw">max</span>(xt)<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">-</span><span class="dv">1</span>, <span class="kw">c</span>(<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0, T0, tau0))</span>
<span id="cb85-47"><a href="rnn.html#cb85-47"></a>x.vali  &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(xt[<span class="dv">2</span>,,,]<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">/</span>(<span class="kw">max</span>(xt)<span class="op">-</span><span class="kw">min</span>(xt))<span class="op">-</span><span class="dv">1</span>, <span class="kw">c</span>(<span class="kw">ncol</span>(toy_rates)<span class="op">-</span>tau0, T0, tau0))</span>
<span id="cb85-48"><a href="rnn.html#cb85-48"></a>y.train &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>YT[<span class="dv">1</span>,]</span>
<span id="cb85-49"><a href="rnn.html#cb85-49"></a>(y0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y.train))</span>
<span id="cb85-50"><a href="rnn.html#cb85-50"></a>y.vali  &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>YT[<span class="dv">2</span>,]</span>
<span id="cb85-51"><a href="rnn.html#cb85-51"></a><span class="kw">dim</span>(x.train)</span>
<span id="cb85-52"><a href="rnn.html#cb85-52"></a><span class="kw">length</span>(y.train);<span class="kw">length</span>(y.vali)</span>
<span id="cb85-53"><a href="rnn.html#cb85-53"></a><span class="co"># x.age.train&lt;-as.matrix(0:0)</span></span>
<span id="cb85-54"><a href="rnn.html#cb85-54"></a><span class="co"># x.training&lt;-list(x.train,x.age.train)</span></span>
<span id="cb85-55"><a href="rnn.html#cb85-55"></a><span class="co"># x.age.valid&lt;-as.matrix(0:0)</span></span>
<span id="cb85-56"><a href="rnn.html#cb85-56"></a><span class="co"># x.validation&lt;-list(x.vali,x.age.valid)</span></span>
<span id="cb85-57"><a href="rnn.html#cb85-57"></a><span class="co">### examples</span></span>
<span id="cb85-58"><a href="rnn.html#cb85-58"></a>tau1 &lt;-<span class="st"> </span><span class="dv">5</span>    <span class="co"># dimension of the outputs z_t^(1) first RNN layer</span></span>
<span id="cb85-59"><a href="rnn.html#cb85-59"></a>tau2 &lt;-<span class="st"> </span><span class="dv">4</span>    <span class="co"># dimension of the outputs z_t^(2) second RNN layer</span></span>
<span id="cb85-60"><a href="rnn.html#cb85-60"></a>CBs &lt;-<span class="st"> </span><span class="kw">callback_model_checkpoint</span>(<span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model&quot;</span>, <span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">verbose =</span> <span class="dv">0</span>,  <span class="dt">save_best_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_weights_only =</span> <span class="ot">TRUE</span>,<span class="dt">save_freq =</span> <span class="ot">NULL</span>)</span>
<span id="cb85-61"><a href="rnn.html#cb85-61"></a>model &lt;-<span class="st"> </span><span class="kw">LSTM2</span>(T0, tau0, tau1, tau2, y0, <span class="st">&quot;nadam&quot;</span>)     </span>
<span id="cb85-62"><a href="rnn.html#cb85-62"></a><span class="kw">summary</span>(model)</span>
<span id="cb85-63"><a href="rnn.html#cb85-63"></a><span class="co"># takes 40 seconds on my laptop</span></span>
<span id="cb85-64"><a href="rnn.html#cb85-64"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb85-65"><a href="rnn.html#cb85-65"></a>  fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="dt">x=</span>x.train, <span class="dt">y=</span>y.train, <span class="dt">validation_data=</span><span class="kw">list</span>(x.vali, y.vali), <span class="dt">batch_size=</span><span class="dv">10</span>, <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">verbose=</span><span class="dv">1</span>, <span class="dt">callbacks=</span>CBs)</span>
<span id="cb85-66"><a href="rnn.html#cb85-66"></a> <span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb85-67"><a href="rnn.html#cb85-67"></a><span class="kw">plot</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>val_loss,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>), <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;early stopping rule&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>),<span class="dt">xlab=</span><span class="st">&quot;epochs&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MSE loss&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb85-68"><a href="rnn.html#cb85-68"></a><span class="kw">lines</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb85-69"><a href="rnn.html#cb85-69"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.1</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</span>
<span id="cb85-70"><a href="rnn.html#cb85-70"></a><span class="kw">legend</span>(<span class="dt">x=</span><span class="st">&quot;bottomleft&quot;</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;in-sample loss&quot;</span>, <span class="st">&quot;out-of-sample loss&quot;</span>))</span>
<span id="cb85-71"><a href="rnn.html#cb85-71"></a><span class="kw">load_model_weights_hdf5</span>(model, <span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model&quot;</span>)</span>
<span id="cb85-72"><a href="rnn.html#cb85-72"></a>Yhat.train1 &lt;-<span class="st"> </span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.train))</span>
<span id="cb85-73"><a href="rnn.html#cb85-73"></a>Yhat.vali1 &lt;-<span class="st"> </span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.vali))</span>
<span id="cb85-74"><a href="rnn.html#cb85-74"></a><span class="kw">c</span>(<span class="kw">round</span>(<span class="kw">mean</span>((Yhat.train1<span class="op">-</span>y.train)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>), <span class="kw">round</span>(<span class="kw">mean</span>((Yhat.vali1<span class="op">-</span>y.vali)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>))</span></code></pre></div>
</div>
<div id="rnn-1" class="section level3">
<h3><span class="header-section-number">6.5.5</span> RNN</h3>
<ol style="list-style-type: decimal">
<li>数据预处理</li>
</ol>
<ul>
<li><p>观察值：1950-1999年数据；预测值：2000-2016年对数死亡率</p></li>
<li><p>分别对男性和女性建立模型，先选定一个性别</p></li>
<li><p>参数设置：回顾周期<span class="math inline">\(T=10\)</span>；<span class="math inline">\(\tau_{0}=5\)</span></p></li>
<li><p>定义解释变量（需扩充年龄界限——复制边缘特征值）：</p>
<p>对于<span class="math inline">\(0 \leq x \leq 99\)</span>，<span class="math inline">\(1950 \leq t \leq 1999\)</span>，有</p>
<p><strong>解释变量</strong><span class="math inline">\(\boldsymbol{x}_{t, x}=\left(\log \left(M_{t,(x-2) \vee 0}\right), \log \left(M_{t,(x-1) \vee 0}\right), \log \left(M_{t, x}\right), \log \left(M_{t,(x+1) \wedge 99}\right), \log \left(M_{t,(x+2) \wedge 99}\right)\right)^{\top} \in \mathbb{R}^{5}\)</span></p>
<p>该式中：<span class="math inline">\(x_{0}\vee x_{1}=\text {max}\{x_{0},x_{1}\}\)</span>;<span class="math inline">\(x_{0}\wedge x_{1}=\text {min}\{x_{0},x_{1}\}\)</span></p></li>
<li><p>定义训练数据<span class="math inline">\(\mathcal{T}\)</span>和验证数据<span class="math inline">\(\mathcal{V}\)</span>：</p>
<p><strong>训练数据</strong><span class="math inline">\(\mathcal{T}=\{(\boldsymbol{x}_{t-T,x}, \ldots,\boldsymbol{x}_{t-1,x},\boldsymbol{Y}_{t, x});0 \leq x \leq 99\ , 1950+T \leq t \leq 1999\}\)</span></p>
<p>其中，<span class="math inline">\(\boldsymbol{Y}_{t, x}=\text {log}(M_{t,x})\)</span></p>
<p><strong>验证数据</strong>:</p>
<p><span class="math inline">\(s&gt;1999\)</span>的特征值要用相应的预测值替代</p></li>
</ul>
<p><span class="math display">\[
\widehat{\boldsymbol{x}}_{s, x}=\left(\log \left(\widehat{M}_{s,(x-2) \vee 0}\right), \log \left(\widehat{M}_{s,(x-1) \vee 0}\right), \log \left(\widehat{M}_{s, x}\right), \log \left(\widehat{M}_{s,(x+1) \wedge 99}\right), \log \left(\widehat{M}_{s,(x+2) \wedge 99}\right)\right)^{\top} \in \mathbb{R}^{5}
\]</span></p>
<p>因此验证数据<span class="math inline">\(\mathcal{V}=\{(\boldsymbol{x}_{t-T,x}, \ldots,\boldsymbol{x}_{1999,x},\widehat{\boldsymbol{x}}_{2000,x}, \ldots,\widehat{\boldsymbol{x}}_{t-1,x},\boldsymbol{Y}_{t, x});0 \leq x \leq 99\ , 2000 \leq t \leq 2016\}\)</span></p>
<ul>
<li><p>基于<strong>训练数据</strong>所有特征值的最大最小值对训练数据和验证数据应用<code>MinMaxScaler</code></p></li>
<li><p>切换响应变量符号</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>建立单个性别的RNN</li>
</ol>
<ul>
<li><p>将训练数据<span class="math inline">\(\mathcal T\)</span>随机划分学习集<span class="math inline">\(\mathcal T_{0}\)</span>(包含80%数据)以及测试集<span class="math inline">\(\mathcal T_{1}\)</span>(包含20%数据)<span class="math inline">\(\mathcal T_{0}\)</span>用于追踪样本内过拟合；</p></li>
<li><p>建立具有三个隐藏层的LSTM3和GRU3；</p></li>
<li><p>超参数设置：<span class="math inline">\(T=10\)</span>；<span class="math inline">\(\tau_{0}=5\)</span>；<span class="math inline">\(\tau_{1}=20\)</span>；<span class="math inline">\(\tau_{2}=15\)</span>；<span class="math inline">\(\tau_{3}=10\)</span>；</p></li>
<li><p>下图显示了分别对男性和女性建立两个模型的收敛行为</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:loss2"></span>
<img src="plots/6/loss2.png" alt="模型的样本内外损失" width="60%"  />
<p class="caption">
Figure 6.5: 模型的样本内外损失
</p>
</div>
<p>该图显示：GRU结构会导致更快的收敛，但后续会证实GRU结构不稳定，因此LSTM会更受欢迎</p>
<ul>
<li>下表给出三个模型(LSTM3/GRU3/LC)分性别的样本内外损失</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|cc|cc|cc|}
\hline &amp;  {\text { in-sample }} &amp; {\text { in-sample }}  &amp; {\text { out-of-sample }} &amp; {\text { out-of-sample }}&amp;  {\text { run times }}&amp;  {\text { run times }} \\
&amp; \text { female } &amp; \text { male } &amp; \text { female } &amp; \text { male } &amp; \text { female } &amp; \text { male } \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 2.5222 &amp; 6.9458 &amp; 0.3566 &amp; 1.3507 &amp; 225 \mathrm{s} &amp; 203 \mathrm{s} \\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 2.8370 &amp; 7.0907 &amp; 0.4788 &amp; 1.2435 &amp; 185 \mathrm{s} &amp; 198 \mathrm{s} \\
\hline \hline \text { LC model with SVD } &amp; 3.7573 &amp; 8.8110 &amp; 0.6045 &amp; 1.8152 &amp; - &amp; - \\
\hline
\end{array}
\]</span></p>
<p>能够看到，所有被选择RNN模型都优于LC模型的预测</p>
<ol start="3" style="list-style-type: decimal">
<li>探索RNN的预测中隐含的漂移项</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>中心化RNN预测的对数死亡率</li>
</ol>
<p><span class="math display">\[
\log \left(\widehat{M}_{t, x}^{\circ}\right)=\log(\widehat M_{t,x})-\widehat a_{x}
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>利用下式求得<span class="math inline">\(2000 \leq t \leq 2016\)</span>对应的<span class="math inline">\(k_{t}\)</span></li>
</ol>
<p><span class="math display">\[
\underset{k_{t}}{\arg \min } \sum_{x}\left(\log \left(\widehat{M}_{t, x}^{\circ}\right)-\widehat{b}_{x} k_{t}\right)^{2}
\]</span></p>
<p>式中，<span class="math inline">\((\widehat{b}_{x})_{x}\)</span>是从LC模型估计得到的。</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>估计结果如下图所示</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:kt2"></span>
<img src="plots/6/kt2.png" alt="三种模型下kt的估计与预测值" width="60%"  />
<p class="caption">
Figure 6.6: 三种模型下kt的估计与预测值
</p>
</div>
<p>该图显示：对于女性，LSTM3的预测与LC的预测基本一致；而对于男性，LSTM3的预测的斜率略大于LC的预测并收敛与LC的预测；但是GRU3的结果并不令人信服，可能是因为从LC模型中得到的不随时间变化的参数bx的估计与GRU3模型产生的预测不符。</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="rnn.html#cb86-1"></a><span class="co"># load corresponding data</span></span>
<span id="cb86-2"><a href="rnn.html#cb86-2"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb86-3"><a href="rnn.html#cb86-3"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb86-4"><a href="rnn.html#cb86-4"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb86-5"><a href="rnn.html#cb86-5"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb86-6"><a href="rnn.html#cb86-6"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;</span>)</span>
<span id="cb86-7"><a href="rnn.html#cb86-7"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R&quot;</span>)</span>
<span id="cb86-8"><a href="rnn.html#cb86-8"></a><span class="co"># choice of parameters</span></span>
<span id="cb86-9"><a href="rnn.html#cb86-9"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb86-10"><a href="rnn.html#cb86-10"></a>tau0 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb86-11"><a href="rnn.html#cb86-11"></a>gender &lt;-<span class="st"> &quot;Female&quot;</span></span>
<span id="cb86-12"><a href="rnn.html#cb86-12"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">1999</span></span>
<span id="cb86-13"><a href="rnn.html#cb86-13"></a><span class="co"># training data pre-processing </span></span>
<span id="cb86-14"><a href="rnn.html#cb86-14"></a>data1 &lt;-<span class="st"> </span><span class="kw">data.preprocessing.RNNs</span>(all_mort, gender, T0, tau0, ObsYear)</span>
<span id="cb86-15"><a href="rnn.html#cb86-15"></a><span class="kw">dim</span>(data1[[<span class="dv">1</span>]])</span>
<span id="cb86-16"><a href="rnn.html#cb86-16"></a><span class="kw">dim</span>(data1[[<span class="dv">2</span>]])</span>
<span id="cb86-17"><a href="rnn.html#cb86-17"></a><span class="co"># validation data pre-processing</span></span>
<span id="cb86-18"><a href="rnn.html#cb86-18"></a>all_mort2 &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>((all_mort<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>(ObsYear<span class="dv">-10</span>))<span class="op">&amp;</span>(Gender<span class="op">==</span>gender)),]</span>
<span id="cb86-19"><a href="rnn.html#cb86-19"></a>all_mortV &lt;-<span class="st"> </span>all_mort2</span>
<span id="cb86-20"><a href="rnn.html#cb86-20"></a>vali.Y &lt;-<span class="st"> </span>all_mortV[<span class="kw">which</span>(all_mortV<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb86-21"><a href="rnn.html#cb86-21"></a> </span>
<span id="cb86-22"><a href="rnn.html#cb86-22"></a><span class="co"># MinMaxScaler data pre-processing</span></span>
<span id="cb86-23"><a href="rnn.html#cb86-23"></a>x.min &lt;-<span class="st"> </span><span class="kw">min</span>(data1[[<span class="dv">1</span>]])</span>
<span id="cb86-24"><a href="rnn.html#cb86-24"></a>x.max &lt;-<span class="st"> </span><span class="kw">max</span>(data1[[<span class="dv">1</span>]])</span>
<span id="cb86-25"><a href="rnn.html#cb86-25"></a>x.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(data1[[<span class="dv">1</span>]]<span class="op">-</span>x.min)<span class="op">/</span>(x.min<span class="op">-</span>x.max)<span class="op">-</span><span class="dv">1</span>, <span class="kw">dim</span>(data1[[<span class="dv">1</span>]]))</span>
<span id="cb86-26"><a href="rnn.html#cb86-26"></a>y.train &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>data1[[<span class="dv">2</span>]]</span>
<span id="cb86-27"><a href="rnn.html#cb86-27"></a>y0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y.train)</span>
<span id="cb86-28"><a href="rnn.html#cb86-28"></a><span class="co"># LSTM architectures</span></span>
<span id="cb86-29"><a href="rnn.html#cb86-29"></a><span class="co"># network architecture deep 3 network</span></span>
<span id="cb86-30"><a href="rnn.html#cb86-30"></a>tau1 &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb86-31"><a href="rnn.html#cb86-31"></a>tau2 &lt;-<span class="st"> </span><span class="dv">15</span></span>
<span id="cb86-32"><a href="rnn.html#cb86-32"></a>tau3 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb86-33"><a href="rnn.html#cb86-33"></a>optimizer &lt;-<span class="st"> &#39;adam&#39;</span></span>
<span id="cb86-34"><a href="rnn.html#cb86-34"></a><span class="co"># choose either LSTM or GRU network</span></span>
<span id="cb86-35"><a href="rnn.html#cb86-35"></a>RNN.type &lt;-<span class="st"> &quot;LSTM&quot;</span></span>
<span id="cb86-36"><a href="rnn.html#cb86-36"></a><span class="co">#RNN.type &lt;- &quot;GRU&quot;</span></span>
<span id="cb86-37"><a href="rnn.html#cb86-37"></a>{<span class="cf">if</span> (RNN.type<span class="op">==</span><span class="st">&quot;LSTM&quot;</span>){model &lt;-<span class="st"> </span><span class="kw">LSTM3</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}<span class="cf">else</span>{model &lt;-<span class="st"> </span><span class="kw">GRU3</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}</span>
<span id="cb86-38"><a href="rnn.html#cb86-38"></a> name.model &lt;-<span class="st"> </span><span class="kw">paste</span>(RNN.type,<span class="st">&quot;3_&quot;</span>, tau0, <span class="st">&quot;_&quot;</span>, tau1, <span class="st">&quot;_&quot;</span>, tau2, <span class="st">&quot;_&quot;</span>, tau3, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb86-39"><a href="rnn.html#cb86-39"></a> file.name &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_&quot;</span>, name.model,<span class="st">&quot;_&quot;</span>, gender, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb86-40"><a href="rnn.html#cb86-40"></a> <span class="kw">summary</span>(model)}</span>
<span id="cb86-41"><a href="rnn.html#cb86-41"></a><span class="co"># define callback</span></span>
<span id="cb86-42"><a href="rnn.html#cb86-42"></a>CBs &lt;-<span class="st"> </span><span class="kw">callback_model_checkpoint</span>(file.name, <span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">verbose =</span> <span class="dv">0</span>,  <span class="dt">save_best_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_weights_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_freq =</span> <span class="ot">NULL</span>)</span>
<span id="cb86-43"><a href="rnn.html#cb86-43"></a><span class="co"># gradient descent fitting: takes roughly 200 seconds on my laptop</span></span>
<span id="cb86-44"><a href="rnn.html#cb86-44"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb86-45"><a href="rnn.html#cb86-45"></a>  fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="dt">x=</span>x.train, <span class="dt">y=</span>y.train, <span class="dt">validation_split=</span><span class="fl">0.2</span>,</span>
<span id="cb86-46"><a href="rnn.html#cb86-46"></a>                                        <span class="dt">batch_size=</span><span class="dv">100</span>, <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">verbose=</span><span class="dv">1</span>, <span class="dt">callbacks=</span>CBs)                                        </span>
<span id="cb86-47"><a href="rnn.html#cb86-47"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb86-48"><a href="rnn.html#cb86-48"></a><span class="co"># plot loss figures</span></span>
<span id="cb86-49"><a href="rnn.html#cb86-49"></a><span class="kw">plot.losses</span>(name.model, gender, fit[[<span class="dv">2</span>]]<span class="op">$</span>val_loss, fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)</span>
<span id="cb86-50"><a href="rnn.html#cb86-50"></a><span class="co"># calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)</span></span>
<span id="cb86-51"><a href="rnn.html#cb86-51"></a><span class="kw">load_model_weights_hdf5</span>(model, file.name)</span>
<span id="cb86-52"><a href="rnn.html#cb86-52"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((<span class="kw">exp</span>(<span class="op">-</span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.train)))<span class="op">-</span><span class="kw">exp</span>(<span class="op">-</span>y.train))<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span>
<span id="cb86-53"><a href="rnn.html#cb86-53"></a><span class="co"># calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)</span></span>
<span id="cb86-54"><a href="rnn.html#cb86-54"></a>pred.result &lt;-<span class="st"> </span><span class="kw">recursive.prediction</span>(ObsYear, all_mort2, gender, T0, tau0, x.min, x.max, model)</span>
<span id="cb86-55"><a href="rnn.html#cb86-55"></a>vali &lt;-<span class="st"> </span>pred.result[[<span class="dv">1</span>]][<span class="kw">which</span>(all_mort2<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb86-56"><a href="rnn.html#cb86-56"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali.Y<span class="op">$</span>mx)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span></code></pre></div>
</div>
<div id="引入性别协变量" class="section level3">
<h3><span class="header-section-number">6.5.6</span> 引入性别协变量</h3>
<ol style="list-style-type: decimal">
<li>数据预处理</li>
</ol>
<ul>
<li><p>添加性别指示变量：0表示女性；1表示男性</p></li>
<li><p>在训练数据时交替使用性别</p></li>
<li><p>应用MinMaxScaler时的最大最小值是同时考虑两种性别所有训练数据的情况下得到的。</p></li>
<li><p>模型结构及超参数设置与单个性别RNN相同</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>建立模型</li>
</ol>
<ul>
<li>基于使得测试损失最小的LSTM3和GRU模型，预测1999年之后的死亡率，并在特征变量中加入性别指标，下表列出了相应的损失</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|c|cc|c|}
\hline &amp;  {\text { in-sample }} &amp; {\text { out-of-sample }} &amp; {\text { out-of-sample }}&amp;  {\text { run times }}\\
&amp; \text {both genders} &amp; \text { female } &amp; \text { male } &amp; \text {both genders} \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.7643 &amp; 0.3402 &amp; 1.1346 &amp; 404 \mathrm{s}\\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.6311 &amp; 0.4646 &amp; 1.2571 &amp;  379 \mathrm{s} \\
\hline \hline \text { LC model with SVD } &amp; 6.2841 &amp; 0.6045 &amp;1.8152 &amp;  - \\
\hline
\end{array}
\]</span></p>
<p>与LC模型相比，得到了一个有很大改进的模型，至少对未来16年的预测是这样的；此外，引入性别协变量的LSTM模型也优于单个性别的模型。</p>
<ol start="3" style="list-style-type: decimal">
<li>隐含的漂移项</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:kt3"></span>
<img src="plots/6/kt3.png" alt="引入性别协变量建模的kt的估计与预测值" width="60%"  />
<p class="caption">
Figure 6.7: 引入性别协变量建模的kt的估计与预测值
</p>
</div>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="rnn.html#cb87-1"></a><span class="co"># load corresponding data</span></span>
<span id="cb87-2"><a href="rnn.html#cb87-2"></a>path.data &lt;-<span class="st"> &quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CHE_mort.csv&quot;</span>           <span class="co"># path and name of data file</span></span>
<span id="cb87-3"><a href="rnn.html#cb87-3"></a>region &lt;-<span class="st"> &quot;CHE&quot;</span>                    <span class="co"># country to be loaded (code is for one selected country)</span></span>
<span id="cb87-4"><a href="rnn.html#cb87-4"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_a package - load data.R&quot;</span>)</span>
<span id="cb87-5"><a href="rnn.html#cb87-5"></a><span class="kw">str</span>(all_mort)</span>
<span id="cb87-6"><a href="rnn.html#cb87-6"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_b package - network definitions.R&quot;</span>)</span>
<span id="cb87-7"><a href="rnn.html#cb87-7"></a><span class="kw">source</span>(<span class="dt">file=</span><span class="st">&quot;6 - Lee and Carter go Machine Learning Recurrent Neural Networks/00_c package - data preparation RNNs.R&quot;</span>)</span>
<span id="cb87-8"><a href="rnn.html#cb87-8"></a><span class="co"># choice of parameters</span></span>
<span id="cb87-9"><a href="rnn.html#cb87-9"></a>T0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb87-10"><a href="rnn.html#cb87-10"></a>tau0 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb87-11"><a href="rnn.html#cb87-11"></a>ObsYear &lt;-<span class="st"> </span><span class="dv">1999</span></span>
<span id="cb87-12"><a href="rnn.html#cb87-12"></a><span class="co"># training data pre-processing </span></span>
<span id="cb87-13"><a href="rnn.html#cb87-13"></a>data1 &lt;-<span class="st"> </span><span class="kw">data.preprocessing.RNNs</span>(all_mort, <span class="st">&quot;Female&quot;</span>, T0, tau0, ObsYear)</span>
<span id="cb87-14"><a href="rnn.html#cb87-14"></a>data2 &lt;-<span class="st"> </span><span class="kw">data.preprocessing.RNNs</span>(all_mort, <span class="st">&quot;Male&quot;</span>, T0, tau0, ObsYear)</span>
<span id="cb87-15"><a href="rnn.html#cb87-15"></a>xx &lt;-<span class="st"> </span><span class="kw">dim</span>(data1[[<span class="dv">1</span>]])[<span class="dv">1</span>]</span>
<span id="cb87-16"><a href="rnn.html#cb87-16"></a>x.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">*</span>xx, <span class="kw">dim</span>(data1[[<span class="dv">1</span>]])[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]))</span>
<span id="cb87-17"><a href="rnn.html#cb87-17"></a>y.train &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">*</span>xx))</span>
<span id="cb87-18"><a href="rnn.html#cb87-18"></a>gender.indicator &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), xx)</span>
<span id="cb87-19"><a href="rnn.html#cb87-19"></a><span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>xx){</span>
<span id="cb87-20"><a href="rnn.html#cb87-20"></a>   x.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span>,,] &lt;-<span class="st"> </span>data1[[<span class="dv">1</span>]][l,,]</span>
<span id="cb87-21"><a href="rnn.html#cb87-21"></a>   x.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">2</span>,,] &lt;-<span class="st"> </span>data2[[<span class="dv">1</span>]][l,,]</span>
<span id="cb87-22"><a href="rnn.html#cb87-22"></a>   y.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="op">-</span>data1[[<span class="dv">2</span>]][l]</span>
<span id="cb87-23"><a href="rnn.html#cb87-23"></a>   y.train[(l<span class="dv">-1</span>)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">2</span>] &lt;-<span class="st"> </span><span class="op">-</span>data2[[<span class="dv">2</span>]][l]</span>
<span id="cb87-24"><a href="rnn.html#cb87-24"></a>          }</span>
<span id="cb87-25"><a href="rnn.html#cb87-25"></a><span class="co"># MinMaxScaler data pre-processing</span></span>
<span id="cb87-26"><a href="rnn.html#cb87-26"></a>x.min &lt;-<span class="st"> </span><span class="kw">min</span>(x.train)</span>
<span id="cb87-27"><a href="rnn.html#cb87-27"></a>x.max &lt;-<span class="st"> </span><span class="kw">max</span>(x.train)</span>
<span id="cb87-28"><a href="rnn.html#cb87-28"></a>x.train &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">array</span>(<span class="dv">2</span><span class="op">*</span>(x.train<span class="op">-</span>x.min)<span class="op">/</span>(x.min<span class="op">-</span>x.max)<span class="op">-</span><span class="dv">1</span>, <span class="kw">dim</span>(x.train)), gender.indicator)</span>
<span id="cb87-29"><a href="rnn.html#cb87-29"></a>y0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y.train)</span>
<span id="cb87-30"><a href="rnn.html#cb87-30"></a><span class="co"># validation data pre-processing</span></span>
<span id="cb87-31"><a href="rnn.html#cb87-31"></a>all_mort2.Female &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>((all_mort<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>(ObsYear<span class="dv">-10</span>))<span class="op">&amp;</span>(Gender<span class="op">==</span><span class="st">&quot;Female&quot;</span>)),]</span>
<span id="cb87-32"><a href="rnn.html#cb87-32"></a>all_mortV.Female &lt;-<span class="st"> </span>all_mort2.Female</span>
<span id="cb87-33"><a href="rnn.html#cb87-33"></a>vali.Y.Female &lt;-<span class="st"> </span>all_mortV.Female[<span class="kw">which</span>(all_mortV.Female<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-34"><a href="rnn.html#cb87-34"></a>all_mort2.Male &lt;-<span class="st"> </span>all_mort[<span class="kw">which</span>((all_mort<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>(ObsYear<span class="dv">-10</span>))<span class="op">&amp;</span>(Gender<span class="op">==</span><span class="st">&quot;Male&quot;</span>)),]</span>
<span id="cb87-35"><a href="rnn.html#cb87-35"></a>all_mortV.Male &lt;-<span class="st"> </span>all_mort2.Male</span>
<span id="cb87-36"><a href="rnn.html#cb87-36"></a>vali.Y.Male &lt;-<span class="st"> </span>all_mortV.Male[<span class="kw">which</span>(all_mortV.Male<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-37"><a href="rnn.html#cb87-37"></a><span class="co"># LSTM architectures</span></span>
<span id="cb87-38"><a href="rnn.html#cb87-38"></a><span class="co"># network architecture deep 3 network</span></span>
<span id="cb87-39"><a href="rnn.html#cb87-39"></a>tau1 &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb87-40"><a href="rnn.html#cb87-40"></a>tau2 &lt;-<span class="st"> </span><span class="dv">15</span></span>
<span id="cb87-41"><a href="rnn.html#cb87-41"></a>tau3 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb87-42"><a href="rnn.html#cb87-42"></a>optimizer &lt;-<span class="st"> &#39;adam&#39;</span></span>
<span id="cb87-43"><a href="rnn.html#cb87-43"></a><span class="co"># choose either LSTM or GRU network</span></span>
<span id="cb87-44"><a href="rnn.html#cb87-44"></a>RNN.type &lt;-<span class="st"> &quot;LSTM&quot;</span></span>
<span id="cb87-45"><a href="rnn.html#cb87-45"></a><span class="co">#RNN.type &lt;- &quot;GRU&quot;</span></span>
<span id="cb87-46"><a href="rnn.html#cb87-46"></a>{<span class="cf">if</span> (RNN.type<span class="op">==</span><span class="st">&quot;LSTM&quot;</span>){model &lt;-<span class="st"> </span><span class="kw">LSTM3.Gender</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}<span class="cf">else</span>{model &lt;-<span class="st"> </span><span class="kw">GRU3.Gender</span>(T0, tau0, tau1, tau2, tau3, y0, optimizer)}</span>
<span id="cb87-47"><a href="rnn.html#cb87-47"></a> name.model &lt;-<span class="st"> </span><span class="kw">paste</span>(RNN.type,<span class="st">&quot;3_&quot;</span>, tau0, <span class="st">&quot;_&quot;</span>, tau1, <span class="st">&quot;_&quot;</span>, tau2, <span class="st">&quot;_&quot;</span>, tau3, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb87-48"><a href="rnn.html#cb87-48"></a> <span class="co">#file.name &lt;- paste(&quot;./Model_Full_Param/best_model_&quot;, name.model, sep=&quot;&quot;)</span></span>
<span id="cb87-49"><a href="rnn.html#cb87-49"></a> file.name &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;./6 - Lee and Carter go Machine Learning Recurrent Neural Networks/CallBack/best_model_&quot;</span>, name.model, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb87-50"><a href="rnn.html#cb87-50"></a> <span class="kw">summary</span>(model)}</span>
<span id="cb87-51"><a href="rnn.html#cb87-51"></a><span class="co"># define callback</span></span>
<span id="cb87-52"><a href="rnn.html#cb87-52"></a>CBs &lt;-<span class="st"> </span><span class="kw">callback_model_checkpoint</span>(file.name, <span class="dt">monitor =</span> <span class="st">&quot;val_loss&quot;</span>, <span class="dt">verbose =</span> <span class="dv">0</span>,  <span class="dt">save_best_only =</span> <span class="ot">TRUE</span>, <span class="dt">save_weights_only =</span> <span class="ot">TRUE</span>,<span class="dt">save_freq =</span> <span class="ot">NULL</span>)</span>
<span id="cb87-53"><a href="rnn.html#cb87-53"></a><span class="co"># gradient descent fitting: takes roughly 400 seconds on my laptop</span></span>
<span id="cb87-54"><a href="rnn.html#cb87-54"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb87-55"><a href="rnn.html#cb87-55"></a>  fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="dt">x=</span>x.train, <span class="dt">y=</span>y.train, <span class="dt">validation_split=</span><span class="fl">0.2</span>,</span>
<span id="cb87-56"><a href="rnn.html#cb87-56"></a>                                        <span class="dt">batch_size=</span><span class="dv">100</span>, <span class="dt">epochs=</span><span class="dv">500</span>, <span class="dt">verbose=</span><span class="dv">1</span>, <span class="dt">callbacks=</span>CBs)                                        </span>
<span id="cb87-57"><a href="rnn.html#cb87-57"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb87-58"><a href="rnn.html#cb87-58"></a><span class="co"># plot loss figures</span></span>
<span id="cb87-59"><a href="rnn.html#cb87-59"></a><span class="kw">plot.losses</span>(name.model, <span class="st">&quot;Both&quot;</span>, fit[[<span class="dv">2</span>]]<span class="op">$</span>val_loss, fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)</span>
<span id="cb87-60"><a href="rnn.html#cb87-60"></a><span class="co"># calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)</span></span>
<span id="cb87-61"><a href="rnn.html#cb87-61"></a><span class="kw">load_model_weights_hdf5</span>(model, file.name)</span>
<span id="cb87-62"><a href="rnn.html#cb87-62"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((<span class="kw">exp</span>(<span class="op">-</span><span class="kw">as.vector</span>(model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(x.train)))<span class="op">-</span><span class="kw">exp</span>(<span class="op">-</span>y.train))<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span>
<span id="cb87-63"><a href="rnn.html#cb87-63"></a><span class="co"># calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)</span></span>
<span id="cb87-64"><a href="rnn.html#cb87-64"></a><span class="co"># Female</span></span>
<span id="cb87-65"><a href="rnn.html#cb87-65"></a>pred.result &lt;-<span class="st"> </span><span class="kw">recursive.prediction.Gender</span>(ObsYear, all_mort2.Female, <span class="st">&quot;Female&quot;</span>, T0, tau0, x.min, x.max, model)</span>
<span id="cb87-66"><a href="rnn.html#cb87-66"></a>vali &lt;-<span class="st"> </span>pred.result[[<span class="dv">1</span>]][<span class="kw">which</span>(all_mort2.Female<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-67"><a href="rnn.html#cb87-67"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali.Y.Female<span class="op">$</span>mx)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span>
<span id="cb87-68"><a href="rnn.html#cb87-68"></a><span class="co"># Male</span></span>
<span id="cb87-69"><a href="rnn.html#cb87-69"></a>pred.result &lt;-<span class="st"> </span><span class="kw">recursive.prediction.Gender</span>(ObsYear, all_mort2.Male, <span class="st">&quot;Male&quot;</span>, T0, tau0, x.min, x.max, model)</span>
<span id="cb87-70"><a href="rnn.html#cb87-70"></a>vali &lt;-<span class="st"> </span>pred.result[[<span class="dv">1</span>]][<span class="kw">which</span>(all_mort2.Male<span class="op">$</span>Year <span class="op">&gt;</span><span class="st"> </span>ObsYear),]</span>
<span id="cb87-71"><a href="rnn.html#cb87-71"></a><span class="kw">round</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">4</span><span class="op">*</span><span class="kw">mean</span>((vali<span class="op">$</span>mx<span class="op">-</span>vali.Y.Male<span class="op">$</span>mx)<span class="op">^</span><span class="dv">2</span>),<span class="dv">4</span>)</span></code></pre></div>
</div>
<div id="稳健性" class="section level3">
<h3><span class="header-section-number">6.5.7</span> 稳健性</h3>
<p>使用梯度下降法的早期停止解决方案的一个问题是由此产生的校准依赖于算法种子点（起始值）的选择</p>
<p>下图展示使用相同RNN结构、相同超参数和相同校准策略，针对100个不同种子点的选择所画的损失的箱线图</p>
<div class="figure" style="text-align: center"><span id="fig:Box"></span>
<img src="plots/6/Box.png" alt="100个不同种子下的损失箱线图" width="60%"  />
<p class="caption">
Figure 6.8: 100个不同种子下的损失箱线图
</p>
</div>
<ul>
<li><p>红色表示联合性别的LSTM结构中的预测结果；蓝色表示联合性别的GRU结构中的预测；橙色水平线表示的是LC的预测</p></li>
<li><p>结论：</p></li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>左侧给出了样本内损失，与LC模型相比，两种RNN结构的样本内损失都有显著减少，平均而言，LSTM的损失比GRU的小，波动性也更小；</p></li>
<li><p>中间和右边分别表示女性和男性的样本外损失：不论男性还是女性，LSTM在几乎所有的100次迭代中都比LC模型好；GRU结构尽在大约一半次数的迭代中比LC模型表现好。</p></li>
</ol>
<ul>
<li><strong>改进方法</strong>：将不同种子点下得到的预测值进行平均，结果如下:</li>
</ul>
<p><span class="math display">\[
\begin{array}{|l|c|cc|c|}
\hline &amp;  {\text { in-sample }} &amp; {\text { out-of-sample }} &amp; {\text { out-of-sample }}&amp;  {\text { run times }}\\
&amp; \text {both genders} &amp; \text { female } &amp; \text { male } &amp; \text {both genders} \\
\hline \hline \text { LSTM3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.7643 &amp; 0.3402 &amp; 1.1346 &amp; 404 \mathrm{s}\\
\text { GRU3 }\left(T=10,\left(\tau_{0}, \tau_{1}, \tau_{2}, \tau_{3}\right)=(5,20,15,10)\right) &amp; 4.6311 &amp; 0.4646 &amp; 1.2571 &amp;  379 \mathrm{s} \\
\hline\hline \text { LSTM3 averaged over 100 different seeds} &amp; - &amp; 0.2451 &amp; 1.2093 &amp; 100 \cdot 404\mathrm{s}\\
\text { GRU3 averaged over 100 different seeds } &amp; - &amp; 0.2341&amp; 1.2746 &amp;  100 \cdot 379 \mathrm{s} \\
\hline \hline \text { LC model with SVD } &amp; 6.2841 &amp; 0.6045 &amp;1.8152 &amp;  - \\
\hline
\end{array}
\]</span></p>
<p>能够看到，平均之后得到了更稳健的解决方案，预测结果也很好，箱线图中绿色水平线表示的就是平均之后的预测损失，显示仅有极少数的在绿色水平线之下的种子点的选择在单独进行校准时效果会更好。</p>
</div>
<div id="预测结果图" class="section level3">
<h3><span class="header-section-number">6.5.8</span> 预测结果图</h3>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="plots/6/mortality.png" alt="对数死亡率的观察与预测值" width="60%"  />
<p class="caption">
Figure 6.9: 对数死亡率的观察与预测值
</p>
</div>
<p>结论：</p>
<ol style="list-style-type: lower-alpha">
<li><p>20-40岁之间LSTM方法能够更好的捕捉到死亡率的改善，左边观察值清楚的表明LSTM这样的改善是合理的；</p></li>
<li><p>年龄较小人群的死亡率实际改善情况比按照本文方法预测的大，这可能是因为训练数据的青年死亡率改善情况无法代表2000年之后的青年死亡率改善情况。</p></li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="unsupervised-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": "github"
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
